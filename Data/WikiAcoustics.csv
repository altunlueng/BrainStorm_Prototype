,ID,Title,Text,Category,Group
0,1,Noise reduction coefficient,"The Noise Reduction Coefficient (commonly abbreviated NRC) is a scalar representation of the amount of sound energy absorbed upon striking a particular surface. An NRC of 0 indicates perfect reflection; an NRC of 1 indicates perfect absorption.
Due to the formulas used, the coefficient is not a percentage and values larger than one are possible (and common).

Technical definition
It is the arithmetic average, rounded to the nearest multiple of 0.05, of the absorption coefficients for a specific material and mounting condition determined at the octave band center frequencies of 250, 500, 1000 and 2000 Hz. The absorption coefficients of materials are commonly determined through use of standardized testing procedures, such as ASTM C423 that is used to evaluate the absorption of materials in eighteen one-third octave frequency bands with center frequencies ranging from 100 Hz to 5000 Hz. Absorption coefficients used to calculate NRC are commonly determined in reverberation rooms of qualified acoustical laboratory test facilities using samples of the particular materials of specified size and appropriate mounting.

Applications
NRC is most commonly used to rate general acoustical properties of acoustic ceiling tiles, baffles, and banners, office screens, and acoustic wall panels. It is occasionally used to rate floor coverings and construction materials.
NRC is intended to be a simplified acoustical rating of room construction and finish materials when the acoustical objectives of the space are less than sensitive. In certain applications, such as designs of music rehearsal rooms, performance spaces, and rooms employed for critical speech, it is usually more appropriate to consider the sound absorption coefficients at the individual one-third octave band frequencies, including those above and below the bands used to compute NRC.

New standards
NRC is being replaced by the Sound Absorption Average (SAA), which is described in the current ASTM C423-09a. The SAA is a single-number rating of sound absorption properties of a material similar to NRC, except that the sound absorption values employed in the averaging are taken at the twelve one-third octave bands from 200 Hz to 2500 Hz, inclusive, and rounding is to the nearest multiple of 0.01.


== References ==",Category:Acoustics,1
1,2,Precedence effect,"The precedence effect or law of the first wavefront is a binaural psychoacoustic effect. When a sound is followed by another sound separated by a sufficiently short time delay (below the listener's echo threshold), listeners perceive a single fused auditory image; its perceived spatial location is dominated by the location of the first-arriving sound (the first wave front). The lagging sound also affects the perceived location. However, its effect is suppressed by the first-arriving sound.
The Haas effect is a psychoacoustic effect, described in 1949 by Helmut Haas in his Ph.D. thesis. It is often equated with the underlying precedence effect.

History
The ""law of the first wavefront"" was described and named in 1948 by Lothar Cremer.
The ""precedence effect"" was described and named in 1949 by Wallach et al. They showed that when two identical sounds are presented in close succession they will be heard as a single fused sound. In their experiments, fusion occurred when the lag between the two sounds was in the range 1 to 5 ms for clicks, and up to 40 ms for more complex sounds such as speech or piano music. When the lag was longer, the second sound was heard as an echo.
Additionally, Wallach et al. demonstrated that when successive sounds coming from sources at different locations were heard as fused, the apparent location of the perceived sound was dominated by the location of the sound that reached the ears first (i.e. the first-arriving wavefront). The second-arriving sound had only a very small (albeit measurable) effect on the perceived location of the fused sound. They designated this phenomenon as the precedence effect, and noted that it explains why sound localization is possible in the typical situation where sounds reverberate from walls, furniture and the like, thus providing multiple, successive stimuli. They also noted that the precedence effect is an important factor in the perception of stereophonic sound.
Wallach et al. did not systematically vary the intensities of the two sounds, although they cited research by Langmuir et al. which suggested that if the second-arriving sound is at least 15 dB louder than the first, the precedence effect breaks down.
The ""Haas effect"" derives from a 1951 paper by Helmut Haas. In 1951 Haas examined how the perception of speech is affected in the presence of a single, coherent sound reflection. To create anechoic conditions, the experiment was carried out on the rooftop of a freestanding building. Another test was carried out in a room with a reverberation time of 1.6 ms. The test signal (recorded speech) was emitted from two similar loudspeakers at locations 45° to the left and to the right in 3 m distance to the listener.
Haas found that humans localize sound sources in the direction of the first arriving sound despite the presence of a single reflection from a different direction. A single auditory event is perceived. A reflection arriving later than 1 ms after the direct sound increases the perceived level and spaciousness (more precisely the perceived width of the sound source). A single reflection arriving within 5 to 30 ms can be up to 10 dB louder than the direct sound without being perceived as a secondary auditory event (echo). This time span varies with the reflection level. If the direct sound is coming from the same direction the listener is facing, the reflection's direction has no significant effect on the results. A reflection with attenuated higher frequencies expands the time span that echo suppression is active. Increased room reverberation time also expands the time span of echo suppression.

Appearance
The precedence effect appears if the subsequent wave fronts arrive between 2 ms and about 50 ms later than the first wave front. This range is signal dependent. For speech the precedence effect disappears for delays above 50 ms, but for music the precedence effect can also appear for delays of some 100 ms.
In two-click lead–lag experiments, localization effects include aspects of summing localization, localization dominance, and lag discrimination suppression. The last two are generally considered to be aspects of the precedence effect:
Summing localization: for time delays below 2 ms, listeners only perceive one sound; its direction is between the locations of the lead and lag sounds. An application for summing localization is the intensity stereophony, where two loudspeakers emit the same signal with different levels, resulting in the localized sound direction between both loudspeakers. The localized direction depends on the level difference between the loudspeakers.
Localization dominance: for delays between 2 and 5 ms, listeners also perceive one sound; its location is determined by the location of the leading sound.
Lag discrimination suppression: for short time delays, listeners are less capable of discriminating the location of the lagging sound.
For time delays above 50 ms (for speech) or some 100 ms (for music) the delayed sound is perceived as an echo of the first-arriving sound. Both sound directions are localized correctly. The time delay for perceiving echoes depends on the signal characteristics. For signals with impulse characteristics, echoes are perceived for delays above 50 ms. For signals with a nearly constant amplitude, the echo threshold can be enhanced up to time differences of 1 to 2 seconds.
A special appearance of the precedence effect is the Haas effect. Haas showed that the precedence effect appears even if the level of the delayed sound is up to 10 dB higher than the level of the first wave front. In this case, the range of delays, where the precedence effect works, is reduced to delays between 10 and 30 ms.

Applications
The precedence effect is important for the hearing in enclosed rooms. With the help of this effect, it remains possible to determine the direction of a sound source (e.g. the direction of a speaker) even in the presence of wall reflections.

Sound reinforcement systems
Haas' findings can be applied to sound reinforcement systems and public address systems. The signal for loudspeakers placed at distant locations from a stage may be delayed electronically by an amount equal to the time sound takes to travel through the air from the stage to the distant location, plus about 10 to 20 milliseconds and played at a level up to 10 dB louder than sound emanating from the stage. The first arrival of sound from the source on stage determines perceived localization whereas the slightly later sound from delayed loudspeakers simply increases the perceived sound level without negatively affecting localization. In this configuration, the listener will localize all sound from the direction of the direct sound, but he will benefit from the higher sound level, which has been enhanced by the loudspeakers.

Ambience extraction
The precedence effect can be employed to increase the perception of ambience during the playback of stereo recordings. If two speakers are placed to the left and right of the listener (in addition to the main speakers), and fed with the program material delayed by 10 to 20 milliseconds, the random-phase ambience components of the sound will become sufficiently decorrelated that they cannot be localized. This effectively extracts the recording's existing ambience, while leaving its foreground ""direct"" sounds still appearing to come from the front.

Multichannel audio decoding
The effect was taken into account and exploited in the psychoacoustics of the Fosgate Tate 101A SQ decoder, developed by Jim Fosgate in consultation with Peter Scheiber and Martin Willcocks, to produce much better spatiality and directionality in matrix decoding of 4-2-4 (SQ quadraphonic) audio.

Haas kicker
Many older LEDE (""live end, dead end"") control room designs featured so-called ""Haas kickers"" – reflective panels placed at the rear to create specular reflections which were thought to provide a wider stereo listening area or raise intelligibility. However, what is beneficial for one type of sound is detrimental to others, so Haas kickers, like compression ceilings, are no longer commonly found in control rooms.

See also
Binaural fusion
Franssen effect

References
Further reading
Floyd Toole ""Sound Reproduction"", Focal Press (July 25, 2008), Chapter 6
Blauert ""Spatial Hearing - Revised Edition: The Psychophysics of Human Sound Localization"", The MIT Press; Rev Sub edition (October 2, 1996)
Litovsky et al. (1999), ""The precedence effect"" J. Acoustic. Soc. Am., Vol. 106, No. 4",Category:Audio engineering,1
2,3,Inharmonicity,"In music, inharmonicity is the degree to which the frequencies of overtones (also known as partials or partial tones) depart from whole multiples of the fundamental frequency (harmonic series).
Acoustically, a note perceived to have a single distinct pitch in fact contains a variety of additional overtones. Many percussion instruments, such as cymbals, tam-tams, and chimes, create complex and inharmonic sounds.
However, in stringed instruments such as the piano, violin, and guitar, or in some Indian drums such as tabla, the overtones are close to—or in some cases, quite exactly—whole number multiples of the fundamental frequency. Any departure from this ideal harmonic series is known as inharmonicity. The less elastic the strings are (that is, the shorter, thicker, smaller tension or stiffer they are), the more inharmonicity they exhibit.
Music harmony and intonation depends strongly on the harmonicity of tones. An ideal, homogeneous, infinitesimally thin or infinitely flexible string or column of air has exactly harmonic modes of vibration. In any real musical instrument, the resonant body that produces the music tone—typically a string, wire, or column of air—deviates from this ideal and has some small or large amount of inharmonicity. For instance, a very thick string behaves less as an ideal string and more like a cylinder (a tube of mass), which has natural resonances that are not whole number multiples of the fundamental frequency.
When a string is bowed or tone in a wind instrument initiated by vibrating reed or lips, a phenomenon called mode-locking counteracts the natural inharmonicity of the string or air column and causes the overtones to lock precisely onto integer multiples of the fundamental pitch, even though these are slightly different from the natural resonance points of the instrument. For this reason, a single tone played by a bowed string instrument, brass instrument, or reed instrument does not necessarily exhibit inharmonicity.
However, when a string is struck or plucked, as with a piano string that is struck by a hammer, a violin string played pizzicato, or a guitar string that is plucked by a finger or plectrum, the string will exhibit inharmonicity. The inharmonicity of a string depends on its physical characteristics, such as tension, stiffness, and length. For instance, a stiff string under low tension (such as those found in the bass notes of small upright pianos) exhibits a high degree of inharmonicity, while a thinner string under higher tension (such as a treble string in a piano) or a more flexible string (such as a gut or nylon string used on a guitar or harp) will exhibit less inharmonicity. A wound string generally exhibits less inharmonicity than the equivalent solid string, and for that reason wound strings are often preferred.

Pianos
Sound quality of inharmonicity
In 1943, Schuck and Young were the first scientists to measure the spectral inharmonicity in piano tones. They found that the spectral partials in piano tones are progressively stretched—that is to say, the lowest partials are stretched the least and higher partials are progressively stretched further.
Inharmonicity is not necessarily unpleasant. In 1962, research by Harvey Fletcher and his collaborators indicated that the spectral inharmonicity is important for tones to sound piano-like. They proposed that inharmonicity is responsible for the ""warmth"" property common to real piano tones. According to their research synthesized piano tones sounded more natural when some inharmonicity was introduced. In general, electronic instruments that duplicate acoustic instruments must duplicate both the inharmonicity and the resulting stretched tuning of the original instruments.

Inharmonicity leads to stretched tuning
When pianos are tuned by piano tuners, the technician sometimes listens for the sound of ""beating"" when two notes are played together, and tunes to the point that minimizes roughness between tones. Piano tuners must deal with the inharmonicity of piano strings, which is present in different amounts in all of the ranges of the instrument, but especially in the bass and high treble registers. The result is that octaves are tuned slightly wider than the harmonic 2:1 ratio. The exact amount octaves are stretched in a piano tuning varies from piano to piano and even from register to register within a single piano—depending on the exact inharmonicity of the strings involved.
Because of the problem of inharmonicity, electronic piano tuning devices used by piano technicians are not designed to tune according to a simple harmonic series. Rather, the devices use various means to duplicate the stretched octaves and other adjustments a technician makes by ear. The most sophisticated devices allow a technician to make custom inharmonicity measurements—simultaneously considering all partials for pitch and volume to determine the most appropriate stretch to employ for a given instrument. Some include an option to simply record a tuning that a technician has completed by ear; the technician can then duplicate that tuning on the same piano (or others of similar make and model) more easily and quickly.
The issues surrounding setting the stretch by ear vs machine have not been settled; machines are better at deriving the absolute placement of semitones within a given chromatic scale, whereas non-machine tuners prefer to adjust these locations preferentially due to their temptation to make intervals more sonorous. The result is that pianos tuned by ear and immediately checked with a machine tend to vary from one degree to another from the purely theoretical semitone (mathematically the 12th root of two) due to human error and perception. (If pleasing the ear is the goal of an aural tuning, then pleasing the math is the goal of a machine tuning.) This is thought to be because strings can vary somewhat from note to note and even from neighbors within a unison. This non-linearity is different from true falseness where a string creates false harmonics and is more akin to minor variations in string thickness, string sounding length or minor bridge inconsistencies.
Piano tuning is a compromise—both in terms of choosing a temperament to minimize out-of-tuneness in the intervals and chords that will be played, and in terms of dealing with inharmonicity. For more information, see Piano acoustics and Piano tuning.
Another factor that can cause problems is the presence of rust on the strings or dirt in the windings. These factors can slightly raise the frequency of the higher modes, resulting in more inharmonicity.

Guitar
While piano tuning is normally done by trained technicians, guitars such as acoustic guitars, electric guitars, and electric bass guitars are usually tuned by the guitarist themselves. When a guitarist tunes a guitar by ear, they have to take both temperament and string inharmonicity into account. The inharmonicity in guitar strings can ""cause stopped notes to stop sharp, meaning they will sound sharper both in terms of pitch and beating, than they ""should"" do. This is distinct from any temperament issue."" Even if a guitar is built so that there are no ""fret or neck angle errors, inharmonicity can make the simple approach of tuning open strings to notes stopped on the fifth or fourth frets"" unreliable. Inharmonicity also demands that some of the ""octaves may need to be compromised minutely."" 
When strobe tuners became available in the 1970s, and then inexpensive electronic tuners in the 1980s reached the mass market, it did not spell the end of tuning problems for guitarists. Even if an electronic tuner indicates that the guitar is ""perfectly"" in tune, some chords may not sound in tune when they are strummed, either due to string inharmonicity from worn or dirty strings, a misplaced fret, a mis-adjusted bridge, or other problems. Due to the range of factors in play, getting a guitar to sound in tune is an exercise in compromise. ""Worn or dirty strings are also inharmonic and harder to tune"", a problem that can be partially resolved by cleaning strings.
Some performers choose to focus the tuning towards the key of the piece, so that the tonic and dominant chords will have a clear, resonant sound. However, since this compromise may lead to muddy-sounding chords in sections of a piece that stray from the main key (e.g., a bridge section that modulates a semitone down), some performers choose to make a broader compromise, and ""split the difference"" so that all chords will sound acceptable.

Mode-locking
Other stringed instruments such as the violin, viola, cello, and double bass also exhibit inharmonicity when notes are plucked using the pizzicato technique. However, this inharmonicity disappears when the strings are bowed, because the bow's stick-slip action is periodic, so it drives all of the resonances of the string at exactly harmonic ratios, even if it has to drive them slightly off their natural frequency. As a result, the operating mode of a bowed string playing a steady note is a compromise among the tunings of all of the (slightly inharmonic) string resonances, which is due to the strong non-linearity of the stick-slip action. Mode locking also occurs in the human voice and in reed instruments such as the clarinet.

List of instruments
Perfectly harmonic

Bowed string instruments (violin, cello, erhu, ...)
Brass instruments (trumpet, horn, trombone, ...)
Reed aerophones (oboe, clarinet, ...)

Nearly harmonic

Plucked string instruments (guitar, harpsichord, harp...)

Approximately harmonic

Tuned percussion

Not harmonic

Untuned percussion

See also
Anharmonicity
Pseudo-octave
Subharmonic

Further reading
B. C. J. Moore, R.W. Peters, and B. C. Glasberg, “Thresholds for the detection of inharmonicity in complex tones,” Journal of the Acoust. Soc. Am., vol. 77, no. 5, pp. 1861–1867, 1985.
F. Scalcon, D. Rocchesso, and G. Borin, “Subjective evaluation of the inharmonicity of synthetic piano tones,” in Proc. Int. Comp. Music Conf. ICMC’98, pp. 53–56, 1998.
A. Galembo and L. Cuddy, “String inharmonicity and the timbral quality of piano bass tones: Fletcher, Blackham, and Stratton (1962) revisited.” Report to the 3rd US Conference on Music Perception and Cognition, MIT, Cambridge, MA, July - August 1997.

References
External links
""Pitch Paradoxical"", iHear.com.",Category:Acoustics,1
3,4,Creeping wave,,Category:All articles lacking sources,1
4,5,Particle velocity,"Particle velocity is the velocity of a particle (real or imagined) in a medium as it transmits a wave. The SI unit of particle velocity is the metre per second (m/s). In many cases this is a longitudinal wave of pressure as with sound, but it can also be a transverse wave as with the vibration of a taut string.
When applied to a sound wave through a medium of a fluid like air, particle velocity would be the physical speed of a parcel of fluid as it moves back and forth in the direction the sound wave is travelling as it passes.
Particle velocity should not be confused with the speed of the wave as it passes through the medium, i.e. in the case of a sound wave, particle velocity is not the same as the speed of sound. The wave moves relatively fast, while the particles oscillate around their original position with a relatively small particle velocity. Particle velocity should also not be confused with the velocity of individual molecules.
In applications involving sound, the particle velocity is usually measured using a logarithmic decibel scale called particle velocity level. Mostly pressure sensors (microphones) are used to measure sound pressure which is then propagated to the velocity field using Green's function.

Mathematical definition
Particle velocity, denoted v, is defined by

  
    
      
        
          v
        
        =
        
          
            
              ?
              
                ?
              
            
            
              ?
              t
            
          
        
      
    
    {\displaystyle \mathbf {v} ={\frac {\partial \mathbf {\delta } }{\partial t}}}
  
where ? is the particle displacement.

Progressive sine waves
The particle displacement of a progressive sine wave is given by

  
    
      
        ?
        (
        
          r
        
        ,
        
        t
        )
        =
        
          ?
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            ?
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle \delta (\mathbf {r} ,\,t)=\delta _{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}),}
  
where
?m is the amplitude of the particle displacement;

  
    
      
        
          ?
          
            ?
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{\delta ,0}}
   is the phase shift of the particle displacement;
k is the angular wavevector;
? is the angular frequency.
It follows that the particle velocity and the sound pressure along the direction of propagation of the sound wave x are given by

  
    
      
        v
        (
        
          r
        
        ,
        
        t
        )
        =
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            k
          
          ?
          
            r
          
          ?
          ?
          t
          +
          
            ?
            
              ?
              ,
              0
            
          
          +
          
            
              ?
              2
            
          
          )
        
        =
        
          v
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            v
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle v(\mathbf {r} ,\,t)={\frac {\partial \delta }{\partial t}}(\mathbf {r} ,\,t)=\omega \delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=v_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{v,0}),}
  

  
    
      
        p
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        
          c
          
            2
          
        
        
          
            
              ?
              ?
            
            
              ?
              x
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          c
          
            2
          
        
        
          k
          
            x
          
        
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            k
          
          ?
          
            r
          
          ?
          ?
          t
          +
          
            ?
            
              ?
              ,
              0
            
          
          +
          
            
              ?
              2
            
          
          )
        
        =
        
          p
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            p
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle p(\mathbf {r} ,\,t)=-\rho c^{2}{\frac {\partial \delta }{\partial x}}(\mathbf {r} ,\,t)=\rho c^{2}k_{x}\delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=p_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{p,0}),}
  
where
vm is the amplitude of the particle velocity;

  
    
      
        
          ?
          
            v
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}}
   is the phase shift of the particle velocity;
pm is the amplitude of the acoustic pressure;

  
    
      
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{p,0}}
   is the phase shift of the acoustic pressure.
Taking the Laplace transforms of v and p with respect to time yields

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          v
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle {\hat {v}}(\mathbf {r} ,\,s)=v_{\mathrm {m} }{\frac {s\cos \varphi _{v,0}-\omega \sin \varphi _{v,0}}{s^{2}+\omega ^{2}}},}
  

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          p
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\hat {p}}(\mathbf {r} ,\,s)=p_{\mathrm {m} }{\frac {s\cos \varphi _{p,0}-\omega \sin \varphi _{p,0}}{s^{2}+\omega ^{2}}}.}
  
Since 
  
    
      
        
          ?
          
            v
            ,
            0
          
        
        =
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}=\varphi _{p,0}}
  , the amplitude of the specific acoustic impedance is given by

  
    
      
        
          z
          
            
              m
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          |
        
        z
        (
        
          r
        
        ,
        
        s
        )
        
          |
        
        =
        
          |
          
            
              
                
                  
                    
                      p
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
              
                
                  
                    
                      v
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
            
          
          |
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              v
              
                
                  m
                
              
            
          
        
        =
        
          
            
              ?
              
                c
                
                  2
                
              
              
                k
                
                  x
                
              
            
            ?
          
        
        .
      
    
    {\displaystyle z_{\mathrm {m} }(\mathbf {r} ,\,s)=|z(\mathbf {r} ,\,s)|=\left|{\frac {{\hat {p}}(\mathbf {r} ,\,s)}{{\hat {v}}(\mathbf {r} ,\,s)}}\right|={\frac {p_{\mathrm {m} }}{v_{\mathrm {m} }}}={\frac {\rho c^{2}k_{x}}{\omega }}.}
  
Consequently, the amplitude of the particle velocity is related to those of the particle displacement and the sound pressure by

  
    
      
        
          v
          
            
              m
            
          
        
        =
        ?
        
          ?
          
            
              m
            
          
        
        ,
      
    
    {\displaystyle v_{\mathrm {m} }=\omega \delta _{\mathrm {m} },}
  

  
    
      
        
          v
          
            
              m
            
          
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              
                z
                
                  
                    m
                  
                
              
              (
              
                r
              
              ,
              
              s
              )
            
          
        
        .
      
    
    {\displaystyle v_{\mathrm {m} }={\frac {p_{\mathrm {m} }}{z_{\mathrm {m} }(\mathbf {r} ,\,s)}}.}

Particle velocity level
Sound velocity level (SVL) or acoustic velocity level or particle velocity level is a logarithmic measure of the effective particle velocity of a sound relative to a reference value.
Sound velocity level, denoted Lv and measured in dB, is defined by

  
    
      
        
          L
          
            v
          
        
        =
        ln
        
        
          (
          
            
              v
              
                v
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        2
        
          log
          
            10
          
        
        
        
          (
          
            
              v
              
                v
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              v
              
                v
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{v}=\ln \!\left({\frac {v}{v_{0}}}\right)\!~\mathrm {Np} =2\log _{10}\!\left({\frac {v}{v_{0}}}\right)\!~\mathrm {B} =20\log _{10}\!\left({\frac {v}{v_{0}}}\right)\!~\mathrm {dB} ,}
  
where
v is the root mean square particle velocity;
v0 is the reference particle velocity;
1 Np = 1 is the neper;
1 B = 1/2 ln 10 is the bel;
1 dB = 1/20 ln 10 is the decibel.
The commonly used reference particle velocity in air is

  
    
      
        
          v
          
            0
          
        
        =
        5
        ×
        
          10
          
            ?
            8
          
        
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle v_{0}=5\times 10^{-8}~\mathrm {m/s} .}
  
The proper notations for sound velocity level using this reference are Lv/(5 × 10?8 m/s) or Lv (re 5 × 10?8 m/s), but the notations dB SVL, dB(SVL), dBSVL, or dBSVL are very common, even if they are not accepted by the SI.

See also
Sound
Sound particle
Particle displacement
Particle acceleration

References
External links
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
The particle Velocity Can Be Directly Measured with a Microflown
Acoustic Particle-Image Velocimetry. Development and Applications",Category:Physical quantities,1
5,6,Seismic communication,"Seismic or vibrational communication is a process of conveying information through mechanical (seismic) vibrations of the substrate. The substrate may be the earth, a plant stem or leaf, the surface of a body of water, a spider’s web, a honeycomb, or any of the myriad types of soil substrates. Seismic cues are generally conveyed by surface Rayleigh or bending waves generated through vibrations on the substrate, or acoustical waves that couple with the substrate. Vibrational communication is an ancient sensory modality and it is widespread in the animal kingdom where it has evolved several times independently. It has been reported in mammals, birds, reptiles, amphibians, insects, arachnids, crustaceans and nematode worms. Vibrations and other communication channels are not necessarily mutually exclusive, but can be used in multi-modal communication.

Functions
Communication requires a sender, a message, and a recipient, although neither the sender or receiver need be present or aware of the others intent to communicate at the time of communication.

Intra-specific communication
Vibrations can provide cues to conspecifics about specific behaviours being performed, predator warning and avoidance, herd or group maintenance, and courtship. The Middle East blind mole-rat (Spalax ehrenbergi) was the first mammal for which vibrational communication was documented. These fossorial rodents bang their head against the walls of their tunnels, which was initially interpreted as part of their tunnel building behaviour, however, it was eventually realised they generate temporally patterned vibrational signals for long-distance communication with neighbouring mole-rats. Footdrumming is used widely as a predator warning or defensive action. It is used primarily by fossorial or semi-fossorial rodents, but has also been recorded for spotted skunks (Spilogale putorius), deer (e.g. white-tailed deer Odocoileus virginianus), marsupials (e.g. tammar wallabies Macropus eugenii), rabbits (e.g. European rabbits Oryctolagus cuniculus) and elephant shrews (Macroscelididae). Banner-tailed kangaroo rats (Dipodomys spectabilis) footdrum in the presence of snakes as a form of individual defense and parental care. Several studies have indicated intentional use of ground vibrations as a means of intra-specific communication during courtship among the Cape mole-rat (Georychus capensis). Footdrumming has been reported to be involved in male-male competition where the dominant male indicates its resource holding potential by drumming, thus minimising physical contact with potential rivals. The Asian elephant (Elephas maximus) uses seismic communication in herd or group maintenance and many social insects use seismic vibrations to coordinate the behaviour of group members, for example in cooperative foraging. Other insects use vibrational communication to search for and attract mates, like North American treehoppers, Enchenopa binotata. Males of this species use their abdomen to send vibrations through their host plant's stem. Females perceive these signals and respond to them to initiate a duet.

Inter-specific communication
The banner-tailed kangaroo rat, (Dipodomys spectabilis), produces several complex footdrumming patterns in a number of different contexts, one of which is when it encounters a snake. The footdrumming may alert nearby offspring but most likely conveys that the rat is too alert for a successful attack, thus preventing the snake's predatory pursuit. Vibrations caused by stampeding animals may be sensed by other species to alert them to danger, thereby increasing the size of the stampede and reducing the risk of danger to an individual.

Eavesdropping
Some animals use eavesdropping to either catch their prey or to avoid being caught by predators. Some snakes are able to perceive and react to substrate-borne vibrations. The vibrations are transmitted through the lower jaw, which is often rested on the ground and is connected with the inner ear. They also detect vibrations directly with receptors on their body surface. Studies on horned desert vipers (Cerastes cerastes) showed they strongly rely on vibrational cues for capturing prey. Localisation of the prey is probably aided by the two halves of the lower jaw being independent.
Vibrational cues can even indicate the life stage of prey thereby aiding optimal prey selection by predators, e.g. larval vibrations can be distinguished from those generated by pupae, or, adults from juveniles. Although some species can conceal or mask their movements, substrate-borne vibrations are generally more difficult to avoid producing than airborne vibrations. The common angle moth (Semiothisa aemulataria) caterpillar escapes predation by lowering itself to safety by a silk thread in response to vibrations produced by approaching predators.

Mimicry
Several animals have learnt to capture prey species by mimicking the vibrational cues of their predators. Wood turtles (Clemmys insculpta), European herring gulls (Larus argentatus), and humans have learnt to vibrate the ground causing earthworms to rise to the surface where they can be easily caught. It is believed that deliberately produced surface vibrations mimic the seismic cues of moles moving through the ground to prey on the worms; the worms respond to these naturally produced vibrations by emerging from their burrows and fleeing across the surface.
Other animals mimic the vibrational cues of prey, only to ambush the predator when it is lured towards the mimic. Assassin bugs (Stenolemus bituberus) hunt web-building spiders by invading the web and plucking the silk to generate vibrations that mimic prey of the spider. This lures the resident spider into striking range of the bug. Spiders from at least five different families routinely invade the webs of other spiders and lure them as prey with vibratory signals (e.g. Pholcus or ‘daddy long-leg’ spiders; salticid ‘jumping’ spiders from the genera Portia, Brettus, Cyrba and Gelotia).
Portia fimbriata jumping spiders lure female Euryattus species by mimicking male courtship vibrations.

Habitat sensing
The wandering spider (Cupiennius salei) can discriminate vibrations created by rain, wind, prey, and potential mates. The creeping grasshopper can escape predation by this spider if it produces vibrations similar enough to those of wind. Thunderstorms and earthquakes produce vibrational cues; these may be used by elephants and birds to attract them to water or avoid earthquakes. Mole rats use reflected, self-generated seismic waves to detect and bypass underground obstacles - a form of ""seismic echolocation"".
However, these type of use is not considered communication in the strictest sense.

Production of vibrational cues
Vibrational cues can be produced in three ways, through percussion (drumming) on the substrate, vibrations of the body or appendages transmitted to the substrate, or, acoustical waves that couple with the substrate. The strength of these cues depends mostly on the size and muscular power of the animal producing the signal.
Percussion
Percussion, or drumming, can produce both short-and long-distance vibrational cues. Direct percussion of the substrate can yield a much stronger signal than an airborne vocalization that couples with the substrate, however, the strength of the percussive cue is related directly to the mass of the animal producing the vibration. Large size is often associated with greater source amplitudes, leading to a greater propagation range. A wide range of vertebrates perform drumming with some part of their body either on the surface or within burrows. Individuals bang heads, rap trunks or tails, stamp or drum with front feet, hind feet or teeth, thump a gular pouch, and basically employ available appendages to create vibrations on the substrates where they live. Insects use percussion by drumming (or scraping) with the head, hind legs, fore legs, mid legs, wings, abdomen, gaster, antennae or maxillary palps.
Tremulation 
Tremulation is performed by a range of insects. This process involves rocking of the entire body with the subsequent vibrations being transferred through the legs to the substrate.
Stridulation
Insects and other arthropods stridulate by rubbing together two parts of the body.

These are referred to generically as the stridulatory organs. Vibrations are transmitted to the substrate through the legs or body.
Tymbal vibrations
Insects possess tymbals which are regions of the exoskeleton modified to form a complex membrane with thin, membranous portions and thickened ""ribs"". These membranes vibrate rapidly, producing audible sound and vibrations that are transmitted to the substrate.
Acoustically coupled
Elephants produce low-frequency vocalizations at high amplitudes such that they couple with the ground and travel along the surface of the earth. Direct percussion can produce a much stronger signal than airborne vocalizations that couple with the ground, as shown in the Cape mole rat and the Asian elephant. However, the power that an animal can couple into the ground at low frequencies is related directly to its mass. Animals of low mass cannot generate low-frequency vibrational surface waves; thus the mole rat could not produce a vibrational signal at 10–20 Hz like the elephant. Some invertebrates e.g. prairie mole cricket (Gryllotalpa major), bushcricket (Tettigoniidae), and cicada produce acoustic communications and substrate vibrations that may be due to acoustic coupling.
For acoustic coupling, low-frequency, high-amplitude vocalizations are necessary for long-distance transmission. It has been suggested that other large mammals such as the lion and rhinoceros may produce acoustically coupled vibrational cues similar to elephants.

Reception of vibrational cues
Vibrational cues are detected by various body parts. Snakes receive signals by sensors in the lower jaw or body, invertebrates by sensors in the legs or body (earthworms), birds by sensors in the legs (pigeons) or bill-tip (shorebirds, kiwis and ibises), mammals by sensors in the feet or lower jaw (mole rats) and kangaroos by sensors in the legs. The star-nosed mole (Condylura cristata), has evolved an elaborate nose structure which may detect seismic waves.
The sensory organs are generically known as somatosensory mechanoreceptors. In insects these sensors are known as campaniform sensillae located near the joints, the subgenual organ in the tibia and Johnston's organ located in the antennae. Arachnids use slit sense organ. In vertebrate animals the sensors are Pacinian corpuscles in placental mammals, similar lamellated corpuscles in marsupials, Herbst corpuscles in birds and a variety of encapsulated or naked nerve endings in other animals.
These sensory receivers detect vibrations in the skin and joints, from which they are typically transmitted as nerve impulses (action potentials) to and through spinal nerves to the spinal cord and then the brain; in snakes, the nerve impulses could be carried through cranial nerves. Alternatively, the sensory receivers may be centralized in the cochlea of the inner ear. Vibrations are transmitted from the substrate to the cochlea through the body (bones, fluids, cartilage, etc.) in an ‘extra-tympanic’ pathway that bypasses the eardrum, and sometimes, even the middle ear. Vibrations then project to the brain along with cues from airborne sound received by the eardrum.

Propagation of vibrational cues
Documented cases of vibrational communication are almost exclusively restricted to Rayleigh waves or bending waves. Seismic energy in the form of Rayleigh waves transmits most efficiently between 10 and 40 Hz. This is the range in which elephants may communicate seismically. In areas with little to no human-generated seismic noise, frequencies around 20 Hz are relatively noise-free, other than vibrations associated with thunder or earth tremors, making it a reasonably quiet communication channel. Both airborne and vibrational waves are subject to interference and alteration from environmental factors. Factors such as wind and temperature influence airborne sound propagation, whereas propagation of seismic signals are affected by the substrate type and heterogeneity. Airborne sound waves spread spherically rather than cylindrically, attenuate more rapidly (losing 6 dB for every doubling of distance) than ground surface waves such as Rayleigh waves (3 dB loss for every doubling of distance), and thus ground surface waves maintain integrity longer. Vibrational signals are probably not very costly to produce for small animals, whereas the generation of air-borne sound is limited by body size.
Benefits and costs of vibrational communication to the signaler are dependent on the function of the signal. For social signaling, daylight and line-of-sight are not required for seismic communication as they are for visual signaling. Likewise, flightless individuals may spend less time locating a potential mate by following the most direct route defined by substrate-borne vibrations, rather than by following sound or chemicals deposited on the path.
Most insects are herbivorous and usually live on plants, therefore the majority of vibrational signals are transmitted through plant stems. Here, communication typically ranges from 0.3 m-2.0 m. It has been suggested that vibrational signals might be adapted to transmit through particular plants.

Examples
White-lipped frog
One of the earliest reports of vertebrate signaling using vibrational communication is the bimodal system of sexual advertisement of the white-lipped frog (Leptodactylus albilabris). Males on the ground sing airborne advertisement songs that target receptive females, but instead of supporting themselves on their front limbs as other frogs often do, they partially bury themselves in soft soil. As they inflate their vocal sacs to produce the airborne call, the gular pouch impacts the soil as a ‘thump’ that sets up Rayleigh waves which propagate 3–6 m through the substrate. Advertising males space themselves at distances of 1–2 m, thus, the nearest neighbour males are able to receive and respond to substrate-borne vibrations created by other males.

Namib Desert golden mole
Predators may use vibrational communication to detect and capture prey. The Namib Desert golden mole (Eremitalpa granti namibensis) is a blind mammal whose eyelids fuse early in development. The ear lacks a pinna, the reduced ear opening is hidden under fur and the organization of the middle ear indicates it would be sensitive to vibrational cues. The Namib Desert golden mole actively forages at night by dipping its head and shoulders into the sand in conjunction with ‘sand swimming’ as it navigates in search of termite prey producing head-banging alarms. Experimental evidence supports the hypothesis that substrate-borne vibrations produced as wind blows through grassy hummocks influence these moles as they forage on termites associated with the grassy mounds, which are spaced at distances of 20–25 m. The exact mechanism of extracting directional information from the vibrations has not been confirmed.

Elephants
In the late 90's, Caitlin O'Connell-Rodwell first argued that elephants communicate over long distances using low-pitched rumbles that are barely audible to humans. Further pioneering research in elephant infrasound communication was done by Katy Payne of the Elephant Listening Project and detailed in her book Silent Thunder. This research is helping our understanding of behaviours such as how elephants can find distant potential mates and how social groups are able to coordinate their movements over extensive ranges. Joyce Poole has also begun decoding elephant utterances that have been recorded over many years of observation, hoping to create a lexicon based on a systematic catalogue of elephant sounds.
Seismic energy transmits most efficiently between 10-40 Hz, i.e. in the same range as the fundamental frequency and 2nd harmonic of an elephant rumble. For Asian elephants, these calls have a frequency of 14–24 Hz, with sound pressure levels of 85–90 dB and last 10–15 seconds. For African elephants, calls range from 15–35 Hz and can be as loud as 117 dB, allowing communication over many kilometers. It seems that when an elephant rumbles, the infrasound that is produced couples with the surface of the earth and then propagates through the ground. In this way, elephants are able to use seismic vibrations at infrasound frequencies for communication. These vibrations can be detected by the skin of an elephant's feet and trunk, which relay the resonant vibrations, similar to the skin on a drum. To listen attentively, individuals will lift one foreleg from the ground, possibly triangulating the source, and face the source of the sound. Occasionally, attentive elephants can be seen to lean forward, putting more weight on their front feet. These behaviours presumably increase the ground contact and sensitivity of the legs. Sometimes, the trunk will be laid on the ground.
Elephants possess several adaptations suited for vibratory communication. The cushion pads of the feet contain cartilaginous nodes and have similarities to the acoustic fat (melon) found in marine mammals like toothed whales and sirenians. In addition, the annular muscle surrounding the ear canal can constrict the passageway, thereby dampening acoustic signals and allowing the animal to hear more seismic signals.
Elephants appear to use vibrational communication for a number of purposes. An elephant running or mock charging can create seismic signals that can be heard at great distances. Vibrational waveforms produced by locomotion appear to travel at distances of up to 32 km (20 mi) while those from vocalizations travel 16 km (9.9 mi). When detecting the vibrational cues of an alarm call signaling danger from predators, elephants enter a defensive posture and family groups will congregate. Vibrational cues are also thought to aid their navigation by use of external sources of infrasound. After the 2004 Boxing Day tsunami in Asia, there were reports that trained elephants in Thailand had become agitated and fled to higher ground before the devastating wave struck, thus saving their own lives and those of the tourists riding on their backs. Because earthquakes and tsunamis generate low-frequency waves, O'Connell-Rodwell and other elephant experts have begun to explore the possibility that the Thai elephants were responding to these events.

See also
Animal communication
Hearing range
Lateral line
Sense


== References ==",Category:Seismology,1
6,7,Transmission loss (duct acoustics),"Transmission loss (TL) in duct acoustics, together with insertion loss (IL), describes the acoustic performances of a muffler like system. It is frequently used in the industry areas such as muffler manufacturers and NVH department of automobile manufacturers. Generally the higher transmission loss of a system it has, the better it will perform in terms of noise cancellation.

Introduction
Transmission loss (TL) (more specifically in duct acoustics) is defined as the difference between the power incident on a duct acoustic device (muffler) and that transmitted downstream into an anechoic termination. Transmission loss is independent of the source and presumes (or requires) an anechoic termination at the downstream end.
Transmission loss does not involve the source impedance and the radiation impedance inasmuch as it represents the difference between incident acoustic energy and that transmitted into an anechoic environment. Being made independent of the terminations, TL finds favor with researchers who are sometimes interested in finding the acoustic transmission behavior of an element or a set of elements in isolation of the terminations. But measurement of the incident wave in a standing wave acoustic field requires uses of impedance tube technology, may be quite laborious, unless one makes use of the two-microphone method with modern instrumentation.

Mathematical definition
By definition the TL on an acoustic component, for example a muffler, is described as:

  
    
      
        T
        L
        =
        
          L
          
            W
            i
          
        
        ?
        
          L
          
            W
            o
          
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          |
          
            
              
                
                  S
                  
                    i
                  
                
                
                  p
                  
                    i
                    +
                  
                
                
                  v
                  
                    i
                    +
                  
                
              
              2
            
          
          
            
              2
              
                
                  S
                  
                    o
                  
                
                
                  p
                  
                    o
                  
                
                
                  v
                  
                    0
                  
                
              
            
          
          |
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          |
          
            
              
                
                  S
                  
                    i
                  
                
                
                  p
                  
                    i
                    +
                  
                  
                    2
                  
                
              
              
                
                  S
                  
                    o
                  
                
                
                  p
                  
                    o
                  
                  
                    2
                  
                
              
            
          
          |
        
      
    
    {\displaystyle TL=L_{Wi}-L_{Wo}=10\log _{10}\left\vert {S_{i}p_{i+}v_{i+} \over 2}{2 \over S_{o}p_{o}v_{0}}\right\vert =10\log _{10}\left\vert {S_{i}p_{i+}^{2} \over S_{o}p_{o}^{2}}\right\vert }
  
where:

  
    
      
        
          L
          
            W
            i
          
        
      
    
    {\displaystyle L_{Wi}}
   is the incident sound power in the inlet coming towards muffler;

  
    
      
        
          L
          
            W
            o
          
        
      
    
    {\displaystyle L_{Wo}}
   is the transmitted sound power going downstream in the outlet out of the muffler;

  
    
      
        
          S
          
            i
          
        
        ,
        
          S
          
            o
          
        
      
    
    {\displaystyle S_{i},S_{o}}
   stand for the cross-sectional area of the inlet and outlet of muffler;

  
    
      
        
          p
          
            i
            +
          
        
      
    
    {\displaystyle p_{i+}}
   is the acoustic pressure of the incident wave in the inlet, towards muffler;

  
    
      
        
          p
          
            o
          
        
      
    
    {\displaystyle p_{o}}
   is the acoustic pressure of the transmitted wave in the outlet, away from muffler.

  
    
      
        
          v
          
            i
            +
          
        
      
    
    {\displaystyle v_{i+}}
   is the particle velocity of the incident wave in the inlet, towards muffler;

  
    
      
        
          v
          
            o
          
        
      
    
    {\displaystyle v_{o}}
   is the particle velocity of the transmitted wave in the outlet, away from muffler.
Note that 
  
    
      
        
          p
          
            i
            +
          
        
      
    
    {\displaystyle p_{i+}}
   cannot be measured directly in isolation from the reflected wave pressure 
  
    
      
        
          p
          
            i
            ?
          
        
      
    
    {\displaystyle p_{i-}}
   (in the inlet, away from muffler). One has to resort to impedance tube technology or two-microphone method with modern instrumentation. However at the downstream side of the muffler, 
  
    
      
        
          p
          
            o
          
        
        =
        
          p
          
            o
            +
          
        
      
    
    {\displaystyle p_{o}=p_{o+}}
   in view of the anechoic termination, which ensures 
  
    
      
        
          p
          
            o
            ?
          
        
        =
        0
      
    
    {\displaystyle p_{o-}=0}
  .
And in most muffler applications, Si and So, the area of the exhaust pipe and tail pipe, are generally made equal, thus we have:

  
    
      
        T
        L
        =
        20
        
          log
          
            10
          
        
        ?
        
          |
          
            
              
                p
                
                  i
                  +
                
              
              
                p
                
                  o
                
              
            
          
          |
        
      
    
    {\displaystyle TL=20\log _{10}\left\vert {p_{i+} \over p_{o}}\right\vert }
  
Thus, TL equals 20 times the logarithm (to the base 10) of the ratio of the acoustic pressure associated with the incident wave (in the exhaust pipe) and that of the transmitted wave (in the tail pipe), with the two pipes having the same cross-sectional area and the tail pipe terminating anechoically. However this anechoic condition is normally difficult to meet under practical industry environment, thus it is usually more convenient for the muffler manufacturers to measure insertions loss during their muffler performance tests under working conditions (mounted on an engine).
Also, since the transmitted sound power cannot possibly exceed the incident sound power (or 
  
    
      
        
          |
          
            p
            
              i
              +
            
          
          |
        
      
    
    {\displaystyle \left\vert p_{i+}\right\vert }
   is always larger than 
  
    
      
        
          |
          
            p
            
              o
            
          
          |
        
      
    
    {\displaystyle \left\vert p_{o}\right\vert }
  ), it is known that TL will never be less than 0 dB.

Transmission matrix description
The low-frequency approximation implies that each subsystem is an acoustic two-port (or four-pole system) with two (and only two) unknown parameters, the complex amplitudes of two interfering waves travelling in opposite directions. Such a system can be described by its transmission matrix (or four-pole matrix), as follows

  
    
      
        
          
            [
            
              
                
                  
                    
                      
                        
                          p
                          ^
                        
                      
                    
                    
                      i
                    
                  
                
              
              
                
                  
                    
                      
                        
                          q
                          ^
                        
                      
                    
                    
                      i
                    
                  
                
              
            
            ]
          
        
        =
        
          
            [
            
              
                
                  A
                
                
                  B
                
              
              
                
                  C
                
                
                  D
                
              
            
            ]
          
        
        
          
            [
            
              
                
                  
                    
                      
                        
                          p
                          ^
                        
                      
                    
                    
                      o
                    
                  
                
              
              
                
                  
                    
                      
                        
                          q
                          ^
                        
                      
                    
                    
                      o
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}{\hat {p}}_{i}\\{\hat {q}}_{i}\end{bmatrix}}={\begin{bmatrix}A&B\\C&D\end{bmatrix}}{\begin{bmatrix}{\hat {p}}_{o}\\{\hat {q}}_{o}\end{bmatrix}}}
  ,
where 
  
    
      
        
          
            
              
                p
                ^
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\hat {p}}_{i}}
  ,
  
    
      
        
          
            
              
                p
                ^
              
            
          
          
            o
          
        
      
    
    {\displaystyle {\hat {p}}_{o}}
  ,
  
    
      
        
          
            
              
                q
                ^
              
            
          
          
            i
          
        
      
    
    {\displaystyle {\hat {q}}_{i}}
   and 
  
    
      
        
          
            
              
                q
                ^
              
            
          
          
            o
          
        
      
    
    {\displaystyle {\hat {q}}_{o}}
   are the sound pressures and volume velocities at the input and at the output. A, B, C and D are complex numbers. With this representation it can be prove that the transmission loss (TL) of this subsystem can be calculated as,

  
    
      
        T
        L
        =
        10
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                1
                4
              
            
            
              
                |
                
                  A
                  +
                  B
                  
                    
                      S
                      
                        ?
                        c
                      
                    
                  
                  +
                  C
                  
                    
                      
                        ?
                        c
                      
                      S
                    
                  
                  +
                  D
                
                |
              
              
                2
              
            
          
          )
        
      
    
    {\displaystyle TL=10\log _{10}\left({{1 \over 4}\left\vert {A+B{S \over \rho c}+C{\rho c \over S}+D}\right\vert ^{2}}\right)}
  ,
where:

  
    
      
        S
      
    
    {\displaystyle S}
   is inlet and outlet cross-sectional area;

  
    
      
        ?
        c
      
    
    {\displaystyle \rho c}
   are media density and sound velocity.

A simple example

Considering we have the most simplest reactive silencer with only one expansion chamber (length l and cross-sectional area S2), with inlet and outlet both having cross-sectional area S1). As we know the transmission matrix of a tube (in this case, the expansion chamber) is

  
    
      
        
          
            [
            
              
                
                  A
                
                
                  B
                
              
              
                
                  C
                
                
                  D
                
              
            
            ]
          
        
        =
        
          
            [
            
              
                
                  
                    cos
                    ?
                    k
                    l
                  
                
                
                  
                    j
                    
                      
                        
                          ?
                          c
                        
                        
                          S
                          
                            2
                          
                        
                      
                    
                    sin
                    ?
                    k
                    l
                  
                
              
              
                
                  
                    j
                    
                      
                        
                          S
                          
                            2
                          
                        
                        
                          ?
                          c
                        
                      
                    
                    sin
                    ?
                    k
                    l
                  
                
                
                  
                    cos
                    ?
                    k
                    l
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}A&B\\C&D\end{bmatrix}}={\begin{bmatrix}{\cos kl}&{j{\rho c \over S_{2}}\sin kl}\\{j{S_{2} \over \rho c}\sin kl}&{\cos kl}\end{bmatrix}}}
  .
Substitute to the equation of TL above, it can be seen that the TL of this simple reactive silencer is

  
    
      
        
          
            
              
                T
                L
              
              
                
                =
                10
                
                  log
                  
                    10
                  
                
                ?
                
                  (
                  
                    
                      
                        1
                        4
                      
                    
                    
                      
                        |
                        
                          
                            cos
                            ?
                            k
                            l
                          
                          +
                          
                            j
                            
                              
                                
                                  S
                                  
                                    1
                                  
                                
                                
                                  S
                                  
                                    2
                                  
                                
                              
                            
                            sin
                            ?
                            k
                            l
                          
                          +
                          
                            j
                            
                              
                                
                                  S
                                  
                                    2
                                  
                                
                                
                                  S
                                  
                                    1
                                  
                                
                              
                            
                            sin
                            ?
                            k
                            l
                          
                          +
                          
                            cos
                            ?
                            k
                            l
                          
                        
                        |
                      
                      
                        2
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                10
                
                  log
                  
                    10
                  
                
                ?
                
                  (
                  
                    
                      
                        cos
                        
                          2
                        
                      
                      ?
                      k
                      l
                    
                    +
                    
                      
                        1
                        4
                      
                    
                    
                      
                        (
                        h
                        +
                        
                          
                            1
                            h
                          
                        
                        )
                      
                      
                        2
                      
                    
                    
                      
                        sin
                        
                          2
                        
                      
                      ?
                      k
                      l
                    
                  
                  )
                
              
            
            
              
              
                
                =
                10
                
                  log
                  
                    10
                  
                
                ?
                
                  (
                  
                    1
                    +
                    
                      
                        1
                        4
                      
                    
                    
                      
                        (
                        h
                        ?
                        
                          
                            1
                            h
                          
                        
                        )
                      
                      
                        2
                      
                    
                    
                      
                        sin
                        
                          2
                        
                      
                      ?
                      k
                      l
                    
                  
                  )
                
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}TL&=10\log _{10}\left({{1 \over 4}\left\vert {{\cos kl}+{j{S_{1} \over S_{2}}\sin kl}+{j{S_{2} \over S_{1}}\sin kl}+{\cos kl}}\right\vert ^{2}}\right)\\&=10\log _{10}\left({{\cos ^{2}kl}+{1 \over 4}\left(h+{1 \over h}\right)^{2}{\sin ^{2}kl}}\right)\\&=10\log _{10}\left({1+{1 \over 4}\left(h-{1 \over h}\right)^{2}{\sin ^{2}kl}}\right),\end{aligned}}}
  
where 
  
    
      
        h
      
    
    {\displaystyle h}
   is the ratio of the cross-sectional areas and 
  
    
      
        l
      
    
    {\displaystyle l}
   is the length of the chamber. 
  
    
      
        k
        =
        ?
        
          /
        
        c
      
    
    {\displaystyle k=\omega /c}
   is the wave number while 
  
    
      
        c
      
    
    {\displaystyle c}
   is the sound speed. Note that the transmission loss is zero when 
  
    
      
        l
      
    
    {\displaystyle l}
   is a multiple of half a wavelength.
As a simple example, consider a one chamber silencer with h=S1/S2=1/3, at around 400 °C the sound speed is about 520 m/s, with l=0.5 m, one easily calculate the TL result shown on the plot on the right. Note that the TL equals zero when frequency is a multiple of 
  
    
      
        
          c
        
        
          2
          l
        
      
    
    {\displaystyle c \over {2l}}
   and TL peaks when frequency is 
  
    
      
        
          
            c
            
              4
              l
            
          
        
        +
        
          n
          ?
          
            
              c
              
                2
                l
              
            
          
        
      
    
    {\displaystyle {c \over {4l}}+{n*{c \over {2l}}}}
  .
Also note that the above calculation is only valid for low-frequency range because at low-frequency range the sound wave can be treated as a plane wave. The TL calculation will start losing its accuracy when the frequency goes above the cutoff frequency, which can be calculated as 
  
    
      
        
          f
          
            c
          
        
        =
        1.84
        
          
            c
            
              ?
              D
            
          
        
      
    
    {\displaystyle f_{c}=1.84{c \over {\pi D}}}
  , where D is diameter of the largest pipe in the structure. In the case above, if for example the muffler body has a diameter of 300mm, then the cut-off frequency is then 1.84*520/pi/0.3=1015 Hz.


== References ==",Category:Physical quantities,1
7,8,Acoustic network,"An acoustic network is a method of positioning equipment using sound waves. It is primarily used in water, and can be as small or as large as required by the users specifications.

Size of network
The simplest acoustic network consists of one measurement resulting in a single range between sound source and sound receiver.
Bigger networks are only limited by the amount of equipment available, and computing power needed to resolve the resulting data. The latest acoustic networks used in the marine seismic industry can resolve a network of some 16,000 individual ranges in a matter of seconds.

The principle
The principle behind all acoustic networks is the same. Distance = speed x travel time. If the travel time and speed of the sound signal are known, we can calculate the distance between source and receiver. In most networks, the speed of the acoustic signal is assumed at a specific value. This value is either derived from measuring a signal between two known points, or by using specific equipment to calculate it from environmental conditions.
The diagram below shows the basic operation of measuring a single range.

At a specified time the processor issues a signal to the source, which then sends out the sound wave.
Once the sound wave is received another signal is received at the processor resulting in a time difference between transmission and reception. This gives the travel time.
Using the travel time and assumed speed of the signal, the processor can calculate the distance between source and receiver.
If the operator is using acoustic ranges to position items in unknown locations they will need to use more than the single range example shown above.
As there is only one measurement, the receiver could be anywhere on a circle with a radius equal to the calculated range and centered on the transmitter.

Acoustic Processing
Audition and acoustic solutions for environmental sounds and noises acquisition and signal processing applications, designs, markets, manufactures and sells low complexity embeddable solutions for capture and other acoustic processing applications: - Multi-channel A/D conversion - Beamforming and front-end acoustic processing solutions for low noise audio capture - Standalone and Embeddable - System for acoustic ambient intelligent applications
If a second transmitter is added to the system the number of possible positions for the receiver is reduced to two.
It is only when three or more ranges are introduced into the system, is the position of the receiver achieved.",Category:All articles lacking sources,1
8,9,Acoustic shock,"Acoustic shock is the symptoms a person may experience after hearing an unexpected, loud sound. The loud sound, called an Acoustic Incident, can be caused by feedback oscillation, fax tones, or signalling tones. Telemarketers and call centre employees are thought to be most at risk.

Reported symptoms
During the exposure, most people will experience discomfort and pain. After the exposure, some people might report shock, nausea and anxiety or depression. Headache, fatigue, hypersensitivity to loud noise and tinnitus may continue for days, weeks or indefinitely. It has not been established how such unrelated symptoms might be caused by an acoustic exposure, or whether such symptoms are even a direct result of exposure.

Physiological mechanisms
It has been suggested that the tensor tympani is involved in causing the disorder. In particular, the tonic tensor tympani syndrome. In France, researchers report the study of a case of acoustic shock in a scientific publication. They suggest that these symptoms may result from a loop involving the middle ear muscles, peripheral inflammatory processes, activation and sensitization of the trigeminal nerve, the autonomic nervous system, and central feedbacks.

Prevention
There are many methods of attempting to reduce the risk of AS. Several devices attempt to remove potentially harmful sound signals by digital signal processing. None has yet been shown to be fully effective. Devices which solely limit noise levels to about 85 dB have been shown in field trials to be ineffective (data from these trials has not been released into the public domain). Limiting background noise and office stress may also reduce the chance of an Acoustic Shock. Proper use of the headset and preventing mobile phones from being used in call centers reduces the chance of feedback.

Legal Action
84 BT employees suffering from depression, headaches and other health problems, are demanding compensation for injury sustained from acoustic shock at work. BT has already paid £90,000 to one worker that suffered from tinnitus.


== References ==",Category:Acoustics,1
9,10,Ultrasound computer tomography,"Ultrasound computer tomography (USCT), sometimes also Ultrasound computed tomography, Ultrasound computerized tomography or just Ultrasound tomography, is a form of medical ultrasound tomography utilizing ultrasound waves as physical phenomenon for imaging. It is mostly in use for soft tissue medical imaging, especially breast imaging.

Description
Ultrasound computer tomographs use ultrasound waves for creating images. In the first measurement step a defined ultrasound wave is generated with typically Piezoelectric ultrasound transducers, transmitted in direction of the measurement object and received with other or the same ultrasound transducers. While traversing and interacting with the object the ultrasound wave is changed by the object and carries now information about the object. After being recorded the information from the modulated waves can be extracted and used to create an image of the object in a second step. Unlike X-ray or other physical properties which provide typically only one information, ultrasound provides multiple information of the object for imaging: the attenuation the wave's sound pressure experiences indicate on the object's attenuation coefficient, the time-of-flight of the wave gives speed of sound information, and the scattered wave indicates on the echogenicity of the object (e.g. refraction index, surface morphology, etc.). Unlike conventional ultrasound sonography, which uses phased array technology for beamforming, most USCT systems utilize unfocused spherical waves for imaging. Most USCT systems aiming for 3D-imaging, either by synthesizing (""stacking"") 2D images or by full 3D aperture setups. Another aim is quantitative imaging instead of only qualitative imaging.
The idea of Ultrasound computer tomography goes back to the 1950s with analogue compounding setups, in the mid 1970s the first ""computed"" USCT systems were built up, utilizing digital technology. The ""computer"" in the USCT concept indicates the heavy reliance on computational intensive advanced digital signal processing, image reconstruction and image processing algorithms for imaging. Successfully realization of USCT systems in the last decades was possible through the continuously growing availability of computing power and data bandwidth provided by the digital revolution.

Setup
USCT systems designed for medical imaging of soft tissue typically aim for resolution in order of centimeters to millimeters and require therefore ultrasound waves in the order of mega-hertz frequency. This requires typically water as low-attenuating transmission medium between ultrasound transducers and object to retain suitable sound pressures.
USCT systems share with the common tomography the fundamental architectural similarity that the aperture, the active imaging elements, surround the object. For the distribution of ultrasound transducers around the measurement object, forming the aperture, multiple design approaches exist. There exist mono-, bi- and multistatic setups of transducer configurations. Common are 1D- or 2D- linear arrays of ultrasound transducers acting as emitters on one side of the object, on the opposing side of the object a similar array acting as receiver is placed, forming a parallel setup. Sometimes accompanied with the additional ability to be moved to gather more information from additional angles. While cost efficient to build the main disadvantage of such an setup is the limited ability (or inability) of gathering reflectivity information, as such an aperture is limited to only transmission information. Another aperture approach is a ring of transducers, sometimes with the degree of freedom of motorized lifting for gathering additional information over the height for 3D imaging (""stacking""). Full 3D setups, with no inherent need for aperture movements, exist in form of apertures formed by semi-spherical distributed transducers. While the most expensive setup they offer the advantage of nearly-uniform data, gathered from many directions. Also, they are fast in data taking as they don't require time-costly mechanical movements.

Imaging methods and algorithms
Tomographic reconstruction methods used in USCT systems for transmission information based imaging are classical inverse radon transform and fourier slice theorem and derived algorithms (cone beam etc.). As advanced alternative also ART-based approaches are utilized. For high resolution and speckle noise reduced reflectivity imaging Synthetic Aperture Focusing Techniques (SAFT), similar to radar's SAR and sonar's SAS, are widely used. Iterative wave equation inversion approaches as imaging method coming from the seismology are under academic research, but usage for real world applications is due to the enormous computational and memory burden still a challenge.

Application and usage
Many USCT systems are designed for soft tissue imaging and for breast cancer diagnosis specifically. As ultrasound based method with low sound pressures, USCT is a harmless and risk-free imaging method, suitable for periodical screening. As USCT setups are fixed or motor moved without direct contact with the breast the reproduction of images is easier as with common, manually guided methods (e.g. Breast ultrasound) which rely on the individual examiners performance and experience. In comparison with conventional screening methods like mammography, USCT systems offer potentially an increased specificity for breast cancer detection, as multiple breast cancer characteristic properties are imaged at the same time: speed-of-sound, attenuation and morphology.

See also
Medical ultrasound
Tomography
Ultrasound transmission tomography
Ultrasound-modulated optical tomography

References

External Links",Category:Medical equipment,1
10,11,Acoustic tweezers,"Acoustic tweezers is a technology that is able to control the movement of objects by sound waves. In a standing acoustic field, objects will experience an acoustic radiation force that moves the objects to special regions of the acoustic field. Depending on the properties (density, compressibility) of the objects, they can be moved to either acoustic pressure nodes (minimum pressure regions) or pressure antinodes (maximum pressure regions). As a result, precise manipulation of objects using sound waves is feasible by controlling the position of pressure nodes. Acoustic tweezers does not require expensive equipment and complex experimental setups. More importantly, acoustic waves have been proven safe to biological objects, making it an ideal tool for biomedical applications. In recent years, acoustic tweezers has found many important applications in the area of manipulating sub-millimeter objects, such as flow cytometry, cell separation, cell trapping, single-cell manipulation, and nanomaterial manipulation. The use of one-dimensional standing waves to manipulate small particles was reported for the first time in ""Ultrasonic inspection of fiber suspensions"".

Fundamental theory
Particles in acoustic field will be moved by forces originated from the interaction among the acoustic waves, fluid and the particles. These forces, including acoustic radiation force, secondary field force between particles, and Stokes drag force, create the phenomena of acoustophoresis, which is the foundation of acoustic tweezers technology.

Acoustic radiation force
When a particle is suspended in the field of a sound wave, a so-called acoustic radiation force, which arises from the scattering of the acoustic waves on the particle, will exert on the particle. The studies of acoustic radiation forces on suspended particles have a long history. The force was first modelled and analyzed for incompressible particles in an ideal fluid by King in 1934. Yosioka and Kawasima calculated the acoustic radiation force on compressible particles in a plane wave field in 1955. Gorkov summarized the previous work and proposed equations to determine the average force acting on a particle in an arbitrary acoustical field when its size is much smaller than the wavelength of the sound. Recently, Bruus revisited the problem and gave detailed derivation for the acoustic radiation force.

As shown in Figure 1, the acoustic radiation force on a small particle results from a non-uniform flux of momentum in the near-field region around the particle, 
  
    
      
        
          
            
              F
              
                r
                a
                d
              
            
          
        
        =
        ?
        ?
        
          
            U
          
        
      
    
    {\displaystyle \mathbf {\mathit {F^{rad}}} =-\nabla {\mathit {U}}}
   which is caused by the incoming acoustic waves and the scattering on the surface of the particle when acoustic waves propagate through it. For a compressible spherical particle with a diameter much smaller than the wavelength of acoustic waves in an ideal fluid, the acoustic radiation force can be calculated by 
  
    
      
        
          
            
              F
              
                r
                a
                d
              
            
          
        
        =
        ?
        ?
        
          
            U
          
        
      
    
    {\displaystyle \mathbf {\mathit {F^{rad}}} =-\nabla {\mathit {U}}}
  , where 
  
    
      
        
          
            U
          
        
      
    
    {\displaystyle {\mathit {U}}}
   is a given quantity, also called acoustic potential energy. The acoustic potential energy is expressed as:

  
    
      
        U
        =
        
          
            V
            
              0
            
          
        
        (
        
          
            
              
                
                  p
                  
                    i
                    n
                  
                  
                    2
                  
                
                ¯
              
            
            
              2
              
                
                  ?
                  
                    f
                  
                
              
              
                c
                
                  f
                
                
                  2
                
              
            
          
        
        
          
            f
            
              1
            
          
        
        ?
        
          
            
              3
              
                
                  ?
                  
                    f
                  
                
              
              
                
                  
                    v
                    
                      i
                      n
                    
                    
                      2
                    
                  
                  ¯
                
              
            
            4
          
        
        
          
            f
            
              2
            
          
        
        )
      
    
    {\displaystyle U={V_{0}}({{\overline {p_{in}^{2}}} \over {2{\rho _{f}}c_{f}^{2}}}{f_{1}}-{{3{\rho _{f}}{\overline {v_{in}^{2}}}} \over 4}{f_{2}})}
  
where
• 
  
    
      
        
          
            
              V
              
                
                  0
                
              
            
          
        
      
    
    {\displaystyle {\mathit {V_{\mathit {0}}}}}
   is the particle volume,
• 
  
    
      
        
          
            
              p
              
                
                  i
                  n
                
              
            
          
        
      
    
    {\displaystyle {\mathit {p_{\mathit {in}}}}}
   is the acoustic pressure,
• 
  
    
      
        
          
            
              v
              
                
                  i
                  n
                
              
            
          
        
      
    
    {\displaystyle {\mathit {v_{\mathit {in}}}}}
   is the velocity of acoustic particles,
• 
  
    
      
        
          
            
              ?
            
          
          
            
              f
            
          
        
      
    
    {\displaystyle {\mathit {\rho }}_{\mathit {f}}}
   is the fluid mass density,
• 
  
    
      
        
          
            
              c
              
                
                  f
                
              
            
          
        
      
    
    {\displaystyle {\mathit {c_{\mathit {f}}}}}
   is the speed of sound of the fluid,
• 
  
    
      
        
          
            
              f
              
                
                  1
                
              
            
          
        
      
    
    {\displaystyle {\mathit {f_{\mathit {1}}}}}
  , 
  
    
      
        
          
            
              f
              
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\mathit {f_{\mathit {2}}}}}
   are two coefficients.
• 
  
    
      
        
          
            
              <
              ?
              >
            
            ¯
          
        
      
    
    {\displaystyle {\overline {<*>}}}
   is the time-average term, 
  
    
      
        
          
            1
            T
          
        
        
          ?
          
            0
          
          
            T
          
        
        
          (
          ?
          )
        
        d
        t
      
    
    {\displaystyle {1 \over T}\int \limits _{0}^{T}{(*)}dt}
  
The two coefficients 
  
    
      
        
          
            
              f
              
                
                  1
                
              
            
          
        
      
    
    {\displaystyle {\mathit {f_{\mathit {1}}}}}
   and 
  
    
      
        
          
            
              f
              
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\mathit {f_{\mathit {2}}}}}
   can be calculated by 
  
    
      
        
          
            f
            
              1
            
          
        
        =
        1
        ?
        
          
            
              
                
                  ?
                  
                    f
                  
                
              
              
                c
                
                  f
                
                
                  2
                
              
            
            
              
                
                  ?
                  
                    p
                  
                
              
              
                c
                
                  p
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {f_{1}}=1-{{{\rho _{f}}c_{f}^{2}} \over {{\rho _{p}}c_{p}^{2}}}}
  , 
  
    
      
        
          
            f
            
              2
            
          
        
        =
        
          
            
              2
              (
              
                
                  ?
                  
                    p
                  
                
              
              ?
              
                
                  ?
                  
                    f
                  
                
              
              )
            
            
              2
              
                
                  ?
                  
                    p
                  
                
              
              +
              
                
                  ?
                  
                    f
                  
                
              
            
          
        
      
    
    {\displaystyle {f_{2}}={{2({\rho _{p}}-{\rho _{f}})} \over {2{\rho _{p}}+{\rho _{f}}}}}
  
where
• 
  
    
      
        
          
            
              ?
            
          
          
            
              p
            
          
        
      
    
    {\displaystyle {\mathit {\rho }}_{\mathit {p}}}
   is the mass density of the particle,
• 
  
    
      
        
          
            
              c
            
          
          
            
              p
            
          
        
      
    
    {\displaystyle {\mathit {c}}_{\mathit {p}}}
   is the speed of sound of the particle.

Acoustic radiation force in standing waves
The standing waves can form stable acoustic potential energy field, so they are able to create stable acoustic radiation force distribution, which is desirable for many acoustic tweezers applications. For 1-D planar standing waves, the acoustic fields are given by:

  
    
      
        
          
            A
            
              i
              n
            
          
        
        (
        x
        ,
        t
        )
        =
        
          
            
              ?
              
                P
                
                  0
                
              
            
            
              
                
                  ?
                  
                    f
                  
                
              
              ?
            
          
        
        sin
        ?
        (
        k
        x
        )
        sin
        ?
        (
        ?
        t
        )
      
    
    {\displaystyle {A_{in}}(x,t)={{-P_{0}} \over {{\rho _{f}}\omega }}\sin(kx)\sin(\omega t)}
  ,

  
    
      
        
          
            p
            
              i
              n
            
          
        
        (
        x
        ,
        t
        )
        =
        
          
            P
            
              0
            
          
        
        cos
        ?
        (
        k
        x
        )
        sin
        ?
        (
        ?
        t
        )
      
    
    {\displaystyle {p_{in}}(x,t)={P_{0}}\cos(kx)\sin(\omega t)}
  ,

  
    
      
        
          
            
              v
            
          
          
            
              in
            
          
        
        (
        x
        ,
        t
        )
        =
        
          
            
              ?
              
                P
                
                  0
                
              
            
            
              
                ?
                
                  f
                
              
              
                c
                
                  f
                
              
            
          
        
        sin
        ?
        (
        k
        x
        )
        cos
        ?
        (
        ?
        t
        )
        )
        
          
            
              e
            
          
          
            
              x
            
          
        
      
    
    {\displaystyle {\textbf {v}}_{\textbf {in}}(x,t)={\frac {-P_{0}}{\rho _{f}c_{f}}}\sin(kx)\cos(\omega t)){\textbf {e}}_{\textbf {x}}}
  ,
where
• 
  
    
      
        
          
            
              A
              
                
                  i
                  n
                
              
            
          
        
      
    
    {\displaystyle {\mathit {A_{\mathit {in}}}}}
   is the displacement of acoustic particle,
• 
  
    
      
        
          
            
              P
              
                
                  0
                
              
            
          
        
      
    
    {\displaystyle {\mathit {P_{\mathit {0}}}}}
   is the acoustic pressure amplitude,
• 
  
    
      
        
          
            ?
          
        
      
    
    {\displaystyle {\mathit {\omega }}}
   is the angular velocity,
• 
  
    
      
        
          
            k
          
        
      
    
    {\displaystyle {\mathit {k}}}
   is the wave number.
With these fields, the time-average terms can be obtained, and they are:

  
    
      
        
          
            
              p
              
                i
                n
              
              
                2
              
            
            ¯
          
        
        =
        
          
            1
            2
          
        
        
          P
          
            0
          
          
            2
          
        
        
          
            cos
            
              2
            
          
        
        (
        k
        x
        )
      
    
    {\displaystyle {\overline {p_{in}^{2}}}={1 \over 2}P_{0}^{2}{\cos ^{2}}(kx)}
  ,

  
    
      
        
          
            
              v
              
                i
                n
              
              
                2
              
            
            ¯
          
        
        =
        
          
            
              
                P
                
                  0
                
                
                  2
                
              
            
            
              2
              
                ?
                
                  f
                
                
                  2
                
              
              
                c
                
                  f
                
                
                  2
                
              
            
          
        
        
          
            sin
            
              2
            
          
        
        (
        k
        x
        )
      
    
    {\displaystyle {\overline {v_{in}^{2}}}={{P_{0}^{2}} \over {2\rho _{f}^{2}c_{f}^{2}}}{\sin ^{2}}(kx)}
  ,
Thus, the acoustic potential energy is:

  
    
      
        U
        =
        
          
            V
            
              0
            
          
        
        
          
            
              
                P
                
                  0
                
                
                  2
                
              
            
            
              4
              
                
                  ?
                  
                    f
                  
                
              
              
                c
                
                  f
                
                
                  2
                
              
            
          
        
        [
        
          
            cos
            
              2
            
          
        
        (
        k
        x
        )
        
          
            f
            
              1
            
          
        
        ?
        
          
            3
            2
          
        
        
          
            sin
            
              2
            
          
        
        (
        k
        x
        )
        
          
            f
            
              2
            
          
        
        ]
      
    
    {\displaystyle U={V_{0}}{{P_{0}^{2}} \over {4{\rho _{f}}c_{f}^{2}}}[{\cos ^{2}}(kx){f_{1}}-{3 \over 2}{\sin ^{2}}(kx){f_{2}}]}
  ,
Then, the acoustic radiation force is found by differentiation:

  
    
      
        
          
            
              F
              
                x
              
            
          
          
            r
            a
            d
          
        
        =
        ?
        
          
            ?
            
              x
            
          
        
        U
        =
        
          
            V
            
              0
            
          
        
        k
        
          
            E
            
              a
              c
            
          
        
        sin
        ?
        (
        2
        k
        x
        )
        ?
      
    
    {\displaystyle {F_{x}}^{rad}=-{\partial _{x}}U={V_{0}}k{E_{ac}}\sin(2kx)\Phi }
  ,

  
    
      
        
          
            E
            
              a
              c
            
          
        
        =
        
          
            
              
                P
                
                  0
                
                
                  2
                
              
            
            
              4
              
                
                  ?
                  
                    f
                  
                
              
              
                c
                
                  f
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {E_{ac}}={{P_{0}^{2}} \over {4{\rho _{f}}c_{f}^{2}}}}
  , 
  
    
      
        ?
        =
        
          
            f
            
              1
            
          
        
        +
        
          
            3
            2
          
        
        
          
            f
            
              2
            
          
        
        =
        
          
            
              5
              
                
                  ?
                  
                    p
                  
                
              
              ?
              2
              
                
                  ?
                  
                    f
                  
                
              
            
            
              2
              
                
                  ?
                  
                    p
                  
                
              
              +
              
                
                  ?
                  
                    f
                  
                
              
            
          
        
        ?
        
          
            
              
                
                  ?
                  
                    f
                  
                
              
              
                c
                
                  f
                
                
                  2
                
              
            
            
              
                
                  ?
                  
                    p
                  
                
              
              
                c
                
                  p
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \Phi ={f_{1}}+{3 \over 2}{f_{2}}={{5{\rho _{p}}-2{\rho _{f}}} \over {2{\rho _{p}}+{\rho _{f}}}}-{{{\rho _{f}}c_{f}^{2}} \over {{\rho _{p}}c_{p}^{2}}}}
  ,

where 
  
    
      
        
          
            
              E
              
                
                  a
                  c
                
              
            
          
        
      
    
    {\displaystyle {\mathit {E_{\mathit {ac}}}}}
   is the acoustic energy density, and 
  
    
      
        ?
      
    
    {\displaystyle \Phi }
   is acoustophoretic contrast factor. The term 
  
    
      
        sin
        ?
        (
        
          
            2
            k
            x
          
        
        )
      
    
    {\displaystyle \sin({\mathit {2kx}})}
   shows that the radiation force period is one half of the pressure period. Also, the contrast factor can be positive or negative depending on the properties of particles and fluid. For positive value of 
  
    
      
        ?
      
    
    {\displaystyle \Phi }
  , the radiation force points from the pressure antinodes to the pressure nodes, as shown in Figure 2, and the particles will be pushed to the pressure nodes.

Secondary acoustic forces
When multiple particles in a suspension are exposed to a standing wave field, they will not only experience acoustic radiation force, but also secondary acoustic forces caused by waves scattered by particles. The interparticle forces are sometimes called Bjerknes forces. A simplified equation for the interparticle forces:

  
    
      
        
          
            F
            
              B
            
          
        
        (
        x
        )
        =
        4
        ?
        
          
            a
            
              6
            
          
        
        [
        
          
            
              
                
                  
                    (
                    
                      
                        ?
                        
                          p
                        
                      
                    
                    ?
                    
                      
                        ?
                        
                          f
                        
                      
                    
                    )
                  
                  
                    2
                  
                
              
              (
              3
              
                
                  
                    cos
                  
                  
                    2
                  
                
              
              ?
              ?
              1
              )
            
            
              6
              
                
                  ?
                  
                    f
                  
                
              
              
                
                  d
                  
                    4
                  
                
              
            
          
        
        
          
            
              v
              
                i
                n
              
            
          
          
            2
          
        
        (
        x
        )
        ?
        
          
            
              
                ?
                
                  2
                
              
              
                
                  ?
                  
                    f
                  
                
              
            
            
              9
              
                
                  d
                  
                    2
                  
                
              
            
          
        
        
          (
          
            
              1
              
                
                  
                    ?
                    
                      p
                    
                  
                
                
                  c
                  
                    p
                  
                  
                    2
                  
                
              
            
          
          ?
          
            
              1
              
                
                  
                    ?
                    
                      f
                    
                  
                
                
                  c
                  
                    f
                  
                  
                    2
                  
                
              
            
          
          
            )
            
              2
            
          
        
        
          p
          
            i
            n
          
          
            2
          
        
        (
        x
        )
        ]
      
    
    {\displaystyle {F_{B}}(x)=4\pi {a^{6}}[{{{{({\rho _{p}}-{\rho _{f}})}^{2}}(3{{\cos }^{2}}\theta -1)} \over {6{\rho _{f}}{d^{4}}}}{v_{in}}^{2}(x)-{{\omega ^{2}{\rho _{f}}} \over {9{d^{2}}}}{({1 \over {{\rho _{p}}c_{p}^{2}}}-{1 \over {{\rho _{f}}c_{f}^{2}}})^{2}}p_{in}^{2}(x)]}
  
where
• 
  
    
      
        
          
            a
          
        
      
    
    {\displaystyle {\mathit {a}}}
   is the radius of the particle,
• 
  
    
      
        
          
            d
          
        
      
    
    {\displaystyle {\mathit {d}}}
   is the distance between the particles,
• 
  
    
      
        
          
            ?
          
        
      
    
    {\displaystyle {\mathit {\theta }}}
   is the angle between the central line of the particles and the direction of propagation of the incident acoustic wave.
The sign of the force represents the direction of the force. A negative sign means an attractive interparticle force, whereas a positive sign means a repulsive force. The left side of the equation depends on the acoustic particle velocity amplitude 
  
    
      
        
          
            
              v
            
          
          
            
              i
              n
            
          
        
        (
        
          
            x
          
        
        )
      
    
    {\displaystyle {\mathit {v}}_{\mathit {in}}({\mathit {x}})}
   and the right side depends on the acoustic pressure amplitude 
  
    
      
        
          
            
              p
            
          
          
            
              i
              n
            
          
        
        (
        
          
            x
          
        
        )
      
    
    {\displaystyle {\mathit {p}}_{\mathit {in}}({\mathit {x}})}
  . When particles are lined up in the direction of the acoustic wave propagation (?=0?) the velocity-dependent term is repulsive, and likewise attractive when the particles are perpendicular (?=90?) to the incident wave propagation. The pressure-dependent term is not affected by the particle orientation at all and is always attractive. In the case of positive contrast factor, the velocity-dependent term diminishes as particles are driven to the velocity node (pressure antinode), as in the case of air bubbles and lipid vesicles. In a similar way the pressure-dependent term diminishes as particles are driven towards the pressure node (velocity antinode), as are most solid particles in aqueous solutions. The influence of the secondary forces is usually very weak, due to the distance term d in the denominator, which means that it is only effective when the distance between particles is very small. The secondary force becomes important in aggregation and sedimentation applications, where particles initially are gathered in nodes by the acoustic radiation force, and as interparticle distances become smaller the secondary forces assist in a further aggregation until the clusters finally become heavy enough for the gravity to overcome the buoyancy and start the sedimentation process.

Acoustic streaming
Acoustic streaming is a steady flow generated by a nonlinear effect in an acoustic field. Depending on the mechanisms, the acoustic streaming can be categorized into two general types, Eckert streaming and Rayleigh streaming. Eckert streaming is driven by a time-average momentum flux created when high amplitude acoustic wave propagates and attenuates in fluid. Rayleigh streaming, also called “boundary driven streaming”, is forced by a shear viscosity close to a solid boundary. Both of the driven mechanisms come from a time-average nonlinear effect. Regarding to the nonlinearity of acoustic streaming, a so-called perturbation approach is used to analyze this phenomenon. The governing equations for this problem are mass conservation and Navier-Stokes equations?

  
    
      
        
          ?
          
            
              t
            
          
        
        
          
            ?
          
        
        =
        ?
        ?
        ?
        (
        
          
            ?
          
        
        
          
            v
          
        
        )
      
    
    {\displaystyle \partial _{\mathit {t}}{\mathit {\rho }}=-\nabla \cdot ({\mathit {\rho }}{\textbf {v}})}
  ,

  
    
      
        
          
            ?
          
        
        [
        
          ?
          
            
              t
            
          
        
        
          
            v
          
        
        +
        (
        
          
            v
          
        
        ?
        ?
        )
        
          
            v
          
        
        ]
        =
        ?
        ?
        
          
            p
          
        
        +
        
          
            ?
          
        
        
          ?
          
            2
          
        
        
          
            v
          
        
        +
        
          
            ?
          
        
        
          
            ?
          
        
        ?
        (
        ?
        ?
        
          
            v
          
        
        )
        )
      
    
    {\displaystyle {\mathit {\rho }}[\partial _{\mathit {t}}{\textbf {v}}+({\textbf {v}}\cdot \nabla ){\textbf {v}}]=-\nabla {\mathit {p}}+{\mathit {\mu }}\nabla ^{2}{\textbf {v}}+{\mathit {\beta }}{\mathit {\mu }}\nabla (\nabla \cdot {\textbf {v}}))}
  
where
• 
  
    
      
        
          
            ?
          
        
      
    
    {\displaystyle {\mathit {\rho }}}
   is the density of fluid,
• 
  
    
      
        
          
            v
          
        
      
    
    {\displaystyle {\textbf {v}}}
   is the velocity of fluid particle,
• 
  
    
      
        
          
            p
          
        
      
    
    {\displaystyle {\mathit {p}}}
   is the pressure
• 
  
    
      
        
          
            ?
          
        
      
    
    {\displaystyle {\mathit {\mu }}}
   is the dynamic viscosity of fluid
• 
  
    
      
        
          
            ?
          
        
      
    
    {\displaystyle {\mathit {\beta }}}
   is the viscosity ratio.
The perturbation series can be written as 
  
    
      
        p
        =
        
          
            p
            
              0
            
          
        
        +
        
          
            p
            
              1
            
          
        
        +
        
          
 ",Category:Acoustics,1
11,12,Transient (acoustics),"In acoustics and audio, a transient is a high amplitude, short-duration sound at the beginning of a waveform that occurs in phenomena such as musical sounds, noises or speech. It can sometimes contain a high degree of non-periodic components and a higher magnitude of high frequencies than the harmonic content of that sound. Transients do not necessarily directly depend on the frequency of the tone they initiate.
Transients are more difficult to encode with many audio compression algorithms, causing pre-echo.

Sonar
The term transient is used by military sonar operators to describe unexpected sounds emanating from another vessel such as operating machinery, a metal hatch being slammed, or the flooding and pressurization of torpedo or vertical launch tubes.

See also
Prefix (acoustics)
Impulse function
Onset (audio)
Transient response – a common electrical engineering term that may be the source of the idea of an acoustic ""transient""


== References ==",Category:Acoustics,1
12,13,Acoustic cryptanalysis,"Acoustic cryptanalysis is a type of side channel attack that exploits sounds emitted by computers or other devices.
Most of the modern acoustic cryptanalysis focuses on the sounds produced by computer keyboards and internal computer components, but historically it has also been applied to impact printers, and electromechanical deciphering machines.

History
Victor Marchetti and John D. Marks eventually negotiated the declassification of CIA acoustic intercepts of the sounds of cleartext printing from encryption machines. Technically this method of attack dates to the time of FFT hardware being cheap enough to perform the task—in this case the late 1960s to mid-1970s. However, using other more primitive means such acoustical attacks were made in the mid-1950s.
In his book Spycatcher, former MI5 operative Peter Wright discusses use of an acoustic attack against Egyptian Hagelin cipher machines in 1956. The attack was codenamed ""ENGULF"".

Known attacks
In 2004, Dmitri Asonov and Rakesh Agrawal of the IBM Almaden Research Center announced that computer keyboards and keypads used on telephones and automated teller machines (ATMs) are vulnerable to attacks based on the sounds produced by different keys. Their attack employed a neural network to recognize the key being pressed. By analyzing recorded sounds, they were able to recover the text of data being entered. These techniques allow an attacker using covert listening devices to obtain passwords, passphrases, personal identification numbers (PINs), and other information entered via keyboards. In 2005, a group of UC Berkeley researchers performed a number of practical experiments demonstrating the validity of this kind of threat.
Also in 2004, Adi Shamir and Eran Tromer demonstrated that it may be possible to conduct timing attacks against a CPU performing cryptographic operations by analyzing variations in acoustic emissions. Analyzed emissions were ultrasonic noise emanating from capacitors and inductors on computer motherboards, not electromagnetic emissions or the human-audible humming of a cooling fan. Shamir and Tromer, along with new collaborator Daniel Genkin and others, then went on to successfully implement the attack on a laptop running a version of GnuPG (an RSA implementation), using either a mobile phone located close to the laptop, or a laboratory-grade microphone located up to 4 m away, and published their experimental results in December 2013.
Acoustic emissions occur in coils and capacitors because of small movements when a current surge passes through them. Capacitors in particular change diameter slightly as their many layers experience electrostatic attraction/repulsion or piezoelectric size change. A coil or capacitor which emits acoustic noise will, conversely, also be microphonic, and the high-end audio industry takes steps with coils and capacitors to reduce these microphonics (emissions) because they can muddy a hi-fi amplifier's sound.
In March 2015, it was made public that some inkjet printers using ultrasonic heads can be read back using high frequency MEMS microphones to record the unique acoustic signals from each nozzle and using timing reconstruction with known printed data, that is, ""confidential"" in 12-point font. Thermal printers can also be read using similar methods but with less fidelity as the signals from the bursting bubbles are weaker. The hack also involved implanting a microphone, chip storage IC and burst transmitter with long-life Li+ battery into doctored cartridges substituted for genuine ones sent by post to the target, typically a bank, then retrieved from the garbage using challenge-response RFID chip. A similar work on reconstructing printouts made by dot-matrix printers was publicized in 2011.

Countermeasures
This kind of cryptanalysis can be defeated by generating sounds that are in the same spectrum and same form as keypresses. If you randomly replay sounds of actual keypresses, it may be possible to totally defeat such kinds of attacks. It is advisable to use at least 5 different recorded variations (36 x 5 = 180 variations) for each keypress to get around the issue of FFT fingerprinting. Alternatively, white noise of a sufficient volume (which may be simpler to generate for playback) will also mask the acoustic emanations of individual keypresses.

See also
TEMPEST
ACOUSTINT


== References ==",Category:Acoustics,1
13,14,Schlieren imaging,"Schlieren imaging is a method to visualize density variations in transparent media.

The term ""schlieren imaging"" is commonly used as a synonym for schlieren photography, though this article particularly treats visualization of the pressure field produced by ultrasonic transducers, generally in water or tissue-mimicking media. The method provides a two-dimensional (2D) projection image of the acoustic beam in real-time (""live video""). The unique properties of the method enable the investigation of specific features of the acoustic field (e.g. focal point in HIFU transducers), detection of acoustic beam-profile irregularities (e.g. due to defects in transducer) and on-line identification of time-dependent phenomena  (e.g. in phased array transducers). Some researchers say that schlieren imaging is equivalent to an X-ray radiograph of the acoustic field.

Setup
The optical setup of a schlieren imaging system may comprise the following main sections: Parallel beam, focusing element, stop (sharp edge) and a camera. The parallel beam may be achieved by a point-like light source (a laser focused into a pinhole is sometimes used) placed in the focal point of a collimating optical element (lens or mirror). The focusing element may be a lens or a mirror. The optical stop may be realized by a razor placed horizontally or vertically in the focal point of the focusing element, carefully positioned to block the light spot image on its edge. The camera is positioned behind the stop and may be equipped with a suitable lens.

Physics
Ray optics description
A parallel beam is described as a group of straight and parallel 'rays'. The rays cross through the transparent medium while potentially interacting with the contained acoustic field, and finally reach the focusing element. Note that the principle of a focusing element is directing (i.e. focusing) rays that are parallel - into a single point on the focal plane of the element. Thus, the population of rays crossing the focal plane of the focusing element can be divided into two groups: those that interacted with the acoustic field and those that didn't. The latter group is undisturbed by the acoustic field, so it remains parallel and forms a point in a well-defined position in the focal plane. The optical stop is positioned exactly at that point, so as to prevent all corresponding rays from further propagating through the system and to the camera. Thus we get rid of the portion of light that crossed the acoustic field without interaction. However, there are also rays that did interact with the acoustic field in the following manner: If a ray travels through a region of nonuniform density whose spatial gradient has a component orthogonal to the ray, that ray is deflected from its original orientation, as if it were passing through a prism. This ray is no longer parallel, so it doesn't intersect the focal point of the focusing element and is not blocked by the knife. In some circumstances the deflected ray escapes the knife-blade and reaches the camera to create a point-like image on the camera-sensor, with a position and intensity related to the inhomogeneity experienced by the ray. An image is formed in this way, exclusively by rays that interacted with the acoustic field, providing a mapping of the acoustic field.

Physical optics description
The acousto-optic effect couples the optical refractive index of the medium with its density and pressure. Thus, spatial and temporal variations in pressure (e.g., due to ultrasound radiation) induces corresponding variations in refractive index. Optical wavelength and wavenumber in medium depend on refractive index. The phase acquired by electromagnetic wave traveling through the medium is related to the line-integral of the wavenumber along the propagation line. For a plane-wave electromagnetic radiation traveling parallel to the Z-axis, the XY planes are iso-phase manifolds (regions of constant phase; the phase does not depend on coordinates (x,y)). However, when the wave emerges from the acoustic field, XY planes are not iso-phase manifoldes anymore; the information about the accumulated pressure along each (x,y) line resides in the phase of the emerging radiation, forming a phase image (phasor) in the XY plane. The phase information is given by the Raman-Nath parameter:

  
    
      
        v
        (
        x
        ,
        y
        )
        =
        
          
            
              2
              ?
              ?
            
            ?
          
        
        ?
        
          p
          (
          x
          ,
          y
          ,
          z
          )
        
        
        d
        z
      
    
    {\displaystyle v(x,y)={\frac {2\pi \kappa }{\lambda }}\int {p(x,y,z)}\,dz}
  
with 
  
    
      
        ?
      
    
    {\displaystyle \kappa }
   - the piezooptic coefficient, 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   the optical wavelength and 
  
    
      
        p
        (
        x
        ,
        y
        ,
        z
        )
      
    
    {\displaystyle p(x,y,z)}
   the three-dimensional pressure field. The schlieren technique converts the phase information into an intensity image, detectable by a camera or a screen.

Application
The accepted gold-standard for quantitative acoustic measurement is the hydrophone. However, scanning the acoustic field with a hydrophone suffers from several limitations, giving rise to supplementary evaluation methods such as the schlieren imaging. The importance of the schlieren imaging technique is prominent in HIFU research and development.  Advantages of schlieren imaging include:
Free field: the investigated acoustic field is not distorted by the measuring probe.
High intensity measurements: the method is compatible with high acoustic intensities.
Real time: Schlieren imaging system provides on-line, live video of the acoustic field.

References
External links
A presentation of schlieren imaging on YouTube
Acoustic Field Characteriztation with Schlieren System - a short presentation",Category:Optics,1
14,15,Sound energy density,"Sound energy density or sound density is the sound energy per unit volume. The SI unit of sound energy density is the pascal (Pa), that is the joule per cubic metre (J/m3) in SI based units.

Mathematical definition
Sound energy density, denoted w, is defined by

  
    
      
        w
        =
        
          
            
              p
              v
            
            c
          
        
      
    
    {\displaystyle w={\frac {pv}{c}}}
  
where
p is the sound pressure;
v is the particle velocity in the direction of propagation;
c is the speed of sound.
The terms instantaneous energy density, maximum energy density, and peak energy density have meanings analogous to the related terms used for sound pressure. In speaking of average energy density, it is necessary to distinguish between the space average (at a given instant) and the time average (at a given point).

Sound energy density level
The sound energy density level gives the ratio of a sound incidence as a sound energy value in comparison to a reference level of 0 dB (DIN 45630). It is a logarithmic measure of the ratio of two sound energy densities.
The energy produced by vibrations is known as sound

  
    
      
        L
        (
        E
        )
        =
        10
        
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                E
                
                  1
                
              
              
                E
                
                  0
                
              
            
          
          )
        
        
          
            d
            B
          
        
      
    
    {\displaystyle L(E)=10\,\log _{10}\left({\frac {E_{1}}{E_{0}}}\right){\rm {dB}}}
  
where E1 and E0 are the energy densities. The unit of the sound energy density level is the decibel (dB).
If E0 is the standard reference sound energy density of

  
    
      
        
          E
          
            0
          
        
        =
        
          10
          
            ?
            12
          
        
        
          
            J
            
              m
              
                3
              
            
          
        
      
    
    {\displaystyle E_{0}=10^{-12}\mathrm {\frac {J}{m^{3}}} }

See also
Particle velocity level
Sound intensity level

References
External links
Conversion: sound intensity to sound intensity level
Ohm's law as acoustic equivalent - calculations
Relationships of acoustic quantities associated with a plane progressive acoustic sound wave - pdf",Category:Physical quantities,1
15,16,Directional sound,"See sound from ultrasound for a beam of ultrasound that makes audible sound in a restricted target area without a receiving set.
Directional Sound refers to the notion of using various devices to create fields of sound which spread less than most (small) traditional loudspeakers. Several techniques are available to accomplish this, and each has its benefits and drawbacks. Ultimately, choosing a directional sound device depends greatly on the environment in which it is deployed as well as the content that will be reproduced. Keeping these factors in mind will yield the best results through any evaluation of directional sound technologies.
Systems which guide evacuees during an emergency by the emission of pink noise to the exits are often also called ""directional sound"" systems.

Basic theory
In all wave-producing sources, the directivity of any source, at maximum, corresponds to the size of the source compared to the wavelengths it is generating: The larger the source is compared to the wavelength of the sound waves, the more directional beam results. The specific transduction method has no impact on the directivity of the resulting sound field; the analysis relies only on the aperture function of the source, per the Huygens–Fresnel principle.
The ultrasonic devices achieve high directivity by modulating audible sound onto high frequency ultrasound. The higher frequency sound waves have a shorter wavelength and thus don't spread out as rapidly. For this reason, the resulting directivity of these devices is far higher than physically possible with any loudspeaker system. However, they are reported to have limited low-frequency reproduction abilities. See sound from ultrasound for more information.

Speaker arrays
While a large loudspeaker is naturally more directional because of its large size, a source with equivalent directivity can be made by utilizing an array of traditional small loudspeakers, all driven together in-phase. Acoustically equal to a large speaker, this creates a larger source size compared to wavelength, and the resulting sound field is narrowed compared to a single small speaker. Large speaker arrays have been used in hundreds of arena sound systems to mitigate noise that would ordinarily travel to adjoining neighborhoods, as well as limited applications in other applications where some degree of directivity is helpful, such as museums or similar display applications that can tolerate large speaker dimensions.
Traditional speaker arrays can be fabricated in any shape or size, but a reduced physical dimension (relative to wavelength) will inherently sacrifice directivity in that dimension. The larger the speaker array, the more directional, and the smaller the size of the speaker array, the less directional it is. This is fundamental physics, and cannot be bypassed, even by using phased arrays or other signal processing methods. This is because the directivity pattern of any wave source is the Fourier Transform of the source function. Phased array design is, however, sometimes useful for beamsteering, or for sidelobe mitigation, but making these compromises necessarily reduces directivity.
Acoustically, speaker arrays are essentially the same as sound domes, which have also been available for decades; the size of the dome opening mimics the acoustic properties of a large speaker of the same diameter (or, equivalently, a large speaker array of the same diameter). Domes, however, tend to weigh much less than the weight of comparable speaker arrays (15 lbs vs. 37 lbs, per the manufacturer's websites), and are far less expensive.
Other types of large speaker panels, such as electrostatic loudspeakers, tend to be more directional than small speakers, for the same reasons as above; they are somewhat more directional only because they tend to be physically larger than most common loudspeakers. Correspondingly, an electrostatic loudspeaker the size of a small traditional speaker would be non-directional.
The directivity for various source sizes and shapes is given in. The directivity is shown to be a function only of the source size and shape, not of the specific type of transducer used.

See also
Loudspeaker
Parabolic loudspeaker
Sonic weaponry
Sound from ultrasound


== References ==",Category:Sound,1
16,17,Decibel,"The decibel (symbol: dB) is a logarithmic unit used to express the ratio of one value of a physical property to another, and may be used to express a change in value (e.g., +1 dB or -1 dB) or an absolute value. In the latter case, it expresses the ratio of a value to a reference value; when used in this way, the decibel symbol should be appended with a suffix that indicates the reference value or some other property. For example, if the reference value is 1 volt, then the suffix is ""V"" (i.e., ""20 dBV""), and if the reference value is one milliwatt, then the suffix is ""m"" (i.e., ""20 dBm""). However, sound pressure level is referenced to the ""threshold of hearing"" (generally given as 20 micropascals at 1 kHz), and the suffix is ""SPL"" (i.e., ""60 dB SPL"").
There are two different scales used when expressing a ratio in decibels depending on the nature of the quantities: field, power, and root-power. When expressing power quantities, the number of decibels is ten times the logarithm to base 10 of the ratio of two power quantities. That is, a change in power by a factor of 10 corresponds to a 10 dB change in level. When expressing field quantities, a change in amplitude by a factor of 10 corresponds to a 20 dB change in level. The extra factor of two is due to the logarithm of the quadratic relationship between power and amplitude. The decibel scales differ so that direct comparisons can be made between related power and field quantities when they are expressed in decibels.
The definition of the decibel is based on the measurement of power in telephony of the early 20th century in the Bell System in the United States. One decibel is one tenth (deci-) of one bel, named in honor of Alexander Graham Bell; however, the bel is seldom used. Today, the decibel is used for a wide variety of measurements in science and engineering, most prominently in acoustics, electronics, and control theory. In electronics, the gains of amplifiers, attenuation of signals, and signal-to-noise ratios are often expressed in decibels.
In the International System of Quantities, the decibel is defined as a unit of measurement for quantities of type level or level difference, which are defined as the logarithm of the ratio of power- or field-type quantities.

History
The decibel originates from methods used to quantify signal loss in telegraph and telephone circuits. The unit for loss was originally Miles of Standard Cable (MSC). 1 MSC corresponded to the loss of power over a 1 mile (approximately 1.6 km) length of standard telephone cable at a frequency of 5000 radians per second (795.8 Hz), and matched closely the smallest attenuation detectable to the average listener. The standard telephone cable implied was ""a cable having uniformly distributed resistance of 88 ohms per loop mile and uniformly distributed shunt capacitance of 0.054 microfarad per mile"" (approximately 19 gauge).
In 1924, Bell Telephone Laboratories received favorable response to a new unit definition among members of the International Advisory Committee on Long Distance Telephony in Europe and replaced the MSC with the Transmission Unit (TU). 1 TU was defined such that the number of TUs was ten times the base-10 logarithm of the ratio of measured power to a reference power level. The definition was conveniently chosen such that 1 TU approximated 1 MSC; specifically, 1 MSC was 1.056 TU. In 1928, the Bell system renamed the TU into the decibel, being one tenth of a newly defined unit for the base-10 logarithm of the power ratio. It was named the bel, in honor of the telecommunications pioneer Alexander Graham Bell. The bel is seldom used, as the decibel was the proposed working unit.
The naming and early definition of the decibel is described in the NBS Standard's Yearbook of 1931:

Since the earliest days of the telephone, the need for a unit in which to measure the transmission efficiency of telephone facilities has been recognized. The introduction of cable in 1896 afforded a stable basis for a convenient unit and the ""mile of standard"" cable came into general use shortly thereafter. This unit was employed up to 1923 when a new unit was adopted as being more suitable for modern telephone work. The new transmission unit is widely used among the foreign telephone organizations and recently it was termed the ""decibel"" at the suggestion of the International Advisory Committee on Long Distance Telephony.
The decibel may be defined by the statement that two amounts of power differ by 1 decibel when they are in the ratio of 100.1 and any two amounts of power differ by N decibels when they are in the ratio of 10N(0.1). The number of transmission units expressing the ratio of any two powers is therefore ten times the common logarithm of that ratio. This method of designating the gain or loss of power in telephone circuits permits direct addition or subtraction of the units expressing the efficiency of different parts of the circuit...

In April 2003, the International Committee for Weights and Measures (CIPM) considered a recommendation for the inclusion of the decibel in the International System of Units (SI), but decided against the proposal. However, the decibel is recognized by other international bodies such as the International Electrotechnical Commission (IEC) and International Organization for Standardization (ISO). The IEC permits the use of the decibel with field quantities as well as power and this recommendation is followed by many national standards bodies, such as NIST, which justifies the use of the decibel for voltage ratios. The term field quantity is deprecated by ISO 80000-1, which favors root-power. In spite of their widespread use, suffixes (such as in dBA or dBV) are not recognized by the IEC or ISO.

Definition
ISO 80000-3 describes definitions for quantities and units of space and time. The decibel for use in acoustics is defined in ISO 80000-8. The major difference from the article below is that for acoustics the decibel has no absolute value.
The ISO Standard 80000-3:2006 defines the following quantities. The decibel (dB) is one-tenth of a bel: 1 dB = 0.1 B. The bel (B) is ?1?2 ln(10) nepers: 1 B = ?1?2 ln(10) Np. The neper is the change in the level of a field quantity when the field quantity changes by a factor of e, that is 1 Np = ln(e) = 1, thereby relating all of the units as nondimensional natural log of field-quantity ratios, 1 dB = 0.11513… Np = 0.11513…. Finally, the level of a quantity is the logarithm of the ratio of the value of that quantity to a reference value of the same kind of quantity.
Therefore, the bel represents the logarithm of a ratio between two power quantities of 10:1, or the logarithm of a ratio between two field quantities of ?10:1.
Two signals whose levels differ by one decibel have a power ratio of 101/10, which is approximately 1.25893, and an amplitude (field quantity) ratio of 10?1?20 (1.12202).
The bel is rarely used either without a prefix or with SI unit prefixes other than deci; it is preferred, for example, to use hundredths of a decibel rather than millibels. Thus, five one-thousandths of a bel would normally be written '0.05 dB', and not '5 mB'.
The method of expressing a ratio as a level in decibels depends on whether the measured property is a power quantity or a field quantity; see Field, power, and root-power quantities for details.

Power quantities
When referring to measurements of power quantities, a ratio can be expressed as a level in decibels by evaluating ten times the base-10 logarithm of the ratio of the measured quantity to reference value. Thus, the ratio of P (measured power) to P0 (reference power) is represented by LP, that ratio expressed in decibels, which is calculated using the formula:

  
    
      
        
          L
          
            P
          
        
        =
        
          
            1
            2
          
        
        ln
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{P}={\frac {1}{2}}\ln \!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {Np} =10\log _{10}\!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {dB} .}
  
The base-10 logarithm of the ratio of the two power levels is the number of bels. The number of decibels is ten times the number of bels (equivalently, a decibel is one-tenth of a bel). P and P0 must measure the same type of quantity, and have the same units before calculating the ratio. If P = P0 in the above equation, then LP = 0. If P is greater than P0 then LP is positive; if P is less than P0 then LP is negative.
Rearranging the above equation gives the following formula for P in terms of P0 and LP:

  
    
      
        P
        =
        
          10
          
            
              
                L
                
                  P
                
              
              
                10
                
                
                  d
                  B
                
              
            
          
        
        
          P
          
            0
          
        
        .
      
    
    {\displaystyle P=10^{\frac {L_{P}}{10\,\mathrm {dB} }}P_{0}.}

Field quantities and root-power quantities
When referring to measurements of field quantities, it is usual to consider the ratio of the squares of F (measured field) and F0 (reference field). This is because in most applications power is proportional to the square of field, and it is desirable for the two decibel formulations to give the same result in such typical cases. Thus, the following definition is used:

  
    
      
        
          L
          
            F
          
        
        =
        ln
        
        
          (
          
            
              F
              
                F
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              
                F
                
                  2
                
              
              
                F
                
                  0
                
                
                  2
                
              
            
          
          )
        
        
         
        
          d
          B
        
        =
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            
              F
              
                F
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{F}=\ln \!\left({\frac {F}{F_{0}}}\right)\!~\mathrm {Np} =10\log _{10}\!\left({\frac {F^{2}}{F_{0}^{2}}}\right)\!~\mathrm {dB} =20\log _{10}\left({\frac {F}{F_{0}}}\right)\!~\mathrm {dB} .}
  
The formula may be rearranged to give

  
    
      
        F
        =
        
          10
          
            
              
                L
                
                  F
                
              
              
                20
                
                
                  d
                  B
                
              
            
          
        
        
          F
          
            0
          
        
        .
      
    
    {\displaystyle F=10^{\frac {L_{F}}{20\,\mathrm {dB} }}F_{0}.}
  
Similarly, in electrical circuits, dissipated power is typically proportional to the square of voltage or current when the impedance is held constant. Taking voltage as an example, this leads to the equation:

  
    
      
        
          G
          
            
              d
              B
            
          
        
        =
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              V
              
                V
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle G_{\mathrm {dB} }=20\log _{10}\!\left({\frac {V}{V_{0}}}\right)\!~\mathrm {dB} ,}
  
where V is the voltage being measured, V0 is a specified reference voltage, and GdB is the power gain expressed in decibels. A similar formula holds for current.
The term root-power quantity is introduced by ISO Standard 80000-1:2009 as a substitute of field quantity. The term field quantity is deprecated by that standard.

Conversions
Since logarithm differences measured in these units are used to represent power ratios and field ratios, the values of the ratios represented by each unit are also included in the table.

Examples
All of these examples yield dimensionless answers in dB because they are relative ratios expressed in decibels. The unit dBW is often used to denote a ratio for which the reference is 1 W, and similarly dBm for a 1 mW reference point.
Calculating the ratio of 1 kW (one kilowatt, or 1000 watts) to 1 W in decibels yields:

  
    
      
        
          G
          
            
              d
              B
            
          
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                1000
                 
                
                  W
                
              
              
                1
                 
                
                  W
                
              
            
          
          )
        
        =
        30.
      
    
    {\displaystyle G_{\mathrm {dB} }=10\log _{10}\left({\frac {1000~\mathrm {W} }{1~\mathrm {W} }}\right)=30.}
  

The ratio of ?1000 V ? 31.62 V to 1 V in decibels is

  
    
      
        
          G
          
            
              d
              B
            
          
        
        =
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                31.62
                 
                
                  V
                
              
              
                1
                 
                
                  V
                
              
            
          
          )
        
        =
        30.
      
    
    {\displaystyle G_{\mathrm {dB} }=20\log _{10}\left({\frac {31.62~\mathrm {V} }{1~\mathrm {V} }}\right)=30.}
  

(31.62 V/1 V)2 ? 1 kW/1 W, illustrating the consequence from the definitions above that GdB has the same value, 30, regardless of whether it is obtained from powers or from amplitudes, provided that in the specific system being considered power ratios are equal to amplitude ratios squared.
The ratio of 1 mW (one milliwatt) to 10 W in decibels is obtained with the formula

  
    
      
        
          G
          
            
              d
              B
            
          
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                0.001
                 
                
                  W
                
              
              
                10
                 
                
                  W
                
              
            
          
          )
        
        =
        ?
        40.
      
    
    {\displaystyle G_{\mathrm {dB} }=10\log _{10}\left({\frac {0.001~\mathrm {W} }{10~\mathrm {W} }}\right)=-40.}
  

The power ratio corresponding to a 3 dB change in level is given by

  
    
      
        G
        =
        
          10
          
            
              3
              10
            
          
        
        ×
        1
        =
        1.99526...
        ?
        2.
      
    
    {\displaystyle G=10^{\frac {3}{10}}\times 1=1.99526...\approx 2.}
  

A change in power ratio by a factor of 10 corresponds to a change in level of 10 dB. A change in power ratio by a factor of 2 or ?1?2 is approximately a change of 3 dB. More precisely, the change is ±3.0103 dB, but this is almost universally rounded to ""3 dB"" in technical writing. This implies an increase in voltage by a factor of ?2 ? 1.4142. Likewise, a doubling or halving of the voltage, and quadrupling or quartering of the power, is commonly described as ""6 dB"" rather than ±6.0206 dB.
Should it be necessary to make the distinction, the number of decibels is written with additional significant figures. 3.00 dB is a power ratio of 10?3?10, or 1.9953, about 0.24% different from exactly 2, and a voltage ratio of 1.4125, 0.12% different from exactly ?2. Similarly, an increase of 6.00 dB is the power ratio is 10?6?10 ? 3.9811, about 0.5% different from 4.

Properties
The decibel has the following properties:
The logarithmic scale nature of the decibel means that a very large range of ratios can be represented by a convenient number, in a similar manner to scientific notation. This allows one to clearly visualize huge changes of some quantity. See Bode plot and semi-log plot. For example, 120 dB SPL may be clearer than ""a trillion times more intense than the threshold of hearing"".
Level values in decibels can be added instead of multiplying the underlying power values, which means that the overall gain of a multi-component system, such as a series of amplifier stages, can be calculated by summing the gains in decibels of the individual components, rather than multiply the amplification factors; that is, log(A × B × C) = log(A) + log(B) + log(C). Practically, this means that, armed only with the knowledge that 1 dB is approximately 26% power gain, 3 dB is approximately 2× power gain, and 10 dB is 10× power gain, it is possible to determine the power ratio of a system from the gain in dB with only simple addition and multiplication. For example:
A system consists of 3 amplifiers in series, with gains (ratio of power out to in) of 10 dB, 8 dB, and 7 dB respectively, for a total gain of 25 dB. Broken into combinations of 10, 3, and 1 dB, this is:
25 dB = 10 dB + 10 dB + 3 dB + 1 dB + 1 dB

With an input of 1 watt, the output is approximately
1 W x 10 x 10 x 2 x 1.26 x 1.26 ? 317.5 W

Calculated exactly, the output is 1 W x 10?25?10 = 316.2 W. The approximate value has an error of only +0.4% with respect to the actual value which is negligible given the precision of the values supplied and the accuracy of most measurement instrumentation.

Advantages and disadvantages
Supporting arguments
According to Mitschke, ""The advantage of using a logarithmic measure is that in a transmission chain, there are many elements concatenated, and each has its own gain or attenuation. To obtain the total, addition of decibel values is much more convenient than multiplication of the individual factors.""
The human perception of the intensity of sound and light approximates the logarithm of intensity rather than a linear relationship (Weber–Fechner law), making the dB scale a useful measure.

Criticism
Various published articles have criticized the unit decibel as having shortcomings that hinder its understanding and use: According to its critics, the decibel creates confusion, obscures reasoning, is more related to the era of slide rules than to modern digital processing, and is cumbersome and difficult to interpret.
Representing the equivalent of zero watts is not possible, causing problems in conversions. Hickling concludes ""Decibels are a useless affectation, which is impeding the development of noise control as an engineering discipline"".
A common source of confusion in using the decibel occurs when deciding about the use of 10 × log or 20 × log. In the original definition, it was a power measurement, and as employed in that context, the formulation 10 × log should be used, as deci means one tenth. The user must be clear whether the quantity expressed is power or amplitude. It is useful to consider how power or energy is expressed, e.g., current × current × resistance, ?1?2 × velocity × velocity × mass. Where the power is a square function of a field variable (such as voltage, current, or pressure), then 10 × log is the correct expression for the square, or 20 × log for the field variable itself.
Quantities in decibels are not necessarily additive, thus being ""of unacceptable form for use in dimensional analysis"".
For the same reason that humans excel at additive operation over multiplication, decibels are awkward in inherently additive operations: ""if two machines each individually produce a [sound pressure] level of, say, 90 dB at a certain point, then when both are operating together we should expect the combined sound pressure level to increase to 93 dB, but certainly not to 180 dB!"" ""suppose that the noise from a machine is measured (including the contribution of background noise) and found to be 87 dBA but when the machine is switched off the background noise alone is measured as 83 dBA. ... the machine noise [level (alone)] may be obtained by 'subtracting' the 83 dBA background noise from the combined level of 87 dBA; i.e., 84.8 dBA."" ""in order to find a representative value of the sound level in a room a number of measurements are taken at different positions within the room, and an average value is calculated. (...) Compare the logarithmic and arithmetic averages of ... 70 dB and 90 dB: logarithmic average = 87 dB; arithmetic average = 80 dB.""

Uses
Acoustics
The decibel is commonly used in acoustics as a unit of sound pressure level. The reference pressure for sound in air is set at the typical threshold of perception of an average human and there are common comparisons used to illustrate different levels of sound pressure. Sound pressure is a field quantity, therefore the field version of the unit definition is used:

  
    
      
        
          L
          
            p
          
        
        =
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              
                p
                
                  
                    r
                    m
                    s
                  
                
              
              
                p
                
                  
                    r
                    e
                    f
                  
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{p}=20\log _{10}\!\left({\frac {p_{\mathrm {rms} }}{p_{\mathrm {ref} }}}\right)\!~\mathrm {dB} ,}
  
where prms is the root mean square of the measured sound pressure in pascals and pref is the standard reference sound pressure of 20 micropascals in air and 1 micropascal in water.
The human ear has a large dynamic range in sound reception. The ratio of the sound intensity that causes permanent damage during short exposure to that of the quietest sound that the ear can hear is greater than or equal to 1 trillion (1012). Such large measurement ranges are conveniently expressed in logarithmic scale: the base-10 logarithm of 1012 is 12, which is expressed as a sound pressure level of 120 dB re 20 ?Pa.
Since the human ear is not equally sensitive to all sound frequencies, noise levels at maximum human sensitivity, somewhere between 2 and 4 kHz, are factored more heavily into some measurements using frequency weighting. (See also Stevens' power law.)

Electronics
In electronics, the decibel is often used to express power or amplitude ratios (gains), in preference to arithmetic ratios or percentages. One advantage is that the total decibel gain of a series of components (such as amplifiers and attenuators) can be calculated simply by summing the decibel gains of the individual components. Similarly, in telecommunications, decibels denote signal gain or loss from a transmitter to a receiver through some medium (free space, waveguide, coaxial cable, fiber optics, etc.) using a link budget.
The decibel unit can also be combined with a suffix to create an absolute unit of electric power. For example, it can be combined with ""m"" for ""milliwatt"" to produce the ""dBm"". Zero dBm is the level corresponding to one milliwatt, and 1 dBm is one decibel greater (about 1.259 mW).
In professional audio specifications, a popular unit is the dBu. This is relative to the root mean square voltage which delivers 1 mW (0 dBm) into a 600-ohm resistor, or ?1 mW×600 ? ? 0.775 VRMS. When used in a 600-ohm circuit (historically, the standard reference impedance in telephone circuits), dBu and dBm are identical.

Optics
In an optical link, if a known amount of optical power, in dBm (referenced to 1 mW), is launched into a fiber, and the losses, in dB (decibels), of each component (e.g., connectors, splices, and lengths of fiber) are known, the overall link loss may be quickly calculated by addition and subtraction of decibel quantities.
In spectrometry and optics, the blocking unit used to measure optical density is equivalent to ?1 B.

Video and digital imaging
In connection with video and digital image sensors, decibels generally represent ratios of video voltages or digitized light levels, using 20 log of the ratio, even when the represented optical power is directly proportional to the voltage or level, not to its square, as in a CCD imager where response voltage is linear in intensity. Thus, a camera signal-to-noise ratio or dynamic range of 40 dB represents a power ratio of 100:1 between signal power and noise power, not 10,000:1. Sometimes the 20 log ratio definition is applied to electron counts or photon counts directly, which are proportional to intensity without the need to consider whether the voltage response is linear.
However, as mentioned above, the 10 log intensity convention prevails more generally in physical optics, including fiber optics, so the terminology can become murky between the conventions of digital photographic technology and physics. Most commonly, quantities called ""dynamic range"" or ""signal-to-noise"" (of the camera) would be specified in 20 log dB, but in related contexts (e.g. attenuation, gain, intensifier SNR, or rejection ratio) the term should be interpreted cautiously, as confusion of the two units can result in very large misunderstandings of the value.
Photographers typically use an alternative base-2 log unit, the stop, to describe light intensity ratios or dynamic range.

Suffixes and reference values
Suffixes are commonly attached to the basic dB unit in order to indicate the reference value by which the ratio is calculated. For example, dBm indicates power measurement relative to 1 milliwatt.
In cases where the unit value of the reference is stated, the decibel value is known as ""absolute"". If the unit value of the reference is not explicitly stated, as in the dB gain of an amplifier, then the decibel value is considered relative.
The SI does not permit attaching qualifiers to units, whether as suffix or prefix, other than standard SI prefixes. Therefore, even though the decibel is accepted for use alongside SI units, the practice of attaching a suffix to the basic dB unit, forming compound units such as dBm, dBu, dBA, etc., is not. The proper way, according to the IEC 60027-3, is either as Lx (re xref) or as Lx/xref, where x is the quantity symbol and xref is the value of the reference quantity, e.g., LE (re 1 ?V/m) = LE/(1 ?V/m) for the electric field strength E relative to 1 ?V/m reference value.
Outside of documents adhering to SI units, the practice is very common as illustrated by the following examples. There is no general rule, with various discipline-specific practices. Sometimes the suffix is a unit symbol (""W"",""K"",""m""), sometimes it is a transliteration of a unit symbol (""uV"" instead of ?V for microvolt), sometimes it is an acronym for the unit's name (""sm"" for square meter, ""m"" for milliwatt), other times it is a mnemonic for the type of quantity being calculated (""i"" for antenna gain with respect to an isotropic antenna, ""?"" for anything normalized by the EM wavelength), or otherwise a general attribute or identifier about the nature of the quantity (""A"" for A-weighted sound pressure level). The suffix is often connected with a dash (dB-Hz), with a space (dB HL), with no intervening character (dBm), or enclosed in parentheses, dB(sm).

Voltage
Since the decibel is defined with respect to power, not amplitude, conversions of voltage ratios to decibels must square the amplitude, or use the factor of 20 instead of 10, as discussed above.

dBV
dB(VRMS) – voltage relative to 1 volt, regardless of impedance.
dBu or dBv
RMS voltage relative to 
  
    
      
        
          
            0.6
          
        
        
        
          V
        
        
        ?
        0.7746
        
        
          V
        
        
        ?
        ?
        2.218
        
        
          d
          B
          V
        
      
    
    {\displaystyle {\sqrt {0.6}}\,\mathrm {V} \,\approx 0.7746\,\mathrm {V} \,\approx -2.218\,\mathrm {dBV} }
  . Originally dBv, it was changed to dBu to avoid confusion with dBV. The ""v"" comes from ""volt"", while ""u"" comes from the volume unit used in the VU meter. dBu can be used as a measure of voltage, regardless of impedance, but is derived from a 600 ? load dissipating 0 dBm (1 mW). The reference voltage comes from the computation 
  
    
      
        V
        =
        
          
            600
            
            ?
            ?
            0.001
            
            
              W
            
          
        
      
    
    {\displaystyle V={\sqrt {600\,\Omega \cdot 0.001\,\mathrm {W} }}}
  .

In professional audio, equipment may be calibrated to indicate a ""0"" on the VU meters some finite time after a signal has been applied at an amplitude of +4 dBu. Consumer equipment typically uses a lower ""nominal"" signal level of -10 dBV. Therefore, many devices offer dual voltage operation (with different gain or ""trim"" settings) for interoperability reasons. A switch or adjustment that covers at least the range between +4 dBu and -10 dBV is common in professional equipment.
dBmV
dB(mVRMS) – voltage relative to 1 millivolt across 75 ?. Widely used in cable television networks, where the nominal strength of a single TV signal at the receiver terminals is about 0 dBmV. Cable TV uses 75 ? coaxial cable, so 0 dBmV corresponds to ?78.75 dBW (?48.75 dBm) or approx. 13 nW.
dB?V or dBuV
dB(?VRMS) – voltage relative to 1 microvolt. Widely used in television and aerial amplifier specifications. 60 dB?V = 0 dBmV.

Acoustics
Probably the most common usage of ""decibels"" in reference to sound level is dB SPL, sound pressure level referenced to the nominal threshold of human hearing: The measures of pressure (a field quantity) use the factor of 20, and the measures of power (e.g. dB SIL and dB SWL) use the factor of 10.
dB SPL
dB SPL (sound pressure level) – for sound in air and other gases, relative to 20 micropascals (?Pa) = 2×10?5 Pa, approximately the quietest sound a human can hear. For sound in water and other liquids, a reference pressure of 1 ?Pa is used.
An RMS sound pressure of one pascal corresponds to a level of 94 dB SPL.
dB SIL
dB sound intensity level – relative to 10?12 W/m2, which is roughly the threshold of human hearing in air.
dB SWL
dB sound power level – relative to 10?12 W.
dBA, dBB, and dBC
These symbols are often used to denote the use of different weighting filters, used to approximate the human ear's response to sound, although the measurement is still in dB (SPL). These measurements usually refer to noise and its effects on humans and other animals, and they are widely used in industry whi",Category:Articles with unsourced statements from March 2008,1
17,18,Acoustic enhancement,"Acoustic enhancement is a subtle type of sound reinforcement system used to augment direct, reflected, or reverberant sound. While sound reinforcement systems are usually used to increase the sound level of the sound source (like a person speaking into a microphone, or musical instruments in a pop ensemble), acoustic enhancement systems are typically used to increase the acoustic energy in the venue. These systems are often associated with acoustic sound sources like a chamber orchestra, symphony orchestra, or opera, but have also found acceptance in a variety of applications and venues that include rehearsal rooms, recording facilities conference rooms, sound stages, sports arenas, and outdoor venues.

Design and application
Acoustic enhancement systems use microphones, amplifiers, and loudspeakers interconnected with some form of processing. The number, type, and placement of microphones and loudspeakers varies according to both the application, as well as the physics limitations that are imposed by the inherent operating principles associated with each manufacturer's equipment. In most instances, however, these systems employ at least one array of loudspeakers that are distributed throughout the venue.
As concertgoers have become aware of the use of these systems, debates have arisen, because ""...purists maintain that the natural acoustic sound of [Classical] voices [or] instruments in a given hall should not be altered."" When employed properly, however, acoustic enhancement can improve listening quality in ways that would be impossible for architectural treatments to accomplish, and deliver sound quality that the concertgoer desires to experience.
At the Vienna Festival in May, 1995, a LARES system was used outdoors to augment the Vienna Philharmonic's performance of Beethoven's Symphony No. 9 conducted by Zubin Mehta. ""This was the first time on this location with classical music that we were not criticised for spoiling the music by amplifying it. ""Alfred Toegel, Sound Department, Vienna Festival. Commenting on a performance by the Grant Park Orchestra at the Jay Pritzker Pavilion at Millennium Park Chicago IL, Senior V.P. of WFMT Radio Steve Robinson stated ""I have never in my life heard sound projected so faithfully and beautifully over such a great distance; it was an ethereal experience""
Kai Harada's article Opera's Dirty Little Secret states that opera houses have begun using electronic acoustic enhancement systems ""...to compensate for flaws in a venue's acoustical architecture."" Despite the uproar that has arisen amongst operagoers, Harada points out that none of the major opera houses using acoustic enhancement systems ""...use traditional, Broadway-style sound reinforcement, in which most if not all singers are equipped with radio microphones mixed to a series of unsightly loudspeakers scattered throughout the theatre.""
Instead, most opera houses use the sound reinforcement system for subtle boosting of offstage voices, onstage dialogue, and sound effects (e.g., church bells in Tosca or thunder in Wagnerian operas). Acoustic Enhancement systems are most often employed in traditional opera houses to improve the sound of the orchestra, and have little if any effect on the sound of the voices. In a review of the State Opera of South Australia's performance of Wagners' Ring cycle at the Adelaide Festival Center Theatre, Michael Kennedy of The Sunday Telegraph, London, wrote: “The balance between the orchestra and the voices has been ideal.” The live recording of ""Wagner: Die Walküre"", the world's first 6 channel SACD ""blitzed the 2005 Helpmann Awards, winning ten of its eleven nominations and earning critical accolades."" - and the recording of ""Wagner: Götterdämmerung"" was nominated for a 2008 Grammy award. ""

Types
There exist different types of acoustic enhancement systems: In-line and feedback systems with or without electronic reverberators.
In-line systems are also called non-regenerative (i.e. no feedback). Feedback systems are also called regenerative. Electronic reverberators can be added in addition using various methods such as convolution and FIR filtering.
In-line systems with electronic reverberators: In-line acoustic enhancement systems include E-coustic LARES (Lexicon Acoustic Reinforcement and Enhancement System), SIAP, the System for Improved Acoustic Performance and ACS, Acoustic Control Systems. These systems use microphones, digital signal processing ""with delay, phase, and frequency-response changes,"" and then send the signal ""... to a large number of loudspeakers placed in extremities of the performance venue."" The Deutsche Staatsoper in Berlin and the Hummingbird Centre in Toronto use a LARES system. The Ahmanson Theatre in Los Angeles, the Royal National Theatre in London, and the Vivian Beaumont Theater in New York City use the SIAP system.   
Feedback systems with electronic reverberators: Feedback acoustic enhancement systems include Meyer Constellation (formerly VRAS) (Variable Room Acoustics System) which uses ""...different algorithms based on microphones placed around the room."" Yamaha's AFC3 Active Field Control system which ""enhances the architectural acoustic characteristics of a room and optimizes reverberation time performance."" Also XLNT's MCR system (Multiple Channel Reverberation).   
Feedback systems without electronic reverberators: CARMEN developed by CSTB comprises a number of electro acoustic active cells (approximately from 16 to 40), each of them being composed of a microphone, an electronic filtering unit, a power amplifier and a loudspeaker. Placed around the walls and ceiling of the auditorium, the cells form virtual walls depending on the architecture and the acoustic problem to solve. They only communicate between each other by the acoustic way.  Another newer variant for smaller halls is CARMENCITA. 


== References ==",Category:Acoustics,1
18,19,Auditory event,"Auditory events describe the subjective perception, when listening to a certain sound situation. This term was introduced by Jens Blauert (Ruhr-University Bochum) in 1966, in order to distinguish clearly between the physical sound field and the auditory perception of the sound. 
Auditory events are the central objects of psychoacoustical investigations. Focus of these investigations is the relationship between the characteristics of a physical sound field and the corresponding perception of listeners. From this relationship conclusions can be drawn about the processing methods of the human auditory system.
Aspects of auditory event investigations can be:
is there an auditory event?
Is a certain sound noticeable?
=> Determination of perception thresholds like hearing threshold, auditory masking thresholds etc.
Which characteristics has the auditory event?
=> Determination of loudness, pitch, sound, harshness etc.
How is the spatial impression of the auditory event?
=> Determination of sound localization, lateralization, perceived direction etc.
When can differences in auditory events be noticed?
How big are the discrimination possibilities of the auditory system?
=> Determination of just noticeable differences

Relationships between sound field and auditory events
The sound field is described by physical quantities, while auditory events are described by quantities of psychoacoustical perception. Below you can find a list with physical sound field quantities and the related psychoacoustical quantities of corresponding auditory events. Mostly there is no simple or proportional relationship between sound field characteristics and auditory events. For example the auditory event property loudness depends not only on the physical quantity sound pressure but also on the spectral characteristics of the sound and on the sound history.


== References ==",Category:Acoustics,1
19,20,Sound pressure,"Sound pressure or acoustic pressure is the local pressure deviation from the ambient (average or equilibrium) atmospheric pressure, caused by a sound wave. In air, sound pressure can be measured using a microphone, and in water with a hydrophone. The SI unit of sound pressure is the pascal (Pa).

Mathematical definition
A sound wave in a transmission medium causes a deviation (sound pressure, a dynamic pressure) in the local ambient pressure, a static pressure.
Sound pressure, denoted p, is defined by

  
    
      
        
          p
          
            
              t
              o
              t
              a
              l
            
          
        
        =
        
          p
          
            
              s
              t
              a
              t
            
          
        
        +
        p
        ,
      
    
    {\displaystyle p_{\mathrm {total} }=p_{\mathrm {stat} }+p,}
  
where
ptotal is the total pressure;
pstat is the static pressure.

Sound measurements
Sound intensity
In a sound wave, the complementary variable to sound pressure is the particle velocity. Together, they determine the sound intensity of the wave.Sound intensity, denoted I and measured in W·m?2 in SI units, is defined by

  
    
      
        
          I
        
        =
        p
        
          v
        
        ,
      
    
    {\displaystyle \mathbf {I} =p\mathbf {v} ,}
  
where
p is the sound pressure;
v is the particle velocity.

Acoustic impedance
Acoustic impedance, denoted Z and measured in Pa·m?3·s in SI units, is defined by

  
    
      
        Z
        (
        s
        )
        =
        
          
            
              
                
                  
                    p
                    ^
                  
                
              
              (
              s
              )
            
            
              
                
                  
                    Q
                    ^
                  
                
              
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle Z(s)={\frac {{\hat {p}}(s)}{{\hat {Q}}(s)}},}
  
where

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {p}}(s)}
   is the Laplace transform of sound pressure;

  
    
      
        
          
            
              Q
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {Q}}(s)}
   is the Laplace transform of sound volume flow rate.
Specific acoustic impedance, denoted z and measured in Pa·m?1·s in SI units, is defined by

  
    
      
        z
        (
        s
        )
        =
        
          
            
              
                
                  
                    p
                    ^
                  
                
              
              (
              s
              )
            
            
              
                
                  
                    v
                    ^
                  
                
              
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle z(s)={\frac {{\hat {p}}(s)}{{\hat {v}}(s)}},}
  
where

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {p}}(s)}
   is the Laplace transform of sound pressure;

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {v}}(s)}
   is the Laplace transform of particle velocity.

Particle displacement
The particle displacement of a progressive sine wave is given by

  
    
      
        ?
        (
        
          r
        
        ,
        
        t
        )
        =
        
          ?
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            ?
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle \delta (\mathbf {r} ,\,t)=\delta _{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}),}
  
where

  
    
      
        
          ?
          
            
              m
            
          
        
      
    
    {\displaystyle \delta _{\mathrm {m} }}
   is the amplitude of the particle displacement;

  
    
      
        
          ?
          
            ?
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{\delta ,0}}
   is the phase shift of the particle displacement;
k is the angular wavevector;
? is the angular frequency.
It follows that the particle velocity and the sound pressure along the direction of propagation of the sound wave x are given by

  
    
      
        v
        (
        
          r
        
        ,
        
        t
        )
        =
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        
          v
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            v
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle v(\mathbf {r} ,\,t)={\frac {\partial \delta }{\partial t}}(\mathbf {r} ,\,t)=\omega \delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=v_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{v,0}),}
  

  
    
      
        p
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        
          c
          
            2
          
        
        
          
            
              ?
              ?
            
            
              ?
              x
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          c
          
            2
          
        
        
          k
          
            x
          
        
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        
          p
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            p
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle p(\mathbf {r} ,\,t)=-\rho c^{2}{\frac {\partial \delta }{\partial x}}(\mathbf {r} ,\,t)=\rho c^{2}k_{x}\delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=p_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{p,0}),}
  
where
vm is the amplitude of the particle velocity;

  
    
      
        
          ?
          
            v
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}}
   is the phase shift of the particle velocity;
pm is the amplitude of the acoustic pressure;

  
    
      
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{p,0}}
   is the phase shift of the acoustic pressure.
Taking the Laplace transforms of v and p with respect to time yields

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          v
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle {\hat {v}}(\mathbf {r} ,\,s)=v_{\mathrm {m} }{\frac {s\cos \varphi _{v,0}-\omega \sin \varphi _{v,0}}{s^{2}+\omega ^{2}}},}
  

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          p
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\hat {p}}(\mathbf {r} ,\,s)=p_{\mathrm {m} }{\frac {s\cos \varphi _{p,0}-\omega \sin \varphi _{p,0}}{s^{2}+\omega ^{2}}}.}
  
Since 
  
    
      
        
          ?
          
            v
            ,
            0
          
        
        =
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}=\varphi _{p,0}}
  , the amplitude of the specific acoustic impedance is given by

  
    
      
        
          z
          
            
              m
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          |
        
        z
        (
        
          r
        
        ,
        
        s
        )
        
          |
        
        =
        
          |
          
            
              
                
                  
                    
                      p
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
              
                
                  
                    
                      v
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
            
          
          |
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              v
              
                
                  m
                
              
            
          
        
        =
        
          
            
              ?
              
                c
                
                  2
                
              
              
                k
                
                  x
                
              
            
            ?
          
        
        .
      
    
    {\displaystyle z_{\mathrm {m} }(\mathbf {r} ,\,s)=|z(\mathbf {r} ,\,s)|=\left|{\frac {{\hat {p}}(\mathbf {r} ,\,s)}{{\hat {v}}(\mathbf {r} ,\,s)}}\right|={\frac {p_{\mathrm {m} }}{v_{\mathrm {m} }}}={\frac {\rho c^{2}k_{x}}{\omega }}.}
  
Consequently, the amplitude of the particle displacement is related to that of the acoustic velocity and the sound pressure by

  
    
      
        
          ?
          
            
              m
            
          
        
        =
        
          
            
              v
              
                
                  m
                
              
            
            ?
          
        
        ,
      
    
    {\displaystyle \delta _{\mathrm {m} }={\frac {v_{\mathrm {m} }}{\omega }},}
  

  
    
      
        
          ?
          
            
              m
            
          
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              ?
              
                z
                
                  
                    m
                  
                
              
              (
              
                r
              
              ,
              
              s
              )
            
          
        
        .
      
    
    {\displaystyle \delta _{\mathrm {m} }={\frac {p_{\mathrm {m} }}{\omega z_{\mathrm {m} }(\mathbf {r} ,\,s)}}.}

Inverse-proportional law
When measuring the sound pressure created by an object, it is important to measure the distance from the object as well, since the sound pressure of a spherical sound wave decreases as 1/r from the centre of the sphere (and not as 1/r2, like the sound intensity):

  
    
      
        p
        (
        r
        )
        ?
        
          
            1
            r
          
        
        .
      
    
    {\displaystyle p(r)\propto {\frac {1}{r}}.}
  
This relationship is an inverse-proportional law.
If the sound pressure p1 is measured at a distance r1 from the centre of the sphere, the sound pressure p2 at another position r2 can be calculated:

  
    
      
        
          p
          
            2
          
        
        =
        
          
            
              r
              
                1
              
            
            
              r
              
                2
              
            
          
        
        
        
          p
          
            1
          
        
        .
      
    
    {\displaystyle p_{2}={\frac {r_{1}}{r_{2}}}\,p_{1}.}
  
The inverse-proportional law for sound pressure comes from the inverse-square law for sound intensity:

  
    
      
        I
        (
        r
        )
        ?
        
          
            1
            
              r
              
                2
              
            
          
        
        .
      
    
    {\displaystyle I(r)\propto {\frac {1}{r^{2}}}.}
  
Indeed,

  
    
      
        I
        (
        r
        )
        =
        p
        (
        r
        )
        v
        (
        r
        )
        =
        p
        (
        r
        )
        [
        p
        ?
        
          z
          
            ?
            1
          
        
        ]
        (
        r
        )
        ?
        
          p
          
            2
          
        
        (
        r
        )
        ,
      
    
    {\displaystyle I(r)=p(r)v(r)=p(r)[p*z^{-1}](r)\propto p^{2}(r),}
  
where

  
    
      
        ?
      
    
    {\displaystyle *}
   is the convolution operator;
z ?1 is the convolution inverse of the specific acoustic impedance,
hence the inverse-proportional law:

  
    
      
        p
        (
        r
        )
        ?
        
          
            1
            r
          
        
        .
      
    
    {\displaystyle p(r)\propto {\frac {1}{r}}.}
  
The sound pressure may vary in direction from the centre of the sphere as well, so measurements at different angles may be necessary, depending on the situation. An obvious example of a sound source whose spherical sound wave varies in level in different directions is a bullhorn.

Sound pressure level
Sound pressure level (SPL) or acoustic pressure level is a logarithmic measure of the effective pressure of a sound relative to a reference value.
Sound pressure level, denoted Lp and measured in dB, is defined by

  
    
      
        
          L
          
            p
          
        
        =
        ln
        
        
          (
          
            
              p
              
                p
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        2
        
          log
          
            10
          
        
        
        
          (
          
            
              p
              
                p
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              p
              
                p
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{p}=\ln \!\left({\frac {p}{p_{0}}}\right)\!~\mathrm {Np} =2\log _{10}\!\left({\frac {p}{p_{0}}}\right)\!~\mathrm {B} =20\log _{10}\!\left({\frac {p}{p_{0}}}\right)\!~\mathrm {dB} ,}
  
where
p is the root mean square sound pressure;
p0 is the reference sound pressure;
1 Np is the neper;
1 B = (1/2 ln 10) Np is the bel;
1 dB = (1/20 ln 10) Np is the decibel.
The commonly used reference sound pressure in air is

  
    
      
        
          p
          
            0
          
        
        =
        20
         
        
          ?
          P
          a
        
        ,
      
    
    {\displaystyle p_{0}=20~\mathrm {\mu Pa} ,}
  
which is often considered as the threshold of human hearing (roughly the sound of a mosquito flying 3 m away). The proper notations for sound pressure level using this reference are Lp/(20 ?Pa) or Lp (re 20 ?Pa), but the suffix notations dB SPL, dB(SPL), dBSPL, or dBSPL are very common, even if they are not accepted by the SI.
Most sound level measurements will be made relative to this reference, meaning 1 Pa will equal an SPL of 94 dB. In other media, such as underwater, a reference level of 1 ?Pa is used. These references are defined in ANSI S1.1-1994.

Examples
The lower limit of audibility is defined as SPL of 0 dB, but the upper limit is not as clearly defined. While 1 atm (194 dB Peak or 191 dB SPL) is the largest pressure variation an undistorted sound wave can have in Earth's atmosphere, larger sound waves can be present in other atmospheres or other media such as under water, or through the Earth.

Ears detect changes in sound pressure. Human hearing does not have a flat spectral sensitivity (frequency response) relative to frequency versus amplitude. Humans do not perceive low- and high-frequency sounds as well as they perceive sounds between 3,000 and 4,000 Hz, as shown in the equal-loudness contour. Because the frequency response of human hearing changes with amplitude, three weightings have been established for measuring sound pressure: A, B and C. A-weighting applies to sound pressures levels up to 55 dB, B-weighting applies to sound pressures levels between 55 dB and 85 dB, and C-weighting is for measuring sound pressure levels above 85 dB.
In order to distinguish the different sound measures a suffix is used: A-weighted sound pressure level is written either as dBA or LA. B-weighted sound pressure level is written either as dBB or LB, and C-weighted sound pressure level is written either as dBC or LC. Unweighted sound pressure level is called ""linear sound pressure level"" and is often written as dBL or just L. Some sound measuring instruments use the letter ""Z"" as an indication of linear SPL.

Distance
The distance of the measuring microphone from a sound source is often omitted when SPL measurements are quoted, making the data useless. In the case of ambient environmental measurements of ""background"" noise, distance need not be quoted as no single source is present, but when measuring the noise level of a specific piece of equipment the distance should always be stated. A distance of one metre (1 m) from the source is a frequently used standard distance. Because of the effects of reflected noise within a closed room, the use of an anechoic chamber allows for sound to be comparable to measurements made in a free field environment.
According to the inverse proportional law, when sound level Lp1 is measured at a distance r1, the sound level Lp2 at the distance r2 is

  
    
      
        
          L
          
            
              p
              
                2
              
            
          
        
        =
        
          L
          
            
              p
              
                1
              
            
          
        
        +
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              
                r
                
                  1
                
              
              
                r
                
                  2
                
              
            
          
          )
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{p_{2}}=L_{p_{1}}+20\log _{10}\!\left({\frac {r_{1}}{r_{2}}}\right)\!~\mathrm {dB} .}

Multiple sources
The formula for the sum of the sound pressure levels of n incoherent radiating sources is

  
    
      
        
          L
          
            ?
          
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              
                
                  
                    
                      p
                      
                        1
                      
                    
                  
                  
                    2
                  
                
                +
                
                  
                    
                      p
                      
                        2
                      
                    
                  
                  
                    2
                  
                
                +
                …
                +
                
                  
                    
                      p
                      
                        n
                      
                    
                  
                  
                    2
                  
                
              
              
                
                  
                    p
                    
                      0
                    
                  
                
                
                  2
                
              
            
          
          )
        
        
         
        
          d
          B
        
        =
        10
        
          log
          
            10
          
        
        
        
          [
          
            
              
                (
                
                  
                    
                      p
                      
                        1
                      
                    
                    
                      p
                      
                        0
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    
                      p
                      
                        2
                      
                    
                    
                      p
                      
                        0
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            …
            +
            
              
                (
                
                  
                    
                      p
                      
                        n
                      
                    
                    
                      p
                      
                        0
                      
                    
                  
                
                )
              
              
                2
              
            
          
          ]
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{\Sigma }=10\log _{10}\!\left({\frac {{p_{1}}^{2}+{p_{2}}^{2}+\ldots +{p_{n}}^{2}}{{p_{0}}^{2}}}\right)\!~\mathrm {dB} =10\log _{10}\!\left[\left({\frac {p_{1}}{p_{0}}}\right)^{2}+\left({\frac {p_{2}}{p_{0}}}\right)^{2}+\ldots +\left({\frac {p_{n}}{p_{0}}}\right)^{2}\right]\!~\mathrm {dB} .}
  
Inserting the formulas

  
    
      
        
          
            (
            
              
                
                  p
                  
                    i
                  
                
                
                  p
                  
                    0
                  
                
              
            
            )
          
          
            2
          
        
        =
        
          10
          
            
              
                L
                
                  i
                
              
              
                10
                
                
                  d
                  B
                
              
            
          
        
        ,
        
        i
        =
        1
        ,
        
        2
        ,
        
        …
        ,
        
        n
        ,
      
    
    {\displaystyle \left({\frac {p_{i}}{p_{0}}}\right)^{2}=10^{\frac {L_{i}}{10\,\mathrm {dB} }},\quad i=1,\,2,\,\ldots ,\,n,}
  
in the formula for the sum of the sound pressure levels yields

  
    
      
        
          L
          
            ?
          
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              10
              
                
                  
                    L
                    
                      1
                    
                  
                  
                    10
                    
                    
                      d
                      B
                    
                  
                
              
            
            +
            
              10
              
                
                  
                    L
                    
                      2
                    
                  
                  
                    10
                    
                    
                      d
                      B
                    
                  
                
              
            
            +
            …
            +
            
              10
              
                
                  
                    L
                    
                      n
                    
                  
                  
                    10
                    
                    
                      d
                      B
                    
                  
                
              
            
          
          )
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{\Sigma }=10\log _{10}\!\left(10^{\frac {L_{1}}{10\,\mathrm {dB} }}+10^{\frac {L_{2}}{10\,\mathrm {dB} }}+\ldots +10^{\frac {L_{n}}{10\,\mathrm {dB} }}\right)\!~\mathrm {dB} .}

Examples of sound pressure
*All values listed are the effective sound pressure unless otherwise stated.

See also
Acoustics
Phon (unit)
Loudness
Sone (unit)
Sound level meter
Stevens' power law
Weber–Fechner law, especially The case of sound

References
General
Beranek, Leo L., Acoustics (1993), Acoustical Society of America, ISBN 0-88318-494-X.
Daniel R. Raichel, The Science and Applications of Acoustics (2006), Springer New York, ISBN 1441920803.

External links
Sound Pressure and Sound Power, Effect and Cause
Conversion of Sound Pressure to Sound Pressure Level and Vice Versa
Table of Sound Levels, Corresponding Sound Pressure and Sound Intensity
Ohm's Law as Acoustic Equivalent, Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
Sound Pressure and Sound Power, Two Commonly Confused Characteristics of Sound
How Many Decibels Is Twice as Loud? Sound Level Change and the Respective Factor of Sound Pressure or Sound Intensity
Decibel (Loudness) Comparison Chart",Category:Physical quantities,1
20,21,Undertone series,"In music, the undertone series or subharmonic series is a sequence of notes that results from inverting the intervals of the overtone series. While overtones naturally occur with the physical production of music on instruments, undertones must be produced in unusual ways. The overtone series being a harmonic series, the undertone series is based on arithmetic division.
The hybrid term subharmonic is used in music and dynamics in a few different ways. In its pure sense, the term subharmonic refers strictly to any member of the subharmonic series (1/1, 1/2, 1/3, 1/4, etc.). When the subharmonic series is used to refer to frequency relationships, it is written with f representing some highest known reference frequency (f/1, f/2, f/3, f/4, etc.). The complex tones of acoustic instruments do not produce partials that resemble the subharmonic series. However, such tones can be produced artificially with audio software and electronics. Subharmonics can be contrasted with harmonics. While harmonics can ""...occur in any nonlinear system"", there are ""...only fairly restricted conditions"" that will lead to the ""nonlinear phenomenon known as subharmonic generation"". One way to define subharmonics is that they are ""...integral submultiples of the fundamental (driving) frequency"".
In a second sense, subharmonic does not relate to the subharmonic series, but instead describes an instrumental technique for lowering the pitch of an acoustic instrument below what would be expected for the resonant frequency of that instrument, such as a violin string that is driven and damped by increased bow pressure to produce a fundamental frequency lower than the normal pitch of the same open string. The human voice can also be forced into a similar driven resonance, also called “undertone singing” (which similarly has nothing to do with the undertone series), to extend the range of the voice below what is normally available. However, the frequency relationships of the component partials of the tone produced by the acoustic instrument or voice played in such a way still resemble the harmonic series, not the subharmonic series. In this sense, subharmonic is a term created by reflection from the second sense of the term harmonic, which in that sense refers to an instrumental technique for making an instrument's pitch seem higher than normal by eliminating some lower partials by damping the resonator at the antinodes of vibration of those partials (such as placing a finger lightly on a string at certain locations).
In a very loose third sense, subharmonic is sometimes used or misused to represent any frequency lower than some other known frequency or frequencies, no matter what the frequency relationship is between those frequencies and no matter the method of production.

Methods for producing an undertone series
The overtone series can be produced physically in two ways—either by overblowing a wind instrument, or by dividing a monochord string. If a monochord string is lightly damped at the halfway point, then at 1/3, then 1/4, 1/5, etc., then the string will produce the overtone series, which includes the major triad. If instead, the length of the string is doubled in the opposite ratios, the undertones series is produced. Similarly, on a wind instrument, if the holes are equally spaced, each successive hole covered will produce the next note in the undertone series.
In addition, undertones can be made through the use of a simple oscillator such as a tuning fork. If the oscillator is gently forced to vibrate against a sheet of paper ""it will naturally make contact at various audible modes of vibration."" Because the tuning fork produces a sine tone, it normally vibrates at its fundamental frequency (e.g. 440 Hz), but ""momentarily"", it will make contact only at every other oscillation (220 Hz), or at every third oscillation (147 Hz), and so on. This produces audible ""subharmonic spectra"", (e.g. 1:1 yields A4 (440 Hz), 1:2 yields A3 (220 Hz), 1:3 yields D3 (147 Hz), 1:4 yields A2 (110 Hz), 1:5 yields F2 (88 Hz), etc.). José Sotorrio claims it is possible to sustain these ""subspectra"" using a sine wave generator through a speaker cone making contact with a flexible (flappable) surface, and also on string instruments ""through skillful manipulation of the bow"", but that this rarely sustains noticeably beyond the ""sub-octave or twelfth"".
String quartets by composers George Crumb and Daniel James Wolf, as well as works by violinist and composer Mari Kimura, include undertones, ""produced by bowing with great pressure to create pitches below the lowest open string on the instrument."" These require string instrument players to bow with sufficient pressure that the strings vibrate in a manner causing the sound waves to modulate and demodulate by the instruments resonating horn with frequencies corresponding to subharmonics.
The tritare, a guitar with Y shaped strings, cause subharmonics too. This can also be achieved by the extended technique of crossing two strings as some experimental jazz guitarists have developed. Also third bridge preparations on guitars cause timbres consisting of sets of high pitched overtones combined with a subharmonic resonant tone of the unplugged part of the string.
Subharmonics can be produced by signal amplification through loudspeakers.

Comparison to the overtone series
Subharmonic frequencies are frequencies below the fundamental frequency of an oscillator in a ratio of 1/n, with n a positive integer number. For example, if the fundamental frequency of an oscillator is 440 Hz, sub-harmonics include 220 Hz (1/2), ~146.6 Hz (1/3) and 110 Hz (1/4). Thus, they are a mirror image of the harmonic series, the undertone series.

Notes in the series
In the overtone series, if we consider C as the fundamental, the first five notes that follow are: C (one octave higher), G (perfect fifth higher than previous note), C (perfect fourth higher than previous note), E (major third higher than previous note), and G (minor third higher than previous note).
The pattern occurs in the same manner using the undertone series. Again we will start with C as the fundamental. The first five notes that follow will be: C (one octave lower), F (perfect fifth lower than previous note), C (perfect fourth lower than previous note), A? (major third lower than previous note), and F (minor third lower than previous note).

Triads
If the first five notes of both series are compared, a pattern is seen:
Overtone series: C C G C E G
Undertone series: C C F C A? F
The undertone series in C contains the F minor triad. Elizabeth Godley argued that the minor triad is also implied by the undertone series and is also a naturally occurring thing in acoustics. ""According to this theory the upper and not the lower tone of a minor chord is the generating tone on which the unity of the chord is conditioned."" Whereas the major chord consists of a generator with upper major third and perfect fifth, the minor chord consists of a generator with lower major third and fifth. Sotorrio, however, said that since this minor triad is not built under the fundamental (C) but under the fifth below (F), major and minor tonalities cannot be said to grow out of this 'polarity'. For this idea to be true, the minor triad would be formed under the Fundamental (C), or else the major triad would be built on the fifth above the fundamental (G) in the overtone series.

Resonance
Hermann von Helmholtz observed in On the Sensations of Tone that the tone of a string tuned to C on a piano changes more noticeably when the notes of its undertone series (c, F, C, A flat, F, D, C, etc.) are struck than those of its overtones. Helmholtz argued that sympathetic resonance is at least as active in under partials as in over partials. Henry Cowell in ""New Musical Resources"" (pg 21-23) discusses a ""Professor Nicolas Garbusov of the Moscow Institute for Musicology"" who created an instrument ""on which at least the first nine undertones could be heard without the aid of resonators."" The phenomenon is described as occurring in resonators of instruments; ""the original sounding body does not produce the undertones but it is difficult to avoid them in resonation...such resonators under certain circumstances respond to only every other vibration producing a half tone.. even if the resonator responds normally to every vibration... under other circumstances the body resonates at only every third vibration...the fact that such underpartials are often audible in music makes them of importance in understanding certain musical relationships...the subdominant... the minor triad.""

Importance in musical composition
First proposed by Zarlino in Instituzione armoniche (1558), the undertone series has been appealed to by theorists such as Riemann and D'Indy to explain phenomena such as the minor chord that the overtone series does not explain. However, while the overtone series occurs naturally as a result of wave propagation and sound acoustics, musicologists such as Paul Hindemith considered the undertone series to be a purely theoretical 'intervallic reflection' of the overtone series. This assertion rests on the fact that undertones do not sound simultaneously with its fundamental tone as the overtone series does.
Harry Partch, on the other hand, argued that the overtone series and the undertone series are equally fundamental, and his concept of Otonality and Utonality is based on this idea. Similarly, Graham H. Jackson in his book The Spiritual Basis of Musical Harmony (2006) suggests that the overtone and undertone series must be seen as a real polarity, representing on the one hand the outer ""material world"" and on the other, our subjective ""inner world"". This view is largely based on the fact that the overtone series has been accepted because it can be explained by materialistic science, while the prevailing conviction about the undertone series is that it can only be achieved by taking subjective experience seriously. For instance, the minor triad is usually heard as sad, or at least pensive, because humans habitually hear all chords as based from below. If feelings are instead based on the high ""fundamental"" of an undertone series, then descending into a minor triad is not felt as melancholy, but rather as overcoming, conquering something. The overtones, by contrast, are then felt as penetrating from outside. With the help of Rudolf Steiner’s work, Jackson traces the history of these two series, as well as the main other system created by the circle of fifths, and argues that in hidden form the series are balanced out in Bach's harmony.
Kathleen Schlesinger, in her 1939 book, The Greek Aulos, pointed out that since the ancient Greek aulos, or reed-blown flute, had holes bored at equal distances, it must have produced a section of the undertone series. She said that this discovery not only cleared up many riddles about the original Greek modes, but indicated that many ancient systems around the world must have also been based on this principle.
In 1868, Adolf von Thimus showed that an indication by a 1st-century Pythagorean, Nicomachus of Gerasa, taken up by Iamblichus in the 4th century, and then worked out by von Thimus, revealed that Pythagoras already had a diagram that could fill a page with interlocking over- and undertone series.
One area of conjecture is that the undertone series might be part of the compositional design phase of the compositional process. The overtone and undertone series can be considered two different arrays, with smaller arrays that contain different major and minor triads. Experiments with undertones to date have focused largely upon improvisation and performance, not compositional design.

See also
Harmonic
Overtone
Subharmonic mixer
Subharmonic synthesizer
Combination tone
Missing fundamental

References
Further reading
Gurewitsch, Matthew (May 13, 2011). ""For a Violinist, Success Means a New Low Point"". The New York Times. Retrieved January 23, 2012.

External links
Website of Mari Kimura with audio clips
Article on Mari Kimura",Category:Acoustics,1
21,22,Acousto-optics,"Acousto-optics is a branch of physics that studies the interactions between sound waves and light waves, especially the diffraction of laser light by ultrasound (or sound in general) through an ultrasonic grating.

Introduction
Optics has had a very long and full history, from ancient Greece, through the renaissance and modern times. As with optics, acoustics has a history of similar duration, again starting with the ancient Greeks. In contrast, the acousto-optic effect has had a relatively short history, beginning with Brillouin predicting the diffraction of light by an acoustic wave, being propagated in a medium of interaction, in 1922. This was then confirmed with experimentation in 1932 by Debye and Sears, and also by Lucas and Biquard.
The particular case of diffraction on the first order, under a certain angle of incidence, (also predicted by Brillouin), has been observed by Rytow in 1935. Raman and Nath (1937) have designed a general ideal model of interaction taking into account several orders. This model was developed by Phariseau (1956) for diffraction including only one diffraction order.
In general, acousto-optic effects are based on the change of the refractive index of a medium due to the presence of sound waves in that medium. Sound waves produce a refractive index grating in the material, and it is this grating that is ""seen"" by the light wave. These variations in the refractive index, due to the pressure fluctuations, may be detected optically by refraction, diffraction, and interference effects, reflection may also be used.
The acousto-optic effect is extensively used in the measurement and study of ultrasonic waves. However, the growing principal area of interest is in acousto-optical devices for the deflection, modulation, signal processing and frequency shifting of light beams. This is due to the increasing availability and performance of lasers, which have made the acousto-optic effect easier to observe and measure. Technical progress in both crystal growth and high frequency piezoelectric transducers has brought valuable benefits to acousto-optic components' improvements.
Along with the current applications, acousto-optics presents interesting possible application. It can be used in nondestructive testing, structural health monitoring and biomedical applications, where optically generated and optical measurements of ultrasound gives a non-contact method of imaging.

Acousto-optic effect
The acousto-optic effect is a specific case of photoelasticity, where there is a change of a material's permittivity, 
  
    
      
        ?
      
    
    {\displaystyle \varepsilon }
  , due to a mechanical strain 
  
    
      
        a
      
    
    {\displaystyle a}
  . Photoelasticity is the variation of the optical indicatrix coefficients 
  
    
      
        
          B
          
            i
          
        
      
    
    {\displaystyle B_{i}}
   caused by the strain 
  
    
      
        
          a
          
            j
          
        
      
    
    {\displaystyle a_{j}}
   given by,

  
    
      
        (
        1
        )
         
        ?
        
          B
          
            i
          
        
        =
        
          p
          
            i
            j
          
        
        
          a
          
            j
          
        
        ,
        
      
    
    {\displaystyle (1)\ \Delta B_{i}=p_{ij}a_{j},\,}
  
where 
  
    
      
        
          p
          
            i
            j
          
        
      
    
    {\displaystyle p_{ij}}
   is the photoelastic tensor with components, 
  
    
      
        i
      
    
    {\displaystyle i}
  ,
  
    
      
        j
      
    
    {\displaystyle j}
   = 1,2,…,6.
Specifically in the acousto-optic effect, the strains 
  
    
      
        
          a
          
            j
          
        
      
    
    {\displaystyle a_{j}}
   are a result of the acoustic wave which has been excited within a transparent medium. This then gives rise to the variation of the refractive index. For a plane acoustic wave propagating along the z axis, the change in the refractive index can be expressed as,

  
    
      
        (
        2
        )
         
        n
        (
        z
        ,
        t
        )
        =
        
          n
          
            0
          
        
        +
        ?
        n
        cos
        ?
        (
        ?
        t
        ?
        K
        z
        )
        ,
        
      
    
    {\displaystyle (2)\ n(z,t)=n_{0}+\Delta n\cos(\Omega t-Kz),\,}
  
where 
  
    
      
        
          n
          
            0
          
        
      
    
    {\displaystyle n_{0}}
   is the undisturbed refractive index, 
  
    
      
        ?
      
    
    {\displaystyle \Omega }
   is the angular frequency, 
  
    
      
        K
      
    
    {\displaystyle K}
   is the wavenumber of the acoustic wave, and 
  
    
      
        ?
        n
      
    
    {\displaystyle \Delta n}
   is the amplitude of variation in the refractive index generated by the acoustic wave, and is given as,

  
    
      
        (
        3
        )
         
        ?
        n
        =
        ?
        
          
            1
            2
          
        
        
          ?
          
            j
          
        
        
          n
          
            0
          
          
            3
          
        
        
          p
          
            z
            j
          
        
        
          a
          
            j
          
        
        ,
      
    
    {\displaystyle (3)\ \Delta n=-{\frac {1}{2}}\sum _{j}n_{0}^{3}p_{zj}a_{j},}
  
The generated refractive index, (2), gives a diffraction grating moving with the velocity given by the speed of the sound wave in the medium. Light which then passes through the transparent material, is diffracted due to this generated refraction index, forming a prominent diffraction pattern. This diffraction pattern corresponds with a conventional diffraction grating at angles 
  
    
      
        
          ?
          
            n
          
        
      
    
    {\displaystyle \theta _{n}}
   from the original direction, and is given by,

  
    
      
        (
        4
        )
         
        ?
        sin
        ?
        (
        
          ?
          
            m
          
        
        )
        =
        m
        ?
        ,
        
      
    
    {\displaystyle (4)\ \Lambda \sin(\theta _{m})=m\lambda ,\,}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   is the wavelength of the optical wave, 
  
    
      
        ?
      
    
    {\displaystyle \Lambda }
   is the wavelength of the acoustic wave and 
  
    
      
        m
      
    
    {\displaystyle m}
   is the integer order maximum.
Light diffracted by an acoustic wave of a single frequency produces two distinct diffraction types. These are Raman-Nath diffraction and Bragg diffraction.
Raman-Nath diffraction is observed with relatively low acoustic frequencies, typically less than 10 MHz, and with a small acousto-optic interaction length, ?, which is typically less than 1 cm. This type of diffraction occurs at an arbitrary angle of incidence, 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \theta _{0}}
  .
In contrast, Bragg diffraction occurs at higher acoustic frequencies, usually exceeding 100 MHz. The observed diffraction pattern generally consists of two diffraction maxima; these are the zeroth and the first orders. However, even these two maxima only appear at definite incidence angles close to the Bragg angle, 
  
    
      
        
          ?
          
            B
          
        
      
    
    {\displaystyle \theta _{B}}
  . The first order maximum or the Bragg maximum is formed due to a selective reflection of the light from the wave fronts of ultrasonic wave. The Bragg angle is given by the expression,

  
    
      
        (
        5
        )
         
        sin
        ?
        
          ?
          
            B
          
        
        =
        ?
        
          
            
              ?
              f
            
            
              2
              
                n
                
                  i
                
              
              ?
            
          
        
        
          [
          1
          +
          
            
              
                ?
                
                  2
                
              
              
                
                  ?
                  
                    2
                  
                
                
                  f
                  
                    2
                  
                
              
            
          
          
            (
            
              n
              
                i
              
              
                2
              
            
            ?
            
              n
              
                d
              
              
                2
              
            
            )
          
          ]
        
        ,
      
    
    {\displaystyle (5)\ \sin \theta _{B}=-{\frac {\lambda f}{2n_{i}\nu }}\left[1+{\frac {\nu ^{2}}{\lambda ^{2}f^{2}}}\left(n_{i}^{2}-n_{d}^{2}\right)\right],}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   is the wavelength of the incident light wave (in a vacuum), 
  
    
      
        f
      
    
    {\displaystyle f}
   is the acoustic frequency, 
  
    
      
        v
      
    
    {\displaystyle v}
   is the velocity of the acoustic wave, 
  
    
      
        
          n
          
            i
          
        
      
    
    {\displaystyle n_{i}}
   is the refractive index for the incident optical wave, and 
  
    
      
        
          n
          
            d
          
        
      
    
    {\displaystyle n_{d}}
   is the refractive index for the diffracted optical waves.
In general, there is no point at which Bragg diffraction takes over from Raman-Nath diffraction. It is simply a fact that as the acoustic frequency increases, the number of observed maxima is gradually reduced due to the angular selectivity of the acousto-optic interaction. Traditionally, the type of diffraction, Bragg or Raman-Nath, is determined by the conditions Q >> 1 and Q << 1 respectively, where Q is given by,

  
    
      
        (
        6
        )
         
        Q
        =
        
          
            
              2
              ?
              ?
              ?
              
                f
                
                  2
                
              
            
            
              n
              
                ?
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle (6)\ Q={\frac {2\pi \lambda \ell f^{2}}{n\nu ^{2}}},}
  
which is known as the Klein-Cook parameter. Since, in general, only the first order diffraction maximum is used in acousto-optic devices, Bragg diffraction is preferable due to the lower optical losses. However, the acousto-optic requirements for Bragg diffraction limit the frequency range of acousto-optic interaction. As a consequence, the speed of operation of acousto-optic devices is also limited.

Acousto-optic devices
Three categories of acousto-optic devices will be discussed. They include the acousto-optic modulator, filter and deflector.

Acousto-optic modulator
By varying the parameters of the acoustic wave, including the amplitude, phase, frequency and polarization, properties of the optical wave may be modulated. The acousto-optic interaction also makes it possible to modulate the optical beam by both temporal and spatial modulation.
A simple method of modulating the optical beam travelling through the acousto-optic device is done by switching the acoustic field on and off. When off the light beam is undiverted, the intensity of light directed at the Bragg diffraction angle is zero. When switched on and Bragg diffraction occurs, the intensity at the Bragg angle increases. So the acousto-optic device is modulating the output along the Bragg diffraction angle, switching it on and off. The device is operated as a modulator by keeping the acoustic wavelength (frequency) fixed and varying the drive power to vary the amount of light in the deflected beam.
There are several limitations associated with the design and performance of acousto-optic modulators. The acousto-optic medium must be designed carefully to provide maximum light intensity in a single diffracted beam. The time taken for the acoustic wave to travel across the diameter of the light beam gives a limitation on the switching speed, and hence limits the modulation bandwidth. The finite velocity of the acoustic wave means the light cannot be fully switched on or off until the acoustic wave has traveled across the light beam. So to increase the bandwidth the light must be focused to a small diameter at the location of the acousto-optic interaction. This minimum focused size of the beam represents the limit for the bandwidth.

Acousto-optic filter
The principle behind the operation of acousto-optic filters is based on the wavelength of the diffracted light being dependent on the acoustic frequency. By tuning the frequency of the acoustic wave, the desired wavelength of the optical wave can be diffracted acousto-optically.
There are two types of the acousto-optic filters, the collinear and non-collinear filters. The type of filter depends on geometry of acousto-optic interaction.
The polarization of the incident light can be either ordinary or extraordinary. For the definition, we assume ordinary polarization. Here the following list of symbols is used,

  
    
      
        ?
      
    
    {\displaystyle \alpha }
  : the angle between the acoustic wave vector and the crystallographic axis z of the crystal;

  
    
      
        ?
      
    
    {\displaystyle \gamma }
  : the wedge angle between the input and output faces of the filter cell (the wedge angle is necessary for eliminating the angular shift of the diffracted beam caused by frequency changing);

  
    
      
        ?
      
    
    {\displaystyle \varphi }
  : the angle between the incident light wave vector and [110] axis of the crystal;

  
    
      
        
          ?
          
            ?
          
        
      
    
    {\displaystyle \alpha _{\ell }}
  : the angle between the input face of the cell and acoustic wave vector;

  
    
      
        ?
      
    
    {\displaystyle \beta }
  : the angle between deflected and non-deflected light at the central frequency;

  
    
      
        ?
      
    
    {\displaystyle \ell }
  : the transducer length.
The incidence angle 
  
    
      
        ?
      
    
    {\displaystyle \varphi }
   and the central frequency 
  
    
      
        
          f
          
            i
          
        
      
    
    {\displaystyle f_{i}}
   of the filter are defined by the following set of equations,

  
    
      
        (
        7
        )
         
        
          n
          
            ?
          
        
        =
        
          
            
              
                n
                
                  0
                
              
              
                n
                
                  e
                
              
            
            
              
                n
                
                  0
                
                
                  2
                
              
              cos
              ?
              ?
              +
              
                n
                
                  e
                
                
                  2
                
              
              
                sin
                
                  2
                
              
              ?
              ?
            
          
        
      
    
    {\displaystyle (7)\ n_{\varphi }={\frac {n_{0}n_{e}}{\sqrt {n_{0}^{2}\cos \varphi +n_{e}^{2}\sin ^{2}\varphi }}}}
  

  
    
      
        (
        8
        )
         
        
          f
          
            i
          
        
        (
        ?
        )
        =
        
          
            ?
            ?
          
        
        
          [
          
            n
            
              ?
            
          
          cos
          ?
          (
          ?
          +
          ?
          )
          ±
          
            
              
                n
                
                  0
                
                
                  2
                
              
              ?
              
                n
                
                  ?
                
                
                  2
                
              
              (
              ?
              )
              
                sin
                
                  2
                
              
              ?
              (
              ?
              +
              ?
              )
            
          
          ]
        
      
    
    {\displaystyle (8)\ f_{i}(\varphi )={\frac {\nu }{\lambda }}\left[n_{\varphi }\cos(\varphi +\alpha )\pm {\sqrt {n_{0}^{2}-n_{\varphi }^{2}(\varphi )\sin ^{2}(\varphi +\alpha )}}\right]}
  
Refractive indices of the ordinary (
  
    
      
        
          n
          
            0
          
        
      
    
    {\displaystyle n_{0}}
  ) and extraordinary (
  
    
      
        
          n
          
            e
          
        
      
    
    {\displaystyle n_{e}}
  ) polarized beams are determined by taking into account their dispersive dependence.
The sound velocity, 
  
    
      
        v
      
    
    {\displaystyle v}
  , depends on the angle ?, such that,

  
    
      
        (
        9
        )
         
        ?
        (
        ?
        )
        =
        
          ?
          
            110
          
        
        
          
            
              cos
              
                2
              
            
            ?
            ?
            +
            
              
                (
                
                  
                    
                      ?
                      
                        001
                      
                    
                    
                      ?
                      
                        110
                      
                    
                  
                
                )
              
              
                2
              
            
            
              sin
              
                2
              
            
            ?
            ?
          
        
      
    
    {\displaystyle (9)\ \nu (\alpha )=\nu _{110}{\sqrt {\cos ^{2}\alpha +\left({\frac {\nu _{001}}{\nu _{110}}}\right)^{2}\sin ^{2}\alpha }}}
  

  
    
      
        
          v
          
            110
          
        
      
    
    {\displaystyle v_{110}}
   and 
  
    
      
        
          v
          
            001
          
        
      
    
    {\displaystyle v_{001}}
   are the sound velocities along the axes [110] and [001], consecutively. The value of 
  
    
      
        
          ?
          
            1
          
        
      
    
    {\displaystyle \alpha _{1}}
   is determined by the angles 
  
    
      
        ?
      
    
    {\displaystyle \varphi }
   and 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
  ,

  
    
      
        (
        10
        )
         
        
          ?
          
            ?
          
        
        =
        ?
        +
        ?
      
    
    {\displaystyle (10)\ \alpha _{\ell }=\varphi +\alpha }
  
The angle 
  
    
      
        ?
      
    
    {\displaystyle \beta }
   between the diffracted and non-diffracted beams defines the view field of the filter; it can be calculated from the formula,

  
    
      
        (
        11
        )
         
        ?
        =
        arcsin
        ?
        
          (
          
            
              
                ?
                
                  f
                  
                    0
                  
                
              
              
                
                  n
                  
                    0
                  
                
                ?
              
            
          
          sin
          ?
          ?
          +
          ?
          )
        
      
    
    {\displaystyle (11)\ \beta =\arcsin \left({\frac {\lambda f_{0}}{n_{0}\nu }}\sin \alpha +\varphi \right)}
  
Input light need not be polarized for a non-collinear design. Unpolarized input light is scattered into orthogonally polarized beams separated by the scattering angle for the particular design and wavelength. If the optical design provides an appropriate beam block for the unscattered light, then two beams (images) are formed in an optical passband that is nearly equivalent in both orthogonally linearly polarized output beams (differing by the Stokes and Anti-Stokes scattering parameter). Because of dispersion, these beams move slightly with scanning rf frequency.

Acousto-optic deflectors
An acousto-optic deflector spatially controls the optical beam. In the operation of an acousto-optic deflector the power driving the acoustic transducer is kept on, at a constant level, while the acoustic frequency is varied to deflect the beam to different angular positions. The acousto-optic deflector makes use of the acoustic frequency dependent diffraction angle, where a change in the angle 
  
    
      
        ?
        
          ?
          
            d
          
        
      
    
    {\displaystyle \Delta \theta _{d}}
   as a function of the change in frequency 
  
    
      
        ?
        f
      
    
    {\displaystyle \Delta f}
   is given as,

  
    
      
        (
        12
        )
         
        ?
        
          ?
          
            d
          
        
        =
        
          
            ?
            ?
          
        
        ?
        f
      
    
    {\displaystyle (12)\ \Delta \theta _{d}={\frac {\lambda }{\nu }}\Delta f}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   is the optical wavelength of the beam and 
  
    
      
        ?
      
    
    {\displaystyle \nu }
   is the velocity of the acoustic wave.
AOD technology has made practical the Bose–Einstein condensation for which the 2001 Nobel Prize in Physics was awarded to Eric A. Cornell, Wolfgang Ketterle and Carl E. Wieman. Another application of acoustic-optical deflection is optical trapping of small molecules.
AODs are essentially the same as acousto-optic modulators (AOMs). In an AOM, only the amplitude of the sound wave is modulated (to modulate the intensity of the diffracted laser beam), whereas in an AOD, both the amplitude and frequency are adjusted, making the engineering requirements tighter for an AOD than an AOM.

Materials
Some materials displaying acousto-optic effect include fused silica,lithium niobate, arsenic trisulfide, tellurium dioxide and tellurite glasses, lead silicate, Ge55As12S33, mercury(I) chloride, lead(II) bromide, and other materials.

See also
Acousto-optic modulator
Acousto-optic deflector
Nonlinear optics
Sonoluminescence
Schaefer-Bergmann diffraction


== References ==",Category:Diffraction,1
22,23,Laser ultrasonics,"Laser-ultrasonics uses lasers to generate and detect ultrasonic waves. It is a non-contact technique used to measure materials thickness, detect flaws and carry out materials characterization. The basic components of a laser-ultrasonic system are a generation laser, a detection laser and a detector.

Ultrasound generation by laser
The generation lasers are short pulse (from tens of nanoseconds to femtoseconds) and high peak power lasers. Common lasers used for ultrasound generation are solid state Q-Switched Nd:YAG and gas lasers (CO2 or Excimers) . The physical principle is of thermal expansion (also called thermoelastic regime) or ablation. In the thermoelastic regime, the ultrasound is generated by the sudden thermal expansion due to the heating of a tiny surface of the material by the laser pulse. If the laser power is sufficient to heat the surface above the material boiling point, some material is evaporated (typically some nanometres) and ultrasound is generated by the recoil effect of the expanding material evaporated. In the ablation regime, a plasma is often formed above the material surface and its expansion can make a substantial contribution to the ultrasonic generation. consequently the emissivity patterns and modal content are different for the two different mechanisms.
The frequency content of the generated ultrasound is partially determined by the frequency content of the laser pulses with shorter pulses giving higher frequencies. For very high frequency generation (up to 100sGHz) fs lasers are used often in a pump-probe configuration with the detection system (see picosecond ultrasonics).

Ultrasound detection by laser
Ultrasound waves may be detected optically by a variety of techniques. Most techniques use continuous or long pulse (typically of tens of microseconds) lasers but some use short pulses to down convert very high frequencies to DC in a classic pump-probe configuration with the generation. Some techniques (notably conventional Fabry–Pérot detectors) require high frequency stability and this usually implies long coherence length. Common detection techniques include: interferometry (homodyne or heterodyne  or Fabry–Pérot) and optical beam deflection (GCLAD) or knife edge detection.
With GCLAD, (Gas-coupled laser acoustic detection), a laser beam is passed through a region where one wants to measure or record the acoustic changes. The ultrasound waves create changes in the air's index of refraction. When the laser encounters these changes, the beam slightly deflects and displaces to a new course. This change is detected and converted to an electric signal by a custom-built photodetector. This enables high sensitivity detection of ultrasound on rough surfaces for frequencies up to 10 MHz.
In practice the choice of technique is often determined by the physical optics and the sample (surface) condition. Many techniques fail to work well on rough surfaces (e.g. simple interferometers) and there are many different schemes to overcome this problem. For instance, photorefractive crystals and four wave mixing are used in an interferometer to compensate for the effects of the surface roughness. These techniques are usually expensive in terms of monetary cost and in terms of light budget (thus requiring more laser power to achieve the same signal to noise under ideal conditions).
At low to moderate frequencies (say < 1 GHz), the mechanism for detection is the movement of the surface of the sample. At high frequencies (say >1 GHz), other mechanisms may come into play (for instance modulation of the sample refractive index with stress).
Under ideal circumstances most detection techniques can be considered theoretically as interferometers and, as such, their ultimate sensitivities are all roughly equal. This is because, in all these techniques, interferometry is used to linearize the detection transfer function and when linearized, maximum sensitivity is achieved. Under these conditions, photon shot noise dominates the sensitivity and this is fundamental to all the optical detection techniques. However, the ultimate limit is determined by the phonon shot noise. Since the phonon frequency is many orders of magnitude lower than the photon frequency, the ultimate sensitivity of ultrasonic detection can be much higher. The usual method for increasing the sensitivity of optical detection is to use more optical power. However, the shot noise limited SNR is proportional to the square root of the total detection power. Thus, increasing optical power has limited effect, and damaging power levels are easily reached before achieving an adequate SNR. Consequently, optical detection frequent has lower SNR than non-optical contacting techniques. Optical generation (at least in the firmly thermodynamic regime) is proportional to the optical power used and it is generally more efficient to improve the generation rather than the detection (again the limit is the damage threshold).
Techniques like CHOTs can overcome the limit of optical detection sensitivity by passively amplifying the amplitude of vibration before optical detection and can result in an increase in sensitivity by several orders of magnitude.

Industrial applications
Well established applications of laser-ultrasonics are composite inspections for the aerospace industry and on-line hot tube thickness measurements for the metallurgical industry.


== References ==",Category:Optics,1
23,24,High-frequency impulse-measurement,"HFIM, acronym for high-frequency-impulse-measurement, is a certain kind of measurement technique in acoustics, where structure-borne sound signals are detected and processed with certain emphasis on short-lived signals as they are indicative for crack formation in a solid body, mostly steel. The basic idea is to use mathematical signal processing methods such as Fourier analysis in combination with suitable computer hardware to allow for real-time measurements of acoustic signal amplitudes as well as their distribution in frequency space. The main benefit of this technique is the enhanced signal-to-noise ratio when it comes to the separation of acoustic emission from a certain source and other, unwanted contamination by any kinds of noise. The technique is therefore mostly applied in industrial production processes, e.g. cold forming or machining, where a 100 percent quality control is required or in condition monitoring for e.g. quantifying tool wear.

Physical basics
High-frequency-impulse measurement is an algorithm for obtaining frequency information of any structure- or air-borne sound source on the basis of discrete signal transformations. This is mostly done using [Fourier series] to quantify the distribution of the energy content of a sound signal in frequency space. On the software side, the tool used for this is the fast Fourier transform (FFT) implementation of this mathematical transformation. This allows, in combination with specific hardware, to directly obtain frequency information so that this is accessible in-line, e.g. during a production process. Contrary to classical, off-line frequency analysis methods, the signal is not unfolded before transformation but is directly fed into the FFT computation. Single events, such as cracks, are hence depicted as extremely short-lived signals covering the entire frequency range (the Fourier transform of a single impulse is a signal covering the entire observed frequency space). Therefore, such single events are easily separable from other noises, even if they are much more energetic.

Applications
Because of its in-line capabilities, HFIM is mostly applied in industrial production processes when it comes to high quality standards e.g. for auto parts that are relevant for crash behavior of a car:
Cold forming: In cold forming applications, HFIM is mostly used to detect cracks during the forming process. Since such cracks are vastly due to stress in the manufactured part, the spontaneous formation of a crack is accompanied by a very sharp, impulse-like signal in the HFIM process alndscape which can easily be deparated from other noise. Therefore, HFIM is the standard technology for crack detection in the automotove sector all over the world.
Machining: In many machining applications, HFIM is used to either monitor the status of tool wear and hence enable pedicitive maintenance or to prevent chatter.
Plastic injection molding: Here, HFIM is used to monitor the status of the molds which are usually very complex. In particular, breaking off of small pins or other parts of the mold can be detected in-line.
Welding: In contrast to most classical monitoring systems for the welding process which usually measure currents or voltages on the welding device, HFIM measures the energy acting directly on the welded workpiece. That allows for detection of various weld imperfactions such as burn-through.
There are also several applications of HFIM devices in materials science laboratories where the exact timing of crack formation is relevant, for instance when determining the plasticity of a new kind of steel.

References
S. Barteldes, F. Walther, W. Holweger: Wälzlagerdiagnose und Detektion von White Etching Cracks mit Barkhausen-Rauschen und Hochfrequenz-Impuls-Messung. In: AKIDA. 10. Aachener Kolloquium für Instandhaltung, Diagnose und Anlagenüberwachung. (= Aachener Schriften zur Rohstoff- und Entsorgungstechnik des Instituts für Maschinentechnik der Rohstoffindustrie. Band 84). Zillekens, Stolberg 2014, ISBN 978-3-941277-21-2, S. 435 ff.
D. Hülsbusch, F.Walther: Damage detection and fatigue strength estimation of carbon fibre reinforced polymers (CFRP) using combined electrical and high-frequency impulse measurements. In: 6th International Symposium on NDT in Aerospace, 12-14th November 2014, Madrid, Spain.
A. Ujma, B. Walder: Werkzeugwartung zur rechten Zeit. In: Kunststoffe. Ausgabe 2/2013, Carl Hanser Verlag.
F. Özkan, D. Hülsbusch, F. Walther: High-frequency impulse measurements (HFIM) for damage detection and fatigue strength estimation of carbon fiber reinforced polymers (CFRP). In: Materials Science and Engineering. Darmstadt, Sept. 2014, S. 23–25.

External links and further reading
Website of QASS, a German company and manufacturer of HFIM measurement devices. (incl. photos, videos and further links)
Talk by Dr. Peter-Christian Zinn concerning different applications of HFIM in the context of Smart Factories.",Category:Acoustics,1
24,25,JASCO Applied Sciences,"JASCO Applied Sciences is a group of international companies that provides services and manufactures equipment to measure underwater sound. JASCO provides services on projects worldwide, operating out of 8 locations internationally, to the oil and gas, marine construction, energy, fisheries, maritime transport and defence sectors. The head office is located in Halifax, NS Canada. JASCO employs acousticians, bioacousticians, physicists, marine mammal scientists, engineers, and project managers.

Services
The firm deploys calibrated sound recorders to measure underwater sound levels. Projects may be long-term, wide-area acoustic monitoring programs or short-term measurements of industrial sources or marine vessels. The data collected are then analysed to determine the acoustic signatures of the sound sources, characterize the ambient noise conditions at the measurement site, or detect and identify marine mammal vocalizations. JASCO measures underwater anthropogenic noise from many sources, including:
Pile driving
Seismic survey operations
Marine vessels
Construction (offshore) and nearshore)
High-frequency sonar such as side-scan sonar, multibeam sonar, and echosounders
The firm also conducts numerical modelling studies to predict the underwater sound field of noise sources, which are often required for environmental impact assessments of industrial projects. The large sound levels produced by activities such as pile driving and seismic surveys can disturb and even injure marine mammals and fish. The results of underwater acoustic modelling are commonly expressed as safety radii (or exclusion zone radii) that are used by marine mammal observers during operations to ensure animals are not exposed to harmful levels of noise. Results are also provided as contour maps of the sound levels around the noise source. These maps can be used to assess or mitigate the impacts of the noise on marine mammals, fish, and other aquatic wildlife.

Products
AMAR G3
The Autonomous Multichannel Acoustic Recorder Generation 3 (AMAR G3) is an underwater acoustic and oceanographic data recorder. It consists of recording electronics housed inside a watertight pressure housing. The AMAR can be connected to up to 8 hydrophones sampled at 24-bit resolution at rates up to 128 kHz, and another high-frequency hydrophone sampled at 16-bit resolution at rates up to 687.5 kHz. Oceanographic sensors (e.g., dissolved oxygen, salinity, acidity, temperature) can also be connected, allowing the system to be used as a mini ocean observatory. Two AMARs are used on the East Node of the VENUS ocean observatory in the Strait of Georgia, providing publicly available underwater sound recordings.

References
External links
JASCO Official Website",Category:Acoustics,1
25,26,Sympathetic resonance,"Sympathetic resonance or sympathetic vibration is a harmonic phenomenon wherein a formerly passive string or vibratory body responds to external vibrations to which it has a harmonic likeness. The classic example is demonstrated with two similar tuning-forks of which one is mounted on a wooden box. If the other one is struck and then placed on the box, then muted, the un-struck mounted fork will be heard. In similar fashion, strings will respond to the external vibrations of a tuning-fork when sufficient harmonic relations exist between the respective vibratory modes. A unison or octave will provoke the largest response as there is maximum likeness in vibratory motion. Other links through shared resonances occur at the fifth and, though with much less effect, at the major third. The principle of sympathetic resonance has been applied in musical instruments from many cultures and times. Apart from the basic principle at work on instruments with many undamped strings, such as harps, guitars and pianos with the dampers raised, other instruments are fitted with extra choirs of sympathetic strings, which respond with a silvery halo to the tones played on the main strings.
According to The New Grove Dictionary of Music and Musicians:

The property of sympathetic vibration is encountered in its direct form in room acoustics in the rattling of window panes, light shades and movable panels in the presence of very loud sounds, such as may occasionally be produced by a full organ. As these things rattle (or even if they do not audibly rattle) sound energy is being converted into mechanical energy, and so the sound is absorbed. Wood paneling and anything else that is lightweight and relatively unrestrained have the same effect. Absorptivity is at its highest at the resonance frequency, usually near or below 100 Hz.

String resonance in music instruments

String resonance occurs on string instruments. Strings or parts of strings may resonate at their fundamental or overtone frequencies when other strings are sounded. For example, an A string at 440 Hz will cause an E string at 330 Hz to resonate, because they share an overtone of 1320 Hz (3rd harmonic of A and 4th harmonic of E).
According to Grove Music Online (2007) article on Duplex Scaling, Steinway progressed a system of aliquot scaling to provide sympathetic resonance with the intention of enriching the treble register of the piano. In the 'octave duplex' piano by Hoerr of Toronto, each note had four strings, of which two, three or four could potentially be struck by the hammer depending on the depression of one of four pedals. Steinway’s duplex scale was precipitated a half century earlier by an experiment conducted by the German piano maker Wilhelm Leberecht Petzoldt, in which a small bridge was placed behind the standard larger one with the intention of maximizing the potential additional resonance of a sympathetically vibrating additional length of string.


== References ==",Category:Articles needing additional references from August 2016,1
26,27,Musica universalis,"Musica universalis (literally universal music), also called Music of the spheres or Harmony of the Spheres, is an ancient philosophical concept that regards proportions in the movements of celestial bodies—the Sun, Moon, and planets—as a form of musica (the Medieval Latin term for music). This ""music"" is not usually thought to be literally audible, but a harmonic, mathematical or religious concept. The idea continued to appeal to thinkers about music until the end of the Renaissance, influencing scholars of many kinds, including humanists. Further scientific exploration has determined specific proportions in some orbital motion, described as orbital resonance.

History
The discovery of the precise relation between the pitch of the musical note and the length of the string that produces it is attributed to Pythagoras. The Music of the Spheres incorporates the metaphysical principle that mathematical relationships express qualities or ""tones"" of energy which manifest in numbers, visual angles, shapes and sounds – all connected within a pattern of proportion. Pythagoras first identified that the pitch of a musical note is in inverse proportion to the length of the string that produces it, and that intervals between harmonious sound frequencies form simple numerical ratios. In a theory known as the Harmony of the Spheres, Pythagoras proposed that the Sun, Moon and planets all emit their own unique hum based on their orbital revolution, and that the quality of life on Earth reflects the tenor of celestial sounds which are physically imperceptible to the human ear. Subsequently, Plato described astronomy and music as ""twinned"" studies of sensual recognition: astronomy for the eyes, music for the ears, and both requiring knowledge of numerical proportions.
Aristotle is the first in giving a critical exposition of the pythagorean notion of the harmony of the spheres. “We should see evidently, after all that proceedes, that, when they talk to us about a harmony resultant of the movement of those bodies, same as the harmony of the sounds that are entwined, there is brilliant comparison being done undoubtedly, but vain; that is not the truth nowise. There are indeed people [the pythagoreans] that figures out that the movement of bodies so big (the planets) must produce necessarily noise, because we hear all around us the noises that bodies without mass nor a speed as equal as the Sun or the Moon. Because of that, one believes being authorized to conclude that stars so numerous and immense cannot go by there making translations moves without making any intense noise. Admitting at first this hypothesis, and supposing that these bodies, thanks to its respective distances, they are for its speed in the same proportion than harmonies, this philosophers come to pretend that the star's voice, which moves into circles, is harmonious. But as it would be very surprising that we did not hear that pretentious voice, it explain us the cause, saying that noise is in our ears since our birth date. This causes that we cannot tell the noise, the thing is, we have never had the contrast of silence which would be its opposite: because voice and silence make themselves distinguishable the one to the other"".
The three branches of the Medieval concept of musica were presented by Boethius in his book De Musica:
musica mundana (sometimes referred to as musica universalis)
musica humana (the internal music of the human body)
musica quae in quibusdam constituta est instrumentis (sounds made by singers and instrumentalists)
According to Max Heindel's Rosicrucian writings, the heavenly ""music of the spheres"" is heard in the Region of Concrete Thought, the lower region of the mental plane, which is an ocean of harmony.

Use in recent music
The connection between music, mathematics, and astronomy had a profound impact on history. It resulted in music's inclusion in the Quadrivium, the medieval curriculum that included arithmetic, geometry, music, and astronomy, and along with the Trivium (grammar, logic,and rhetoric) made up the seven liberal arts, which are still the basis for higher education today. A small number of recent compositions either make reference to or are based on the concepts of Musica Universalis or Harmony of the Spheres. Among these are Music of the Spheres by Mike Oldfield, Om by the Moody Blues, The Earth Sings Mi Fa Mi album by The Receiving End of Sirens, Music of the Spheres by Ian Brown, and Björk's single Cosmogony, included in her 2011 album Biophilia. Earlier, in the 1910s, Danish composer Rued Langgaard composed a pioneering orchestral work titled Music of the Spheres.

See also
Asteroseismology
Esoteric cosmology
Harmonices Mundi
Orbital resonance
This Is My Father's World
Titius–Bode law
Ray of Creation
Sacred geometry
Shabd
Space music
Thema Mundi

Notes
Sources
Davis, Henry, 1901. The Republic The Statesman of Plato. London: M. W. Dunne 1901; Nabu Press reprint, 2010. ISBN 978-1-146-97972-6.
Hackett, Jeremiah, 1997. Roger Bacon and the sciences: commemorative essays. Brill. ISBN 978-90-04-10015-2.
Kepler, Johannes, 1619. The Harmony of the World, translated by E.J. Aiton, A.M. Duncan and J.V. Field (1997). Philadelphia: American Philosophical Society. ISBN 0-87169-209-0.
Pliny the Elder, 77AD. Natural History, books I-II, translated by H. Rackham (1938). Harvard University Press. ISBN 0-674-99364-0.
Smith, Mark A., 2006. Ptolemy's theory of visual perception: an English translation of the Optics. Philadelphia: American Philosophical Society. ISBN 978-0-87169-862-9.
Weiss, Piero and Taruskin, Richard, 2008. Music in the Western World: a history in documents. Cengage Learning. ISBN 978-0-534-58599-0.

Further reading
Calter, Paul. ""Pythagoras & Music of the Spheres"". Geometry in Art & Architecture. Dartmouth College. Retrieved November 26, 2011. 
Plant, David. ""Johannes Kepler & the Music of the Spheres"". Skyscript.co.uk. Retrieved November 26, 2011. 
Wille G. Musica Romana. Die Bedeutung der Musik im Leben der Römer. Amsterdam, 1967.
Burkert W. Weisheit und Wissenschaft: Studien zu Pythagoras, Philolaos und Platon. Nürnberg, 1962
Richter L. Tantus et tam dulcis sonus. Die Lehre von der Sphärenharmonie in Rom und ihre griechischen Quellen // Geschichte der Musiktheorie. Bd. 2. Darmstadt, 2006, SS.505-634.

External links
Music of the Spheres",Category:Early scientific cosmologies,1
27,28,Acoustic wave,"Acoustic waves (also known as sound waves) are a type of longitudinal waves that propagate by means of adiabatic compression and decompression. Longitudinal waves are waves that have the same direction of vibration as their direction of travel. Important quantities for describing acoustic waves are sound pressure, particle velocity, particle displacement and sound intensity. Acoustic waves travel with the speed of sound which depends on the medium they're passing through.

Wave properties
Acoustic waves are longitudinal waves that exhibit phenomena like diffraction, reflection and interference. Sound waves however don't have any polarization since they oscillate along the same direction as they move.

Acoustic wave equation
The acoustic wave equation describes the propagation of sound waves. The acoustic wave equation for sound pressure in one dimension is given by

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              p
            
            
              ?
              
                x
                
                  2
                
              
            
          
        
        ?
        
          
            1
            
              c
              
                2
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              p
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        =
        0
      
    
    {\displaystyle {\partial ^{2}p \over \partial x^{2}}-{1 \over c^{2}}{\partial ^{2}p \over \partial t^{2}}=0}
  
where

  
    
      
        p
      
    
    {\displaystyle p}
   is sound pressure in Pa

  
    
      
        x
      
    
    {\displaystyle x}
   is particle displacement in m

  
    
      
        c
      
    
    {\displaystyle c}
   is speed of sound in m/s

  
    
      
        t
      
    
    {\displaystyle t}
   is time in s
The wave equation for particle velocity has the same shape and is given by

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              u
            
            
              ?
              
                x
                
                  2
                
              
            
          
        
        ?
        
          
            1
            
              c
              
                2
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              u
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        =
        0
      
    
    {\displaystyle {\partial ^{2}u \over \partial x^{2}}-{1 \over c^{2}}{\partial ^{2}u \over \partial t^{2}}=0}
  
where

  
    
      
        u
      
    
    {\displaystyle u}
   is particle velocity in m/s
For lossy media, more intricate models need to be applied in order to take into account frequency-dependent attenuation and phase speed. Such models include acoustic wave equations that incorporate fractional derivative terms, see also the acoustic attenuation article.
D'Alembert gave the general solution for the lossless wave equation. For sound pressure, a solution would be

  
    
      
        p
        =
        R
        cos
        ?
        (
        ?
        t
        ?
        k
        x
        )
        +
        (
        1
        ?
        R
        )
        cos
        ?
        (
        ?
        t
        +
        k
        x
        )
      
    
    {\displaystyle p=R\cos(\omega t-kx)+(1-R)\cos(\omega t+kx)}
  
where

  
    
      
        ?
      
    
    {\displaystyle \omega }
   is angular frequency in rad/s

  
    
      
        t
      
    
    {\displaystyle t}
   is time in s

  
    
      
        k
      
    
    {\displaystyle k}
   is wave number in rad·m?1

  
    
      
        R
      
    
    {\displaystyle R}
   is a coefficient without unit
For 
  
    
      
        R
        =
        1
      
    
    {\displaystyle R=1}
   the wave becomes a travelling wave moving rightwards, for 
  
    
      
        R
        =
        0
      
    
    {\displaystyle R=0}
   the wave becomes a travelling wave moving leftwards. A standing wave can be obtained by 
  
    
      
        R
        =
        0.5
      
    
    {\displaystyle R=0.5}
  .

Phase
In a travelling wave pressure and particle velocity are in phase, which means the phase angle between the two quantities is zero.
This can be easily proven using the ideal gas law

  
    
      
         
        p
        V
        =
        n
        R
        T
      
    
    {\displaystyle \ pV=nRT}
  
where

  
    
      
        p
      
    
    {\displaystyle p}
   is pressure in Pa

  
    
      
        V
      
    
    {\displaystyle V}
   is volume in m3

  
    
      
        n
      
    
    {\displaystyle n}
   is amount in mol

  
    
      
        R
      
    
    {\displaystyle R}
   is the universal gas constant with value 
  
    
      
        8.314
        
        472
        (
        15
        )
         
        
          
            
              J
            
            
              m
              o
              l
               
              K
            
          
        
      
    
    {\displaystyle 8.314\,472(15)~{\frac {\mathrm {J} }{\mathrm {mol~K} }}}
  
Consider a volume 
  
    
      
        V
      
    
    {\displaystyle V}
  . As an acoustic wave propagates through the volume, adiabatic compression and decompression occurs. For adiabatic change the following relation between volume 
  
    
      
        V
      
    
    {\displaystyle V}
   of a parcel of fluid and pressure 
  
    
      
        p
      
    
    {\displaystyle p}
   holds

  
    
      
        
          
            
              ?
              V
            
            
              V
              
                m
              
            
          
        
        =
        
          
            
              ?
              1
            
            
               
              ?
            
          
        
        
          
            
              ?
              p
            
            
              p
              
                m
              
            
          
        
      
    
    {\displaystyle {\partial V \over V_{m}}={-1 \over \ \gamma }{\partial p \over p_{m}}}
  
where

  
    
      
        ?
      
    
    {\displaystyle \gamma }
   is the adiabatic index without unit
As a sound wave propagates through a volume, displacement of particles 
  
    
      
        x
      
    
    {\displaystyle x}
   occurs along the wave propagation direction.

  
    
      
        
          
            
              ?
              x
            
            
              V
              
                m
              
            
          
        
        A
        =
        
          
            
              ?
              V
            
            
              V
              
                m
              
            
          
        
        =
        
          
            
              ?
              1
            
            
               
              ?
            
          
        
        
          
            
              ?
              p
            
            
              p
              
                m
              
            
          
        
      
    
    {\displaystyle {\partial x \over V_{m}}A={\partial V \over V_{m}}={-1 \over \ \gamma }{\partial p \over p_{m}}}
  
where

  
    
      
        A
      
    
    {\displaystyle A}
   is cross-sectional area in m2
From this equation it can be seen that when pressure is at its maximum, displacement reaches zero. As mentioned before, the oscillating pressure for a rightward travelling wave can be given by

  
    
      
        p
        =
        
          p
          
            0
          
        
        c
        o
        s
        (
        ?
        t
        ?
        k
        x
        )
      
    
    {\displaystyle p=p_{0}cos(\omega t-kx)}
  
Since displacement is maximum when pressure is zero there is a 90 degrees phase difference, so displacement is given by

  
    
      
        x
        =
        
          x
          
            0
          
        
        s
        i
        n
        (
        ?
        t
        ?
        k
        x
        )
      
    
    {\displaystyle x=x_{0}sin(\omega t-kx)}
  
Particle velocity is the first derivative of particle displacement. Differentiation of a sine gives a cosine again

  
    
      
        u
        =
        
          u
          
            0
          
        
        c
        o
        s
        (
        ?
        t
        ?
        k
        x
        )
      
    
    {\displaystyle u=u_{0}cos(\omega t-kx)}
  
During adiabatic change, temperature changes with pressure as well following

  
    
      
        
          
            
              ?
              T
            
            
              T
              
                m
              
            
          
        
        =
        
          
            
              ?
              ?
              1
            
            
               
              ?
            
          
        
        
          
            
              ?
              p
            
            
              p
              
                m
              
            
          
        
      
    
    {\displaystyle {\partial T \over T_{m}}={\gamma -1 \over \ \gamma }{\partial p \over p_{m}}}
  
This fact is exploited within the field of thermoacoustics.

Propagation speed
The propagation speed of acoustic waves is given by the speed of sound. In general, the speed of sound c is given by the Newton-Laplace equation:

  
    
      
        c
        =
        
          
            
              C
              ?
            
          
        
        
      
    
    {\displaystyle c={\sqrt {\frac {C}{\rho }}}\,}
  
where
C is a coefficient of stiffness, the bulk modulus (or the modulus of bulk elasticity for gas mediums),

  
    
      
        ?
      
    
    {\displaystyle \rho }
   is the density in kg/m3
Thus the speed of sound increases with the stiffness (the resistance of an elastic body to deformation by an applied force) of the material, and decreases with the density. For general equations of state, if classical mechanics is used, the speed of sound 
  
    
      
        c
      
    
    {\displaystyle c}
   is given by

  
    
      
        
          c
          
            2
          
        
        =
        
          
            
              ?
              p
            
            
              ?
              ?
            
          
        
      
    
    {\displaystyle c^{2}={\frac {\partial p}{\partial \rho }}}
  
where differentiation is taken with respect to adiabatic change.
where 
  
    
      
        p
      
    
    {\displaystyle p}
   is the pressure and 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   is the density

Interference
Interference is the addition of two or more waves that results in a new wave pattern. Interference of sound waves can be observed when two loudspeakers transmit the same signal. At certain locations constructive interference occurs, doubling the local sound pressure. And at other locations destructive interference occurs, causing a local sound pressure of zero pascals.

Standing wave
A standing wave is a special kind of wave that can occur in a resonator. In a resonator superposition of the incident and reflective wave occurs, causing a standing wave. Pressure and particle velocity are 90 degrees out of phase in a standing wave.
Consider a tube with two closed ends acting as a resonator. The resonator has normal modes at frequencies given by

  
    
      
        f
        =
        
          
            
              N
              c
            
            
              2
              d
            
          
        
        
        
        N
        ?
        {
        1
        ,
        2
        ,
        3
        ,
        …
        }
      
    
    {\displaystyle f={\frac {Nc}{2d}}\qquad \qquad N\in \{1,2,3,\dots \}}
  
where

  
    
      
        c
      
    
    {\displaystyle c}
   is the speed of sound in m/s

  
    
      
        d
      
    
    {\displaystyle d}
   is the length of the tube in m
At the ends particle velocity becomes zero since there can be no particle displacement. Pressure however doubles at the ends because of interference of the incident wave with the reflective wave. As pressure is maximum at the ends while velocity is zero, there is a 90 degrees phase difference between them.

Reflection
An acoustic travelling wave can be reflected by a solid surface. If a travelling wave is reflected, the reflected wave can interfere with the incident wave causing a standing wave in the near field. As a consequence, the local pressure in the near field is doubled, and the particle velocity becomes zero.
Attenuation causes the reflected wave to decrease in power as distance from the reflective material increases. As the power of the reflective wave decreases compared to the power of the incident wave, interference also decreases. And as interference decreases, so does the phase difference between sound pressure and particle velocity. At a large enough distance from the reflective material, there is no interference left anymore. At this distance one can speak of the far field.
The amount of reflection is given by the reflection coefficient which is the ratio of the reflected intensity over the incident intensity

  
    
      
        R
        =
        
          
            
              I
              
                
                  r
                  e
                  f
                  l
                  e
                  c
                  t
                  e
                  d
                
              
            
            
              I
              
                
                  i
                  n
                  c
                  i
                  d
                  e
                  n
                  t
                
              
            
          
        
      
    
    {\displaystyle R={\frac {I_{\mathrm {reflected} }}{I_{\mathrm {incident} }}}}

Absorption
Acoustic waves can be absorbed. The amount of absorption is given by the absorption coefficient which is given by

  
    
      
        ?
        =
        1
        ?
        
          R
          
            2
          
        
      
    
    {\displaystyle \alpha =1-R^{2}}
  
where

  
    
      
        ?
      
    
    {\displaystyle \alpha }
   is the absorption coefficient without a unit

  
    
      
        R
      
    
    {\displaystyle R}
   is the reflection coefficient without a unit
Often acoustic absorption of materials is given in decibels instead.

Measurement
Sound pressure can be measured directly using a microphone. Particle velocity can be measured directly using a particle velocity probe. It is also possible to measure the quantities indirectly using the opposite instrument. Sound intensity can be measured using different combinations:
Microphone and particle velocity probe (p-u probe)
Two microphones (p-p probe)
Two particle velocity probes (u-u probe)
The sound pressure is measured in pascal, the particle velocity in meters per second, and the sound intensity in watts. Often these quantities are measured as a level in decibels relative to a certain quantity.
The sound pressure level is given by

  
    
      
        
          L
          
            p
          
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                
                  
                    p
                    
                      
                        r
                        m
                        s
                      
                    
                  
                
                
                  2
                
              
              
                
                  
                    p
                    
                      
                        r
                        e
                        f
                      
                    
                  
                
                
                  2
                
              
            
          
          )
        
      
    
    {\displaystyle L_{p}=10\log _{10}\left({\frac {{p_{\mathrm {rms} }}^{2}}{{p_{\mathrm {ref} }}^{2}}}\right)}
  
where

  
    
      
        
          p
          
            
              r
              m
              s
            
          
        
      
    
    {\displaystyle p_{\mathrm {rms} }}
   is the root-mean square pressure in Pa

  
    
      
        
          p
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle p_{\mathrm {ref} }}
   is the reference value of 2*10?5 Pa
The particle velocity level is given by

  
    
      
        
          L
          
            u
          
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                
                  
                    u
                    
                      
                        r
                        m
                        s
                      
                    
                  
                
                
                  2
                
              
              
                
                  
                    u
                    
                      
                        r
                        e
                        f
                      
                    
                  
                
                
                  2
                
              
            
          
          )
        
      
    
    {\displaystyle L_{u}=10\log _{10}\left({\frac {{u_{\mathrm {rms} }}^{2}}{{u_{\mathrm {ref} }}^{2}}}\right)}
  
where

  
    
      
        
          u
          
            
              r
              m
              s
            
          
        
      
    
    {\displaystyle u_{\mathrm {rms} }}
   is the root-mean square particle velocity in m/s

  
    
      
        
          u
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle u_{\mathrm {ref} }}
   is the reference value of 5*10?8 m/s
The sound intensity level is given by

  
    
      
        
          L
          
            I
          
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          
            
              I
              
                
                  r
                  m
                  s
                
              
            
            
              I
              
                
                  r
                  e
                  f
                
              
            
          
        
      
    
    {\displaystyle L_{I}=10\log _{10}{\frac {I_{\mathrm {rms} }}{I_{\mathrm {ref} }}}}
  
where

  
    
      
        
          I
          
            
              r
              m
              s
            
          
        
      
    
    {\displaystyle I_{\mathrm {rms} }}
   is the root-mean square sound intensity in W

  
    
      
        
          I
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle I_{\mathrm {ref} }}
   is the reference value of 1*10?12 W


== See also ==",Category:All articles lacking sources,1
28,29,Noise barrier,"A noise barrier (also called a soundwall, noise wall, sound berm, sound barrier, or acoustical barrier) is an exterior structure designed to protect inhabitants of sensitive land use areas from noise pollution. Noise barriers are the most effective method of mitigating roadway, railway, and industrial noise sources – other than cessation of the source activity or use of source controls.
In the case of surface transportation noise, other methods of reducing the source noise intensity include encouraging the use of hybrid and electric vehicles, improving automobile aerodynamics and tire design, and choosing low-noise paving material. Extensive use of noise barriers began in the United States after noise regulations were introduced in the early 1970s.

History
Noise barriers have been built in the United States since the mid-twentieth century, when vehicular traffic burgeoned. I-680 in Milpitas, California was the first noise barrier. In the late 1960s, analytic acoustical technology emerged to mathematically evaluate the efficacy of a noise barrier design adjacent to a specific roadway. By the 1990s, noise barriers that included use of transparent materials were being designed in Denmark and other western European countries. Below, a researcher collects data to calibrate a roadway noise model for Foothill Expressway.

The best of these early computer models considered the effects of roadway geometry, topography, vehicle volumes, vehicle speeds, truck mix, roadway surface type, and micro-meteorology. Several U.S. research groups developed variations of the computer modeling techniques: Caltrans Headquarters in Sacramento, California; the ESL Inc. group in Palo Alto, California; the Bolt, Beranek and Newman group in Cambridge, Massachusetts, and a research team at the University of Florida. Possibly the earliest published work that scientifically designed a specific noise barrier was the study for the Foothill Expressway in Los Altos, California.
Numerous case studies across the U.S. soon addressed dozens of different existing and planned highways. Most were commissioned by state highway departments and conducted by one of the four research groups mentioned above. The U.S. National Environmental Policy Act effectively mandated the quantitative analysis of noise pollution from every Federal-Aid Highway Act Project in the country, propelling noise barrier model development and application. With passage of the Noise Control Act of 1972, demand for noise barrier design soared from a host of noise regulation spinoff.
By the late 1970s, more than a dozen research groups in the U.S. were applying similar computer modeling technology and addressing at least 200 different locations for noise barriers each year. As of 2006, this technology is considered a standard in the evaluation of noise pollution from highways. The nature and accuracy of the computer models used is nearly identical to the original 1970s versions of the technology.

Design
The acoustical science of noise barrier design is based upon treating an airway or railway as a line source. The theory is based upon blockage of sound ray travel toward a particular receptor; however, diffraction of sound must be addressed. Sound waves bend (downward) when they pass an edge, such as the apex of a noise barrier. Barriers that block line of sight of a highway or other source will therefore block more sound. Further complicating matters is the phenomenon of refraction, the bending of sound rays in the presence of an inhomogeneous atmosphere. Wind shear and thermocline produce such inhomogeneities. The sound sources modeled must include engine noise, tire noise, and aerodynamic noise, all of which vary by vehicle type and speed.
The noise barrier may be constructed on private land, on a public right-of-way, or on other public land. Because sound levels are measured using a logarithmic scale, a reduction of nine decibels is equivalent to elimination of approximately 86 percent of the unwanted sound power.

Materials
Several different materials may be used for sound barriers. These materials can include masonry, earthwork (such as earth berm), steel, concrete, wood, plastics, insulating wool, or composites. Walls that are made of absorptive material mitigate sound differently than hard surfaces. It is now also possible to make noise barriers with active materials such as solar photovoltaic panels to generate electricity while also reducing traffic noise.  
A wall with porous surface material and sound-dampening content material can be absorptive where little or no noise is reflected back towards the source or elsewhere. Hard surfaces such as masonry or concrete are considered to be reflective where most of the noise is reflected back towards the noise source and beyond.
Noise barriers can be effective tools for noise pollution abatement, but certain locations and topographies are not suitable for use of noise barriers. Cost and aesthetics also play a role in the choice of noise barriers. In some cases, a roadway is surrounded by a noise abatement structure or dug into a tunnel using the cut-and-cover method.

Tradeoffs
Potential disadvantages of noise barriers include:
Aesthetic impacts for motorists and neighbors, particularly if scenic vistas are blocked
Costs of design, construction, and maintenance
Necessity to design custom drainage that the barrier may interrupt
Normally, the benefits of noise reduction far outweigh aesthetic impacts for residents protected from unwanted sound. These benefits include lessened sleep disturbance, improved ability to enjoy outdoor life, reduced speech interference, stress reduction, reduced risk of hearing impairment, and a reduction in the elevated blood pressure created by noise (that improves cardiovascular health).
Sound barrier walls vary in cost depending on the type and quality. Concrete is popular due to lower cost. Since they are reflective, they could potentially create noise for those across from the barrier. Absorptive barriers absorb, and thus abate the noise more effectively, but cost more as they are often custom built and are composed of a variety of materials that allow for absorption. With regard to berm construction costs, a major factor is the availability of excess soil in the immediate area which could be used for berm construction. If the soil is present, it is often cheaper to construct an earth berm noise barrier than to haul away the excess dirt, provided there is sufficient land area available for berm construction. Generally a four-to-one ratio of berm cross sectional width to height is required. Thus, for example, to build a 6-foot-high (1.8 m) berm, one needs an available width of 24 feet (7.3 m).

Effects on pollution
Roadside noise barriers have been shown to reduce the near-road air pollution concentration levels. Within 15–50 m from the roadside, air pollution concentration levels at the lee side of the noise barriers may be reduced by up to 50% compared to open road values. 
Noise barriers force the pollution plumes coming from the road to move up and over the barrier creating the effect of an elevated source and enhancing vertical dispersion of the plume. The deceleration and the deflection of the initial flow by the noise barrier force the plume to disperse horizontally. A highly turbulent shear zone characterized by slow velocities and a re-circulation cavity is created in the lee of the barrier which further enhances the dispersion; this mixes ambient air with the pollutants downwind behind the barrier.

See also
Health effects from noise
Soundproofing


== References ==",Category:Articles with disputed statements from January 2017,1
29,30,Acoustic jar,"Acoustic jars are ceramic vessels found set into the walls, and sometimes into the sides of cavities beneath the floors, of medieval churches. They are believed to have been intended to improve the sound of singing, and to have been inspired by the theories of Vitruvius .

History
Tuned bronze vases set in niches were used to modify the acoustics in Greek and Roman theatres. Their use is described by Vitruvius. No original examples survive.
In the Middle Ages the idea of the acoustic vessel re-emerged. Examples have been found in around 200 churches, half of them in France. They vary greatly in shape and positioning, but, unlike those described by Vitruvius they are ceramic, and are found enclosed within the fabric of building. The function of the jars was established with the discovery of a reference in the Chronicle of the Celestins of Metz. The chronicler recorded that, in 1432:

on the vigil of the Assumption, after brother Odo le Roy, the prior, had returned from the before-mentioned general chapter, it was ordered that pots should be put into the choir of the church of this place, he stating that he had seen such in a church elsewhere thinking that they made the singing better, and resound more, they were put up there in one day, by taking as many workmen as were necessary.

Modern experiments have indicated that their effect would have been to absorb the resonance of certain frequencies, rather than to amplify sound.
They use of the jars was said to be common in Brittany, and around Clisson, in the Loire Inferieure. In the Clisson area, they were usually found in horizontal rows at a height of about three metres above floor level. In 1859 a correspondent to Archaeologia Cambrensis reported that at Pallet there was ""a modern chapel, with earthenware vessels inserted in the walls of the choir, expressly for acoustic purposes"". In England, a set of eleven jars survives high on the chancel walls of St Andrews church at Lyddington, Rutland.
At St Peter Mancroft in Norwich two L-shaped trenches accommodating a number of acoustic jars were discovered beneath the wooden floor on which the choir stalls had previously stood. They had rubble walls and concrete bottoms, and the surfaces were rendered over. Earthenware jars were built into the walls at intervals of about three feet, with the mouths facing into the trench. The jars were about 9 ½ inches long, and 8 inches across at their widest, narrowing to 6 inches at the mouth. A similar discovery was made at St Peter Parmentergate, Norwich in the same city. At Fountains Abbey, in Yorkshire several earthenware vessels were discovered mortared into the base of the choir screen, their necks protruding through the stonework.
The effectiveness of the medieval jars has sometimes been doubted. The Chronicler of Metz, in the only medieval source for the purpose of the jars, mocks the prior for believing that they might have improved the sound of the choir, and the archaeologist Ralph Merrifield suggested that their use might have owed more to a tradition of votive deposits than to the theories of Vitruvius.
In 2011, at The Acoustics of Ancient Theatres conference, held in Patra, Greece, P. Karampatzakis and V. Zafranas presented evidence that Vitruvius' theory was in fact correct, and that the reconstruction of an ancient acoustic vase is possible.


== References ==",Category:History of ceramics,1
30,31,Strike tone,"The strike tone, strike note, or tap note, of a bell when struck is the dominant note perceived immediately by the human ear. It is also known as the prime or fundamental note. However, an analysis of the bell's frequency spectrum reveals that the fundamental only exists weakly and its dominance is a human perception of a note built up by the complex series of harmonics that are generated. The correct harmonic tuning of the bell is therefore important in creating a good strike tone.

Composition of the strike tone
When a bell is struck, the energy imparted causes vibration of the bell in a complex manner and a series of tones known as partials or harmonics are generated.
""This atonal strike sound includes many inharmonic partials that die out quickly, giving way to a strike note or strike tone that is dominated by the prominent partials of the bell. Most observers identify the metallic strike note as having a pitch at or near the frequency of the strong second partial (prime or fundamental), but to others its pitch is an octave higher. Finally, as the sound of the bell ebbs, the slowly decaying hum tone (an octave below the prime, see subharmonic) lingers on."" ""When a bell is properly struck, the first note that prominently attracts the attention of the ear is what is known as the strike note, tap note, or fundamental, this is what we call the note of the bell. The low sound heard after the strike note has lost its intensity is called the hum. There are also present a minor third and perfect fifth in the first octave, and a major third and perfect fifth in the second octave.""
Regarding their names: ""When struck by its clapper, a bell vibrates in a complex way...In general, each normal mode of vibration contributes one partial to the sound of the bell. These partials are customarily given names such as hum, prime, minor third (or tierce), fifth (or quint), octave (or nominal), upper octave, etc. The strike note of the bell, which is determined by three partials (the octave, upper fifth, and the upper octave), is generally close to the pitch of the prime in a well-tuned bell."" Bells with good tone are well-tuned.
""From this it will be seen that (1) the hum note should be a perfect octave below the strike note; (2) the nominal should be a perfect octave above the strike note; (3) the third above the strike note is a minor 3rd and the fifth perfect; (4) that all these notes should be in perfect tune with each other. Above the nominal the major 3rd and perfect 5th can be heard in bells of considerable size; in smaller bells they are so weak as not to be worthy of consideration."" However, historical approached to bell tuning meant that in the past ""Very few bells agree with these conditions. Generally the hum note is a sixth or seventh, and in rare cases a ninth below the strike note. The nominal is somewhere about an octave or a ninth above the strike note, and the other notes diverge accordingly. Bells that are swung are more likely to conform to the conditions than those that are struck.""

Tuning a bell
When the strike note or fundamental of a bell is tuned, its harmonic series must be tuned with it. Bells often contain secondary strike tones which are inharmonic, or unrelated to the harmonic series of the original strike note. ""Whether a founder tunes the nominal or the strike note makes little difference, however, because the nominal is one of the main partials that determines the tuning of the strike note,"" the nominal, twelfth, and double octave being the most important in regards to strike note, resembling harmonics 2:3:4.
The hum tone, which should be an octave below the strike tone, is the actual first partial: ""The strike tone appears to be the fifth partial of a rather unusual series: the ear misjudging it for the octave below, and accepting it as the fundamental of the series. That the strike tone is in some sense aural perception is no longer doubted: the most likely explanation is that it is a perceptual effect, possibly a difference tone created subjectively by the ear from two objectively existing partials.""
""It is interesting that the hum tone of a bell is generally not audible at all—the perceived pitch of the bell (called the 'strike tone') is one octave higher than the hum tone, and there is no component in the sound spectrum of the bell corresponding to the strike tone."" ""The strike note is of great interest to psychoacousticians, because it is a subjective tone created by three strong nearly harmonic partials in the bell sound. The octave or nominal, the twelfth, and the upper octave normally have frequencies nearly in the ratios 2:3:4 [See Table]. The ear assumes these to be partials of a missing fundamental, which it hears as the strike note."" In a well-tuned bell the strike note is generally close to the prime.
In chimes, modes 4, 5, and 6 appear to determine the strike tone and have frequencies in the ratios 92:112:132, or 81:121:169, ""which are close enough to the ratios 2:3:4 for the ear to consider them nearly harmonic and to use them as a basis for establishing a virtual pitch.""  Play chime note on C 
Below are the names and relative frequencies of important partials of tuned church bell or carillon bell:


== Sources ==",Category:Bells (instrument),1
31,32,Acoustic metamaterial,"An acoustic metamaterial is a material designed to control, direct, and manipulate sound waves as these might occur in gases, liquids, and solids. The hereditary line into acoustic metamaterials follows from theory and research in negative index material. Furthermore, with acoustic metamaterials, controlling sonic waves can now be extended to the negative refraction domain.
Control of the various forms of sound waves is mostly accomplished through the bulk modulus ?, mass density ?, and chirality. The density and bulk modulus are analogies of the electromagnetic parameters, permittivity and permeability in negative index materials. Related to this is the mechanics of wave propagation in a lattice structure. Also materials have mass, and instrinsic degrees of stiffness. Together these form a resonant system, and the mechanical (sonic) resonance may be excited by appropriate sonic frequencies (for example pulses at audio frequencies).

History
Acoustic metamaterials have developed from the research and results behind metamaterials. The novel material was originally proposed by Victor Veselago in 1967, but not realized until some 33 years later. John Pendry produced the basic elements of metamaterials during the last part of the 1990s. His materials were combined and then negative index materials were realized first in the year 2000 and 2001 which produced a negative refraction thereby broadening possible optical and material responses. Hence, research in acoustic metamaterials has the same goal of broader material response with sound waves.
Research employing acoustic metamaterials began in the year 2000 with the fabrication and demonstration of sonic crystals in a liquid. This was followed by transposing the behavior of the split-ring resonator to research in acoustic metamaterials. After this double negative parameters (negative bulk modulus ?eff and negative density ?eff) were produced by this type of medium. Then a group of researchers presented the design and tested results of an ultrasonic metamaterial lens for focusing 60 kHz.
The earlier studies of acoustics in technology, which is called acoustical engineering, are typically concerned with how to reduce unwanted sounds, noise control, how to make useful sounds for the medical diagnosis, sonar, and sound reproduction and how to measure some other physical properties using sound.
Using acoustic metamaterials the directions of sound through the medium can be controlled by manipulating the refractive index. Therefore, the traditional acoustic technologies are extended and may eventually cloak certain objects from acoustic detection.
First successful industrial applications of acoustic metamaterials are tested for aircraft insulations.

Basic principles
Since the acoustic metamaterials are one of the branch of the metamaterials, the basic principle of the acoustic metamaterials is similar to the principle of metamaterials. These metamaterials usually gain their properties from structure rather than composition, using the inclusion of small inhomogeneities to enact effective macroscopic behavior. Similar to metamaterials research, investigating materials with Negative index metamaterials, the negative index acoustic metamaterials became the primary research. Negative refractive index of acoustic materials can be achieved by changing the bulk modulus and mass density.

Bulk modulus and mass density
Below, the bulk modulus ? of a substance reflects the substance's resistance to uniform compression. It is defined in relation to the pressure increase needed to cause a given relative decrease in volume.
The mass density (or just ""density"") of a material is defined as mass per unit volume and is expressed in grams per cubic centimeter (g/cm3). In all three classic states of matter—gas, liquid, or solid—the density varies with a change in temperature or pressure, and gases are the most susceptible to those changes. The spectrum of densities is wide-ranging: from 1015 g/cm3 for neutron stars, 1.00 g/cm3 for water to 1.2×10?3 g/cm3 for air. Also relevant here are area density which is mass over a (two-dimensional) area, linear density - mass over a one-dimensional line, and relative density, which is a density divided by the density of a reference material, such as water.
For acoustic materials and acoustic metamaterials, both bulk modulus and density are component parameters, which define their refractive index.

Analogues
Scientific research revealed that acoustic metamaterials have analogues to electromagnetic metamaterials when exhibiting the following characteristics:
In certain frequency bands, the effective mass density and bulk modulus may become negative. This results in a negative refractive index. Flat slab focusing, which can result in super resolution, is similar to electromagnetic metamaterials. The double negative parameters are a result of low-frequency resonances. In combination with a well-defined polarization during wave propagation; k = |n|?, is an equation for refractive index as sound waves interact with acoustic metamaterials (below):

  
    
      
        
          n
          
            2
          
        
        =
        
          
            ?
            ?
          
        
      
    
    {\displaystyle n^{2}={\frac {\rho }{\beta }}}
  

The inherent parameters of the medium are the mass density ?, bulk modulus ?, and chirality k. Chirality, or handedness, determines the polarity of wave propagation (wave vector). Hence within the last equation, Veselago-type solutions (n2 = u*?) are possible for wave propagation as the negative or positive state of ? and ? determine the forward or backward wave propagation.
In negative refractive, electromagnetic metamaterials, negative permittivity can be found in natural materials. However, negative permeability has to be intentionally created in the artificial transmission medium. Obtaining a negative refractive index with acoustic materials is different. Neither negative ? nor negative ? are found in naturally occurring materials; they are derived from the resonant frequencies of an artificially fabricated transmission medium (metamaterial), and such negative values are an anomalous response. Negative ? or ? means that at certain frequencies the medium expands when experiencing compression (negative modulus), and accelerates to the left when being pushed to the right (negative density).

Electromagnetic field vs acoustic field
The electromagnetic spectrum extends from below frequencies used for modern radio to gamma radiation at the short-wavelength end, covering wavelengths from thousands of kilometers down to a fraction of the size of an atom. That would be wavelengths from 103 to 10?15 kilometers. The long wavelength limit is the size of the universe itself, while it is thought that the short wavelength limit is in the vicinity of the Planck length, although in principle the spectrum is infinite and continuous.
Infrasonic frequencies range from 20 Hz down to 0.001 Hz. Audible frequencies are 20 Hz to 20 kHz. Ultrasonic range is above 20 kHz. Sound requires a medium. Electromagnetics radiation (EM waves) can travel in a vacuum.

Mechanics of lattice waves
An imaginary demonstration: A hypothetical rigid lattice structure (solid) is composed of 1023 atoms. However, in a real solid these particles could just as easily be ions. In a rigid lattice structure, atoms exert pressure, or a force, on each other in order to maintain equilibrium. Atomic forces maintain rigid lattice structure. Most of them, such as the covalent or ionic bonds, are of electric nature. The magnetic force, and the force of gravity are negligible. Because of bonding between atoms, the displacement of one or more atoms from their equilibrium positions will give rise to a set of vibration waves propagating through the lattice. One such wave is shown in the figure to the right. The amplitude of the wave is given by the displacements of the atoms from their equilibrium positions. The wavelength ? is marked.
There is a minimum possible wavelength, given by the equilibrium separation a between atoms. Any wavelength shorter than this can be mapped onto a wavelength longer than a, due to effects similar to that in aliasing.

Analysis and experiments
The current research on acoustic metamaterials is based not only on prior experience with electromagnetic metamaterials. The key physics in acoustics are sound, ultrasound and infrasound, which are mechanical waves in gases, liquids, and solids. One objective of the inquiry into the properties of acoustic metamaterials is applications in seismic wave reflection and in vibration control technologies related to earthquakes.

Sonic crystals
In the year 2000 the research of Liu et al. paved the way to acoustic metamaterials through sonic crystals. The latter exhibit spectral gaps two orders of magnitude smaller than the wavelength of sound. The spectral gaps prevent the transmission of waves at prescribed frequencies. The frequency can be tuned to desired parameters by varying the size and geometry of the metamaterial.
The fabricated material consisted of a high-density solid lead ball as the core, one centimeter in size, which was coated with a 2.5-mm layer of rubber silicone. These were arranged in a crystal lattice structure of an 8 × 8 × 8 cube. The balls were cemented into the cubic structure with an epoxy. Transmission was measured as a function of frequency from 250 to 1600 Hz for effectively a four-layer sonic crystal. A two-centimeter slab absorbed sound that normally would require a much thicker material, at 400 Hz. A drop in amplitude was observed at 400 and 1100 Hz.
The amplitudes of the sound waves entering the surface were compared with the sound waves at the center of the metamaterial structure. The oscillations of the coated spheres absorbed sonic energy, which created the frequency gap; the sound energy is absorbed exponentially as the thickness of the material is increased. The key result here is a negative elastic constant created from resonant frequencies of the material. Its projected applications, with a future expanded frequency range in elastic wave systems, are seismic wave reflection and ultrasonics.

Split-ring resonators for acoustic metamaterials
In 2004 split-ring resonators (SRR) became the object of acoustic metamaterial research. Prior research with SRRs fabricated as negative index electromagnetic metamaterials was referenced as the progenitor of further research in acoustic metamaterials. An analysis of the frequency band gap characteristics, derived from the inherent limiting properties of artificially created SRRs, paralleled an analysis of sonic crystals. The band gap properties of SRRs were related to sonic crystal band gap properties. Inherent in this inquiry is a description of mechanical properties and problems of continuum mechanics for sonic crystals, as a macroscopically homogeneous substance.
The correlation in bandgap capabilities includes locally resonant elements and elastic moduli which operate in a certain frequency range. Elements which interact and resonate in their respective localized area are embedded throughout the material. In acoustic metamaterials, locally resonant elements would be the interaction of a single 1-cm rubber sphere with the surrounding liquid. The values of the stop band and band gap frequencies can be controlled by choosing the size, types of materials, and the integration of microscopic structures which control the modulation of the frequencies. These materials are then able to shield acoustic signals and attenuate the effects of anti-plane shear waves. By extrapolating these properties to larger scales it could be possible to create seismic wave filters (see Seismic metamaterials).
According to research prior to this analysis, arrayed metamaterials can create filters or polarizers of either electromagnetic or elastic waves. Here a method is shown which can be applied to two-dimensional stop band and bandgap control with either photonic or sonic structures. Similar to photonic and electromagnetic metamaterial fabrication, a sonic metamaterial is embedded with localized sources of mass density ? and the (elastic) bulk modulus ? parameters, which are analogous to permittivity and permeability, respectively. The sonic (or phononic) metamaterials are sonic crystals, as in the previous section. These crystals have a solid lead core and a softer, more elastic silicone coating. The sonic crystals had built-in localized resonances due to the coated spheres which resulted in almost flat dispersion curves. Low-frequency bandgaps and localized wave interactions of the coated spheres were analyzed and presented in.
This method can be used to tune bandgaps inherent in the material and, also, create new low-frequency bandgaps. It is also applicable for designing low-frequency phononic crystal waveguides (radio frequency). Doubly periodic square array of SRRs are used to illustrate the methodology.

Phononic crystal
Phononic crystals are synthetic materials that are formed by periodic variation of the acoustic properties of the material (i.e., elasticity and mass). One of the main properties of the phononic crystals is the possibility of having a phononic bandgap. A phononic crystal with phononic bandgap prevents phonons of selected ranges of frequencies from being transmitted through the material.
To obtain the frequency band structure of a phononic crystal, Bloch theory is applied on a single unit cell in the reciprocal lattice space (Brillouin zone). Several numerical methods are available for this problem, e.g., the planewave expansion method, the finite element method, and the finite difference method. A brief survey of numerical methods for calculating the frequency band structure is provided by Hussein (2009)
In order to speed up the calculation of the frequency band structure, the Reduced Bloch Mode Expansion (RBME) method can be used. The RBME applies ""on top"" of any of the primary expansion numerical methods mentioned above. For large unit cell models, the RBME method can reduce the time for computing the band structure by up to two orders of magnitude.
The basis of phononic crystals dates back to Isaac Newton who imagined that sound waves propagated through air in the same way that an elastic wave would propagate along a lattice of point masses connected by springs with an elastic force constant E. This force constant is identical to the modulus of the material. Of course with phononic crystals of materials with differing modulus the calculations are a little more complicated than this simple model.
Based on Newton’s observation we can conclude that a key factor for acoustic band-gap engineering is impedance mismatch between periodic elements comprising the crystal and the surrounding medium. When an advancing wave-front meets a material with very high impedance it will tend to increase its phase velocity through that medium. Likewise, when the advancing wave-front meets a low impedance medium it will slow down. We can exploit this concept with periodic (and handcrafted) arrangements of impedance mismatched elements to affect acoustic waves in the crystal – essentially band-gap engineering.
The position of the band-gap in frequency space for a phononic crystal is controlled by the size and arrangement of the elements comprising the crystal. The width of the band gap is generally related to the difference in the speed of sound (due to impedance differences) through the materials that comprise the composite.

Double-negative acoustic metamaterial
The electromagnetic (isotropic) metamaterials have built-in resonance structures that exhibit effective negative permittivity and negative permeability for some frequency ranges. In contrast, it is difficult to build composite acoustic materials with built-in resonances such that the two effective response functions are negative within the capability or range of the transmission medium.
The mass density ? and bulk modulus ? are position dependent. Using the formulation of a plane wave the wave vector is:

  
    
      
        
          
            
              k
              ?
            
          
        
        =
        
          
            
               
              
                |
              
              n
              
                |
              
              ?
            
            c
          
        
        .
        
      
    
    {\displaystyle {\vec {k}}={\frac {\ |n|\omega }{c}}.\,}
  

The angular frequency is represented by ? and c is the propagation speed of acoustic signal through the homogeneous medium. With constant density and bulk modulus as constituents of the medium, the refractive index is expressed as n2 = ? / ?. In order to develop a propagating (plane) wave through the material, it is necessary for both ? and ? to be either positive or negative.
When the negative parameters are achieved, the mathematical result of the Poynting vector 
  
    
      
        
          
            
              s
              ?
            
          
        
      
    
    {\displaystyle \scriptstyle {\overleftarrow {s}}}
  . is the opposite direction of the wave vector 
  
    
      
        
          
            
              k
              ?
            
          
        
      
    
    {\displaystyle \scriptstyle {\overrightarrow {k}}}
  . This requires negativity in bulk modulus and density. Physically, it means that the medium displays an anomalous response at some frequencies such that it expands upon compression (negative bulk modulus) and moves to the left when being pushed to the right (negative density) at the same time.
Natural materials do not have a negative density or a negative bulk modulus, but, negative values are mathematically possible, and can be demonstrated when dispersing soft rubber in a liquid.
Even for composite materials, the effective bulk modulus and density should be normally bounded by the values of the constituents, i.e., the derivation of lower and upper bounds for the elastic moduli of the medium. Intrinsic is the expectation for positive bulk modulus and positive density. For example, dispersing spherical solid particles in a fluid results in the ratio governed by the specific gravity when interacting with the long acoustic wavelength (sound). Mathematically, it can be easily proven that ?eff and ?eff are definitely positive for natural materials. The exception occurs at low resonant frequencies.
As an example, acoustic double negativity is theoretically demonstrated with a composite of soft, silicone rubber spheres suspended in water. In soft rubber, sound travels much slower than through the water. The high velocity contrast of sound speeds between the rubber spheres and the water allows for the transmission of very low monopolar and dipolar frequencies. This is an analogue to analytical solution for the scattering of electromagnetic radiation, or electromagnetic plane wave scattering, by spherical particles - dielectric spheres.
Hence, there is a narrow range of normalized frequency 0.035 < ?a/(2?c) < 0.04 where the bulk modulus and negative density are both negative. Here a is the lattice constant if the spheres are arranged in a face-centered cubic (fcc) lattice; ? is frequency and c is speed of the acoustic signal. The effective bulk modulus and density near the static limit are positive as predicted. The monopolar resonance creates a negative bulk modulus above the normalized frequency at about 0.035 while the dipolar resonance creates a negative density above the normalized frequency at about 0.04.
This behavior is analogous to low-frequency resonances produced in SRRs (electromagnetic metamaterial). The wires and split rings create intrinsic electric dipolar and magnetic dipolar response. With this artificially constructed acoustic metamaterial of rubber spheres and water, only one structure (instead of two) creates the low-frequency resonances to achieve double negativity. With monopolar resonance, the spheres expand, which produces a phase shift between the waves passing through rubber and water. This creates the negative response. The dipolar resonance creates a negative response such that the frequency of the center of mass of the spheres is out of phase with the wave vector of the sound wave (acoustic signal). If these negative responses are large enough to compensate the background fluid, one can have both negative effective bulk modulus and negative effective density.
Both the mass density and the reciprocal of the bulk modulus are decreasing in magnitude fast enough so that the group velocity becomes negative (double negativity). This gives rise to the desired results of negative refraction. The double negativity is a consequence of resonance and the resulting negative refraction properties.

Metamaterial with simultaneously negative bulk modulus and mass density
In August 2007 a metamaterial was reported which simultaneously possesses a negative bulk modulus and mass density. This metamaterial is a zinc blende structure consisting of one fcc array of bubble-contained-water spheres (BWSs) and another relatively shifted fcc array of rubber-coated-gold spheres (RGSs) in special epoxy.
Negative bulk modulus is achieved through monopolar resonances of the BWS series. Negative mass density is achieved with dipolar resonances of the gold sphere series. Rather than rubber spheres in liquid, this is a solid based material. This is also as yet a realization of simultaneously negative bulk modulus and mass density in a solid based material, which is an important distinction.

Double C resonators
Double C resonator (DCR) is a ring cut in halves. In 2007, proposals have been made for arrays of DCRs and similar negative acoustic metamaterial. Although linear elasticity is mentioned, the problem is defined around shear waves directed at angles to the plane of the cylinders. The DCR was constructed similar to the SRRs in a multiple cell configuration. The DCR has been improved with stiffer material sheets. Each cell consists of a large rigid disk and two thin ligaments. The DCR cell is a tiny oscillator connected by springs. One spring of the oscillator connects to the mass and is anchored by the other spring. The LC resonator has specified capacitance and inductance. The limitations are expressed with appropriate mathematical equations. In addition to the intended limitations is that the speed of sound in the matrix is expressed as c = ??/µ with a matrix of density ? and shear modulus ?. The resonant frequency is then expressed as ?1/(LC).
A phononic bandgap occurs in association with the resonance of the split cylinder ring. There is a phononic band gap within a range of normalized frequencies. This is when the inclusion moves as a rigid body.
The DCR design produced a suitable band with negative slope in a range of frequencies. This band was obtained by hybridizing the modes of a DCR with the modes of thin stiff bars. Calculations have shown that at these frequencies:
a beam of sound negatively refracts across a slab of such a medium,
the phase vector in the medium possesses real and imaginary parts with opposite signs,
the medium is well impedance-matched with the surrounding medium,
a flat slab of the metamaterial can image a source across the slab like a Veselago lens,
the image formed by the flat slab has considerable sub-wavelength image resolution, and
a double corner of the metamaterial can act as an open resonator for sound.

Acoustic metamaterial superlens
In May 2009 Shu Zhang et al. presented the design and test results of an ultrasonic metamaterial lens for focusing 60 kHz (~2 cm wavelength) sound waves under water. The lens is made of sub-wavelength elements and is therefore potentially more compact than phononic lenses that operate in the same frequency range.
High-resolution acoustic imaging techniques are the essential tools for nondestructive testing and medical screening. However, the spatial resolution of the conventional acoustic imaging methods is restricted by the incident wavelength of ultrasound. This is due to the quickly fading evanescent fields which carry the sub-wavelength features of objects.
The lens consists of a network of fluid-filled cavities called Helmholtz resonators that oscillate at certain sonic frequencies. Similar to a network of inductors and capacitors in electromagnetic metamaterial, the arrangement of Helmholtz cavities designed by Zhang et al. have a negative dynamic modulus for ultrasound waves. Zhang et al. did focus a point source of 60.5 kHz sound to a spot size that is roughly the width of half a wavelength and their design may allow to push the spatial resolution even further. This result is in excellent agreement with the numerical simulation by transmission line model, which derived the effective mass density and compressibility. This metamaterial lens also displays variable focal length at different frequencies.

Acoustic diode
An acoustic diode was introduced in August 2009. An electrical diode allows current to flow in only one direction in a wire; it is an essential electronic device which had no analogues for sound waves. However, the reported design partially fills this role by converting sound to a new frequency and blocking any backwards flow of the original frequency. In practice, it could give designers new flexibility in making ultrasonic sources like those used in medical imaging. The proposed structure combines two components: The first is a sheet of nonlinear acoustic material—one whose sound speed varies with air pressure. An example of such a material is a collection of grains or beads, which becomes stiffer as it is squeezed. The second component is a filter that allows the doubled frequency to pass through but reflects the original.

Acoustic cloaking
An acoustic cloak is a hypothetical device that would make objects impervious towards sound waves. This could be used to build sound proof homes, advanced concert halls, or stealth warships. The mathematics and physics behind acoustic cloaking has been known for several years. The idea of acoustic cloaking is to deviate the sounds waves around the object that has to be cloaked. But realizing it in materials has been difficult, since mechanical metamaterials are needed. The key to this problem are acoustic metamaterials also known as ""Sonic Crystals"". Making a metamaterial for sound means identifying the acoustic analogues to permittivity and permeability in light waves. It turns out that these are the material's mass density and its elastic constant. Researchers from Wuhan University, China in a paper in 2007 reported such a metamaterial which simultaneously possessed a negative bulk modulus and mass density.

Potential applications
If such a material could be commercialized, researchers believe it could have many applications. Walls of the material could be built to soundproof houses or it could be used in concert halls to enhance acoustics or direct noise away from certain areas. The military may also be interested to conceal submarines from detection by sonar or to create a new class of stealth ships.

Metamaterial acoustic cloak
A laboratory metamaterial device that is applicable to ultra-sound waves has been demonstrated in January 2011. It can be applied to sound wavelengths from 40 to 80 kHz.
The metamaterial acoustic cloak is designed to hide objects submerged in water. The metamaterial cloaking mechanism bends and twists sound waves by intentional design.
The cloaking mechanism consists of 16 concentric rings in a cylindrical configuration, and each ring with acoustic circuits. It is intentionally designed to guide sound waves, in two dimensions. The first microwave metamaterial cloak guided electromagnetic waves in two dimensions.
Each ring has a different index of refraction. This causes sound waves to vary their speed from ring to ring. ""The sound waves propagate around the outer ring, guided by the channels in the circuits, which bend the waves to wrap them around the outer layers of the cloak"". This device has been described as an array of cavities which actually slow the speed of the propagating sound waves. An experimental cylinder was submerged in tank, and then it disappeared from sonar. Other objects of various shape and density were also hidden from the sonar. The acoustic cloak demonstrated effectiveness for the sound wavelengths of 40 kHz to 80 kHz.

Phononic metamaterials for thermal management
As phonons are responsible for thermal conduction in solids, acoustic metamaterials may be designed to control heat transfer.

See also
Books

Metamaterials scientists

References
Further reading
Leonhardt, Ulf; Smith, David R (2008). ""Focus on Cloaking and Transformation Optics"". New Journal of Physics. 10 (11): 115019. Bibcode:2008NJPh...10k5019L. doi:10.1088/1367-2630/10/11/115019. 
Fang, Nicholas; Xi, Dongjuan; Xu, Jianyi; Ambati, Muralidhar; Srituravanich, Werayut; Sun, Cheng; Zhang, Xiang (2006). ""Ultrasonic metamaterials with negative modulus"" (PDF). Nature Materials. 5 (6): 452–6. Bibcode:2006NatMa...5..452F. doi:10.1038/nmat1644. PMID 16648856. Archived from the original (PDF) on June 23, 2010. 
Zhang, Shu; Xia, Chunguang; Fang, Nicholas (2011). ""Broadband Acoustic Cloak for Ultrasound Waves"". Physical Review Letters. 106: 024301. arXiv:1009.3310?. Bibcode:2011PhRvL.106b4301Z. doi:10.1103/PhysRevLett.106.024301. PMID 21405230. 
Pendry, J B; Li, Jensen (2008). ""An acoustic metafluid: realizing a broadband acoustic cloak"". New Journal of Physics. 10 (11): 115032. Bibcode:2008NJPh...10k5032P. doi:10.1088/1367-2630/10/11/115032. 
Richard V. Craster, et al.: Acoustic metamaterials: negative refraction, imaging, lensing and cloaking. Springer, Dordrecht 2013, ISBN 978-94-007-4812-5.

External links
Ideas underpinning sound
Acoustic Metamaterials and Devices: Negative, Positive, and Zero Refraction and Super-lensing in Phononic Crystals
http://iopscience.iop.org/1367-2630/10/11/115032
http://news.bbc.co.uk/2/hi/science/nature/7450321.stm
https://www.newscientist.com/blog/technology/2007/08/how-to-build-acoustic-invisibility.html",Category:Articles with dead external links from February 2017,1
32,33,Sound power,"Sound power or acoustic power is the rate at which sound energy is emitted, reflected, transmitted or received, per unit time. The SI unit of sound power is the watt (W). It is the power of the sound force on a surface of the medium of propagation of the sound wave. For a sound source, unlike sound pressure, sound power is neither room-dependent nor distance-dependent. Sound pressure is a measurement at a point in space near the source, while the sound power of a source is the total power emitted by that source in all directions. Sound power passing through an area is sometimes called sound flux or acoustic flux through that area.

Sound power level LWA
Regulations control the maximum sound power level LWA that a device (e.g. vacuum cleaner) is allowed to produce. The A-weighting scale is used in the calculation as the regulation is concerned with the loudness as perceived by the human ear. Measurements are taken at several defined points around the device.
The test environment can be located indoors or outdoors. The ideal environment is on the ground in a large open space or hemi-anechoic chamber (free-field over a reflecting plane ). To account for undesired reflections from nearby objects, walls, and the ceiling, and for any residual background noises, measurement corrections are applied.

Table of selected sound sources
Here is a table of some examples.

Mathematical definition
Sound power, denoted P, is defined by

  
    
      
        P
        =
        
          f
        
        ?
        
          v
        
        =
        A
        p
        
        
          u
        
        ?
        
          v
        
        =
        A
        p
        v
      
    
    {\displaystyle P=\mathbf {f} \cdot \mathbf {v} =Ap\,\mathbf {u} \cdot \mathbf {v} =Apv}
  
where
f is the sound force of unit vector u;
v is the particle velocity of projection v along u;
A is the area;
p is the sound pressure.
In a medium, the sound power is given by

  
    
      
        P
        =
        
          
            
              A
              
                p
                
                  2
                
              
            
            
              ?
              c
            
          
        
        cos
        ?
        ?
        ,
      
    
    {\displaystyle P={\frac {Ap^{2}}{\rho c}}\cos \theta ,}
  
where
A is the area of the surface;
? is the mass density;
c is the sound velocity;
? is the angle between the direction of propagation of the sound and the normal to the surface.
For example, a sound at SPL = 85 dB or p = 0.356 Pa in air (? = 1.2 kg·m?3 and c = 343 m·s?1) through a surface of area A = 1 m2 normal to the direction of propagation (? = 0 °) has a sound energy flux P = 0.3 mW.
This is the parameter one would be interested in when converting noise back into usable energy, along with any losses in the capturing device.

Relationships with other quantities
Sound power is related to sound intensity:

  
    
      
        P
        =
        A
        I
        ,
      
    
    {\displaystyle P=AI,}
  
where
A is the area;
I is the sound intensity.
Sound power is related sound energy density:

  
    
      
        P
        =
        A
        c
        w
        ,
      
    
    {\displaystyle P=Acw,}
  
where
c is the speed of sound;
w is the sound energy density.

Sound power level definition
Sound power level (SWL) or acoustic power level is a logarithmic measure of the power of a sound relative to a reference value.
Sound power level, denoted LW and measured in dB, is defined by

  
    
      
        
          L
          
            W
          
        
        =
        
          
            1
            2
          
        
        ln
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        
          log
          
            10
          
        
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{W}={\frac {1}{2}}\ln \!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {Np} =\log _{10}\!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {B} =10\log _{10}\!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {dB} ,}
  
where
P is the sound power;
P0 is the reference sound power;
1 Np = 1 is the neper;
1 B = 1/2 ln 10 is the bel;
1 dB = 1/20 ln 10 is the decibel.
The commonly used reference sound power in air is

  
    
      
        
          P
          
            0
          
        
        =
        1
         
        
          p
          W
        
        .
      
    
    {\displaystyle P_{0}=1~\mathrm {pW} .}
  
The proper notations for sound power level using this reference are LW/(1 pW) or LW (re 1 pW), but the suffix notations dB SWL, dB(SWL), dBSWL, or dBSWL are very common, even if they are not accepted by the SI.
The reference sound power P0 is defined as the sound power with the reference sound intensity I0 = 1 pW/m2 passing through a surface of area A0 = 1 m2:

  
    
      
        
          P
          
            0
          
        
        =
        
          A
          
            0
          
        
        
          I
          
            0
          
        
        ,
      
    
    {\displaystyle P_{0}=A_{0}I_{0},}
  
hence the reference value P0 = 1 pW.

Relationship with sound pressure level
The generic calculation of sound power from sound pressure is as follows:

  
    
      
        
          L
          
            W
          
        
        =
        
          L
          
            p
          
        
        +
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              
                A
                
                  S
                
              
              
                A
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{W}=L_{p}+10\log _{10}\!\left({\frac {A_{S}}{A_{0}}}\right)\!~\mathrm {dB} ,}
  
where: 
  
    
      
        
          
            A
            
              S
            
          
        
      
    
    {\displaystyle {A_{S}}}
   defines the area of a surface that wholly encompasses the source. This surface may be any shape, but it must fully enclose the source.
In the case of a sound source located in free field positioned over a reflecting plane (i.e. the ground), in air at ambient temperature, the sound power level at distance r from the sound source is approximately related to sound pressure level (SPL) by

  
    
      
        
          L
          
            W
          
        
        =
        
          L
          
            p
          
        
        +
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              
                2
                ?
                
                  r
                  
                    2
                  
                
              
              
                A
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{W}=L_{p}+10\log _{10}\!\left({\frac {2\pi r^{2}}{A_{0}}}\right)\!~\mathrm {dB} ,}
  
where
Lp is the sound pressure level;
A0 = 1 m2;

  
    
      
        
          2
          ?
          
            r
            
              2
            
          
        
        ,
      
    
    {\displaystyle {2\pi r^{2}},}
   defines the surface area of a hemisphere; and
r must be sufficient that the hemisphere fully encloses the source.
Derivation of this equation:

  
    
      
        
          
            
              
                
                  L
                  
                    W
                  
                
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      P
                      
                        P
                        
                          0
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        A
                        I
                      
                      
                        
                          A
                          
                            0
                          
                        
                        
                          I
                          
                            0
                          
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      I
                      
                        I
                        
                          0
                        
                      
                    
                  
                  )
                
                +
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      A
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}L_{W}&={\frac {1}{2}}\ln \!\left({\frac {P}{P_{0}}}\right)\\&={\frac {1}{2}}\ln \!\left({\frac {AI}{A_{0}I_{0}}}\right)\\&={\frac {1}{2}}\ln \!\left({\frac {I}{I_{0}}}\right)+{\frac {1}{2}}\ln \!\left({\frac {A}{A_{0}}}\right)\!.\end{aligned}}}
  
For a progressive spherical wave,

  
    
      
        
          z
          
            0
          
        
        =
        
          
            p
            v
          
        
        ,
      
    
    {\displaystyle z_{0}={\frac {p}{v}},}
  

  
    
      
        A
        =
        4
        ?
        
          r
          
            2
          
        
        ,
      
    
    {\displaystyle A=4\pi r^{2},}
   (the surface area of sphere)
where z0 is the characteristic specific acoustic impedance.
Consequently,

  
    
      
        I
        =
        p
        v
        =
        
          
            
              p
              
                2
              
            
            
              z
              
                0
              
            
          
        
        ,
      
    
    {\displaystyle I=pv={\frac {p^{2}}{z_{0}}},}
  
and since by definition I0 = p02/z0, where p0 = 20 ?Pa is the reference sound pressure,

  
    
      
        
          
            
              
                
                  L
                  
                    W
                  
                
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        p
                        
                          2
                        
                      
                      
                        p
                        
                          0
                        
                        
                          2
                        
                      
                    
                  
                  )
                
                +
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        4
                        ?
                        
                          r
                          
                            2
                          
                        
                      
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                ln
                
                
                  (
                  
                    
                      p
                      
                        p
                        
                          0
                        
                      
                    
                  
                  )
                
                +
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        4
                        ?
                        
                          r
                          
                            2
                          
                        
                      
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  L
                  
                    p
                  
                
                +
                10
                
                  log
                  
                    10
                  
                
                
                
                  (
                  
                    
                      
                        4
                        ?
                        
                          r
                          
                            2
                          
                        
                      
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
                
                 
                
                  d
                  B
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}L_{W}&={\frac {1}{2}}\ln \!\left({\frac {p^{2}}{p_{0}^{2}}}\right)+{\frac {1}{2}}\ln \!\left({\frac {4\pi r^{2}}{A_{0}}}\right)\\&=\ln \!\left({\frac {p}{p_{0}}}\right)+{\frac {1}{2}}\ln \!\left({\frac {4\pi r^{2}}{A_{0}}}\right)\\&=L_{p}+10\log _{10}\!\left({\frac {4\pi r^{2}}{A_{0}}}\right)\!~\mathrm {dB} .\end{aligned}}}
  
The sound power estimated practically does not depend on distance. The sound pressure used in the calculation may be affected by distance due to viscous effects in the propagation of sound unless this is accounted for.

References
External links
Sound power and Sound pressure. Cause and Effect
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
NIOSH Powertools Database
Sound Power Testing",Category:Physical quantities,1
33,34,Biophony,"Biophony (also known as the niche hypothesis) consists of the Greek prefix, bio, meaning life, and the suffix, phon, meaning sound. It specifically refers to the collective sound that vocalizing animals create in each given environment. The term, which refers to one of three components of the soundscape (the others include geophony [non-biological natural sound] and anthrophony [human-induced noise]), was coined by Dr. Bernie Krause. The interrelationship of disciplines informed by natural soundscapes is called soundscape ecology, a further refinement of the older model and term, acoustic ecology.
The study of biophony focuses on the collective impact of all sounds emanating from natural biological origins in a given habitat. The realm of study is focused on the intricate relationships – competitive and/or cooperative – generally between non-human biological sound sources taking into account seasonal variability, weather, and time of day or night, and climate change. It explores new definitions of animal territory as defined by biophony, and addresses changes in density, diversity, and richness of animal populations.
The complete absence of biophony or geophony in a given biome would be expressed as dysphonia (from the Greek meaning the inability to produce a proper collective voice in this case).
The ""niche hypothesis"", an early version of the term, biophony, describes the acoustic bandwidth partitioning process that occurs in still-wild biomes by which non-human organisms adjust their vocalizations by frequency and time-shifting to compensate for vocal territory occupied by other vocal creatures. Thus each species evolves to establish and maintain its own acoustic bandwidth so that its voice is not masked. For instance, notable examples of clear partitioning and species discrimination can be found in the spectrograms derived from the biophonic recordings made in most uncompromised tropical and subtropical rain forests.

See also
Acoustic ecology
Anthropophony
Aphonia
Bioacoustics
Ecoacoustics
Geophony
Soundscape
Soundscape ecology
Spectrogram

References
Further reading
Krause, Bernie (1998). Into a Wild Sanctuary. Berkeley, California: Heyday Books. 
Krause, Bernie (2002). Wild Soundscapes: Discovering the Voice of the Natural World. Berkeley, California: Wilderness Press. 
Krause, Bernie (31 January 2001). Loss of Natural Soundscape: Global Implications of Its Effect on Humans and Other Creatures. World Affairs Council, San Francisco, California. 
Hull J (18 February 2007). ""The Noises of Nature"". Idea Lab. New York Times Magazine. 
Krause B (2008). ""Anatomy of the Soundscape"". Journal of the Audio Engineering Society. 56 (1/2). 
Sueur, Jérome, Cicada acoustic communication: potential sound partitioning in a multi species community from Mexico (Hemiptera: Cicadomorpha: Cicadidae), Biological Journal of the Linnean Society, 2002, 75, 379-394
Bernie Krause, Stuart H. Gage, Wooyeong Joo, Measuring and interpreting the temporal variability in the soundscape at four places in Sequoia National Park, Landscape Ecology, DOI 10.1007/s10980-011-9639-6, Aug. 2011,
Bryan C. Pijanowski, Luis J. Villanueva-Rivera, Sarah L. Dumyahn, Almo Farina, Bernie L. Krause, Brian M. Napoletano, Stuart H. Gage, and Nadia Pieretti,Soundscape Ecology: The Science of Sound in the Landscape, BioScience, March, 2011, Vol. 61 No. 3, 203-216
Krause, Bernie (2012). The Great Animal Orchestra: Finding the Origins of Music in the World's Wild Places. New York, New York: Little Brown. 
Kull, Kalevi 2010. Ecosystems are made of semiosic bonds: Consortia, umwelten, biophony and ecological codes. Biosemiotics 3(3): 347–357.
Farina, Almo (2013). Soundscape Ecology: Principals, Patterns, Methods and Applications. Springer.

External links
Wild Sanctuary — online database of natural sounds. Created by Dr. Bernie Krause.
World Forum For Acoustic Ecology: Soundscape Newsletter Archive
An Introduction To Acoustic Ecology",Category:Articles with topics of unclear notability from July 2017,1
34,35,Ambient noise level,"In atmospheric sounding and noise pollution, ambient noise level (sometimes called background noise level, reference sound level, or room noise level) is the background sound pressure level at a given location, normally specified as a reference level to study a new intrusive sound source.
Ambient sound levels are often measured in order to map sound conditions over a spatial regime to understand their variation with locale. In this case the product of the investigation is a sound level contour map. Alternatively ambient noise levels may be measured to provide a reference point for analyzing an intrusive sound to a given environment. For example, sometimes aircraft noise is studied by measuring ambient sound without presence of any overflights, and then studying the noise addition by measurement or computer simulation of overflight events. Or roadway noise is measured as ambient sound, prior to introducing a hypothetical noise barrier intended to reduce that ambient noise level.
Ambient noise level is measured with a sound level meter. It is usually measured in dB relative to a reference pressure of 0.00002 Pa, i.e., 20 ?Pa (micropascals) in SI units. A pascal is a newton per square meter. The centimeter-gram-second system of units, the reference sound pressure for measuring ambient noise level is 0.0002 dyn/cm2. Most frequently ambient noise levels are measured using a frequency weighting filter, the most common being the A-weighting scale, such that resulting measurements are denoted dB(A), or decibels on the A-weighting scale.

See also
A-weighting
Background noise
Environmental noise
Noise barrier
Noise health effects
Noise pollution
Noise regulation

References
 This article incorporates public domain material from the General Services Administration document ""Federal Standard 1037C"" (in support of MIL-STD-188).",Category:Noise,1
35,36,Sound baffle,"A sound baffle is a construction or device which reduces the strength (level) of airborne sound. Sound baffles are a fundamental tool of noise mitigation, the practice of minimizing noise pollution or reverberation. An important type of sound baffle is the noise barrier constructed along highways to reduce sound levels at properties in the vicinity. Sound baffles are also applied to walls and ceilings in building interiors to absorb sound energy and thus lessen reverberation.

Highway noise barriers
The technology for accurate prediction of the effects of noise barrier design using a computer model to analyze roadway noise has been available since the early 1970s. The earliest published scientific design of a noise barrier may have occurred in Santa Clara County, California in 1970 for a section of the Foothill Expressway in Los Altos, California. The county used a computer model to predict the effects of sound propagation from roadways, with variables consisting of vehicle speed, ratio of trucks to automobiles, road surface type, roadway geometrics, micro-meteorology and the design of proposed soundwalls.

Interior sound baffle design
Since the early 1900s, scientists have been aware of the utility of certain types of interior coatings or baffles to improve the acoustics of concert halls, theaters, conference rooms and other spaces where sound quality is important. By the mid-1950s, Bolt, Beranek and Newman and a few other U.S. research organizations were developing technology to address sound quality's design challenges. This design field draws on several disciplines including acoustical science, computer modeling, architecture and materials science. Sound baffles are also used in speaker cabinets to absorb energy from the pressure created by the speakers, thus reducing cabinet resonance.
In 1973, Pearl P. Randolph, a school bus driver in Virginia, won a new school bus in a national contest held by Wayne Corporation for the suggestion that sound baffles be installed in the ceiling of school buses. In 1981, they were first made mandatory by the state of California.

Vehicle exhaust sound baffles
Baffles are also found in the exhaust pipes of vehicles, particularly motorcycles.

See also
Noise pollution
Noise health effects


== References ==",Category:Noise pollution,1
36,37,Acoustic membrane,"An acoustic membrane is a thin layer that vibrates and is used in acoustics to produce or transfer sound, such as a drum, microphone, or loudspeaker.

See also
Membranophone
Vibrations of a circular membrane",Category:All articles lacking sources,1
37,38,Tone hole,"A tone hole is an opening in the body of a wind instrument which, when alternately closed and opened, changes the pitch of the sound produced. Tone holes may serve specific purposes, such as a trill hole or register hole. A tone hole is, ""in wind instruments[,] a hole that may be stopped by the finger, or a key, to change the pitch of the tone produced.""
The resonant frequencies of the air column in a pipe are inversely proportional to the pipe's effective length. In other words, a shorter pipe produces higher notes. For a pipe with no tone holes but open at both ends, the effective length is the physical length of the pipe plus a little more for the small volumes of air just beyond the ends of the pipe that are also involved in the resonance. An open hole anywhere along the middle of the pipe shortens the pipe's effective length and therefore raises the pitch of the notes it produces. The closer an open hole is to the blowing end, the shorter the remaining effective length is and the more it raises the pitch. Generally, a hole in a given position doesn't reduce the effective length quite as much as cutting the pipe at that position would, and the smaller the hole, the less it reduces the effective length when open. Closing the hole increases the effective length and lowers the pitch again. However, a pipe with a closed tone hole is not acoustically identical to a pipe with no hole; the shape of the fingertip or pad that closes the hole modifies the pipe's internal volume and effective length.
When there are multiple tone holes, the first (closest to the blowing end) open tone hole usually has the largest influence on the pipe's effective length. However, closing holes below the first open hole without closing the first hole can also lower the pitch significantly; such cross fingerings may often be useful. Generally, the pitch and timbre of the notes produced will depend on the positions, sizes, heights, and shapes of all the tone holes, both open and closed. Theoretical models allow these effects to be calculated with some accuracy, but the design of tone holes remains to some degree a matter of trial and error.
Most woodwind instruments rely on tone holes to produce different pitches. Two exceptions are the slide whistle and the overtone flutes. Most brass instruments use valves or a slide instead of tone holes, with the rare keyed bugle and the ophicleide as exceptions.

See also
Saxophone tone hole
Organ pipe


== References ==",Category:Articles needing additional references from December 2009,1
38,39,Overtone,"An overtone is any frequency greater than the fundamental frequency of a sound. Using the model of Fourier analysis, the fundamental and the overtones together are called partials. Harmonics, or more precisely, harmonic partials, are partials whose frequencies are numerical integer multiples of the fundamental (including the fundamental which is 1 times itself). These overlapping terms are variously used when discussing the acoustic behavior of musical instruments. (See etymology below.) The model of Fourier analysis provides for the inclusion of inharmonic partials, which are partials whose frequencies are not whole-number ratios of the fundamental (such as 1.1 or 2.14179).

When a resonant system such as a blown pipe or plucked string is excited, a number of overtones may be produced along with the fundamental tone. In simple cases, such as for most musical instruments, the frequencies of these tones are the same as (or close to) the harmonics. Examples of exceptions include the circular drum, – a timpani whose first overtone is about 1.6 times its fundamental resonance frequency, gongs and cymbals, and brass instruments. The human vocal tract is able to produce highly variable amplitudes of the overtones, called formants, which define different vowels.

Explanation
Most oscillators, from a guitar string to a flute, will naturally vibrate at a series of distinct frequencies known as normal modes. The lowest normal mode frequency is known as the fundamental frequency, while the higher frequencies are called overtones. Often, when an oscillator is excited by, for example, plucking a guitar string, it will oscillate at several of its modal frequencies at the same time. So when a note is played, this gives the sensation of hearing other frequencies (overtones) above the lowest frequency (the fundamental).
Timbre is the quality that gives the listener the ability to distinguish between the sound of different instruments. The timbre of an instrument is determined by which overtones it emphasizes. That is to say, the relative volumes of these overtones to each other determines the specific ""flavor"" or ""color"" of sound of that family of instruments. The intensity of each of these overtones is rarely constant for the duration of a note. Over time, different overtones may decay at different rates, causing the relative intensity of each overtone to rise or fall independent of the overall volume of the sound. A carefully trained ear can hear these changes even in a single note. This is why the timbre of a note may be perceived differently when played staccato or legato.
A driven non-linear oscillator, such as the vocal folds, a blown wind instrument, or a bowed violin string (but not a struck guitar string or bell) will oscillate in a periodic, non-sinusoidal manner. This generates the impression of sound at integer multiple frequencies of the fundamental known as harmonics, or more precisely, harmonic partials. For most string instruments and other long and thin instruments such as a bassoon, the first few overtones are quite close to integer multiples of the fundamental frequency, producing an approximation to a harmonic series. Thus, in music, overtones are often called harmonics. Depending upon how the string is plucked or bowed, different overtones can be emphasized.
However, some overtones in some instruments may not be of a close integer multiplication of the fundamental frequency, thus causing a small dissonance. ""High quality"" instruments are usually built in such a manner that their individual notes do not create disharmonious overtones. In fact, the flared end of a brass instrument is not to make the instrument sound louder, but to correct for tube length “end effects” that would otherwise make the overtones significantly different from integer harmonics. This is illustrated by the following:
Consider a guitar string. Its idealized 1st overtone would be exactly twice its fundamental if its length were shortened by ½, perhaps by lightly pressing a guitar string at the 12th fret; however, if a vibrating string is examined, it will be seen that the string does not vibrate flush to the bridge and nut, but it instead has a small “dead length” of string at each end. This dead length actually varies from string to string, being more pronounced with thicker and/or stiffer strings. This means that halving the physical string length does not halve the actual string vibration length, and, hence, the overtones will not be exact multiples of a fundamental frequency. The effect is so pronounced that properly set up guitars will angle the bridge such that the thinner strings will progressively have a length up to few millimeters shorter than the thicker strings. Not doing so would result in inharmonious chords made up of two or more strings. Similar considerations apply to tube instruments.

Musical usage term
An overtone is a partial (a ""partial wave"" or ""constituent frequency"") that can be either a harmonic partial (a harmonic) other than the fundamental, or an inharmonic partial. A harmonic frequency is an integer multiple of the fundamental frequency. An inharmonic frequency is a non-integer multiple of a fundamental frequency.
An example of harmonic overtones: (absolute harmony)
Some musical instruments produce overtones that are slightly sharper or flatter than true harmonics. The sharpness or flatness of their overtones is one of the elements that contributes to their unique sound. Due to phase inconsistencies between the fundamental and the partial harmonic, this also has the effect of making their waveforms not perfectly periodic.
Musical instruments that can create notes of any desired duration and definite pitch have harmonic partials. A tuning fork, provided it is sounded with a mallet (or equivalent) that is reasonably soft, has a tone that consists very nearly of the fundamental, alone; it has a sinusoidal waveform. Nevertheless, music consisting of pure sinusoids was found to be unsatisfactory in the early 20th century.

Etymology
In Hermann von Helmholtz's classic ""On The Sensations Of Tone"" he used the German ""Obertöne"" which was actually a contraction of ""Oberpartialtöne"", or in English: ""upper partial tones"". According to Alexander Ellis (in pages 24–25 of his definitive English translation of Helmholtz), the similarity of German ""ober"" to English ""over"" caused a Prof. Tyndall to mistranslate Helmholtz' term, thus creating ""overtone"". Ellis disparages the term ""overtone"" for its awkward implications. Because ""overtone"" makes the upper partials seem like such a distinct phenomena, it leads to the mathematical problem described above where the first overtone is the second partial. Also, unlike discussion of ""partials"", the word ""overtone"" has connotations that have led people to wonder about the presence of ""undertones"" (a term sometimes confused with ""difference tones"" but also used in speculation about a hypothetical ""undertone series"").

""Overtones"" in choral music
In barbershop music, the word overtone is often used in a related but particular manner. It refers to a psychoacoustic effect in which a listener hears an audible pitch that is higher than, and different from, the fundamentals of the four pitches being sung by the quartet. The barbershop singer's ""overtone"" is created by the interactions of the upper partial tones in each singer's note (and by sum and difference frequencies created by nonlinear interactions within the ear). Similar effects can be found in other a cappella polyphonic music such as the music of the Republic of Georgia and the Sardinian cantu a tenore. Overtones are naturally highlighted when singing in a particularly resonant space, such as a church; one theory of the development of polyphony in Europe holds that singers of Gregorian chant, originally monophonic, began to hear the overtones of their monophonic song and to imitate these pitches - with the fifth, octave, and major third being the loudest vocal overtones, it is one explanation of the development of the triad and the idea of consonance in music.

String instruments
String instruments can also produce multiphonic tones when strings are divided in two pieces or the sound is somehow distorted. The Sitar has sympathetic strings which help to bring out the overtones while one is playing. The overtones are also highly important in the tanpura, the drone instrument in traditional North and South Indian music, in which loose strings tuned at octaves and fifths are plucked and designed to buzz to create sympathetic resonance and highlight the cascading sound of the overtones. Western string instruments, such as the violin, may be played close to the bridge (a technique called ""sul ponticello"" or ""am Steg"") which causes the note to split into overtones while attaining a distinctive glassy, metallic sound. Various techniques of bow pressure may also be used to bring out the overtones, as well as using string nodes to produce natural harmonics. The most well-known technique on a guitar is playing flageolet tones or using distortion effects. The Ancient Chinese instrument the Guqin contains a scale based on the knotted positions of overtones. Also the Vietnamese ?àn b?u functions on flageolet tones. Other multiphonic extended techniques used are prepared piano, prepared guitar and 3rd bridge.

Wind instruments
Wind instruments manipulate the overtone series significantly in the normal production of sound, but various playing techniques may be used to produce multiphonics which bring out the overtones of the instrument. On many woodwind instruments, alternate fingerings are used. ""Overblowing"", or adding intensely exaggerated air pressure, can also cause notes to split into their overtones. In brass instruments, multiphonics may be produced by singing into the instrument while playing a note at the same time, causing the two pitches to interact - if the sung pitch is at specific harmonic intervals with the played pitch, the two sounds will blend and produce additional notes by the phenomenon of sum and difference tones. Non-western wind instruments also exploit overtones in playing, and some may highlight the overtone sound exceptionally. Instruments like the didgeridoo are highly dependent on the interaction and manipulation of overtones achieved by the performer changing their mouth shape while playing, or singing and playing simultaneously. Likewise, when playing a harmonica or pitch pipe, one may alter the shape of their mouth to amplify specific overtones. Though not a wind instrument, a similar technique is used for playing the jaw harp: the performer amplifies the instrument's overtones by changing the shape, and therefore the resonance, of their vocal tract.

Overtone singing
Overtone singing, also called harmonic singing, occurs when the singer amplifies voluntarily two overtones in the sequence available given the fundamental tone he/she is singing. Overtone singing is a traditional form of singing in many parts of the Himalayas and Altay; Tibetans, Mongols and Tuvans are known for their overtone singing. In these contexts it is often referred to as throat singing or khoomei, though it should not be confused with Inuit throat singing, which is produced by different means.

See also
Harmonic series (music)
Harmonics
Subharmonic
Just intonation
Xenharmonic
Stretched octave
Combination tone
Electronic tuner
Scale of harmonics
Overtone band (in vibrational spectroscopy)

References
External links
Overtones, partials and harmonics from fundamental frequency",Category:Acoustics,1
39,40,Reverse echo,"Reverse echo or reverse reverb, also known as backwards echo and reverse regeneration, is a sound effect created as the result of recording an echo or delayed signal of an audio recording played backwards. The original recording is then played forwards accompanied by the recording of the echo or delayed signal which now precedes the original signal.

Development
Guitarist and producer Jimmy Page claims to have invented the effect, stating that he originally developed the method when recording the single ""Ten Little Indians"" with The Yardbirds in 1967. He later used it on a number of Led Zeppelin tracks, including ""You Shook Me"", ""Whole Lotta Love"", and their cover of ""When the Levee Breaks"". In an interview he gave to Guitar World magazine in 1993, Page explained:
Despite Page's claims, an earlier example of the effect can distinctly be heard towards the end of the 1966 Lee Mallory single ""That's the Way It's Gonna Be"", produced by Curt Boettcher.

Usage in music
Reverse reverb is commonly used in shoegaze, particularly by such bands as My Bloody Valentine and Spacemen 3. Reverse echo is also often used as a lead-in to vocal passages in hardstyle music, and various forms of EDM and pop music. The reverse echo or reverb is applied to the first word or syllable of the vocal for a build-up effect or other-worldly sound.

Use in other media
Reverse echo has been used in filmmaking and television production for an otherworldly effect on voices, especially in horror movies.


== References ==",Category:Audio engineering,1
40,41,Harmonic,"A harmonic is any member of the harmonic series, a divergent infinite series. Its name derives from the concept of overtones, or harmonics in musical instruments: the wavelengths of the overtones of a vibrating string or a column of air (as with a tuba) are derived from the string's (or air column's) fundamental wavelength. Every term of the series (i.e., the higher harmonics) after the first is the ""harmonic mean"" of the neighboring terms. The phrase ""harmonic mean"" likewise derives from music.
The term is employed in various disciplines, including music, physics, acoustics, electronic power transmission, radio technology, and other fields. It is typically applied to repeating signals, such as sinusoidal waves. A harmonic of such a wave is a wave with a frequency that is a positive integer multiple of the frequency of the original wave, known as the fundamental frequency. The original wave is also called the 1st harmonic, the following harmonics are known as higher harmonics. As all harmonics are periodic at the fundamental frequency, the sum of harmonics is also periodic at that frequency. For example, if the fundamental frequency is 50 Hz, a common AC power supply frequency, the frequencies of the first three higher harmonics are 100 Hz (2nd harmonic), 150 Hz (3rd harmonic), 200 Hz (4th harmonic) and any addition of waves with these frequencies is periodic at 50 Hz.

An nth characteristic mode, for n > 1, will have nodes that are not vibrating. For example, the 3rd characteristic mode will have nodes at 
  
    
      
        
          
            
              1
              3
            
          
        
      
    
    {\displaystyle {\tfrac {1}{3}}}
  L and 
  
    
      
        
          
            
              2
              3
            
          
        
      
    
    {\displaystyle {\tfrac {2}{3}}}
  L, where L is the length of the string. In fact, each nth characteristic mode, for n a multiple of 3, will not have nodes at these points. These other characteristic modes will be vibrating at the positions 
  
    
      
        
          
            
              1
              3
            
          
        
      
    
    {\displaystyle {\tfrac {1}{3}}}
  L and 
  
    
      
        
          
            
              2
              3
            
          
        
      
    
    {\displaystyle {\tfrac {2}{3}}}
  L. If the player gently touches one of these positions, then these other characteristic modes will be suppressed. The tonal harmonics from these other characteristic modes will then also be suppressed. Consequently, the tonal harmonics from the nth characteristic modes, where n is a multiple of 3, will be made relatively more prominent.

In music, harmonics are used on string instruments and wind instruments as a way of producing sound on the instrument, particularly to play higher notes and, with strings, obtain notes that have a unique sound quality or ""tone colour"". On strings, harmonics that are bowed have a ""glassy"", pure tone. On stringed instruments, harmonics are played by touching (but not fully pressing down the string) at an exact point on the string while sounding the string (plucking, bowing, etc.); this allows the harmonic to sound, a pitch which is always higher than the fundamental frequency of the string.

Terminology
Harmonics may also be called ""overtones"", ""partials"" or ""upper partials"". The difference between ""harmonic"" and ""overtone"" is that the term ""harmonic"" includes all of the notes in a series, including the fundamental frequency (e.g., the open string of a guitar). The term ""overtone"" only includes the pitches above the fundamental. In some music contexts, the terms ""harmonic"", ""overtone"" and ""partial"" are used fairly interchangeably.

Characteristics
A whizzing, whistling tonal character, distinguishes all the harmonics both natural and artificial from the firmly stopped intervals; therefore their application in connection with the latter must always be carefully considered.

Most acoustic instruments emit complex tones containing many individual partials (component simple tones or sinusoidal waves), but the untrained human ear typically does not perceive those partials as separate phenomena. Rather, a musical note is perceived as one sound, the quality or timbre of that sound being a result of the relative strengths of the individual partials. Many acoustic oscillators, such as the human voice or a bowed violin string, produce complex tones that are more or less periodic, and thus are composed of partials that are near matches to integer multiples of the fundamental frequency and therefore resemble the ideal harmonics and are called ""harmonic partials"" or simply ""harmonics"" for convenience (although it's not strictly accurate to call a partial a harmonic, the first being real and the second being ideal).
Oscillators that produce harmonic partials behave somewhat like one-dimensional resonators, and are often long and thin, such as a guitar string or a column of air open at both ends (as with the modern orchestral transverse flute). Wind instruments whose air column is open at only one end, such as trumpets and clarinets, also produce partials resembling harmonics. However they only produce partials matching the odd harmonics, at least in theory. The reality of acoustic instruments is such that none of them behaves as perfectly as the somewhat simplified theoretical models would predict.
Partials whose frequencies are not integer multiples of the fundamental are referred to as inharmonic partials. Some acoustic instruments emit a mix of harmonic and inharmonic partials but still produce an effect on the ear of having a definite fundamental pitch, such as pianos, strings plucked pizzicato, vibraphones, marimbas, and certain pure-sounding bells or chimes. Antique singing bowls are known for producing multiple harmonic partials or multiphonics.   Other oscillators, such as cymbals, drum heads, and other percussion instruments, naturally produce an abundance of inharmonic partials and do not imply any particular pitch, and therefore cannot be used melodically or harmonically in the same way other instruments can.

Partials, overtones, and harmonics
An overtone is any partial higher than the lowest partial in a compound tone. The relative strengths and frequency relationships of the component partials determine the timbre of an instrument. The similarity between the terms overtone and partial sometimes leads to their being loosely used interchangeably in a musical context, but they are counted differently, leading to some possible confusion. In the special case of instrumental timbres whose component partials closely match a harmonic series (such as with most strings and winds) rather than being inharmonic partials (such as with most pitched percussion instruments), it is also convenient to call the component partials ""harmonics"" but not strictly correct (because harmonics are numbered the same even when missing, while partials and overtones are only counted when present). This chart demonstrates how the three types of names (partial, overtone, and harmonic) are counted (assuming that the harmonics are present):
In many musical instruments, it is possible to play the upper harmonics without the fundamental note being present. In a simple case (e.g., recorder) this has the effect of making the note go up in pitch by an octave, but in more complex cases many other pitch variations are obtained. In some cases it also changes the timbre of the note. This is part of the normal method of obtaining higher notes in wind instruments, where it is called overblowing. The extended technique of playing multiphonics also produces harmonics. On string instruments it is possible to produce very pure sounding notes, called harmonics or flageolets by string players, which have an eerie quality, as well as being high in pitch. Harmonics may be used to check at a unison the tuning of strings that are not tuned to the unison. For example, lightly fingering the node found halfway down the highest string of a cello produces the same pitch as lightly fingering the node ?1?3 of the way down the second highest string. For the human voice see Overtone singing, which uses harmonics.
While it is true that electronically produced periodic tones (e.g. square waves or other non-sinusoidal waves) have ""harmonics"" that are whole number multiples of the fundamental frequency, practical instruments do not all have this characteristic. For example, higher ""harmonics""' of piano notes are not true harmonics but are ""overtones"" and can be very sharp, i.e. a higher frequency than given by a pure harmonic series. This is especially true of instruments other than stringed or brass/woodwind ones, e.g., xylophone, drums, bells etc., where not all the overtones have a simple whole number ratio with the fundamental frequency. The fundamental frequency is the reciprocal of the period of the periodic phenomenon.

On stringed instruments
Harmonics may be singly produced [on stringed instruments] (1) by varying the point of contact with the bow, or (2) by slightly pressing the strig at the nodes, or divisions of its aliquot parts (
  
    
      
        
          
            
              1
              2
            
          
        
      
    
    {\displaystyle {\tfrac {1}{2}}}
  , 
  
    
      
        
          
            
              1
              3
            
          
        
      
    
    {\displaystyle {\tfrac {1}{3}}}
  , 
  
    
      
        
          
            
              1
              4
            
          
        
      
    
    {\displaystyle {\tfrac {1}{4}}}
  , etc.). (1) In the first case, advancing the bow from the usual place where the fundamental note is produced, towards the bridge, the whole scale of harmonics may be produced in succession, on an old and highly resonant instrument. The employment of this means produces the effect called 'sul ponticello.' (2) The production of harmonics by the slight pressure of the finger on the open string is more useful. When produced by pressing slightly on the various nodes of the open strings they are called 'Natural harmonics.' ... Violinists are well aware that the longer the string in proportion to its thickness, the greater the number of upper harmonics it can be made to yield.

The following table displays the stop points on a stringed instrument, such as the guitar (guitar harmonics), at which gentle touching of a string will force it into a harmonic mode when vibrated. String harmonics (flageolet tones) are described as having a ""flutelike, silvery quality"" that can be highly effective as a special color or tone color (timbre) when used and heard in orchestration. It is unusual to encounter natural harmonics higher than the fifth partial on any stringed instrument except the double bass, on account of its much longer strings. Harmonics are widely used in plucked string instruments, such as acoustic guitar, electric guitar and electric bass. On an electric guitar played loudly through a guitar amplifier with distortion, harmonics are more sustained and can be used in guitar solos. In the heavy metal music lead guitar style known as shred guitar, harmonics, both natural and artificial, are widely used.

Table
Artificial harmonics
Although harmonics are most often used on open strings (natural harmonics), occasionally a score will call for an artificial harmonic, produced by playing an overtone on an already stopped string. As a performance technique, it is accomplished by using two fingers on the fingerboard, the first to shorten the string to the desired fundamental, with the second touching the node corresponding to the appropriate harmonic. On fretted instruments, such as an electric guitar, the performer can look at the frets to determine where to stop the string and where to touch the node. On unfretted instruments, such as the violin and related instruments, playing artificial harmonics is an advanced technique, as it requires the performer to find two precise locations on the same string.

Other information
Harmonics may be either used or considered as the basis of just intonation systems. Composer Arnold Dreyblatt is able to bring out different harmonics on the single string of his modified double bass by slightly altering his unique bowing technique halfway between hitting and bowing the strings. Composer Lawrence Ball uses harmonics to generate music electronically.

See also
Aristoxenus
Harmonics (electrical power)
Electronic tuner
Formant
Fourier series
Guitar harmonic
Harmonic oscillator
Harmony
Pure tone
Pythagorean tuning
Scale of harmonics
Spherical harmonics
Stretched octave
Subharmonic
Xenharmonic music

References
External links
Harmonics, partials and overtones from fundamental frequency
Discussion of Sciarrino's violin etudes and notation issues
 Chisholm, Hugh, ed. (1911). ""Harmonic"". Encyclopædia Britannica (11th ed.). Cambridge University Press. 
Harmonics
Hear and see harmonics on a Piano",Category:Telecommunications,1
41,42,Combination tone,"A combination tone (also called resultant or subjective tone) is a psychoacoustic phenomenon of an additional tone or tones that are artificially perceived when two real tones are sounded at the same time. Their discovery is credited to the violinist Giuseppe Tartini (although he was not the first, see Sorge) and so are also called Tartini tones.
There are two types of combination tones: sum tones whose frequencies are found by adding the frequencies of the real tones and difference tones whose frequencies are the difference between the frequencies of the real tones. ""Combination tones are heard when two pure tones (i.e., tones produced by simple harmonic sound waves having no overtones), differing in frequency by about 50 cycles per second [Hertz] or more, sound together at sufficient intensity.""
Combination tones can also be produced electronically by combining two signals in a circuit that has nonlinear distortion, such as an amplifier subject to clipping or a ring modulator.

Explanation
One way a difference tone can be heard is when two tones with fairly complete sets of harmonics make a just fifth. This can be explained as an example of the missing fundamental phenomenon. If 
  
    
      
        f
      
    
    {\displaystyle f}
   is the missing fundamental frequency, then 
  
    
      
        2
        f
      
    
    {\displaystyle 2f}
   would be the frequency of the lower tone, and its harmonics would be 
  
    
      
        4
        f
        ,
        6
        f
        ,
        8
        f
        ,
      
    
    {\displaystyle 4f,6f,8f,}
   etc. Since a fifth corresponds to a frequency ratio of 2:3, the higher tone and its harmonics would then be 
  
    
      
        3
        f
        ,
        6
        f
        ,
        9
        f
        ,
      
    
    {\displaystyle 3f,6f,9f,}
   etc. When both tones are sounded, there are components with frequencies of 
  
    
      
        2
        f
        ,
        3
        f
        ,
        4
        f
        ,
        6
        f
        ,
        8
        f
        ,
        9
        f
        ,
      
    
    {\displaystyle 2f,3f,4f,6f,8f,9f,}
   etc. The missing fundamental is heard because so many of these components refer to it.
The specific phenomenon that Tartini discovered was physical. Sum and difference tones are thought to be caused sometimes by the non-linearity of the inner ear. This causes intermodulation distortion of the various frequencies which enter the ear. They are combined linearly, generating relatively faint components with frequencies equal to the sums and differences of whole multiples of the original frequencies. Any components which are heard are usually lower, with the most commonly heard frequency being just the difference tone, 
  
    
      
        
          f
          
            2
          
        
        ?
        
          f
          
            1
          
        
      
    
    {\displaystyle f_{2}-f_{1}}
  , though this may be a consequence of the other phenomena. Although much less common, the following frequencies may also be heard:

  
    
      
        2
        
          f
          
            1
          
        
        ?
        
          f
          
            2
          
        
        ,
        3
        
          f
          
            1
          
        
        ?
        2
        
          f
          
            2
          
        
        ,
        …
        ,
        
          f
          
            1
          
        
        ?
        k
        (
        
          f
          
            2
          
        
        ?
        
          f
          
            1
          
        
        )
      
    
    {\displaystyle 2f_{1}-f_{2},3f_{1}-2f_{2},\ldots ,f_{1}-k(f_{2}-f_{1})}
  
For a time it was thought that the inner ear was solely responsible whenever a sum or difference tone was heard. However, experiments show evidence that even when using headphones providing a single pure tone to each ear separately, listeners may still hear a difference tone. Since the peculiar, non-linear physics of the ear doesn't come into play in this case, it is thought that this must be a separate, neural phenomenon. Compare binaural beats.
Heinz Bohlen proposed what is now known as the Bohlen–Pierce scale on the basis of combination tones, as well as the 833 cents scale.

Resultant tone
A resultant tone (obsolete) is ""produced when any two loud and sustained musical sounds are heard at the same time.""
In pipe organs, this is done by having two pipes, one pipe of the note being played, and another harmonically related, typically at its fifth, being sounded at the same time. The result is a pitch at a common subharmonic of the pitches played (one octave below the first pitch when the second is the fifth, 3:2, two octaves below when the second is the major third, 5:4). This effect is useful especially in the lowest ranks of the pipe organ where cost or space could prohibit having a rank of such low pitch. For example, a 64' pipe would be costly and take up at least 32' of space (if capped) for each pipe. Using a resultant tone for such low pitches would eliminate the cost and space factor, but would not sound as full as a true 64' pipe.
This effect is most often used in the lowest octave of the organ only. It can vary from highly effective to disappointing depending on several factors, primarily the skill of the organ voicer, and the acoustics of the room the instrument is installed in.

See also
Harmonic series (music)
Missing fundamental
Power chord#Analysis
Ring modulation

References
Further reading
Adrianus J. M. Houtsma, Julius L. Goldstein, ""Percepetion of Musical Intervals: Evidence for the Central Origin of the Pitch of Complex Tones"", Massachusetts Institute of Technology, Research Laboratory of Electronics, Technical Report 484, October 1, 1971.

External links
Titchener Difference Tones Training
Difference tones on the harmonica
Pitch Perception Lecture Notes
Tartini computer program. Uses combination tones for pitch recognition. If certain intervals are played in double-stop, the program can display its Tartini-tone.
http://www.organstops.org/r/Resultant.html",Category:Articles with unsourced statements from October 2007,1
42,43,Statistical energy analysis,"Statistical energy analysis (SEA) is a method for predicting the transmission of sound and vibration through complex structural acoustic systems. The method is particularly well suited for quick system level response predictions at the early design stage of a product, and for predicting responses at higher frequencies. In SEA a system is represented in terms of a number of coupled subsystems and a set of linear equations are derived that describe the input, storage, transmission and dissipation of energy within each subsystem. The parameters in the SEA equations are typically obtained by making certain statistical assumptions about the local dynamic properties of each subsystem (similar to assumptions made in room acoustics and statistical mechanics). These assumptions significantly simplify the analysis and make it possible to analyze the response of systems that are often too complex to analyze using other methods (such as finite element and boundary element methods).

History
The initial derivation of SEA arose from independent calculations made in 1959 by Richard Lyon and Preston Smith as part of work concerned with the development of methods for analyzing the response of large complex aerospace structures subjected to spatially distributed random loading. Lyon's calculation showed that under certain conditions, the flow of energy between two coupled oscillators is proportional to the difference in the oscillator energies (suggesting a thermal analogy exists in structural-acoustic systems). Smith's calculation showed that a structural mode and a diffuse reverberant sound field attain a state of 'equipartition of energy' as the damping of the mode is reduced (suggesting a state of thermal equilibrium can exist in structural-acoustic systems). The extension of the two oscillator results to more general systems is often referred to as the modal approach to SEA. While the modal approach provides physical insights into the mechanisms that govern energy flow it involves assumptions that have been the subject of considerable debate over many decades. In recent years, alternative derivations of the SEA equations based on wave approaches have become available. Such derivations form the theoretical foundation behind a number of modern commercial SEA codes and provide a general framework for calculating the parameters in an SEA model. A number of methods also exist for post-processing FE models to obtain estimates of SEA parameters. Lyon mentioned the use of such methods in his initial SEA text book in 1975 but a number of alternative derivations have been presented over the years

Method
To solve a noise and vibration problem with SEA, the system is partitioned into a number of components (such as plates, shells, beams and acoustic cavities) that are coupled together at various junctions. Each component can support a number of different propagating wavetypes (for example,the bending, longitudinal and shear wavefields in a thin isotropic plate). From an SEA point of view, the reverberant field of each wavefield represents an orthogonal store of energy and so is represented as a separate energy degree of freedom in the SEA equations. The energy storage capacity of each reverberant field is described by a parameter termed the 'modal density', which depends on the average speed with which waves propagate energy through the subsystem (the average group velocity), and the overall dimension of the subsystem. The transmission of energy between different wavefields at a given type of junction is described by parameters termed 'coupling loss factors'. Each coupling loss factor describes the input power to the direct field of a given receiving subsystem per unit energy in the reverberant field of a particular source subsystem. The coupling loss factors are typically calculated by considering the way in which waves are scattered at different types of junctions (for example, point, line and area junctions). Strictly, SEA predicts the average response of a population or ensemble of systems and so the coupling loss factors and modal densities represent ensemble average quantities. To simplify the calculation of the coupling loss factors it is often assumed that there is significant scattering within each subsystem (when viewed across an ensemble) so that direct field transmission between multiple connections to the same subsystem is negligible and reverberant transmission dominates. In practical terms, this means that SEA is often best suited for problems in which each subsystem is large compared with a wavelength (or from a modal point of view, each subsystem contains several modes in a given frequency band of interest). The SEA equations contain a relatively small number of degrees of freedom and so can be easily inverted to find the reverberant energy in each subsystem due to a given set of external input powers. The (ensemble average) sound pressure levels and vibration velocities within each subsystem can then be obtained by superimposing the direct and reverberant fields within each subsystem.

Applications
Over the past half century, SEA has found applications in virtually every industry for which noise and vibration are of concern. Typical applications include:
Interior noise prediction and sound package design in automotive, aircraft, rotorcraft and train applications
Interior and exterior radiated noise in marine applications
Prediction of dynamic environments in launch vehicles and spacecraft
Prediction of noise from consumer goods such as dishwashers, washing machines and refrigerators
Prediction of noise from generators and industrial chillers
Prediction of air-borne and structure-borne noise through buildings
Design of enclosures etc.
Additional examples can be found in the proceedings of conferences such as INTERNOISE, NOISECON, EURONOISE, ICSV, NOVEM, SAE N&V.

Software implementations
Several commercial solutions for Statistical Energy Analysis are available:
SEAM, SEAM 3D from Cambridge Collaborative Inc. USA
wave6 from wave six LLC 
VA One SEA Module (previously AutoSEA) from ESI Group, France
GSSEA-Light from Gothenburg Sound AB, Sweden
SEA+ from InterAC, France distributed by LMS International
Free solutions:
Statistical Energy Analysis Freeware,
SEAlab - open code in Matlab/Octave from Applied Acoustics, Chalmers, Sweden (open source)


== References ==",Category:Acoustics,1
43,44,Category:Acoustics software,,Category:Audio software,1
44,45,Sound masking,"Sound masking is the addition of sound created by special digital generators and distributed by normally unseen speakers through an area to reduce distractions or provide confidentiality where needed. The sound is broad band random that conveys no information about itself to a listener. It is often referred to erroneously as white noise or pink noise; the sound spectrum and level is specially shaped to provide the degree of privacy desired by occupants. Masking operates by covering up or masking unwanted sounds, similar to perfume that covers up other odors. This is in contrast to the technique of active noise control which attempts to eliminate the unwanted sound. Sound masking is used in homes, commercial offices, medical facilities, court rooms, and in secure facilities to provide secrecy.

The Need for Sound Masking
Based on Sound Masking Done Right .

Effects of Noise on People
A seminal work covers the subject in some detail. Noise is defined as unwanted sound. It can have three effects depending mostly on level. At high levels, there are mechanical changes in a person, such as heating of the skin, rupture of the eardrum, or vibration of the eyeballs or internal organs. At lower levels, there are physiological (biological) changes in a person, such as elevation of blood pressure, or stress. At still lower levels, the changes are psychological (subjective) such as annoyance and complaints. Annoyance is based on factors such as the person's evaluation of the necessity of the noise, or whether it can be controlled, or whether it is normal for the environment (see Common Opinions about Sound). The levels of sound masking are sufficiently low that they have no known physical or physiological effects on people. One aim of sound masking design is to make the sound be ""normal"", i.e., acceptable.

Studies of Noise in the Office Environment
Since most sound masking is used in offices, a number of cognitive psychology studies have been made that relate specifically to the office environment. One study found that there was a modest stress (physiological) increase and diminished motivation caused by typical office noises, including speech. It is recommended that the use of sound masking is under the control of the worker. Another study suggested that changes in level are an important factor, but that habituation to the noise can occur. In the office, habituation can be interpreted to mean ""I’ve grown used to the noise and it no longer distracts me"" or ""Since I cannot do anything about it, I will have to live with it."" Another study points out that the specific information within the speech intrusion is not important nor is the ""intensity"" (level) of the sound between 48 and 76 dBA. Since the energy level of the louder sound was 1,000 times that of the least, one must assume that distraction occurred for all levels. For arithmetic tasks, both speech and non-verbal intrusive noises caused significant performance decreases. For ""prose tasks"" it was found that speech caused a greater performance decrease than nonverbal noises. In another study  the author added several significant observations. It was found that ""during a serial recall task, the accuracy of report decreases 30 to 50%."" When the intrusive speech was increasingly filtered to a meaningless mumble, there was a monotonic increase in performance. Finally, the author states: ""Perhaps the single feature that makes the irrelevant speech phenomena so fascinating is that the processing of sound is obligatory; it appears beyond the individual’s control."" Within the references cited above are further references to earlier works on this subject. There are several implications for a sound masking system. The masking must reduce the difference between the steady background level and the transient levels associated with both speech and other sounds. Motivation and productivity are improved when this is accomplished. The masking sound itself must not change rapidly and should be as meaningless as possible.

Common Opinions about Sound
Sound masking must satisfy the persons that listen to it. People ask themselves a number of questions about the acoustical environment. The following questions were deduced from employee comments about their office environment. These are questions the listeners implicitly ask themselves to determine their response to their environment. The design of a sound masking system must take these opinions into account.
Is the sound made by me or made on my behalf?
Is the sound ""normal"" for this environment?
Is the sound necessary and can anything be done to control it?
Does the sound have meaning?
Is the sound frightening?
Will the sound have an adverse effect on my health?
What is the pitch of the sound?
How reverberant is the room?

Complaints about Noise
Since sound masking is a shaped random sound, often erroneously called ""white noise"", It is important for a sound masking system to dispel that this sound is actually noise. The most important finding by Kryter is encapsulated by this statement:
""The general finding that the performance of the more anxious personality types is more affected by noise than that of nonanxious types would attest to the existence of a stimulus-contingency factor. In terms of learning or conditioning, the task becomes disliked and is performed relatively poorly because it is related to or contingent upon the aversive noise.""
""A possible teaching of much of the data presented in this book is that, other than as a damaging agent to the ear and as a masker of auditory information, noise will not harm the organism or interfere with mental or motor performance.""
In well designed sound masking systems, distraction caused by extraneous conversations (loss of privacy) far outweighs any negative response to the sound masking

The Quest for Quiet
The biggest problem with sound masking is that when people are annoyed by the activity sounds around them (noise), they search for ""quiet."" They believe that ""quiet"" is a desirable condition of low background sound level, but what they are really searching for is the freedom from the acoustical distractions that ultimately cause annoyance. The only way to achieve true quiet would be to maintain a low background sound level with no transient sounds; a condition that requires complete isolation from all activity sounds. A better definition of ""quiet"" would be the absence of distracting sounds, not the absence of all sound. This is the definition used in sound masking.

Steady vs. Transient Sounds
Steady sounds are the background sounds in any environment that are reasonably continuous and long term. If a steady sound persists for a long time without change, and the level is relatively low, persons generally accept it as normal. People are seldom aware that an outdoor background sound exists. Steady sound can be tonal or random. If the sound is tonal it will create more annoyance than a random sound of the same level since the latter conveys no information to the listener. Transient sounds are conversation, paging, machine sounds as well as exterior sounds such as passing aircraft and road traffic. They are short term, can vary considerably in level, and generally distract a person's attention if the level is high relative to the steady sound level (a rise of about 10 dB is a common criterion). The distraction is further strengthened if the sound has high information content, such as conversation. At relatively low levels, the major concern is the psychological effect of distraction and annoyance. The primary use of sound masking is to reduce the distraction associated with transient sounds, and in some cases reduce the intelligibility of those transient sounds (closed offices, secure facilities).

What is sound masking?
In the same way that physical masks cover the wearer's face, sound maskers cover up, hide, or disguise other sounds but do not change or eliminate them. True noise cancellation that eliminates sound works only within spatially constrained areas, such as headphones, and cannot be applied to entire rooms. Thus, in larger spaces, one may instead mask distracting sounds, such as conversations, by raising the ambient background sounds up to or above the level of the expected intrusive ones. In the home, the level needed to mask sounds is low, and at sporting events, that level is high. The goal is to provide fewer distractions without the masking sound itself becoming a distraction, which requires persons experienced with this technique to find that optimum level.

The Early History of Sound Masking
It is likely that primitive people did not want acoustical privacy, so they never camped near a rushing stream. They understood that stream noise would mask the approach of enemies or predators. The sound of fountains in Roman villas certainly served to mask the sounds of iron-rimmed chariot wheels on the cobble-stoned streets. Fountain masking has carried over to shopping malls or buildings with large atria. There are stories of a dentist, in the 1940s, applying random sound to patient’s ears through earphones to mask the terrible noise of slow speed drills. An example of a self-contained masker, made in the 1960s, is shown in the figure on the right. The application of electronics and the advent of the open office resulted in the rapid evolution of sound masking. The Quickborner Team of Germany introduced the concept of the open office to the United States in the late 1960s. Geiger-Hamme Laboratories developed a standard for open office acoustics in the 1970s. It was sponsored by the Public Building Service of the General Services Administration for use in open government offices. It included a requirement for sound masking. Many of the major furniture manufacturers, such as Herman Miller, Steelcase, and Haworth, converted much of their production to products for the open office. Herman Miller was the first to have self-contained maskers mounted pointing up on top of open office furniture panels. It did not survive primarily due to the presence of controls available to employees. Owens Corning, a manufacturer of fiberglass products, entered the open office market and introduced a centralized masking system using a speaker called the Sweeny baffle. This speaker was unusual in that the sound spectrum on axis was the same as the electrical spectrum. Unfortunately, off axis the spectrum was different; it no longer exists. Manufacturers of commercial sound systems entered the market in the 1970s; masking was an add-on to their other audio products. Soundolier (now part of Atlas Sound) sold a self contained masker that has survived until recently. The Dukane Corporation sold a masker that had two speakers contained in a heavy triangular enclosure. Companies that considered sound masking as their primary business came into existence about that time. One product, the Lahti masker, was a speaker mounted on the surface of a plastic sphere. Dynasound, Inc. introduced a masker that had a speaker mounted on the lid of two gallon paint can; the handle was used for plenum mounting. K.R. Moeller Associates sold a self-contained masker called Scamp, while the Lencore Corporation sold an equivalent unit, now called Spectra. A document was published in 1980 by the Defense Intelligence Agency. It concerned protection of secure facilities from deliberate audio surveillance; sound masking was one means of protection. Dynasound, Inc. developed a vibration device that could be attached to various surfaces such as doors, walls, and windows, to provide sound masking. In the 1980s, the Bertagni family developed a speaker that could not be distinguished from a fiberglass ceiling tile. The invisibility aspect was favorable to architects. The attempt was made to use this speaker for sound masking, but the cost and the sound radiating characteristics limited its use for that purpose. Armstrong World Industries, a manufacturer of ceiling materials, developed a similar speaker. This speaker has survived as a product of Sound Advance and is now used for applications other than sound masking. An early attitude among owners, designers, and architects was that masking was an excuse for a bad open office design. In early systems, the installers were not knowledgeable about how to provide privacy. As a result, many systems merely provided more noise and were shut off. This was countered by the rise of firms who specialized in the design, installation, and equalization, of masking systems. The evolution of sound masking since the 1980s is described in other sections.

Why Sound Masking is used
Sound masking has some unique advantages over traditional methods of reducing distracting sounds.
Sound masking is dynamic (variable) as are the sounds it is intended to block. Building elements that provide sound attenuation are static (fixed) and cannot adapt to intruding sounds that change in level. Masking can vary from location to location as well as from time to time, to adapt to changing environmental conditions.
Sound masking is by far the least expensive tool for providing privacy.
Sound masking systems are special audio systems that create spatially uniform sound levels. The uniformity can be used to integrate music and paging into a system.
Sound masking works at the listener's ear and is independent of the building structure (acoustically) so concerns about how distracting sounds get from one place to another is unimportant. Structural modifications require such knowledge and are more costly.
It helps to overcome neighborhood noises, snoring sounds from other family members, It provides soothing sleep inducing sounds for babies, afternoon and day time naps are more comfortable; it creates a noise free environment for reading or studying. It provides a personalized and discreet environment for confidential conversations. There are two groups of people that should not be exposed to sound masking: those with significant hearing loss and those with very limited vision. The first group already have considerable privacy which results in masking providing too much privacy. Visually impaired persons use acoustical cues to navigate; sound masking can remove those cues.

Applications of Sound Masking
Commercial Facilities
A number of studies of office acoustics have been done.

Closed offices
Closed offices and conference rooms often appear to provide confidentiality but actually may not. Lightweight, or movable, walls are more sound transparent and most do not extend to the ceiling deck, so speech can pass into the ceiling plenum and then to the next office. Sound masking can be used to make up for such acoustical weaknesses.

Open offices
Open offices can have a background sound level that is too low. Sound level restrictions of air handling units are increasingly stricter. The conversations of others can be clearly understood. Sound masking raises the background level to make up for the absence of walls that would otherwise block the sound.

Less Common Applications
Although the above applications relate to acoustical privacy within an office, there have been applications where sound masking was used to create privacy from sounds exterior to the office. Examples are privacy from elevated freeway traffic, continual siren use in cities, sound from the floor above, and local construction noise. Sound masking is used in court rooms to prevent jurors from hearing attorney conversations with the judge at his bench.

Medical Facilities
Regulations
The Congress of the United States passed the Health Insurance Portability and Accountability Act (HIPAA) into law. It mandates that individually identifiable patient health information be protected. Although written and computer files are obviously to be protected, verbal information must also be protected. ""Covered entities"" (those who must comply with the law) must make reasonable efforts to safeguard patient information from being overheard. The law itself gives no specific guidance on how this is to be accomplished, but a document released by the Department of Health and Human Services provides some clarification. It includes, as part of the protection, the phrase ""health information whether it is on paper, in computers, or communicated orally"". The Office of Civil Rights also has published a document on this issue, stating that the law does not require retrofitting spaces, such as soundproofing of rooms, in order to comply. As a result, many medical facilities have already realized that compliance is wise and have begun retrofitting their facilities. Experience has suggested that most hospital rooms do not need sound proofing, but can benefit from sound masking.

Noise Problems
Noise in hospitals has been a problem for at least fifty years, in part because of the need to have all surfaces hard and cleanable. A large number of measurements and reports in prestigious journals have established the problem  From the patient’s viewpoint the problem has been the distraction and annoyance caused by the noise of people, which results in less rest, poorer sleep, and possibly longer recuperation time. The increased socialization now permitted in hospitals, as well as the increased use of medical machinery, has exacerbated the problem. An extensive survey by the Public Health Service in 1963 showed that patients were frequently disturbed by speech and distress sounds in other rooms as well as staff visits during night hours. Other studies concerned the interference of sleep and recuperation by noise. One study found that the amount and rate of increase in the sound level from the constant background was the main contributor to full awakening or changes in the stage of sleep. It was determined that the magnitude of the change in level, regardless of its median value, was more significant than the level of a steady sound of the same median value. This conclusion was supported by an Environmental Protection Agency document. Suter  expanded this finding by stating ""it is clear that intermittent and impulsive noise is more disturbing than continuous noise of equivalent energy, and that meaningful sounds are more likely to produce sleep disruption than sounds with neutral content."" These conclusions were the same as those found for open offices.

Sound Masking in Medical Facilities
The finding of researchers has shown that the privacy problem in medical facilities is very similar to that in open offices. As a result, sound masking has been used beneficially in a number of locations. Patient rooms, corridors, and nursing areas of hospitals are prime locations. Sound masking can be used in retirement and rehabilitation centers. It is also beneficial in medical suites or patient contact areas of medical insurance providers. Pharmacies can also provide confidential privacy at contact areas with sound masking.

Secure Facilities
Secure facilities require more care when sound masking is used. For commercial offices and even for medical facilities, the listener for confidential conversations are presumed to be casual or accidental. For secure facilities, the listener is presumed to be deliberate and may make use of sophisticated technical listening devices. Many government facilities have made use of structural solutions, i.e., rooms (room-within-a-room) that are shielded from vibration, acoustical, and electromagnetic surveillance. Unfortunately, not all secret conversations take place in such rooms. A less obvious weakness in secure rooms is that modern listening devices can be placed in locations that the building structure cannot protect against (inside wall cavities or remote detection of window vibration). Another weakness in rooms of this type is that designers may presume speech is on a controlled, but low, level. Public address systems, speaker phones, and audio/video presentations require additional protection.

Standards
There are many unclassified government documents that define how secure rooms are to be designed. One type of facility is called a SCIF (Secure Compartmented Information Facility); there are others. There are likely classified documents as well. In almost all of the documents, sound masking is one recommended audio protection method. There is similar standard that applies to financial institutions.

Categories of Surveillance
There are two categories; each must be addressed differently. Uncontrolled areas are those where the persons attempting to protect themselves have little or no control over the surrounding environment. This might be all areas outside the building in which the secure room resides, such as parking lots or other public spaces where it is possible to gain access without detection. Controlled areas are those within the building where the occupant has a measure of control.

Types of Masking Signals
Unlike sound masking in offices or hospitals, it is necessary to consider that sophisticated listeners may have technology to recover speech buried in masking sound. To inhibit such devices, it may be necessary to provide layered audio protection; several different signals are mixed together. Non-stationary random noise should be the first layer; it provides more protection than standard sound masking generators. Music may be used as the second layer; it is buried below the random noise so it is actually inaudible to room occupants. A voice babble generator or actual speech samples may be used as a third layer. The fourth layer, the actual voices to protect, should be sufficiently buried.

Types of Masking Speakers
Standard masking speakers, discussed in Section 2.1.3 must be supplemented with vibration maskers for attachment to a number of surfaces, as noted below.

Locations for Protection
There are a number of locations where surveillance of secure facilities can occur.

Windows
Windows generally face uncontrolled areas (areas not under the control of the secure facility). They require protection. Most persons are not aware that windows respond well in the speech range of frequencies. The vibration caused by room speech is minute but can be detected remotely by laser microphones or directional microphones. The laser microphone system transmitter sends an infrared signal to the window and a special detector picks up the reflection. The reflection has been modified by the window vibration, so when the base frequency is removed (much the same as in radios) the detected vibration is that of a conversation or other internal sound. The location of the detector must be carefully chosen but the distance can be quite far. The directional microphone detects the very low level sound emitted by the window. The emitted sound spreads out, so the microphone angular position is not critical, but the distance to the window must be reasonably close. For protection, a vibration device is attached to the window that causes the window to vibrate with an appropriate masking signal. It is used to protect against vibration detection and radiated sound detection. The figure on the right shows the relative sound levels. The sound emitted within the secure room is insufficient to cause any interference with normal level conversation.

Walls
Exterior walls are generally constructed of heavier material than interior walls and seldom need audio protection. Interior walls are a different story. Speech can excite a wall to vibrate and there are several locations from which conversations can be detected:
remote from the wall with laser or directional microphones.
on the far side of the wall with vibration detectors or direct listening.
within the wall cavity with vibration detectors or fiber optic microphones.
Vibration sound masking devices provide protection against these forms of surveillance when placed within the room to be protected and at the appropriate height and spacing on the wall, In effect, the wall becomes a masking speaker. A diagram of the sound levels in the figure on the right suggest that the masking protects the wall panels, the wall cavity, and beyond the wall, while the masking levels in the secure room do not interfere with speech.

Doors
Doors are weak links in walls; they may be hollow or solid core, metal, or specially built for high sound attenuation. They can open to exterior uncontrolled areas or to internal controlled areas. Every door has a gap around its periphery that may have gaskets. Because carpeting is often used, there may be a significant gap at the bottom. Most surveillance of internal doors is accomplished by direct listening while other methods may be used for external doors. Because of the multiple paths of sound through and around a door, vibration sound maskers are used on doors. The figure on the right shows one such installation.

Ducts
Listening through air ducts is a time honored method of eavesdropping since almost all modern rooms have supply ducts that connect to a multiplicity of rooms. Ducts can be effective speaking tubes; the speech attenuation is particularly weak in unlined metal ducts. In some cases, exhaust ducts will connect to uncontrolled spaces. Surveillance can be accomplished by direct listening or with probe microphones or vibration devices within the duct where they are not visible for inspection. Vibration sound maskers, appropriately located at room perimeters, are attached directly to metallic duct walls and use the duct wall as the masking speaker. The figure on the right shows the masking of conversations in the duct afforded by a vibration masker at the room wall. External speaker maskers must be used at appropriate positions to radiate sound into fiberglass ducts to provide the same protection.

Piping
Normally, liquid filled pipes do not carry significant speech energy nor does conduit piping filled with wires. However, empty conduit pipes are excellent speaking tubes; vibration maskers are attached to them. In some facilities, vibration maskers have been attached to support pipes and columns.

Raised Floors
Raised floors are generally inaccessible for inspection so surveillance can be accomplished there. If the floor is part of the air handling system, probe microphones or fiber-optic microphones can be effective. If the floor is continuous, the high sound attenuation of the material makes surveillance with vibration detectors more advantageous since the stiff metal plates respond to speech. Although vibration maskers can be attached to the floor plates, the preferred protection is to use speaker maskers under the floor.

Plenum Ceilings
Secure rooms often have a suspended ceiling with a plenum above. Sound masking penetrates the ceiling material in open offices, so speech can go in the opposite direction into the plenum. It is likely that the perimeter walls extend to the structural ceiling. If the plenum is part of the air handling system or if there are cable run penetrations, probe microphones can be used for surveillance. If not, a small penetration can be made to insert these devices. As with commercial offices, plenum speaker maskers are installed.

Internal Loudspeakers
Many building codes require the presence of speakers in a secure room for emergency announcements. Although speakers are intended for creating sound, the speaker cone also responds to external sound and the coil generates a minute voltage characteristic of that sound. With proper sensing, that voltage can be converted to speech. Although it is possible to place a speaker masker next to the paging speaker the recommended solution is an optical isolator. It is essentially an audio diode; sound only goes one way.

Computer Keyboards
There is some evidence that the sound of key strokes can be detected to identify the characters being entered. A vibration masker placed under the keyboard will radiate sufficient masking to block surveillance without disturbing the user.

Private Applications
Sound masking generators have been used for personal application for many years. There are several manufacturers of devices for home, hotel, or air travel, primarily for sleeping. The figure on the right shows a masker that has been on the market for many years.

Status of Sound Masking
Advances in Sound Masking
The advent of sound masking made use of various components typical of other types of sound systems, such as amplifiers and loudspeakers. Since that time, use of masking has grown so that manufacturers have added a number of functions to their systems that are beneficial to sound masking. Several are listed below. Chanaud has given a more detailed discussion.

Initial Ramp Up Function
On the initiation of a sound masking system, it is important not raise the background level experienced by occupants from a low existing level to a higher masking level. This function permits the level to be raised slowly and automatically over periods as long as 30 days.

Fast Ramp Up Function
Buildings will have power failures shutting the masking system down. This function prevents the level from jumping up when power is restored. Typical recovery times are in minutes.

Programmed Level Control
The need for privacy varies throughout the day. Persons desire privacy during busy times, but do not need as much when occupancy is low in the evening or on weekends. Security guards do not want privacy as they patrol an office at night. This function automatically and continually alters the overall sound masking during the day. The time history can be different for each day of the week and for each channel of masking so can be used for both open and closed offices. It requires pre-knowledge of the activity in the office to be set correctly. The figure on the right shows an example commonly used for open offices..

Adaptive Function
Programmed level control must presume daily activity levels in an office. In many cases this is adequate. However, in the 2000s, Soft dB was the pioneer of adaptive masking control. This function senses the actual activity levels in real time and automatically adjusts the masking level to continually minimize distractions. The figure on the right shows a time history of activity levels (ambient noise) in an open office and the masking sound response to that activity. Changes in masking level must be sufficiently slow that they are not noticed by occupants.

Addressable Function
Older masking sound systems required visits for major adjustments after initial setup. Newer masking systems now have the capability to be adjusted remotely, either locally or over the internet. Because of this added capability, these systems have software that can adjust the masking level in zones, small groups of speakers or even individual speakers. Some also can adjust the masking spectrum to this level. Some of these systems have an available system zone map that identifies those zones or speakers that are to be modified. This function eliminates the need for older monitor panels, simplifies the facility managers tasks, and the expense of contractor visits.

Rapid Equalization
The performance of sound masking as a privacy tool is determined by the proper setting of the system level and spectrum (equalization, tuning). Improper tuning has been the cause of system rejection in the past. One reason was that the level and spectrum in a zone was set in an iterative manner. One person measured the spectrum and another adjusted the various frequency bands of the generator manually, a slow and unreliable process. Software has been developed that allows a number of spectra to be measured in a zone and appropriately averaged. The average can be uploaded to the generator and internally compared with the desired spectrum. Then the generator automatically adjusts its electrical output to provide the correct acoustical spectrum in the zone. Uploading can be done by physical connection or remotely.

Attributes of Successful Sound Masking Systems
There are a number of factors that contribute to the success of a sound masking system. Good design takes into account each of these factors; a system with more of them will survive longer.

Sound Masking Level
The system should be able to generate sound that masks the intelligibility of speech for the various degrees of privacy. It should also be able to mask the sound of aircraft, the sound of vehicles, the barks of dogs, the music of neighbors, and other sources of annoyance should the need arise. To do this, the equipment must have a broad range of levels and an adequate number of zones.

Sound Masking Spectrum
The system should be able to apply different masking spectra at different locations. To do this, the system must have an adequate number of channels to set the needed spectra. Speakers in open offices, closed offices, and vibration maskers all require different spec",Category:Articles using small message boxes,1
45,46,String resonance,"String resonance occurs on string instruments. Strings or parts of strings may resonate at their fundamental or overtone frequencies when other strings are sounded. For example, an A string at 440 Hz will cause an E string at 330 Hz to resonate, because they share an overtone of 1320 Hz (3rd overtone of A and 4th overtone of E).
Electric guitars can have string trees near the tuning pegs to mute this type of reverberation. The string length behind the bridge also must be as short as possible to prevent the resonance. String resonance is a factor in the timbre of a string instrument. Tailed bridge guitars like the Fender Jaguar differ in timbre from guitars with short bridges, because of their (extended) floating bridge. The Japanese Koto is also an example of an instrument with occurring string resonance.

String resonance in instrument building
Sometimes string resonance is used in the construction of the instrument, like for instance the Sympathetic strings in many Eastern instruments.

Piano
According to a 2007 Grove Music Online article on ""duplex scaling"", Steinway developed a system of Aliquot stringing to provide sympathetic resonance, with the intention of enriching the treble register of the piano. In the ""octave duplex"" piano by Hoerr of Toronto, each note had four strings, of which two, three or four could be struck by the hammer depending on the depression of any of four pedals. Steinway’s duplex scale was inspired a half century earlier by an experiment conducted by the German piano maker Wilhelm Leberecht Petzoldt, in which a small bridge was placed behind the standard larger one with the intention of maximizing the potential additional resonance of a sympathetically vibrating additional length of string.

Overtones due to string resonance on the koto
The following table  shows the created resonating overtones on the koto for various positions on a stopped string (the proportion being between the ""played"" portion of the string and resonant portion, the remaining length of the string).

Instruments that use string resonance
Baryton
Bazantar
Crwth
Dilruba
Esraj
Fender Jaguar
Fender Jazzmaster
Gadulka
Gottuvadhyam
Hardingfele
H'arpeggione
Kithara (of Harry Partch)
Koto
Modern versions of the nyckelharpa
Mohan veena
Moodswinger
Moonlander
Pencilina
Piano (damper pedal)
Prepared guitar
Prepared piano
Rubab
Sarangi
Sarod
Sitar
Springtime
Tambura
Ten-string guitar
Ukelin
Viola d'amore
Tar (lute)
Twister
Setar

References
See also
3rd bridge
Aliquot stringing
Drone
List of meantone intervals
Mechanical resonance
Prepared guitar
Resonance
Scale of harmonics
Sympathetic string
Tailed bridge guitar
Wolf tone",Category:Acoustics,1
46,47,Bass trap,"Bass traps are acoustic energy absorbers which are designed to damp low frequency sound energy with the goal of attaining a flatter low frequency (LF) room response by reducing LF resonances in rooms. They are commonly used in recording studios, mastering rooms, home theatres and other rooms built to provide a critical listening environment. Like all acoustically absorptive devices, they function by turning sound energy into heat through friction.

General description—types
There are generally two types of bass traps: resonant absorbers and porous absorbers. Resonant absorbers are further divided into panel absorbers and Helmholtz resonators.
Both types are effective, but whereas a resonant absorber needs to be mechanically tuned to resonate in sympathy with the frequencies being absorbed, a porous absorber does not resonate and need not be tuned.
Porous absorbers tend to be smaller in size and are easier to design and build as well as less expensive overall than resonant absorbers. However, the deep bass attenuation of a porous absorber is generally inferior, so its usefulness for attenuating lower frequency room resonances is more limited.
Resonating absorbers tend to absorb a narrower spectrum and porous absorbers tend to absorb a broader spectrum. The spectrum of both types can be either narrowed or broadened by design but the generalized difference in bandwidth and tunability dominates their respective performance.
Examples of resonating type bass traps include a rigid container with one or more portholes or slots (i.e. Helmholtz resonator), or a rigid container with a flexible diaphragm (i.e. membrane absorber). Resonating type bass trap achieves absorption of sound by sympathetic vibration of some free element of the device with the air volume of the room.
Resonating absorbers vary in construction, with one type of membrane absorber using a springy sheet of wood that attaches to the enclosure only along the edges/corners, and another using a more floppy sheet of thin material stretched like a drumhead. A Helmholtz resonator can have one port tuned to a single frequency, or several ports tuned to either a single or to multiple frequencies, with round port, slotted port, or even perforated construction. Resonating absorbers often incorporate porous absorption internally to simultaneously lower the resonant frequency and broaden the spectrum of absorption.
Porous absorbers are most commonly made from fiberglass, mineral wool or open cell foam that resists the passage of air molecules through the interstitial space. Porous absorbers often incorporate a foil or paper facing to reflect frequencies above 500Hz. Facing also improves low bass absorption by translating the physical compression of air at the facing into physical compression of the fibers that are in contact with the facing while also maintaining the resistive loss of air as it is driven through the bulk of the fiber by the facing.

Design concepts for building bass traps
Resonating bass traps
Resonating bass traps will absorb sound with high efficiency at their fundamental frequency of resonance. As such, a knowledge of the frequencies of resonances which require damping is helpful before designing and constructing a resonating bass trap. This can be attained by calculation of the room's modes or by direct measurement of the room itself.
Resonating absorbers can be broadened in the frequency range of efficacy to some degree by either introducing porous absorptive material to the interior of the vessel, by constraining the vibrations of the panel or membrane, or by installing an array of resonating devices each tuned to adjacent frequency ranges so that collectively the array functions over a broadened range of sounds. Such devices can be enormously effective over their tuned range, but can take up a great deal of space, especially when installed in arrays, and thus are sometimes not a practical solution.

Panel absorber
A simple panel resonator can be built to hang on a wall by building a wooden frame, adding a couple of inches of mineral wool to the inside and adding a sheet of plywood over the top attached only at the edges. A small gap should be left between the panel and the acoustic insulation so that the panel is free to resonate. Panel resonance can be enhanced by reducing the point of connection between the panel and the frame by means of narrow spacer material such as a loop of wire or welding rod run along the edge of the frame so that the panel is perched on a thin edge. Approximate full sheet [4' × 8'] plywood panel resonances when mounted on a 1×4 frame 3.5"" deep are:
1/8"" plywood = 150 Hz
1/4"" plywood = 110 Hz
3/8"" plywood = 87 Hz

Helmholtz resonator
Other common resonating bass traps are form of the Helmholtz resonator—such as either a stiff walled box with a hole in one side [a port], or a series of slats over-mounted across the face as a stiff-walled box forming narrow openings in the cracks between the slat members.

Porous absorber bass traps
A bass trap generally comprises a core absorbent damp material, a frame, and a covering for aesthetic reasons.
Core: Semi-rigid glass-wool or mineral-wool insulation boards or dense open-cell foam are typically used.
Frame: A steel exterior frame is preferred, although fire resistant wood skeleton frames are common. The frame acts as a housing, allowing mounting to walls and ceilings, and also anchors the covering material.
Covering: Porous fabric (similar to speaker grill cloth) is usually used.

Positioning
Since low frequency resonances in a room have their points of maximum or minimum pressure in the corners of the room, bass traps mounted in these positions will be the most efficient. Bass traps are typically used to attenuate modal resonances and so exact placement depends on which room mode one is trying to target. Bass traps typically combine structural mechanisms that can work at both positions of high particle velocity/low pressure (thick fiberglass) and high pressure/low particle velocity (membranes).
Porous bass trap absorbers need to be very thick to be effective at lower frequencies so they tend to be allocated either as diagonal wedges in the corners or as thick rectangular bulk behind false walls where they are out of the way and less likely to disrupt higher frequencies or room function. Air gap behind a porous panel absorber e.g. straddling a corner also helps to ensure it protrudes more into the room where there is more air velocity, improving its velocity-based absorption and extending its bandwidth while inducing some ripple in its absorption spectrum. Resonant bass trap absorbers need to be at a pressure maximum and tend to be thinner, so they are more conveniently and effectively positioned flat against a wall in a corner where the pressure is maximum, rather than straddling a corner where there is more velocity.
Standard practice is to investigate the applicability of porous bass trap absorbers before investigating resonant or hybridized bass trap absorbers. Complementary methods to use in combination with porous absorption include drywall/stud construction of walls/ceiling and filled with insulation as a form of highly damped resonant bass trapping using the drywall itself as the membrane. The combination of inherently lossy resonant room boundaries and paper or foil faced porous bass trap absorption deep in the tricorners (where three room boundaries meet) is often sufficient to attain acceptable bass response even in listening rooms with somewhat problematic resonances.
Adjusting the listening position within the room boundaries and elevating the seating with a riser that is filled with porous absorption is one more method that can improve bass response without resorting to resonant bass trapping, while simultaneously improving home theater screen visibility.
Small listening rooms suffer a paucity of low frequency resonances with gaps between them, so another complementary method is adding a subwoofer to drive the room resonances from an optimal physical location that minimizes ripple in the frequency response at the listening position. Yet another complementary method is splitting an existing subwoofer allocation up into multiple smaller subwoofers in spatially separated locations that increase the degrees of freedom available to tune the response with. Multiple subwoofers also tend to smooth the bass response across a larger listening area and are often easier to place for good bass response than a single larger subwoofer (or no subwoofer).
If the response is somewhat uniform across all listening positions using these methods, equalization can be used to shape the bass response to the desired target and further smooth out any remaining ripple.
Some combination of porous absorption with these complementary methods is typically preferred for their simplicity, affordability, and convenience, but resonant bass traps are more effective for absorbing strong room resonances where the aforementioned complimentary methods are inadequate or impractical, particularly when the geometry of the room causes problematic narrow-band resonances that affect the low bass and the composition of the room boundaries is highly reflective rather than acoustically lossy.

References
Everest, F. Alton. The Master Handbook of Acoustics, McGraw-Hill, 2000 (ISBN 0-07-136097-2).
Kinsler, Frey, Coppens and Sanders, Fundamentals of Acoustics, Third Edition, John Wiley & Sons, 1999 (ISBN 978-0471847892), Section 10.8: “The Helmholtz Resonator”.
https://www.soundonsound.com/sos/may06/articles/qa0506_3.htm",Category:Acoustics,1
47,48,Piano acoustics,"Piano acoustics are the physical properties of the piano that affect its sound.

String length and mass
The strings of a piano vary in thickness, and therefore in mass per length, with bass strings thicker than treble. A typical range is from 1/30 inch (.85 mm) for the highest treble strings to 1/3 inch (8.5 mm) for the lowest bass. These differences in string thickness follow from well-understood acoustic properties of strings.
Given two strings, equally taut and heavy, one twice as long as the other, the longer would vibrate with a pitch one octave lower than the shorter. However, if one were to use this principle to design a piano it would be impossible to fit the bass strings onto a frame of any reasonable size. Furthermore, in such a hypothetical, gigantic piano, the lowest strings would travel so far in vibrating that they would strike one another. Instead, piano makers take advantage of the fact that a heavy string vibrates more slowly than a light string of identical length and tension; thus, the bass strings on the piano are much thicker than the others.

Inharmonicity and piano size
Any vibrating thing produces vibrations at a number of frequencies above the fundamental pitch. These are called overtones. When the overtones are integer multiples (e.g., 2×, 3× ... 6× ... ) of the fundamental frequency (called harmonics), then - neglecting damping - the oscillation is periodic—i.e., it vibrates exactly the same way over and over. Humans seem to enjoy the sound of periodic oscillations. For this reason, many musical instruments, including pianos, are designed to produce nearly periodic oscillations, that is, to have overtones as close as possible to the harmonics of the fundamental tone.
In an ideal vibrating string, when the wavelength of a wave on a stretched string is much greater than the thickness of the string, the wave velocity on the string is constant and the overtones are at the harmonics. That is why so many instruments are constructed of skinny strings or thin columns of air.
However, for high overtones with short wavelengths that approach the diameter of the string, the string behaves more like a thick metal bar: its mechanical resistance to bending becomes an additional force to the tension, which 'raises the pitch' of the overtones. Only when the bending force is much smaller than the tension of the string, are its wave-speed (and the overtones pitched as harmonics) unchanged. The frequency-raised overtones (above the harmonics), called 'partials' can produce an unpleasant effect called inharmonicity. Basic strategies to reduce inharmonicity include decreasing the thickness of the string or increasing its length, choosing a flexible material with a low bending force, and increasing the tension force so that it stays much bigger than the bending force.
Winding a string allows an effective decrease in the thickness of the string. In a wound string, only the inner core resists bending while the windings function only to increase the linear density of the string. The thickness of the inner core is limited by its strength and by its tension; stronger materials allow for thinner cores at higher tensions, reducing inharmonicity. Hence, piano designers choose high quality steel for their strings, as its strength and durability help them minimize string diameters.
If string diameter, tension, mass, uniformity, and length compromises were the only factors—all pianos could be small, spinet-sized instruments. Piano builders, however, have found that 'longer wires' increase instrument power, harmonicity, and reverberation, and help produce a properly tempered tuning scale.
For longer wires, larger pianos achieve the longer wavelengths and tonal characteristics desired. Piano designers strive to fit the longest strings possible within the case; moreover, all else being equal, the sensible piano buyer tries to obtain the largest instrument compatible with budget and space.
Inharmonicity largely affects the lowest and highest notes in the piano and is one of the limits on the total range of a piano. The lowest strings, which must be longest, are most limited by the size of the piano. The designer of a short piano is forced to use thick strings to increase mass density and is thus driven into inharmonicity.
The highest strings must be under the greatest tension, yet must also be thin to allow for a low mass density. The limited strength of steel forces the piano designer to use very short strings whose short wavelengths thus generate inharmonicity.
The natural inharmonicity of a piano is used by the tuner to make slight adjustments in the tuning of a piano. The tuner stretches the notes, slightly sharpening the high notes and flatting the low notes to make overtones of lower notes have the same frequency as the fundamentals of higher notes.
See also Piano wire, Piano tuning, Psychoacoustics.

The Railsback curve
The Railsback curve, first measured by O.L. Railsback, expresses the difference between normal piano tuning and an equal-tempered scale (one in which the frequencies of successive notes are related by a constant ratio, equal to the twelfth root of two). For any given note on the piano, the deviation between the normal pitch of that note and its equal-tempered pitch is given in cents (hundredths of a semitone).
As the Railsback curve shows, octaves are normally stretched on a well-tuned piano. That is, the high notes are higher, and the low notes lower, than they are in an equal-tempered scale. Railsback discovered that pianos were typically tuned in this manner not because of a lack of precision, but because of inharmonicity in the strings. Ideally, the overtone series of a note consists of frequencies that are integer multiples of the note's fundamental frequency. Inharmonicity as present in piano strings makes successive overtones higher than they ""should"" be.
To tune an octave, a piano technician must reduce the speed of beating between the first overtone of a lower note and a higher note until it disappears. Because of inharmonicity, this first overtone is sharper than a harmonic octave (which has the ratio of 2/1), making either the lower note flatter, or the higher note sharper, depending on which one is tuned relative to the other. And because pianists' tessitura is commonly three octaves, it is critical that any note on the piano be tuned reasonably close to the eighth harmonic of the note three octaves below.
Thus to produce octaves that reflect the temperament and accommodate the inharmonicity of the instrument, the technician begins his stretch from the middle of the piano so that, as the stretch accumulates from register to register, it results in his desired stretch at the top and bottom of the instrument.

Shape of the curve
Because string inharmonicity only makes harmonics sharper, the Railsback curve—which is functionally the integral of the inharmonicity at an octave—is monotonically increasing. A piano is tuned beginning in the center, so the Railsback curve has a shallow slope in this area. But as the piano tuner stretches octaves to compensate for inharmonicity, the stretch accumulates as tuned notes ascend and descend, and their curves become more pronounced.
Inharmonicity in a string is caused primarily by stiffness. Decreased length and increased thickness both contribute to inharmonicity. For the middle to high part of the piano range, string thickness remains constant as length decreases, contributing to greater inharmonicity in the higher notes. For the low range, string thickness drastically increases—especially in shorter pianos, which cannot compensate with longer strings, which produces greater inharmonicity in this range as well.
In the bass register, a second factor affecting the inharmonicity is the resonance caused by the acoustic impedance of the piano sound board. These resonances exhibit positive feedback on the inharmonic effect: if a string vibrates at a frequency just below that of a resonance, the impedance makes it vibrate even lower, and if it vibrates just above a resonance, the impedance makes it vibrate higher. The sounding board has multiple resonant frequencies that are unique to any particular piano. This contributes to the greater variance in the empirically measured Railsback curve in the lower octaves.

Multiple strings
All but the lowest notes of a piano have multiple strings tuned to the same frequency. This allows the piano to have a loud attack with a fast decay but a long sustain in the Attack Decay Sustain Release (ADSR) system.
The three strings create a coupled oscillator with three normal modes (with two polarizations each). Since the strings are only weakly coupled, the normal modes have imperceptibly different frequencies. But they transfer their vibrational energy to the sounding board at significantly different rates.
The normal mode in which the three strings oscillate together is most efficient at transferring energy since all three strings pull in the same direction at the same time. It sounds loud, but decays quickly. This normal mode is responsible for the rapid staccato ""Attack"" part of the note.
In the other two normal modes, strings do not all pull together, e.g., one pulls up while the other two pull down. There is slow transfer of energy to the sounding board, generating a soft but near-constant sustain.

See also
Electronic tuner
Inharmonicity

References
Further reading
Ortiz-Berenguer, Luis I., F. Javier Casajús-Quirós, Marisol Torres-Guijarro, J.A. Beracoechea. Piano Transcription Using Pattern Recognition: Aspects On Parameter Extraction: Proceeds of The International Conference on Digital Audio Effects, Naples, October 2004.
Railsback, O. L. (1938). ""Scale Temperament as Applied to Piano Tuning"". The Journal of the Acoustical Society of America. 9 (3): 274. Bibcode:1938ASAJ....9..274R. doi:10.1121/1.1902056. 
Sundberg, Johan (1991). The Science of Musical Sounds. San Diego: Academic Press. ISBN 0-12-676948-6. 
Weinreich, G. (1977). ""Coupled piano strings"". The Journal of the Acoustical Society of America. 62. doi:10.1121/1.381677. 
Giordano, Nicholas J., Sr (2010). Physics of the Piano. Oxford: Oxford University Press. ISBN 978-0-19-878914-7.

External links
Five lectures on the acoustics of the piano
A. H. Benade Sound Production in Pianos
Robert W. Young, Inharmonicity of Plain Wire Piano Strings' The Journal of the Acoustical Society of America, vol 24 no. 3 (May 1952)
""The Engineering of Concert Grand Pianos,"" by Richard Dain, FRENG
D. Clausen, B. Hughes and W. Stuart ""A design analysis of a Stuart and Sons grand piano frame""",Category:Piano,1
48,49,Nonlinear acoustics,"Nonlinear acoustics (NLA) is a branch of physics and acoustics dealing with sound waves of sufficiently large amplitudes. Large amplitudes require using full systems of governing equations of fluid dynamics (for sound waves in liquids and gases) and elasticity (for sound waves in solids). These equations are generally nonlinear, and their traditional linearization is no longer possible. The solutions of these equations show that, due to the effects of nonlinearity, sound waves are being distorted as they travel.

Introduction
A sound wave propagates through a material as a localized pressure change. Increasing the pressure of a gas or fluid increases its local temperature. The local speed of sound in a compressible material increases with temperature; as a result, the wave travels faster during the high pressure phase of the oscillation than during the lower pressure phase. This affects the wave's frequency structure; for example, in an initially plane sinusoidal wave of a single frequency, the peaks of the wave travel faster than the troughs, and the pulse becomes cumulatively more like a sawtooth wave. In other words, the wave self-distorts. In doing so, other frequency components are introduced, which can be described by the Fourier series. This phenomenon is characteristic of a non-linear system, since a linear acoustic system responds only to the driving frequency. This always occurs but the effects of geometric spreading and of absorption usually overcome the self distortion, so linear behavior usually prevails and nonlinear acoustic propagation occurs only for very large amplitudes and only near the source.
Additionally, waves of different amplitudes will generate different pressure gradients, contributing to the non-linear effect.

Physical analysis
The pressure changes within a medium cause the wave energy to transfer to higher harmonics. Since attenuation generally increases with frequency, a counter effect exists that changes the nature of the nonlinear effect over distance. To describe their level of nonlinearity, materials can be given a nonlinearity parameter, 
  
    
      
        B
        
          /
        
        A
      
    
    {\displaystyle B/A}
  . The values of 
  
    
      
        A
      
    
    {\displaystyle A}
   and 
  
    
      
        B
      
    
    {\displaystyle B}
   are the coefficients of the first and second order terms of the Taylor series expansion of the equation relating the material's pressure to its density. The Taylor series has more terms, and hence more coefficients (C, D, .. etc.) but they are seldom used. Typical values for the nonlinearity parameter in biological mediums are shown in the following table.
In a liquid usually a modified coefficient is used known as 
  
    
      
        ?
        =
        1
        +
        
          
            B
            
              2
              A
            
          
        
      
    
    {\displaystyle \beta =1+{\frac {B}{2A}}}
  .

Mathematical model
Governing Equations to Derive Westervelt Equation
Continuity:

  
    
      
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        +
        ?
        ?
        (
        ?
        
          
            u
          
        
        )
        =
        0
      
    
    {\displaystyle {\frac {\partial \rho }{\partial t}}+\nabla \cdot (\rho {\textbf {u}})=0}
  
Conservation of momentum:

  
    
      
        ?
        
          (
          
            
              
                
                  ?
                  
                    
                      u
                    
                  
                
                
                  ?
                  t
                
              
            
            +
            
              
                u
              
            
            ?
            ?
            
              
                u
              
            
          
          )
        
        +
        ?
        p
        =
        (
        ?
        +
        2
        ?
        )
        ?
        (
        ?
        ?
        
          
            u
          
        
        )
      
    
    {\displaystyle \rho \left({\frac {\partial {\textbf {u}}}{\partial t}}+{\textbf {u}}\cdot \nabla {\textbf {u}}\right)+\nabla p=(\lambda +2\mu )\nabla (\nabla \cdot {\textbf {u}})}
  
with Taylor perturbation expansion on density:

  
    
      
        ?
        =
        
          ?
          
            0
          
          
            ?
          
        
        
          ?
          
            i
          
        
        
          ?
          
            i
          
        
      
    
    {\displaystyle \rho =\sum _{0}^{\infty }\varepsilon ^{i}\rho _{i}}
  
where ? is a small parameter, i.e. the perturbation parameter, the equation of state becomes:

  
    
      
        p
        =
        ?
        
          ?
          
            1
          
        
        
          c
          
            0
          
          
            2
          
        
        
          (
          
            1
            +
            ?
            
              
                B
                
                  2
                  !
                  A
                
              
            
            
              
                
                  ?
                  
                    1
                  
                
                
                  ?
                  
                    0
                  
                
              
            
            +
            O
            (
            
              ?
              
                2
              
            
            )
          
          )
        
      
    
    {\displaystyle p=\varepsilon \rho _{1}c_{0}^{2}\left(1+\varepsilon {\frac {B}{2!A}}{\frac {\rho _{1}}{\rho _{0}}}+O(\varepsilon ^{2})\right)}
  
If the second term in the Taylor expansion of pressure is dropped, the viscous wave equation can be derived. If it is kept, the non-linear term in pressure appears in the Westervelt equation.

Westervelt equation
The general wave equation that accounts for nonlinearity up to the second-order is given by the Westervelt equation

  
    
      
        
        
          ?
          
            2
          
        
        p
        ?
        
          
            1
            
              c
              
                0
              
              
                2
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              p
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        +
        
          
            ?
            
              c
              
                0
              
              
                4
              
            
          
        
        
          
            
              
                ?
                
                  3
                
              
              p
            
            
              ?
              
                t
                
                  3
                
              
            
          
        
        =
        ?
        
          
            ?
            
              
                ?
                
                  0
                
              
              
                c
                
                  0
                
                
                  4
                
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              
                p
                
                  2
                
              
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \,\nabla ^{2}p-{\frac {1}{c_{0}^{2}}}{\frac {\partial ^{2}p}{\partial t^{2}}}+{\frac {\delta }{c_{0}^{4}}}{\frac {\partial ^{3}p}{\partial t^{3}}}=-{\frac {\beta }{\rho _{0}c_{0}^{4}}}{\frac {\partial ^{2}p^{2}}{\partial t^{2}}}}
  
where 
  
    
      
        p
      
    
    {\displaystyle p}
   is the sound pressure, 
  
    
      
        
          c
          
            0
          
        
      
    
    {\displaystyle c_{0}}
   is the small signal sound speed, 
  
    
      
        ?
      
    
    {\displaystyle \delta }
   is the sound diffusivity, 
  
    
      
        ?
      
    
    {\displaystyle \beta }
   is the non-linearity coefficient and 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
   is the ambient density.
The sound diffusivity is given by

  
    
      
        
        ?
        =
        
          
            1
            
              ?
              
                0
              
            
          
        
        
          (
          
            
              
                4
                3
              
            
            ?
            +
            
              ?
              
                B
              
            
          
          )
        
        +
        
          
            k
            
              ?
              
                0
              
            
          
        
        
          (
          
            
              
                1
                
                  c
                  
                    v
                  
                
              
            
            ?
            
              
                1
                
                  c
                  
                    p
                  
                
              
            
          
          )
        
      
    
    {\displaystyle \,\delta ={\frac {1}{\rho _{0}}}\left({\frac {4}{3}}\mu +\mu _{B}\right)+{\frac {k}{\rho _{0}}}\left({\frac {1}{c_{v}}}-{\frac {1}{c_{p}}}\right)}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \mu }
   is the shear viscosity, 
  
    
      
        
          ?
          
            B
          
        
      
    
    {\displaystyle \mu _{B}}
   the bulk viscosity, 
  
    
      
        k
      
    
    {\displaystyle k}
   the thermal conductivity, 
  
    
      
        
          c
          
            v
          
        
      
    
    {\displaystyle c_{v}}
   and 
  
    
      
        
          c
          
            p
          
        
      
    
    {\displaystyle c_{p}}
   the specific heat at constant volume and pressure respectively.

Burgers' equation
The Westervelt equation can be simplified to take a one-dimensional form with an assumption of strictly forward propagating waves and the use of a coordinate transformation to a retarded time frame:

  
    
      
        
          
            
              ?
              p
            
            
              ?
              z
            
          
        
        ?
        
          
            ?
            
              
                ?
                
                  0
                
              
              
                c
                
                  0
                
                
                  3
                
              
            
          
        
        p
        
          
            
              ?
              p
            
            
              ?
              ?
            
          
        
        =
        
          
            ?
            
              2
              
                c
                
                  0
                
                
                  3
                
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              p
            
            
              ?
              
                ?
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial p}{\partial z}}-{\frac {\beta }{\rho _{0}c_{0}^{3}}}p{\frac {\partial p}{\partial \tau }}={\frac {\delta }{2c_{0}^{3}}}{\frac {\partial ^{2}p}{\partial \tau ^{2}}}}
  
where 
  
    
      
        ?
        =
        t
        ?
        z
        
          /
        
        
          c
          
            0
          
        
      
    
    {\displaystyle \tau =t-z/c_{0}}
   is retarded time. This corresponds to a viscous Burgers equation:

  
    
      
        
          
            
              ?
              y
            
            
              ?
              
                t
                ?
              
            
          
        
        +
        y
        
          
            
              ?
              y
            
            
              ?
              x
            
          
        
        =
        d
        
          
            
              
                ?
                
                  2
                
              
              y
            
            
              ?
              
                x
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial y}{\partial t'}}+y{\frac {\partial y}{\partial x}}=d{\frac {\partial ^{2}y}{\partial x^{2}}}}
  .
in the pressure field (y=p), with a mathematical ""time variable"":

  
    
      
        
          t
          ?
        
        =
        
          
            z
            
              c
              
                0
              
            
          
        
      
    
    {\displaystyle t'={\frac {z}{c_{0}}}}
  .
and with a ""space variable"":

  
    
      
        x
        =
        ?
        
          
            
              
                ?
                
                  0
                
              
              
                c
                
                  0
                
                
                  2
                
              
            
            ?
          
        
        ?
      
    
    {\displaystyle x=-{\frac {\rho _{0}c_{0}^{2}}{\beta }}\tau }
  .
and a negative diffusion coefficient:

  
    
      
        d
        =
        ?
        
          
            
              
                ?
                
                  0
                
              
              
                c
                
                  0
                
              
            
            
              2
              
                ?
                
                  2
                
              
            
          
        
        ?
      
    
    {\displaystyle d=-{\frac {\rho _{0}c_{0}}{2\beta ^{2}}}\delta }
  .
The Burgers equation is the simplest equation that describes the combined effects of nonlinearity and losses on the propagation of progressive waves.

KZK equation
An augmentation to the Burgers equation that accounts for the combined effects of non-linearity, diffraction and absorption in directional sound beams is described by the Khokhlov-Zabolotskaya-Kuznetsov (KZK) equation. Solutions to this equation are generally used to model non-linear acoustics.
If the 
  
    
      
        z
      
    
    {\displaystyle z}
   axis is in the direction of the sound beam path and the 
  
    
      
        (
        x
        ,
        y
        )
      
    
    {\displaystyle (x,y)}
   plane is perpendicular to that, the KZK equation can be written

  
    
      
        
        
          
            
              
                ?
                
                  2
                
              
              p
            
            
              ?
              z
              ?
              ?
            
          
        
        =
        
          
            
              c
              
                0
              
            
            2
          
        
        
          ?
          
            ?
          
          
            2
          
        
        p
        +
        
          
            ?
            
              2
              
                c
                
                  0
                
                
                  3
                
              
            
          
        
        
          
            
              
                ?
                
                  3
                
              
              p
            
            
              ?
              
                ?
                
                  3
                
              
            
          
        
        +
        
          
            ?
            
              2
              
                ?
                
                  0
                
              
              
                c
                
                  0
                
                
                  3
                
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              
                p
                
                  2
                
              
            
            
              ?
              
                ?
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \,{\frac {\partial ^{2}p}{\partial z\partial \tau }}={\frac {c_{0}}{2}}\nabla _{\perp }^{2}p+{\frac {\delta }{2c_{0}^{3}}}{\frac {\partial ^{3}p}{\partial \tau ^{3}}}+{\frac {\beta }{2\rho _{0}c_{0}^{3}}}{\frac {\partial ^{2}p^{2}}{\partial \tau ^{2}}}}
  
The equation can be solved for a particular system using a finite difference scheme. Such solutions show how the sound beam distorts as it passes through a non-linear medium.

Common occurrences
Sonic boom
The nonlinear behavior of the atmosphere leads to change of the wave shape in a sonic boom. Generally, this makes the boom more 'sharp' or sudden, as the high-amplitude peak moves to the wavefront.

Acoustic levitation
The practice of acoustic levitation would not be possible without understanding nonlinear acoustic phenomena. The nonlinear effects are particularly evident due to the high-powered acoustic waves involved.

Ultrasonic waves
Because of their relatively high amplitude to wavelength ratio, ultrasonic waves commonly display nonlinear propagation behavior. For example, nonlinear acoustics is a field of interest for medical ultrasonography because it can be exploited to produce better image quality.

Musical acoustics
The physical behavior of musical acoustics is mainly nonlinear. Many attempts are made to model their sound generation from physical modeling of emulating their sound from measurements of their non-linearity.

See also
Cavitation


== References ==",Category:Articles needing expert attention with no reason or talk parameter,1
49,50,Sound intensity probe,"A sound intensity probe is a probe capable of measuring sound intensity. The sound intensity is the product of sound pressure and particle velocity. Common probes measure the sound intensity using two microphones. Another type measures the sound intensity using a microphone and a particle velocity probe.

Sound intensity
The sound intensity (in watts per square meter) is given by

  
    
      
        I
        =
        p
        v
      
    
    {\displaystyle I=pv}
  
where

  
    
      
        p
      
    
    {\displaystyle p}
   is sound pressure in pascals

  
    
      
        v
      
    
    {\displaystyle v}
   is particle velocity in meters per second

Devices
A p-p type of sound intensity probe measures the sound intensity using two phase-matched microphones. These microphones are usually positioned face-to-face and are used to determine a pressure gradient. From this pressure gradient it is possible to calculate the particle velocity. The sound pressure is determined from the average from both microphones output.
A p-u type of sound intensity probe measured both the sound pressure and the particle velocity directly. Sound pressure is measured using a microphone and particle velocity using a particle velocity probe

Applications
Sound intensity probes are used for several applications. A common application is to determine the sound power of an object. Another application is locating a source of noise.

External links
Extensive explanation on the working of a p-p type of sound intensity probe by HP",Category:Acoustics,1
50,51,Direct-field acoustic testing,"Direct-field acoustic testing, or DFAT, is a technique used for acoustic testing of aerospace structures by subjecting them to sound waves created by an array of acoustic drivers. The method uses electro-dynamic acoustic speakers, arranged around the test article to provide a uniform, well-controlled, direct sound field at the surface of the unit under test. The system employs high capability acoustic drivers, powerful audio amplifiers, a narrow-band multiple-input-multiple-output (MIMO) controller and precision laboratory microphones to produce an acoustic environment that can simulate a helicopter, aircraft, jet engine or launch vehicle sound pressure field. A high level system is capable of overall sound pressure levels in the 125–147 dB for more than one minute over a frequency range from 25 Hz to 10 kHz.

Overview
A direct field is generated by audio drivers arranged to encircle the test article. Two different control schemes can be used to perform a direct field test. One method, known as single-input-single-output or SISO, uses a single drive signal to all acoustic drivers with multiple control microphones averaged to produce the control measurement. This method will produce a set of correlated plane waves that may combine to produce large magnitude variations creating local fluctuations on the test article surface. Magnitude variations as much as +/?12dB can be experienced. The second method, known as MIMO, uses multiple independent drive signals to control multiple independent microphone locations. This method produces a more uncorrelated field that is much more uniform than the SISO field. Magnitude variations in the range of +/-3dB are typical when using MIMO control.
The technique uses normal incident plane waves in a shaped spectrum of acoustic noise to impact directly on all exposed test article surfaces without external boundary reflections. Depending on the geometry of the test article this could produce magnitude variations on surfaces due to phasing differences between the plane waves. In the case of large surface area, low mass density test articles the phasing difference may excite primary structure modes in a different way than more conventional reverberant-field testing. This fundamental difference and its impact on the structure must be weighed against the advantages of the DFAT method.
An advantage of DFAT testing over reverberant testing is the portability of the DFAT system. This allows the test equipment to be transported to any location, setup, calibrated, used to perform a High Intensity Acoustic Test and then removed from the test site. The entire process from load-in to load-out can be accomplished in no more than 4 days for a large satellite or similar aerospace structure. The test system uses a “building block” approach to form combinations of equipment to satisfy the environmental requirements. Systems typically include 500 plus speakers, 2 million plus watts of amplification, at least 8 to 16 control microphones, and a closed-loop MIMO acoustic control and data acquisition system. The mobility and “building block” approach allows this method to be tailored for each application and to provide a more timely and cost effective test solution. This method can also be useful for testing articles that are too large to fit inside a traditional acoustic reverberant chamber.

Process
The process requires the transport to and assembly of a speaker circle around the test article. The size of the circle is dependent on the size of the test article. Generally, a circle 12 feet (3.7 m) in diameter larger and 4 feet (1.2 m) taller than the test article is required. The arrangement should avoid symmetry to reduce the potential for adverse coupling of plane waves. The test article can be mounted on a platform or suspended. Multiple microphones, eight to sixteen, should be used for control with either the SISO or MIMO methods. The microphones should be placed randomly around the test article. The distance from the surface of the drivers to the surface of the control microphones should be 1.0–1.5 meters (3.3–4.9 feet). The distance from the control microphones to the surface of the test article should be 0.5–0.75 meters (1.6–2.5 feet). The height of the control microphones should be centered at mid-height of the test item and randomly varied up and down by about one-eighth of the test item height. The orientation of the free-field microphones in a DFAT test arrangement is not critical. However, reflections from the test article can be minimized with the microphone oriented toward the sound source with a 0 degree incidence. Most modern day, quality measurement, free-field microphones are factory adjusted to compensate for incident angle. This phenomenon is most pronounced at high frequencies, above 10 kHz for a 1/4"" microphone, and is inversely proportional to microphone diaphragm diameter.
The speakers are driven by a series of audio amplifiers that are usually powered by a portable diesel generator. The system is safely and accurately controlled by a closed-loop feedback control system that can be used to limit and/or abort if an over-test condition is detected.
A pretest is usually performed using a simulator to confirm the specified overall sound pressure level and spectrum can be achieved. The pre-test is also used to verify any special control features such as; abort tolerances, response limits, field shaping and emergency shut-down procedures. The microphone responses should then be examined to evaluate the resulting field for uniformity, coherence and if available, structural response. Then the simulator is replaced with the actual test item in the speaker circle and the test process is repeated.
The entire operation is usually completed in four days, and only requires the test article for one of those days. All equipment is brought to the test article, assembled, pretested and performance-checked before testing the flight article. The flight article is generally required for only one day of testing depending on the complexity of the test plan. On completion of the flight test, the article is removed and all equipment is disassembled and transported from the site.
Features
Portability: can be set up almost anywhere
Modularity: adapts to multiple configurations
Controllability: safe, repeatable, real-time control
Narrow-band control: can provide control to a spec defined at constant, narrow-band (~3 Hz) increments from 25 to 10 kHz
Response limiting: limit on SPL, acceleration, force or stress response values
Over-test protection: abort on peak or rms input values

Capabilities
The convenience, low cost, and mobility of this method distinguish it from conventional testing and are the primary reasons for its growing popularity. The method is convenient because all required sound system, power generation and distribution and data acquisition and control equipment is brought to the test site. Equipment is usually leased for each test event. There is no large investment in a facility, equipment or personnel required on the part of the customer. A diesel generator is the preferred power source, therefore providing clean on-site electrical power in a consistent configuration for connection to the MSI power distribution equipment. This removes the demand for large quantities of power from the test facility. In addition, testing can be performed at a much lower cost per test compared to the installation, operation and maintenance of a more standard high intensity reverberant acoustic chamber system. Finally, mobility allows this test method to be performed at almost any time and place in the normal test article integration and test flow. The test equipment is completely portable and no special facility or infrastructure is required.

References
Resource chronology
Measurement of Correlation Coefficients in Reverberant Sound Fields, Cook, Waterhouse, Berendt, Edelman, Thompson, J-ASA, Vol.27, No.6, 11/11/1955
The Development of Sonic Environmental Testing, John Van Houten, IEST, 1966
Combined Loads, Vibration and Modal Testing of the QuickScat Spacecraft, Scharton(JPL), Vujcich(Ball), 18th ATS, 3/16-18/1999
Combining Spacecraft Vibration & Acoustic Tests, Terry Scharton, S/C & L/V Dynamic Environments Workshop, June 1999
Direct, Near-Field Acoustic Tests, Larkin & Tsoi, S/C & L/V Dynamic Environments Workshop, June 1999
The Coherence of Reverberant Sound Fields, Jacobson & Rosin, J-ASA, Vol.108, No.1, 03/21/2000
Direct, Near-Field Acoustic Testing at Orbital Sciences Corporation, Paul Larkin, IEST/ESTECH 2000, May 2000
Direct Near-Field Acoustic Testing – Update, Larkin, S/C & L/V Dynamic Environments Workshop, June 2000
Direct Acoustic Test of the QuickSCAT Spacecraft, D. Anthony, T. Scharton, A. Leccese, SAE/AIAA World Aviation Congress, 10/19-21/00
Direct, Near Field Acoustic Testing, Larkin & Whalen, SAE/AIAA World Aviation Congress, 10/19-21/00
An Innovative Acoustic Test Method for the Faster, Better, Cheaper Environment, Paul Larkin,19th Aerospace Testing Seminar, October 2000
Direct Near-Field Acoustic Testing – Work-in-Progress, Paul Larkin, S/C & L/V Dynamic Environments Workshop, June 2001
High-Intensity Acoustics Testing, IEST-RP-DTE040.1, Institute of Environmental Sciences and Technology, January, 2003.
Control of an Acoustical Speaker System in a Reverberant Chamber, Paul Larkin & Dave Smallwood, 21st Aerospace Testing Seminar, October 2003
Rectangular Control of Muli-Shaker Systems: Theory and Some Practical Results, Underwood and Keller, Spectral Dynamics, Inc., San Jose, CA, 2003
JAGUAR Random Acoustic Control and Analysis Operating Note, 2560-0122/A, Spectral Dynamics, Inc., San Jose, CA, 2003
Direct Field Acoustic Test and Simulation Analysis, Fred Hausle, Steve Johnston, John Stadille, S/C & L/V Dynamic Environments Workshop, June 2004
Control of an Acoustical Speaker System in a Reverberant Chamber, Paul Larkin & Dave Smallwood, 21st Aerospace Testing Seminar, 10/21/2004
Direct Acoustic Verses Reverberant Testing of the Cloud Profiling Radar Instrument, Michael O’Connell & Fred Hausle, S/C & L/V Dynamic Environments Workshop, June 2005
Direct Field and Reverberant Chamber Acoustic Test Comparisons, Michael O’Connell, S/C & L/V Dynamic Environments Workshop, June 2007
Investigations Toward Development of Standard Practices for Direct Field Acoustic Testing, Michael B. Van Dyke, 24th Aerospace Testing Seminar, April 2008
Toward Development of Standard Practices in Direct Field Acoustic Testing, Michael B. Van Dyke, S/C & L/V Dynamic Environments Workshop, June 2008.
Direct Field Acoustic Testing, Paul Larkin and Bob Goldstein, 25th AIAA/Space Simulation Conference, October 2008.
Direct Field vs Reverberant Field Acoustic Testing, Gordon Maahs, Spacecraft & Launch Vehicle Dynamic Environments Workshop, June 2009.
Direct Field Acoustic Test (DFAT) - Recommended Practice, Paul Larkin, Spacecraft & Launch Vehicle Dynamic Environments Workshop, June 2009.
Acoustically Induced Vibration of Structures, Reverberant vs. Direct Acoustic Testing, Koliani, O’Connell, Tsoi, 25th Aerospace Testing Seminar, October 2009.
Direct Field Acoustic Test – Recommended Practice, Larkin and Goldstein, 25th Aerospace Testing Seminar, October 2009.
Direct Field Acoustic Test (DFAT), Paul Larkin, AIAA/Working Group on Dynamic Space Simulation, May 2010.
Direct Field vs. Reverberant Field DFAT, Larkin and Maahs, SC & LV Dynamic Environments Workshop, June 2010.
Recent Developments in Direct Field Acoustic Testing, Larkin and Goldstein, 26th Space Simulation Conference. October 2010.
Direct Field Acoustic Testing of a Flight System: Logistics, Challenges and Results, Babuska, Gurule, Skousen, Stasiunas, 81st Shock & Vibration Symposium, October 2010.
Analytical Modeling of the Acoustic Field During a Direct Field Acoustic Test, Mesh, Rouse, Stasiunas, 26th Aerospace Testing Seminar, March 2011.
Small Direct Field Acoustic Noise Facility, Saggini, Tiani, Ribour, Poulain, Herzog, 26th Aerospace Testing Seminar, March 2011.
Acoustic Testing of Flight Hardware Using Loudspeakers: How Much Do We Know About This Method, Kolaini and Kern, 26th Aerospace Testing Seminar, March 2011.
Issues Related to Large Flight Hardware Acoustic Qualification Testing, Kolaini, Kern and Perry, 26th Aerospace Testing Seminar, March 2011.
Spatial Variability Caused by Acoustic Wave Interference in Single-Drive Direct Field Acoustic Testing, VanDyke and Peters, 26th Aerospace Testing Seminar, March 2011.
Small Direct Field Acoustic Noise Test Facility, Saggini,Tiani, Ribour, Poulain and Herzog, 26th Aerospace Testing Seminar, March 2011.
Direct Field Acoustic Testing (DFAT) Recommended Practice (RP) Development, Foss and Larkin, IEST/ESTECH 2011, May 2011.
Vibro-acoustic Predictions: Direct Acoustic vs. Reverberant Acoustic Fields, Ali Kolaini, SC & LV Dynamic Environments Workshop, June 2011.
MIMO Acoustic Control for DFAT, Larkin and Spicer, SC & LV Dynamic Environments Workshop, June 2011.
Temporal Evaluation of DFAT Data Quality, Levi Smith, IEST/ESTECH 2012, May 2012.
Using Narrow-band Difference as a Comparative Metric for Acoustic Fields, Clinton Maldoon, IEST/ESTECH 2012, May 2012.
Direct Field Acoustic Test of the RBSP Spacecraft, Gordon Maahs, 27th Aerospace Testing Seminar, October 2012.
Status of Direct Field Acoustic Testing, Hayes and Larkin, 27th Aerospace Testing Seminar, October 2012.
Some Questions Regarding Aspects of Acoustic Testing and Test Facilities, Arloe Wesley Mayne III, 27th Aerospace Testing Seminar, October 2012.
Experiences in Performing a High-Intensity, Direct Field Acoustic Test on a Contamination-Sensitive System, Stasiunas, Babuska, and Skousen, 27th Aerospace Testing Seminar, October 2012.
Impact of Acoustic Standing Waves on Structural Responses: Reverberant Acoustic Testing (RAT) vs. Direct Field Acoustic Testing (DFAT), Kolaini, Doty, and Chang, 27th Aerospace Testing Seminar,October 2012.
Further Developments Using MIMO Acoustic Control for DFAT, Paul Larkin, 27th Space Simulation Conference, November 2012.",Category:Acoustics,1
51,52,Fessenden oscillator,"A Fessenden oscillator is an electro-acoustic transducer invented by Reginald Fessenden, with development starting in 1912 at the Submarine Signal Company of Boston. It was the first successful acoustical echo ranging device. Similar in operating principle to a dynamic voice coil loudspeaker, it was an early kind of transducer, capable of creating underwater sounds and of picking up their echoes.
The creation of this device was motivated by the RMS Titanic disaster of 1912, which highlighted the need to protect ships from collisions with icebergs, obstacles, and other ships. Because of its relatively low operating frequency, it has been replaced in modern transducers by piezoelectric devices.

Oscillator
The oscillator in the name referred to the fact that the device vibrated and moved water in response to a driving AC current. It was not an electronic oscillator but a mechanical one in that it generated repetitive mechanical vibrations. Electronic oscillators did not yet exist when this device was created. Because the design of the device does not depend on a resonant response, it should not be considered an harmonic oscillator.

Operation
The Fessenden oscillator somewhat resembled a modern dynamic microphone or dynamic loudspeaker in overall construction. A circular metal plate, clamped at its edge, in contact with the water on one side, was attached on the other side to a copper tube, which was free to move in the circular gap of a magnet system. The magnet system had a direct-current winding to provide a polarizing magnetic field in the gap, and an alternating current winding that induced currents in the copper tube. These induced currents produced a magnetic field that reacted against the polarizing field. The resulting force was communicated to the membrane and in turn provided acoustic vibrations into the water.
Unlike previous underwater sound sources such as underwater bells, the Fessenden oscillator was reversible; the AC winding could be connected to a head set and underwater sounds and echoes could be heard. Using this device Fessenden was able to detect icebergs at a distance of about 2 miles, and occasionally detected echoes from the sea floor.
The device could also be used as an underwater telegraph, sending Morse code through the water. The Fessenden underwater signalling apparatus, or more usually just ""The Fessenden"", was fitted to Royal Navy submarines in World War I. British K-series submarines were equipped with Fessenden oscillators starting in 1915. However, a submarine signalling the surface could be heard by any nearby (enemy) hydrophone, so the system had restricted utility during wartime patrols.

Application
During the First World War the Fessenden oscillator was applied to detection of submarines, but its rather low operating frequency of around 1 kilohertz gave it a very broad beam, unsuitable for detecting and localising small targets. In peacetime, the oscillator was used for depth finding, where the lack of directionality was not a concern, and Fessenden designed a commercial fathometer using a carbon microphone as receiver, for the Submarine Signal Company.

See also
Underwater acoustics
Underwater acoustic communication
Hydrophone
List of Reginald Fessenden patents

References
Bibliography
Frost, Gary Lewis (July 2001). ""Inventing Schemes and Strategies: The Making and Selling of the Fessenden Oscillator"". Technology and Culture. Project MUSE. 42 (3): 462–488. doi:10.1353/tech.2001.0109.

Further reading
Fay, H. J. W. (February 1917). ""Submarine Signaling -- Fessenden Oscillator"". Journal of the American Society for Naval Engineers. 29 (1): 101–113. doi:10.1111/j.1559-3584.1917.tb01183.x. 
Rolt, Kenneth D. (1994). ""The Fessenden oscillator: History, electroacoustic model, and performance estimate"". J. Acoust. Soc. Am. 95 (5): 2832. Bibcode:1994ASAJ...95.2832R. doi:10.1121/1.409629.",Category:Maritime safety,1
52,53,Category:Acoustical engineers,,Category:Acoustics,1
53,54,Lighthill mechanism,"In acoustics, Lighthill mechanism refers to the generation of sound by turbulent fluid motions, as proposed by Sir James Lighthill in 1952.

See also
Aeroacoustics

References
M. J. Lighthill, ""On Sound Generated Aerodynamically. I. General Theory,"" Proc. R. Soc. Lond. A 211 (1952) pp. 564-587.
M. J. Lighthill, ""On Sound Generated Aerodynamically. II. Turbulence as a Source of Sound,"" Proc. R. Soc. Lond. A 222 (1954) pp. 1-32.",Category:Acoustics,1
54,55,Diffuse field acoustic testing,"Diffuse field acoustic testing is the testing of the mechanical resistance of a spacecraft to the acoustic pressures during launch.
In the aerospace industry, acoustic chambers are the main facilities for such tests. A chamber is a reverberant room that creates a diffuse sound field and is composed of an empty volume (from 1 m3 to 2900 m3) and a multifrequency sound generation system.

Diffuse field principle
Theoretically, diffuse field is defined as a Sound pressure field where there is no privileged direction of the energy. In other words, when sound pressure is the same everywhere in the room. This is obtained with large rooms with no absorbent materials on walls, ceiling or floor. Diffusionis enhanced in asymmetric rooms. To obtain such conditions, the room must be reverberant. The source's direct field must be negligible compared to the reverberant field, avoiding privileged propagation.

Reverberation time
Reverberation is due to multiple reflections on walls with some delays that come back to the receptor. Summing up these contributions, a reverberant pressure field is created. The more reverberation, the more the field is diffused.
Two oft-used measures of reverberation time quantify this parameter,  : 
  
    
      
        R
        
          T
          
            30
          
        
      
    
    {\displaystyle RT_{30}}
   and 
  
    
      
        R
        
          T
          
            60
          
        
      
    
    {\displaystyle RT_{60}}
  . These values are the interval for the sound pressure level to the lower of 30 or 60 dBSPL. It can be obtained by measuring the sound pressure decrease after a sound impulse or by using approximate formulas such as Sabine's or Eyring's. In the case of a diffuse field (low absorption on the walls, and big volumes) Sabine's formula is used.

  
    
      
        R
        
          T
          
            60
          
        
        =
        0.16
        
          
            V
            A
          
        
      
    
    {\displaystyle RT_{60}=0.16{\frac {V}{A}}}
  
Where 
  
    
      
        A
        =
        ?
        S
        ×
        ?
      
    
    {\displaystyle A=\sum S\times \alpha }
   is the equivalent absorption area involving the surface 
  
    
      
        S
      
    
    {\displaystyle S}
   of the walls and their absorption coefficient 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
  .

Geometrical approach
Many theoretical ways to model sound propagation are used. One of these is the geometrical approach. This represents sound waves as a ray of energy propagating. When it meets an obstacle, this ray has two possible behaviors: It can be reflected following the normal of the plan, specular reflection. Alternatively it can be separated into many rays following a mathematical law (for example Lambert's law), diffused reflection.

Quantities involving diffuse pressure field
Frequency is a main factor of a good diffused field. Some phenomena linked to frequency of the sound pressure field lead to poor homogeneity of a pressure field. The frequency response of a room is the amplification or reduction of some frequencies. It represents the repartition of pressure with respect to frequency. In low frequencies this can lead to mode apparition. These modes are due to standing waves that lead to maximum and minimum pressure according to the geometry of the room. To determine the frequency for which the pressure field can be considered diffused, Schroeder's frequency is commonly used. It is obtained considering the frequency from which the modal overlap exceeds 
  
    
      
        M
        =
        3
      
    
    {\displaystyle M=3}
  . Below this frequency, the field is not diffuse and standing waves create pressure modes

  
    
      
        
          F
          
            s
          
        
        =
        2000
        
          
            
              
                R
                
                  T
                  
                    60
                  
                
              
              V
            
          
        
      
    
    {\displaystyle F_{s}=2000{\sqrt {\frac {RT_{60}}{V}}}}
  
Where 
  
    
      
        R
        
          T
          
            60
          
        
      
    
    {\displaystyle RT_{60}}
   is the reverberation time of the room and 
  
    
      
        V
      
    
    {\displaystyle V}
   its volume.

For example, in the case of a rectangular room, low frequency modes are determined relative to the room dimensions as

  
    
      
        
          f
          
            l
            m
            n
          
        
        =
        
          
            
              c
              
                0
              
            
            2
          
        
        
          
            
              
                (
                
                  
                    l
                    
                      L
                      
                        x
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    m
                    
                      L
                      
                        y
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    n
                    
                      L
                      
                        z
                      
                    
                  
                
                )
              
              
                2
              
            
          
        
      
    
    {\displaystyle f_{lmn}={\frac {c_{0}}{2}}{\sqrt {\left({\frac {l}{L_{x}}}\right)^{2}+\left({\frac {m}{L_{y}}}\right)^{2}+\left({\frac {n}{L_{z}}}\right)^{2}}}}
  

Where 
  
    
      
        l
      
    
    {\displaystyle l}
  , 
  
    
      
        m
      
    
    {\displaystyle m}
   and 
  
    
      
        n
      
    
    {\displaystyle n}
   are respectively the mode of the length 
  
    
      
        
          L
          
            x
          
        
      
    
    {\displaystyle L_{x}}
  , 
  
    
      
        
          L
          
            y
          
        
      
    
    {\displaystyle L_{y}}
   and 
  
    
      
        
          L
          
            z
          
        
      
    
    {\displaystyle L_{z}}
   of the room and 
  
    
      
        
          c
          
            0
          
        
      
    
    {\displaystyle c_{0}}
   the celerity of sound in the working fluid.

Aerospace application
Acoustic tests are mainly use for environmental tests on aircraft structures. Satellites are expensive products with high-engineering built-in components. To improve the resistance of a spacecraft during launch and during its orbital life, analysis is focused on tests in three categories : Thermal, Radio-frequencies and Vibrations. This last test area is focused on the mechanical stresses that the specimen will meet during its life, especially during launch.
Acoustics creates mechanical stresses during the first five seconds. Sound pressure levels can go up to 150 dBSPL. Acoustic tests are used to verify the mechanical resistance of the satellite and its elements to acoustic pressures generated.

Accelerometer measurement
Once the sound generation system is working, acceleration measurement is performed by accelerometers placed on the specimen.

Acoustic chamber
Sound generation
To test a satellite, a sound generation system generates a broadband spectrum ([25 Hz-10000]Hz) simulating the maximum envelope of all launchers that the satellite may fly in. To qualify, three tests are realized with changing global gain compared to launcher spectrum:
Low-Level : - 8 dB SPL
Intermediate : - 4 dB SPL
PFM (Proto Flight Model) : 0 dB SPL
Before testing the satellite, an empty room test is performed to check the chamber's signature.
This pressure field is generated by multifrequency sirens powered by nitrogen or compressed air modulators. This system can generate sound pressure levels up to 160 dBSPL. Each acoustic chamber has its own configurations, but each siren is centered on a frequency where sound pressure levels are the highest. In some cases these sirens can be completed with electroacoustic systems to generate and control midrange and high frequencies. Sirens generate low frequencies, but with high sound pressure levels distortion appears that leads to higher harmonics. Loudspeakers are used in some chambers to control these frequencies.
To produce exact levels, piloting microphones check sound pressure levels and apply a realtime gain correction to adjust the level.

Advantages
Homogeneity : Spatial homogeneity guaranteed for ± 1.5 dBSPL
Low frequency generation : Very efficient low frequency generation (below 50 Hz)
Security : Control with piloting microphones that adjust the level or abort if needed
Representativeness : Faithful to real stresses during launch
Well known process : Used by many aerospace industries

Disadvantages
Gas generation : May require large amounts of nitrogen
High frequency control : If no high frequency sirens or electroacoustic devices are included, only harmonics generated by distortion produce mid and high frequencies.

Examples
Thales Alenia Space (Cannes) : 1000 m3
IABG (Ottobrunn) : 1378 m3
NASA : 2860 m3

References
External links
Thales Alenia Space Official Website
IABG Space Official Website
NASA's Space Power Facility (SPF) Wikipedia page
Intespace's acoustic test facility",Category:Orphaned articles from June 2016,1
55,56,Rayleigh wave,"Rayleigh waves are a type of surface acoustic wave that travel along the surface of solids. They can be produced in materials in many ways, such as by a localized impact or by piezo-electric transduction, and are frequently used in non-destructive testing for detecting defects. Rayleigh waves are part of the seismic waves that are produced on the Earth by earthquakes. When guided in layers they are referred to as Lamb waves, Rayleigh–Lamb waves, or generalized Rayleigh waves.

Characteristics
Rayleigh waves are a type of surface wave that travel near the surface of solids. Rayleigh waves include both longitudinal and transverse motions that decrease exponentially in amplitude as distance from the surface increases. There is a phase difference between these component motions.
The existence of Rayleigh waves was predicted in 1885 by Lord Rayleigh, after whom they were named. In isotropic solids these waves cause the surface particles to move in ellipses in planes normal to the surface and parallel to the direction of propagation – the major axis of the ellipse is vertical. At the surface and at shallow depths this motion is retrograde, that is the in-plane motion of a particle is counterclockwise when the wave travels from left to right. At greater depths the particle motion becomes prograde. In addition, the motion amplitude decays and the eccentricity changes as the depth into the material increases. The depth of significant displacement in the solid is approximately equal to the acoustic wavelength. Rayleigh waves are distinct from other types of surface or guided acoustic waves such as Love waves or Lamb waves, both being types of guided waves supported by a layer, or longitudinal and shear waves, that travel in the bulk.
Rayleigh waves have a speed slightly less than shear waves by a factor dependent on the elastic constants of the material. The typical speed of Rayleigh waves in metals is of the order of 2–5 km/s, and the typical Rayleigh speed in the ground is of the order of 50–300 m/s. For linear elastic materials with positive Poisson ratio (
  
    
      
        ?
        >
        0
      
    
    {\displaystyle \nu >0}
  ), the Rayleigh wave speed can be approximated as 
  
    
      
        
          c
          
            R
          
        
        
          /
        
        
          c
          
            S
          
        
        =
        
          
            
              0.862
              +
              1.14
              ?
            
            
              1
              +
              ?
            
          
        
      
    
    {\displaystyle c_{R}/c_{S}={\frac {0.862+1.14\nu }{1+\nu }}}
  . Since Rayleigh waves are confined near the surface, their in-plane amplitude when generated by a point source decays only as 
  
    
      
        
          1
        
        
          /
        
        
          
            r
          
        
      
    
    {\displaystyle {1}/{\sqrt {r}}}
  , where 
  
    
      
        r
      
    
    {\displaystyle r}
   is the radial distance. Surface waves therefore decay more slowly with distance than do bulk waves, which spread out in three dimensions from a point source. This slow decay is one reason why they are of particular interest to seismologists. Rayleigh waves can circle the globe multiple times after a large earthquake and still be measurably large.
In seismology, Rayleigh waves (called ""ground roll"") are the most important type of surface wave, and can be produced (apart from earthquakes), for example, by ocean waves, by explosions, by railway trains and ground vehicles, or by a sledgehammer impact.

Rayleigh wave dispersion
In isotropic, linear elastic materials described by Lame coefficients 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   and 
  
    
      
        ?
      
    
    {\displaystyle \mu }
  , Rayleigh waves have a speed given by solutions to the equation

  
    
      
        
          ?
          
            3
          
        
        ?
        8
        
          ?
          
            2
          
        
        +
        8
        ?
        (
        3
        ?
        2
        ?
        )
        ?
        16
        (
        1
        ?
        ?
        )
        =
        0
        ,
      
    
    {\displaystyle \zeta ^{3}-8\zeta ^{2}+8\zeta (3-2\eta )-16(1-\eta )=0,}
  
where 
  
    
      
        ?
        =
        
          ?
          
            2
          
        
        
          /
        
        
          k
          
            2
          
        
        
          ?
          
            2
          
        
      
    
    {\displaystyle \zeta =\omega ^{2}/k^{2}\beta ^{2}}
  , 
  
    
      
        ?
        =
        
          ?
          
            2
          
        
        
          /
        
        
          ?
          
            2
          
        
      
    
    {\displaystyle \eta =\beta ^{2}/\alpha ^{2}}
  , 
  
    
      
        ?
        
          ?
          
            2
          
        
        =
        ?
        +
        2
        ?
      
    
    {\displaystyle \rho \alpha ^{2}=\lambda +2\mu }
  , and 
  
    
      
        ?
        
          ?
          
            2
          
        
        =
        ?
      
    
    {\displaystyle \rho \beta ^{2}=\mu }
  . Since this equation has no inherent scale, the boundary value problem giving rise to Rayleigh waves are dispersionless. An interesting special case is the Poisson solid, for which 
  
    
      
        ?
        =
        ?
      
    
    {\displaystyle \lambda =\mu }
  , since this gives a frequency-independent phase velocity equal to 
  
    
      
        ?
        
          /
        
        k
        =
        ?
        
          
            0.8453
          
        
      
    
    {\displaystyle \omega /k=\beta {\sqrt {0.8453}}}
  .
The elastic constants often change with depth, due to the changing properties of the material. This means that the velocity of a Rayleigh wave in practice becomes dependent on the wavelength (and therefore frequency), a phenomenon referred to as dispersion. Waves affected by dispersion have a different wave train shape. Rayleigh waves on ideal, homogeneous and flat elastic solids show no dispersion, as stated above. However, if a solid or structure has a density or sound velocity that varies with depth, Rayleigh waves become dispersive. One example is Rayleigh waves on the Earth's surface: those waves with a higher frequency travel more slowly than those with a lower frequency. This occurs because a Rayleigh wave of lower frequency has a relatively long wavelength. The displacement of long wavelength waves penetrates more deeply into the Earth than short wavelength waves. Since the speed of waves in the Earth increases with increasing depth, the longer wavelength (low frequency) waves can travel faster than the shorter wavelength (high frequency) waves. Rayleigh waves thus often appear spread out on seismograms recorded at distant earthquake recording stations. It is also possible to observe Rayleigh wave dispersion in thin films or multi-layered structures.

Rayleigh waves in non-destructive testing
Rayleigh waves are widely used for materials characterization, to discover the mechanical and structural properties of the object being tested – like the presence of cracking, and the related shear modulus. This is in common with other types of surface waves. The Rayleigh waves used for this purpose are in the ultrasonic frequency range.
They are used at different length scales because they are easily generated and detected on the free surface of solid objects. Since they are confined in the vicinity of the free surface within a depth (~ the wavelength) linked to the frequency of the wave, different frequencies can be used for characterization at different length scales.

Rayleigh waves in electronic devices
Rayleigh waves propagating at high ultrasonic frequencies (10–1000 MHz) are used widely in different electronic devices. In addition to Rayleigh waves, some other types of surface acoustic waves (SAW), e.g. Love waves, are also used for this purpose. Examples of electronic devices using Rayleigh waves are filters, resonators, oscillators, sensors of pressure, temperature, humidity, etc. Operation of SAW devices is based on the transformation of the initial electric signal into a surface wave that, after achieving the required changes to the spectrum of the initial electric signal as a result of its interaction with different types of surface inhomogeneity, is transformed back into a modified electric signal. The transformation of the initial electric energy into mechanical energy (in the form of SAW) and back is usually accomplished via the use of piezoelectric materials for both generation and reception of Rayleigh waves as well as for their propagation.

Rayleigh waves in geophysics
Rayleigh waves from earthquakes
Because Rayleigh waves are surface waves, the amplitude of such waves generated by an earthquake generally decreases exponentially with the depth of the hypocenter (focus). However, large earthquakes may generate Rayleigh waves that travel around the Earth several times before dissipating.
In seismology longitudinal and shear waves are known as P-waves and S-waves, respectively, and are termed body waves. Rayleigh waves are generated by the interaction of P- and S- waves at the surface of the earth, and travel with a velocity that is lower than the P-, S-, and Love wave velocities. Rayleigh waves emanating outward from the epicenter of an earthquake travel along the surface of the earth at about 10 times the speed of sound in air (0.340 km/s), that is ~3 km/s.
Due to their higher speed, the P- and S-waves generated by an earthquake arrive before the surface waves. However, the particle motion of surface waves is larger than that of body waves, so the surface waves tend to cause more damage. In the case of Rayleigh waves, the motion is of a rolling nature, similar to an ocean surface wave. The intensity of Rayleigh wave shaking at a particular location is dependent on several factors:

The size of the earthquake.
The distance to the earthquake.
The depth of the earthquake.
The geologic structure of the crust.
The focal mechanism of the earthquake.
The rupture directivity of the earthquake.
Local geologic structure can serve to focus or defocus Rayleigh waves, leading to significant differences in shaking over short distances.

Rayleigh waves in seismology
Low frequency Rayleigh waves generated during earthquakes are used in seismology to characterise the Earth's interior. In intermediate ranges, Rayleigh waves are used in geophysics and geotechnical engineering for the characterisation of oil deposits. These applications are based on the geometric dispersion of Rayleigh waves and on the solution of an inverse problem on the basis of seismic data collected on the ground surface using active sources (falling weights, hammers or small explosions, for example) or by recording microtremors. Rayleigh ground waves are important also for environmental noise and vibration control since they make a major contribution to traffic-induced ground vibrations and the associated structure-borne noise in buildings.

Other manifestations
Animals
Low frequency (< 20 Hz) Rayleigh waves are inaudible, yet they can be detected by many mammals, birds, insects and spiders. Humans should be able to detect such Rayleigh waves through their Pacinian corpuscles, which are in the joints, although people do not seem to consciously respond to the signals. Some animals seem to use Rayleigh waves to communicate. In particular, some biologists theorize that elephants may use vocalizations to generate Rayleigh waves. Since Rayleigh waves decay slowly, they should be detectable over long distances. Note that these Rayleigh waves have a much higher frequency than Rayleigh waves generated by earthquakes.
After the 2004 Indian Ocean earthquake, some people have speculated that Rayleigh waves served as a warning to animals to seek higher ground, allowing them to escape the more slowly traveling tsunami. At this time, evidence for this is mostly anecdotal. Other animal early warning systems may rely on an ability to sense infrasonic waves traveling through the air.

See also
Linear elasticity
Longitudinal wave
Love wave
P-wave
Phonon
S-wave
Seismology
Surface acoustic wave

Notes
Further reading
Viktorov, I.A. (2013) ""Rayleigh and Lamb Waves: Physical Theory and Applications"", Springer; Reprint of the original 1st 1967 edition by Plenum Press, New York. ISBN 978-1489956835.
Aki, K. and Richards, P. G. (2002). Quantitative Seismology (2nd ed.). University Science Books. ISBN 0-935702-96-2.
Fowler, C. M. R. (1990). The Solid Earth. Cambridge, UK: Cambridge University Press. ISBN 0-521-38590-3.
Lai, C.G., Wilmanski, K. (Eds.) (2005). Surface Waves in Geomechanics: Direct and Inverse Modelling for Soils and Rocks"" Series: CISM International Centre for Mechanical Sciences, Number 481, Springer, Wien, ISBN 978-3-211-27740-9
Y. Sugawara, O. B. Wright, O. Matsuda, M. Takigahira, Y. Tanaka, S. Tamura and V. E. Gusev, ""Watching ripples on crystals"", Phys. Rev. Lett. 88, 185504 (2002)

External links
Real-time imaging of Rayleigh waves",Category:Wave mechanics,1
56,57,Acoustic radiation force,"Acoustic radiation force (ARF), as a well-defined scientific subject, emerged in 1902, after publication of classical work by Lord Rayleigh on the theory of sound. Rayleigh introduced the concept of acoustic radiation pressure which he named “the pressure of vibrations”. The detailed history of ARF, its physical basis and biomedical applications can be found in the reviews.
The ARF is, in general, defined as a period-averaged force exerted on the medium by a sound wave. The mechanisms of acoustic radiation force generations include: (1) change in the density of energy of the propagating wave due to absorption and scattering; (2) reflection from inclusions, walls or other interfaces; and (3) spatial variations in propagation velocity, and (4) spatial variations of energy density in standing acoustic waves.
The first of these mechanisms is the basis of biomedical applications of ARF related to assessing viscoelastic properties of biological tissues and fluids, and specifically to elasticity imaging. The simplified equation for the ARF, F, generated due to absorption and scattering in tissue is given as

  
    
      
        F
        =
        
          
            
              2
              ?
              I
            
            c
          
        
      
    
    {\displaystyle F={\frac {2\alpha I}{c}}}
  
where
? is the absorption coefficient
I is the ultrasound intensity, and
c is the longitudinal wave speed in the medium.
The second of the listed mechanisms is the basis of one of the oldest applications of ARF, proposed by Wood and Loomis in the 1920s to measure total power in ultrasonic beams which later was implemented in radiation balances  that were widely used to measure the power output from physiotherapy ultrasound units. Until now, it is the standard method of measuring output power for physiotherapy systems.
The third mechanism presents generation of ARF in media without attenuation or acoustic wave reflection. Gradients of acoustic properties of medium, such as variations of sound velocity, cause gradients of the energy density in the propagating acoustic wave. As a result, radiation force is generated. Displacement produced by such non-dissipative acoustic radiation force in inhomogeneous media can be in both directions (outward and inward toward the transducer) depending on the sign of the gradient of energy density of the propagating wave. Non-dissipative radiation force, in contrast to the dissipative radiation force, can be generated in tissue at low sub-MHz frequencies despite the low attenuation coefficient of the media. This mechanism is currently used for development of acoustic tweezers.
The fourth mechanism of generation of ARF is related to an actively explored area of manipulation of biological cells and particles in standing ultrasonic wave fields. It has been known since the 19th century that an object in a sound field is affected by a steady-state acoustic radiation force. In a classical experiment, Kundt and Lehman trapped dust particles in a tube by applying a standing-wave field. However, it is only in the last decades the phenomenon has found widespread application ranging from manipulation of cells in suspension, increasing the sensitivity of biosensors and immunochemical tests.

Medical imaging applications
The widest area of biomedical applications of ARF is related to medical diagnostics, to assessing viscoelastic properties of biological tissues and fluids, and more specifically to elasticity imaging. Acoustic radiation force of focused ultrasound became the basis of numerous emerging diagnostic imaging techniques such as Shear Wave Elasticity Imaging (SWEI), Acoustic Radiation Force Impulse Imaging (ARFI), Supersonic Shear Imaging (SSI), Shearwave Dispersion Ultrasound Vibrometry (SDUV), Harmonic Motion Imaging (HMI), Comb-push Ultrasound Shear Elastography (CUSE), and Spatially Modulated Ultrasound Radiation Force (SMURF). One of the most advanced modalities of the ARF-based elastography is Supersonic Shear Imaging (SSI). SSI uses ARF to induce a 'push' inside the tissue of interest generating shear waves and the tissue's stiffness is computed from how fast the shear wave travels through the tissue. Shear wave elasticity imaging has been developed into a clinical imaging modality over the last two decades and the radiation force-based methods are currently implemented in the commercial devices: SuperSonic Imagine Aixplorer, in the Siemens Acuson S2000 and S3000 as Virtual Touch Quantification, and in the General Electric Logiq E9.

References
See also
Elasticity
Elastography
Shear Wave Elasticity Imaging
Soft tissue
Tactile Imaging",Category:Acoustics,1
57,58,Attenuation,"In physics, attenuation or, in some contexts, extinction is the gradual loss of flux intensity through a medium. For instance, dark glasses attenuate sunlight, lead attenuates X-rays, and water and air attenuate both light and sound at variable attenuation rates.
Hearing protectors help reduce acoustic flux from flowing into the ears. This phenomenon is called acoustic attenuation and is measured in decibels (dBs).
In electrical engineering and telecommunications, attenuation affects the propagation of waves and signals in electrical circuits, in optical fibers, and in air. Electrical attenuators and optical attenuators are commonly manufactured components in this field.

Background
In many cases, attenuation is an exponential function of the path length through the medium. In chemical spectroscopy, this is known as the Beer–Lambert law. In engineering, attenuation is usually measured in units of decibels per unit length of medium (dB/cm, dB/km, etc.) and is represented by the attenuation coefficient of the medium in question. Attenuation also occurs in earthquakes; when the seismic waves move farther away from the hypocenter, they grow smaller as they are attenuated by the ground.

Ultrasound
One area of research in which attenuation figures strongly is in ultrasound physics. Attenuation in ultrasound is the reduction in amplitude of the ultrasound beam as a function of distance through the imaging medium. Accounting for attenuation effects in ultrasound is important because a reduced signal amplitude can affect the quality of the image produced. By knowing the attenuation that an ultrasound beam experiences traveling through a medium, one can adjust the input signal amplitude to compensate for any loss of energy at the desired imaging depth.
Ultrasound attenuation measurement in heterogeneous systems, like emulsions or colloids, yields information on particle size distribution. There is an ISO standard on this technique.
Ultrasound attenuation can be used for extensional rheology measurement. There are acoustic rheometers that employ Stokes' law for measuring extensional viscosity and volume viscosity.
Wave equations which take acoustic attenuation into account can be written on a fractional derivative form, see the article on acoustic attenuation or e.g. the survey paper.

Attenuation coefficient
Attenuation coefficients are used to quantify different media according to how strongly the transmitted ultrasound amplitude decreases as a function of frequency. The attenuation coefficient (
  
    
      
        ?
      
    
    {\displaystyle \alpha }
  ) can be used to determine total attenuation in dB in the medium using the following formula:

  
    
      
        
          Attenuation
        
        =
        ?
        [
        
          dB
        
        
          /
        
        (
        
          MHz
        
        ?
        
          cm
        
        )
        ]
        ?
        ?
        [
        
          cm
        
        ]
        ?
        
          f
        
        [
        
          MHz
        
        ]
      
    
    {\displaystyle {\text{Attenuation}}=\alpha [{\text{dB}}/({\text{MHz}}\cdot {\text{cm}})]\cdot \ell [{\text{cm}}]\cdot {\text{f}}[{\text{MHz}}]}
  
As this equation shows, besides the medium length and attenuation coefficient, attenuation is also linearly dependent on the frequency of the incident ultrasound beam. Attenuation coefficients vary widely for different media. In biomedical ultrasound imaging however, biological materials and water are the most commonly used media. The attenuation coefficients of common biological materials at a frequency of 1 MHz are listed below:

There are two general ways of acoustic energy losses: absorption and scattering, for instance light scattering. Ultrasound propagation through homogeneous media is associated only with absorption and can be characterized with absorption coefficient only. Propagation through heterogeneous media requires taking into account scattering. Fractional derivative wave equations can be applied for modeling of lossy acoustical wave propagation, see also acoustic attenuation and Ref.

Light attenuation in water
Shortwave radiation emitted from the sun have wavelengths in the visible spectrum of light that range from 360 nm (violet) to 750 nm (red). When the sun’s radiation reaches the sea-surface, the shortwave radiation is attenuated by the water, and the intensity of light decreases exponentially with water depth. The intensity of light at depth can be calculated using the Beer-Lambert Law.
In clear open waters, visible light is absorbed at the longest wavelengths first. Thus, red, orange, and yellow wavelengths are absorbed at higher water depths, and blue and violet wavelengths reach the deepest in the water column. Because the blue and violet wavelengths are absorbed last compared to the other wavelengths, open ocean waters appear deep-blue to the eye.
In near-shore (coastal) waters, sea water contains more phytoplankton than the very clear central ocean waters. Chlorophyll-a pigments in the phytoplankton absorb light, and the plants themselves scatter light, making coastal waters less clear than open waters. Chlorophyll-a absorbs light most strongly in the shortest wavelengths (blue and violet) of the visible spectrum. In near-shore waters where there are high concentrations of phytoplankton, the green wavelength reaches the deepest in the water column and the color of water to an observer appears green-blue or green.

Seismic waves
The energy with which an earthquake affects a location depends on the running distance. The attenuation in the signal of ground motion intensity plays an important role in the assessment of possible strong groundshaking. A seismic wave loses energy as it propagates through the earth (attenuation). This phenomenon is tied into the dispersion of the seismic energy with the distance. There are two types of dissipated energy:
geometric dispersion caused by distribution of the seismic energy to greater volumes
dispersion as heat, also called intrinsic attenuation or anelastic attenuation.

Electromagnetic
Attenuation decreases the intensity of electromagnetic radiation due to absorption or scattering of photons. Attenuation does not include the decrease in intensity due to inverse-square law geometric spreading. Therefore, calculation of the total change in intensity involves both the inverse-square law and an estimation of attenuation over the path.
The primary causes of attenuation in matter are the photoelectric effect, compton scattering, and, for photon energies of above 1.022 MeV, pair production.

Radiography
See Attenuation coefficient.

Optics
Attenuation in fiber optics, also known as transmission loss, is the reduction in intensity of the light beam (or signal) with respect to distance travelled through a transmission medium. Attenuation coefficients in fiber optics usually use units of dB/km through the medium due to the relatively high quality of transparency of modern optical transmission media. The medium is typically a fiber of silica glass that confines the incident light beam to the inside. Attenuation is an important factor limiting the transmission of a digital signal across large distances. Thus, much research has gone into both limiting the attenuation and maximizing the amplification of the optical signal. Empirical research has shown that attenuation in optical fiber is caused primarily by both scattering and absorption.
Attenuation in fiber optics can be quantified using the following equation:

  
    
      
        
          Attenuation (dB)
        
        =
        10
        ×
        
          log
          
            10
          
        
        ?
        
          (
          
            
              Input intensity (W)
              Output intensity (W)
            
          
          )
        
      
    
    {\displaystyle {\text{Attenuation (dB)}}=10\times \log _{10}\left({\frac {\text{Input intensity (W)}}{\text{Output intensity (W)}}}\right)}

Light scattering
The propagation of light through the core of an optical fiber is based on total internal reflection of the lightwave. Rough and irregular surfaces, even at the molecular level of the glass, can cause light rays to be reflected in many random directions. This type of reflection is referred to as ""diffuse reflection"", and it is typically characterized by wide variety of reflection angles. Most objects that can be seen with the naked eye are visible due to diffuse reflection. Another term commonly used for this type of reflection is ""light scattering"". Light scattering from the surfaces of objects is our primary mechanism of physical observation.   Light scattering from many common surfaces can be modelled by lambertian reflectance.
Light scattering depends on the wavelength of the light being scattered. Thus, limits to spatial scales of visibility arise, depending on the frequency of the incident lightwave and the physical dimension (or spatial scale) of the scattering center, which is typically in the form of some specific microstructural feature. For example, since visible light has a wavelength scale on the order of one micrometer (one millionth of a meter), scattering centers will have dimensions on a similar spatial scale.
Thus, attenuation results from the incoherent scattering of light at internal surfaces and interfaces. In (poly)crystalline materials such as metals and ceramics, in addition to pores, most of the internal surfaces or interfaces are in the form of grain boundaries that separate tiny regions of crystalline order. It has recently been shown that, when the size of the scattering center (or grain boundary) is reduced below the size of the wavelength of the light being scattered, the scattering no longer occurs to any significant extent. This phenomenon has given rise to the production of transparent ceramic materials.
Likewise, the scattering of light in optical quality glass fiber is caused by molecular-level irregularities (compositional fluctuations) in the glass structure. Indeed, one emerging school of thought is that a glass is simply the limiting case of a polycrystalline solid. Within this framework, ""domains"" exhibiting various degrees of short-range order become the building-blocks of both metals and alloys, as well as glasses and ceramics. Distributed both between and within these domains are microstructural defects that will provide the most ideal locations for the occurrence of light scattering. This same phenomenon is seen as one of the limiting factors in the transparency of IR missile domes.

UV-Vis-IR absorption
In addition to light scattering, attenuation or signal loss can also occur due to selective absorption of specific wavelengths, in a manner similar to that responsible for the appearance of color. Primary material considerations include both electrons and molecules as follows:
At the electronic level, it depends on whether the electron orbitals are spaced (or ""quantized"") such that they can absorb a quantum of light (or photon) of a specific wavelength or frequency in the ultraviolet (UV) or visible ranges. This is what gives rise to color.
At the atomic or molecular level, it depends on the frequencies of atomic or molecular vibrations or chemical bonds, how close-packed its atoms or molecules are, and whether or not the atoms or molecules exhibit long-range order. These factors will determine the capacity of the material transmitting longer wavelengths in the infrared (IR), far IR, radio and microwave ranges.
The selective absorption of infrared (IR) light by a particular material occurs because the selected frequency of the light wave matches the frequency (or an integral multiple of the frequency) at which the particles of that material vibrate. Since different atoms and molecules have different natural frequencies of vibration, they will selectively absorb different frequencies (or portions of the spectrum) of infrared (IR) light.

Applications
In optical fibers, attenuation is the rate at which the signal light decreases in intensity. For this reason, glass fiber (which has a low attenuation) is used for long-distance fiber optic cables; plastic fiber has a higher attenuation and, hence, shorter range. There also exist optical attenuators that decrease the signal in a fiber optic cable intentionally.
Attenuation of light is also important in physical oceanography. This same effect is an important consideration in weather radar, as raindrops absorb a part of the emitted beam that is more or less significant, depending on the wavelength used.
Due to the damaging effects of high-energy photons, it is necessary to know how much energy is deposited in tissue during diagnostic treatments involving such radiation. In addition, gamma radiation is used in cancer treatments where it is important to know how much energy will be deposited in healthy and in tumorous tissue.

Radio
Attenuation is an important consideration in the modern world of wireless telecommunication. Attenuation limits the range of radio signals and is affected by the materials a signal must travel through (e.g., air, wood, concrete, rain). See the article on path loss for more information on signal loss in wireless communication.

See also
References
External links
NIST's XAAMDI: X-Ray Attenuation and Absorption for Materials of Dosimetric Interest Database
NIST's XCOM: Photon Cross Sections Database
NIST's FAST: Attenuation and Scattering Tables
Underwater Radio Communication",Category:Acoustics,1
58,59,Franssen effect,"The Franssen effect is an auditory illusion where the listener incorrectly localizes a sound. It was found in 1960 by Nico Valentinus Franssen (1926–1979), a Dutch physicist and inventor. There are two classical experiments, which are related to the Franssen effect, called Franssen effect F1 and Franssen effect F2.

Franssen effect F1
Setup
There are two speakers to the left and right of the listener. Each is about 1 meter in distance from the listener, at approximately 45° angles.

Producing the illusion
The left speaker suddenly begins to produce a sharp pure tone. The two speakers are complementary to each other: i.e., as one increases, the other decreases. The left one is decreased exponentially, and the right speaker becomes the main source of the sound. The interesting illusion achieved here is that the listener perceives the sound as only coming from the left speaker, although the right speaker has been on most of the time.

Franssen effect F2
Experiment
Inside a room (auditorium) there are 2 loudspeakers at different positions. At the beginning of the presentation, loudspeaker 1 emits a sinusoidal signal with a steep attacking slope. Subsequently the power of this loudspeaker remains constant. The listeners can localize this loudspeaker easily. During the stationary part of the envelope the signal is very smoothly faded over from loudspeaker 1 to loudspeaker 2. Although loudspeaker 2 emits all the sound at the end, the listener's auditory events remain at the position of loudspeaker 1. This mislocalization remains, even if the test supervisor plugs off the cables of loudspeaker 1 demonstratively.

Conclusions
This effect gives some information about the capabilities of the human auditory system to localize sound sources in enclosed rooms:
The human auditory system is able to localize a sound source in reverberant sound fields, if there are fast signal changes or signal onsets. (Loudspeaker 1 was correctly localized at the beginning of the experiment.)
The human auditory system is not able to localize signals with a constant amplitude and spectrum in reverberant sound fields. (The fade over to loudspeaker 2 was not recognized by the listeners.)
As long as no sound source can be localized, the direction of the last localized sound source remains as the perceived direction. (The auditory event remained at loudspeaker 1, although loudspeaker 2 emitted all the sound at the end of the experiment.)
When looking at the sound, which arrives at the listener's ears, the following situation appears:
At the beginning of the experiment, when loudspeaker 1 started to emit sound, there was a short time period, where only the direct sound of loudspeaker 1 arrived at the listener's ears. In this time period the localization of loudspeaker 1 was surely possible, because it was not yet disturbed by wall reflections.
Some milliseconds later the sound of the wall reflections arrived and disturbed the localization of sound sources.
During the fade over the level and the spectrum of the emitted sound remained constant. This fade over was overlaid by many wall reflections from the sound situation before. Obviously no sound source localization was possible during this phase.
At the end, when only loudspeaker 2 emitted sound, the situation was quite similar, the sound of the wall reflections, which arrived simultaneously, prevented a localization of this sound source.
As a consequence the auditory system seems only to be able to localize sound sources in reverberant environment at sound onsets or at bigger spectral changes. Then the direct sound of the sound source prevails at least in some frequency ranges and the direction of the sound source can be determined. Some milliseconds later, when the sound of the wall reflections arrives, a sound source localization seems no more to be possible. As long as no new localization is possible, the auditory systems seems to keep the last localized direction as perceived sound source direction.

See also
Binaural fusion
Haas effect

References
External links
Localization of sound in rooms IV: The Franssen effect
The Franssen Effect",Category:Auditory illusions,1
59,60,Sonic soot blowers,"Sonic soot blowers offer a cost-effective and non-destructive means of preventing ash and particulate build-up within the power generation industry. They use high energy – low frequency sound waves that provide 360° particulate de-bonding and at a speed in excess of 344 metres per second. Because they employ non-destructive sound waves, unlike steam soot blowers they eliminate any concerns over corrosion, erosion or mechanical damage and do not produce an effluent stream.

The sonic soot blower can in some ways be compared to a musical reed instrument such as an oboe, where the ‘base tone’ is created by blowing air over a reed and then converting this ‘base tone’ into a particular high or low note, depending on how far the sound wave has to travel along inside the body of the instrument.
The sonic soot blower operates in the same manner, the ‘base tone’ being produced by passing compressed air into a wave generator which houses a titanium diaphragm causing it to oscillate rapidly. This ‘base tone’ is then converted into a range of selected frequencies ranging from 350 Hz down to 60 Hz by the design and length of the horn section, producing the desired sound frequency at a sound level approaching 200 dB. The sonic soot blower is usually ‘sounded’ for a few seconds at intervals of between 3 and 10 minutes. This ‘sounding’ pattern is normally controlled via the plant’s PLC. However, it may also be operated by such means as a SCADA system, individual timers on each solenoid valve or via a manual ball valve.

Construction
Sonic soot blowers are normally constructed from fabricated, 316 grade stainless steel as opposed to some sonic horns which are manufactured from heavy cast iron. For installations in harsher environments, such as high temperature or acidic gas streams, other types of stainless steels are used such as 310, 316 and 825.

Performance
Sonic soot blowers create a rapid series of very powerful sound induced pressure fluctuations which when transmitted into the ash or particulate, cause them to de-bond from other particles and from the heat transfer surface to which they are bonded and so carried away in the gas stream. This prevents the ash from building up and sintering onto the boiler tubes thus significantly reducing thermal efficiency. This is in contrast to the operating principles of steam soot blowers which are usually only employed at most once every eight hours by which time the ash has built up and baked hard onto the heat transfer surfaces. The steam soot blower then tries to blast away his hard deposit, usually only from the leading edge of the steam tubes.

Advantages
Sonic soot blowers are a proven alternative to conventional steam soot blowers in power generation plants which burn a range of fossil fuels and other waste fuels including biofuels. Depending on the application and boiler plant design, sonic soot blowers usually totally replace existing high maintenance steam soot blowers whether of the retractable or rotary type. In a few cases, sonic soot blowers can be used to supplement steam soot blowers. Sonic soot blower cleaning technologies can be applied in superheaters, generating sections, economizers, and airheaters as well as downstream equipment such as electrostatic precipitators, baghouse filters and fans.
The main advantages of sonic soot blowers over steam soot blowers are:-
• Elimination of opacity spikes due to more regular, more efficient cleaning • No structural damage to tube bundles or boiler structure • Elimination of tube corrosion and erosion problems • 360° cleaning of all tube surfaces – not harsh leading edge tube cleaning as with steam soot blowers • Prevention of ash build up and sintering on steam tubes due to sonic soot blowers regular operation • Extremely low maintenance or operational costs • Eco-friendly – helps to combat global climate change and the effect of global warming

References
Sonic soot blowers
Mechanisms of steam soot blower erosion",Category:Audio engineering,1
60,61,Category:Psychoacoustics,,Category:Acoustics,1
61,62,Otoacoustic emission,"An otoacoustic emission (OAE) is a sound which is generated from within the inner ear. Having been predicted by Thomas Gold in 1948, its existence was first demonstrated experimentally by David Kemp in 1978 and otoacoustic emissions have since been shown to arise through a number of different cellular and mechanical causes within the inner ear. Studies have shown that OAEs disappear after the inner ear has been damaged, so OAEs are often used in the laboratory and the clinic as a measure of inner ear health.
Broadly speaking, there are two types of otoacoustic emissions: spontaneous otoacoustic emissions (SOAEs), which can occur without external stimulation, and evoked otoacoustic emissions (EOAEs), which require an evoking stimulus.

Mechanism of occurrence
OAEs are considered to be related to the amplification function of the cochlea. In the absence of external stimulation, the activity of the cochlear amplifier increases, leading to the production of sound. Several lines of evidence suggest that, in mammals, outer hair cells are the elements that enhance cochlear sensitivity and frequency selectivity and hence act as the energy sources for amplification. One theory is that they act to increase the discriminability of signal variations in continuous noise by lowering the masking effect of its cochlear amplification.

Types
Spontaneous
Spontaneous otoacoustic emissions (SOAE)s are sounds that are emitted from the ear without external stimulation and are measurable with sensitive microphones in the external ear canal. At least one SOAE can be detected in approx. 35-50% of the population. The sounds are frequency-stable between 500 Hz and 4500 Hz to have unstable volumes between -30 dB SPL and +10 dB SPL. The majority of the people are unaware of their SOAEs; portions of 1-9% however perceive a SOAE as an annoying tinnitus.

Evoked
Evoked otoacoustic emissions are currently evoked using three different methodologies.
Stimulus Frequency OAEs (SFOAEs) are measured during the application of a pure-tone stimulus, and are detected by the vectorial difference between the stimulus waveform and the recorded waveform (which consists of the sum of the stimulus and the OAE).
Transient-evoked OAEs (TEOAEs or TrOAEs) are evoked using a click (broad frequency range) or toneburst (brief duration pure tone) stimulus. The evoked response from a click covers the frequency range up to around 4 kHz, while a toneburst will elicit a response from the region that has the same frequency as the pure tone.
Distortion product OAEs (DPOAEs) are evoked using a pair of primary tones 
  
    
      
        
          f
          
            1
          
        
      
    
    {\displaystyle f_{1}}
   and 
  
    
      
        
          f
          
            2
          
        
      
    
    {\displaystyle f_{2}}
   with particular intensity (usually either 65 - 55 dBSPL or 65 for both) and ratio (
  
    
      
        
          f
          
            1
          
        
        
           
        
        :
        
           
        
        
          f
          
            2
          
        
      
    
    {\displaystyle f_{1}{\mbox{ }}:{\mbox{ }}f_{2}}
  ).
The evoked responses from these stimuli occur at frequencies (
  
    
      
        
          f
          
            d
            p
          
        
      
    
    {\displaystyle f_{dp}}
  ) mathematically related to the primary frequencies, with the two most prominent being 
  
    
      
        
          f
          
            d
            p
          
        
        =
        2
        
          f
          
            1
          
        
        ?
        
          f
          
            2
          
        
      
    
    {\displaystyle f_{dp}=2f_{1}-f_{2}}
   (the ""cubic"" distortion tone, most commonly used for hearing screening) and 
  
    
      
        
          f
          
            d
            p
          
        
        =
        
          f
          
            2
          
        
        ?
        
          f
          
            1
          
        
      
    
    {\displaystyle f_{dp}=f_{2}-f_{1}}
   (the ""quadratic"" distortion tone, or simple difference tone).

Clinical importance
Otoacoustic emissions are clinically important because they are the basis of a simple, non-invasive test for hearing defects in newborn babies and in children who are too young to cooperate in conventional hearing tests. Many western countries now have national programmes for the universal hearing screening of newborn babies. Periodic early childhood hearing screenings program are also utilizing OAE technology. One excellent example has been demonstrated by the Early Childhood Hearing Outreach Initiative at the National Center for Hearing Assessment and Management (NCHAM) at Utah State University, which has helped hundreds of Early Head Start programs across the United States implement OAE screening and follow-up practices in those early childhood educational settings. The primary screening tool is a test for the presence of a click-evoked OAE. Otoacoustic emissions also assist in differential diagnosis of cochlear and higher level hearing losses (e.g., auditory neuropathy).
The relationships between otoacoustic emissions and tinnitus have been explored. Several studies suggest that in about 6% to 12% of normal-hearing persons with tinnitus and SOAEs, the SOAEs are at least partly responsible for the tinnitus. Studies have found that some subjects with tinnitus display oscillating or ringing EOAEs, and in these cases, it is hypothesized that the oscillating EOAEs and tinnitus are related to a common underlying pathology rather than the emissions being the source of the tinnitus.
In conjunction with audiometric testing, OAE testing can be completed to determine changes in the responses. Studies have found that exposure to noise can cause a decline in OAE responses. In a study, industrial workers who were exposed 84.5 dBA of noise were compared to workers who were exposed to 53.2 dBA of noise by considering hearing thresholds and OAEs before and after 5 days of work. This study revealed that hearing thresholds and OAE results were significantly lower among the workers who were exposed to higher levels of noise.
It has been found that distortion product otoacoustic emissions (DPOAE’s) have provided the most information for detecting mild hearing loss in high frequencies when compared to transient evoked otoacoustic emissions (TEOAE). This is an indication that DPOAE’s can help with detecting an early onset of noise-induced hearing loss. A study measuring audiometric thresholds and DPOAEs among individuals in the military showed that there was a decrease in DPOAEs after noise exposure, but did not show a shift in audiometric threshold. This supports OAEs as predicting early signs of noise damage.

Biometric importance
In 2009, Stephen Beeby of The University of Southampton led research into utilizing otoacoustic emissions for biometric identification. Devices equipped with a microphone could detect these subsonic emissions and potentially identify an individual, thereby providing access to the device, without the need of a traditional password. It is speculated, however, that colds, medication, trimming one's ear hair, or recording and playing back a signal to the microphone could subvert the identification process.

See also
Auditory brainstem response
Entoptic phenomenon
Maryanne Amacher, a composer who used this phenomenon in her music
Pure tone audiometry

References
Further reading
M.S. Robinette and T.J. Glattke (eds., 2007). Otoacoustic Emissions: Clinical Applications, third edition (Thieme).
G.A. Manley, R.R. Fay, and A.N. Popper (eds., 2008). Active Processes and Otoacoustic Emissions (Springer Handbook of Auditory Research, vol. 30).
S. Dhar and J.W. Hall, III (2011). Otoacoustic Emissions: Principles, Procedures, and Protocols (Plural Publishing).",Category:Acoustics,1
62,63,Icophone,,Category:Time–frequency analysis,1
63,64,Equivalent rectangular bandwidth,"The equivalent rectangular bandwidth or ERB is a measure used in psychoacoustics, which gives an approximation to the bandwidths of the filters in human hearing, using the unrealistic but convenient simplification of modeling the filters as rectangular band-pass filters.

Approximations
For moderate sound levels and young listeners, the bandwidth of human auditory filters can be approximated by the polynomial equation:

where f is the center frequency of the filter in kHz and ERB(f) is the bandwidth of the filter in Hz. The approximation is based on the results of a number of published simultaneous masking experiments and is valid from 0.1 to 6.5 kHz.
The above approximation was given in 1983 by Moore and Glasberg, who in 1990 published another (linear) approximation:

where f is in kHz and ERB(f) is in Hz. The approximation is applicable at moderate sound levels and for values of f between 0.1 and 10 kHz.

ERB-rate scale
The ERB-rate scale, or simply ERB scale, can be defined as a function ERBS(f) which returns the number of equivalent rectangular bandwidths below the given frequency f. It can be constructed by solving the following differential system of equations:

  
    
      
        
          
            {
            
              
                
                  
                    E
                    R
                    B
                    S
                  
                  (
                  0
                  )
                  =
                  0
                
              
              
                
                  
                    
                      
                        d
                        f
                      
                      
                        d
                        
                          E
                          R
                          B
                          S
                        
                        (
                        f
                        )
                      
                    
                  
                  =
                  
                    E
                    R
                    B
                  
                  (
                  f
                  )
                
              
            
            
          
        
      
    
    {\displaystyle {\begin{cases}\mathrm {ERBS} (0)=0\\{\frac {df}{d\mathrm {ERBS} (f)}}=\mathrm {ERB} (f)\\\end{cases}}}
  
The solution for ERBS(f) is the integral of the reciprocal of ERB(f) with the constant of integration set in such a way that ERBS(0) = 0.
Using the second order polynomial approximation (Eq.1) for ERB(f) yields:

  
    
      
        
          E
          R
          B
          S
        
        (
        f
        )
        =
        11.17
        ?
        ln
        ?
        
          (
          
            
              
                f
                +
                0.312
              
              
                f
                +
                14.675
              
            
          
          )
        
        +
        43.0
      
    
    {\displaystyle \mathrm {ERBS} (f)=11.17\cdot \ln \left({\frac {f+0.312}{f+14.675}}\right)+43.0}
   
where f is in kHz. The VOICEBOX speech processing toolbox for MATLAB implements the conversion and its inverse as:

  
    
      
        
          E
          R
          B
          S
        
        (
        f
        )
        =
        11.17268
        ?
        ln
        ?
        
          (
          1
          +
          
            
              
                46.06538
                ?
                f
              
              
                f
                +
                14678.49
              
            
          
          )
        
      
    
    {\displaystyle \mathrm {ERBS} (f)=11.17268\cdot \ln \left(1+{\frac {46.06538\cdot f}{f+14678.49}}\right)}
   

  
    
      
        f
        =
        
          
            676170.4
            
              47.06538
              ?
              
                e
                
                  0.08950404
                  ?
                  
                    E
                    R
                    B
                    S
                  
                  (
                  f
                  )
                
              
            
          
        
        ?
        14678.49
      
    
    {\displaystyle f={\frac {676170.4}{47.06538-e^{0.08950404\cdot \mathrm {ERBS} (f)}}}-14678.49}
   
where f is in Hz.
Using the linear approximation (Eq.2) for ERB(f) yields:

  
    
      
        
          E
          R
          B
          S
        
        (
        f
        )
        =
        21.4
        ?
        l
        o
        
          g
          
            10
          
        
        (
        1
        +
        0.00437
        ?
        f
        )
      
    
    {\displaystyle \mathrm {ERBS} (f)=21.4\cdot log_{10}(1+0.00437\cdot f)}
   
where f is in Hz.

See also
Critical bands
Bark scale

References
External links
http://www2.ling.su.se/staff/hartmut/bark.htm",Category:Acoustics,1
64,65,Soundproofing,"Soundproofing is any means of reducing the sound pressure with respect to a specified sound source and receptor. There are several basic approaches to reducing sound: increasing the distance between source and receiver, using noise barriers to reflect or absorb the energy of the sound waves, using damping structures such as sound baffles, or using active antinoise sound generators.
Two distinct soundproofing problems may need to be considered when designing acoustic treatments - to improve the sound within a room (See anechoic chamber), and reduce sound leakage to/from adjacent rooms or outdoors. Acoustic quieting and noise control can be used to limit unwanted noise. Soundproofing can suppress unwanted indirect sound waves such as reflections that cause echoes and resonances that cause reverberation. Soundproofing can reduce the transmission of unwanted direct sound waves from the source to an involuntary listener through the use of distance and intervening objects in the sound path.

Distance
The energy density of sound waves decreases as they spread out, so that increasing the distance between the receiver and source results in a progressively lesser intensity of sound at the receiver. In a normal three-dimensional setting, with a point source and point receptor, the intensity of sound waves will be attenuated according to the inverse square of the distance from the source.

Damping
Damping means to reduce resonance in the room, by absorption or redirection (reflection or diffusion). Absorption will reduce the overall sound level, whereas redirection makes unwanted sound harmless or even beneficial by reducing coherence. Damping can reduce the acoustic resonance in the air, or mechanical resonance in the structure of the room itself or things in the room.

Absorption
Absorbing sound spontaneously converts part of the sound energy to a very small amount of heat in the intervening object (the absorbing material), rather than sound being transmitted or reflected. There are several ways in which a material can absorb sound. The choice of sound absorbing material will be determined by the frequency distribution of noise to be absorbed and the acoustic absorption profile required

Porous absorbers
Porous absorbers, typically open cell rubber foams or melamine sponges, absorb noise by friction within the cell structure. Porous open cell foams are highly effective noise absorbers across a broad range of medium-high frequencies. Performance can be less impressive at lower frequencies.
The exact absorption profile of a porous open cell foam will be determined by a number of factors including the following:
Cell size
Tortuosity
Porosity
Material thickness
Material density

Resonant absorbers
Resonant panels, Helmholtz resonators and other resonant absorbers work by damping a sound wave as they reflect it. Unlike porous absorbers, resonant absorbers are most effective at low-medium frequencies and the absorption of resonant absorbers is always matched to a narrow frequency range.

Reflection
When sound waves hit a medium, the reflection of that sound is dependent on dissimilarity of the surfaces it comes in contact with. Sound hitting a concrete surface will result in a much different reflection than if sound were to hit a softer medium such as fiberglass. In an outdoor environment such as highway engineering, embankments or panelling are often used to reflect sound upwards into the sky.

Diffusion
If a specular reflection from a hard flat surface is giving a problematic echo then an acoustic diffuser may be applied to the surface. It will scatter sound in all directions. This is effective to eliminate pockets of noise in a room.

Room within a room
A room within a room (RWAR) is one method of isolating sound and preventing it from transmitting to the outside world where it may be undesirable.
Most vibration / sound transfer from a room to the outside occurs through mechanical means. The vibration passes directly through the brick, woodwork and other solid structural elements. When it meets with an element such as a wall, ceiling, floor or window, which acts as a sounding board, the vibration is amplified and heard in the second space. A mechanical transmission is much faster, more efficient and may be more readily amplified than an airborne transmission of the same initial strength.
The use of acoustic foam and other absorbent means is less effective against this transmitted vibration. The user is advised to break the connection between the room that contains the noise source and the outside world. This is called acoustic decoupling. Ideal decoupling involves eliminating vibration transfer in both solid materials and in the air, so air-flow into the room is often controlled. This has safety implications: inside decoupled space, proper ventilation must be assured, and gas heaters cannot be used.

Noise cancellation
Noise cancellation generators for active noise control are a relatively modern innovation. A microphone is used to pick up the sound that is then analyzed by a computer; then, sound waves with opposite polarity (180° phase at all frequencies) are output through a speaker, causing destructive interference and cancelling much of the noise.

Residential soundproofing
Residential soundproofing aims to decrease or eliminate the effects of exterior noise. The main focus of residential soundproofing in existing structures is the windows and doors. Solid wood doors are a better sound barrier than hollow doors. Curtains can be used to damp sound either through use of heavy materials or through the use of air chambers known as honeycombs. Single-, double- and triple-honeycomb designs achieve relatively greater degrees of sound damping. The primary soundproofing limit of curtains is the lack of a seal at the edge of the curtain, although this may be alleviated with the use of sealing features, such as hook and loop fastener, adhesive, magnets, or other materials. Thickness of glass will play a role when diagnosing sound leakage. Double-pane windows achieve somewhat greater sound damping than single-pane windows when well sealed into the opening of the window frame and wall.
Significant noise reduction can also be achieved by installing a second interior window. In this case the exterior window remains in place while a slider or hung window is installed within the same wall openings.
The FAA offers soundproofing for homes that fall within a noise contour where the average decibel level is 65 decibels. It is part of their Residential Sound Insulation Program. The program provides Solid-core wood entry doors plus windows and storm doors.

Commercial soundproofing
Restaurants, schools, office businesses, and health care facilities use architectural acoustics to reduce noise for their customers. In the US, OSHA has requirements regulating the length of exposure of workers to certain levels of noise.
Commercial businesses sometimes use soundproofing technology, especially when they are an open office design. There are many reasons why a business might implement soundproofing for their office. One of the biggest hindrances in worker productivity are the distracting noises that comes from people talking such as on the phone, or with their co-workers and boss. Noise soundproofing is important in mitigating people from losing their concentration and focus from their work project. It is also important to keep confidential conversations secure to the intended listeners.
When trying to find places to install soundproofing, acoustic panels should be installed in office areas where lots of traffic corridors, circulation pathways, and open work areas are connected. Successful acoustic panel installations rely on three strategies and techniques to absorb sound, block sound transmission from one place to another, and cover and masking of the sound.

Automotive soundproofing
Automotive soundproofing aims to decrease or eliminate the effects of exterior noise, primarily engine, exhaust and tire noise across a wide frequency range. When constructing a vehicle which includes soundproofing, a panel dampening material is fitted which reduces the vibration of the vehicles body panels when they are excited by one of the many high energy sound sources caused when the vehicle is in use. There are many complex noises created within vehicles which change with the driving environment and speed at which the vehicle travels. Significant noise reductions of up to 8 dB can be achieved by installing a combination of different types of materials.
The automotive environment limits the thickness of materials that can be used, but combinations of dampers, barriers, and absorbers are common. Common materials include felt, foam, polyester, and Polypropylene blend materials. Waterproofing may be necessary based on materials used. Acoustic foam can be applied in different areas of a vehicle during manufacture to reduce cabin noise. Foams also have cost and performance advantages in installation since foam material can expand and fill cavities after application and also prevent leaks and some gases from entering the vehicle. Vehicle soundproofing can reduce wind, engine, road, and tire noise. Vehicle soundproofing can reduce sound inside a vehicle from five to 20 decibels.

Noise barriers as exterior soundproofing
Since the early 1970s, it has become common practice in the United States and other industrialized countries to engineer noise barriers along major highways to protect adjacent residents from intruding roadway noise. The Federal Highway Administration (FHWA) in conjunction with State Highway Administration (SHA) adopted Federal Regulation (23 CFR 772) requiring each state to adopt their own policy in regards to abatement of highway traffic noise. Engineering techniques have been developed to predict an effective geometry for the noise barrier design in a particular real world situation. Noise barriers may be constructed of wood, masonry, earth or a combination thereof. One of the earliest noise barrier designs was in Arlington, Virginia adjacent to Interstate 66, stemming from interests expressed by the Arlington Coalition on Transportation. Possibly the earliest scientifically designed and published noise barrier construction was in Los Altos, California in 1970.

See also


== References ==",Category:Noise reduction,1
65,66,Aeroacoustics,"Aeroacoustics is a branch of acoustics that studies noise generation via either turbulent fluid motion or aerodynamic forces interacting with surfaces. Noise generation can also be associated with periodically varying flows. A notable example of this phenomenon is the Aeolian tones produced by wind blowing over fixed objects.
Although no complete scientific theory of the generation of noise by aerodynamic flows has been established, most practical aeroacoustic analysis relies upon the so-called aeroacoustic analogy, proposed by Sir James Lighthill in the 1950s while at the University of Manchester. whereby the governing equations of motion of the fluid are coerced into a form reminiscent of the wave equation of ""classical"" (i.e. linear) acoustics in the left-hand side with the remaining terms as sources in the right-hand side.

History
The modern discipline of aeroacoustics can be said to have originated with the first publication of Lighthill in the early 1950s, when noise generation associated with the jet engine was beginning to be placed under scientific scrutiny.

Lighthill's equation
Lighthill rearranged the Navier–Stokes equations, which govern the flow of a compressible viscous fluid, into an inhomogeneous wave equation, thereby making a connection between fluid mechanics and acoustics. This is often called ""Lighthill's analogy"" because it presents a model for the acoustic field that is not, strictly speaking, based on the physics of flow-induced/generated noise, but rather on the analogy of how they might be represented through the governing equations of a compressible fluid.
The first equation of interest is the conservation of mass equation, which reads

  
    
      
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        +
        ?
        ?
        
          (
          ?
          
            v
          
          )
        
        =
        
          
            
              D
              ?
            
            
              D
              t
            
          
        
        +
        ?
        ?
        ?
        
          v
        
        =
        0
        ,
      
    
    {\displaystyle {\frac {\partial \rho }{\partial t}}+\nabla \cdot \left(\rho \mathbf {v} \right)={\frac {D\rho }{Dt}}+\rho \nabla \cdot \mathbf {v} =0,}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   and 
  
    
      
        
          v
        
      
    
    {\displaystyle \mathbf {v} }
   represent the density and velocity of the fluid, which depend on space and time, and 
  
    
      
        D
        
          /
        
        D
        t
      
    
    {\displaystyle D/Dt}
   is the substantial derivative.
Next is the conservation of momentum equation, which is given by

  
    
      
        
          ?
        
        
          
            
              ?
              
                v
              
            
            
              ?
              t
            
          
        
        +
        
          ?
          (
          
            v
          
          ?
          ?
          )
          
            v
          
        
        =
        ?
        ?
        p
        +
        ?
        ?
        ?
        ,
      
    
    {\displaystyle {\rho }{\frac {\partial \mathbf {v} }{\partial t}}+{\rho (\mathbf {v} \cdot \nabla )\mathbf {v} }=-\nabla p+\nabla \cdot \sigma ,}
  
where 
  
    
      
        p
      
    
    {\displaystyle p}
   is the thermodynamic pressure, and 
  
    
      
        ?
      
    
    {\displaystyle \sigma }
   is the viscous (or traceless) part of the stress tensor from the Navier–Stokes equations.
Now, multiplying the conservation of mass equation by 
  
    
      
        
          v
        
      
    
    {\displaystyle \mathbf {v} }
   and adding it to the conservation of momentum equation gives

  
    
      
        
          
            ?
            
              ?
              t
            
          
        
        
          (
          ?
          
            v
          
          )
        
        +
        ?
        ?
        (
        ?
        
          v
        
        ?
        
          v
        
        )
        =
        ?
        ?
        p
        +
        ?
        ?
        ?
        .
      
    
    {\displaystyle {\frac {\partial }{\partial t}}\left(\rho \mathbf {v} \right)+\nabla \cdot (\rho \mathbf {v} \otimes \mathbf {v} )=-\nabla p+\nabla \cdot \sigma .}
  
Note that 
  
    
      
        
          v
        
        ?
        
          v
        
      
    
    {\displaystyle \mathbf {v} \otimes \mathbf {v} }
   is a tensor (see also tensor product). Differentiating the conservation of mass equation with respect to time, taking the divergence of the last equation and subtracting the latter from the former, we arrive at

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              ?
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        ?
        
          ?
          
            2
          
        
        p
        +
        ?
        ?
        ?
        ?
        ?
        =
        ?
        ?
        ?
        ?
        (
        ?
        
          v
        
        ?
        
          v
        
        )
        .
      
    
    {\displaystyle {\frac {\partial ^{2}\rho }{\partial t^{2}}}-\nabla ^{2}p+\nabla \cdot \nabla \cdot \sigma =\nabla \cdot \nabla \cdot (\rho \mathbf {v} \otimes \mathbf {v} ).}
  
Subtracting 
  
    
      
        
          c
          
            0
          
          
            2
          
        
        
          ?
          
            2
          
        
        ?
      
    
    {\displaystyle c_{0}^{2}\nabla ^{2}\rho }
  , where 
  
    
      
        
          c
          
            0
          
        
      
    
    {\displaystyle c_{0}}
   is the speed of sound in the medium in its equilibrium (or quiescent) state, from both sides of the last equation and rearranging it results in

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              ?
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        ?
        
          c
          
            0
          
          
            2
          
        
        
          ?
          
            2
          
        
        ?
        =
        ?
        ?
        
          [
          ?
          ?
          (
          ?
          
            v
          
          ?
          
            v
          
          )
          ?
          ?
          ?
          ?
          +
          ?
          p
          ?
          
            c
            
              0
            
            
              2
            
          
          ?
          ?
          ]
        
        ,
      
    
    {\displaystyle {\frac {\partial ^{2}\rho }{\partial t^{2}}}-c_{0}^{2}\nabla ^{2}\rho =\nabla \cdot \left[\nabla \cdot (\rho \mathbf {v} \otimes \mathbf {v} )-\nabla \cdot \sigma +\nabla p-c_{0}^{2}\nabla \rho \right],}
  
which is equivalent to

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              ?
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        ?
        
          c
          
            0
          
          
            2
          
        
        
          ?
          
            2
          
        
        ?
        =
        (
        ?
        ?
        ?
        )
        :
        
          [
          ?
          
            v
          
          ?
          
            v
          
          ?
          ?
          +
          (
          p
          ?
          
            c
            
              0
            
            
              2
            
          
          ?
          )
          
            I
          
          ]
        
        ,
      
    
    {\displaystyle {\frac {\partial ^{2}\rho }{\partial t^{2}}}-c_{0}^{2}\nabla ^{2}\rho =(\nabla \otimes \nabla ):\left[\rho \mathbf {v} \otimes \mathbf {v} -\sigma +(p-c_{0}^{2}\rho )\mathbb {I} \right],}
  
where 
  
    
      
        
          I
        
      
    
    {\displaystyle \mathbb {I} }
   is the identity tensor, and 
  
    
      
        :
      
    
    {\displaystyle :}
   denotes the (double) tensor contraction operator.
The above equation is the celebrated Lighthill equation of aeroacoustics. It is a wave equation with a source term on the right-hand side, i.e. an inhomogeneous wave equation. The argument of the ""double-divergence operator"" on the right-hand side of last equation, i.e. 
  
    
      
        ?
        
          v
        
        ?
        
          v
        
        ?
        ?
        +
        (
        p
        ?
        
          c
          
            0
          
          
            2
          
        
        ?
        )
        
          I
        
      
    
    {\displaystyle \rho \mathbf {v} \otimes \mathbf {v} -\sigma +(p-c_{0}^{2}\rho )\mathbb {I} }
  , is the so-called Lighthill turbulence stress tensor for the acoustic field, and it is commonly denoted by 
  
    
      
        T
      
    
    {\displaystyle T}
  .
Using Einstein notation, Lighthill’s equation can be written as

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              ?
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        ?
        
          c
          
            0
          
          
            2
          
        
        
          ?
          
            2
          
        
        ?
        =
        
          
            
              
                ?
                
                  2
                
              
              
                T
                
                  i
                  j
                
              
            
            
              ?
              
                x
                
                  i
                
              
              ?
              
                x
                
                  j
                
              
            
          
        
        ,
        
        (
        ?
        )
      
    
    {\displaystyle {\frac {\partial ^{2}\rho }{\partial t^{2}}}-c_{0}^{2}\nabla ^{2}\rho ={\frac {\partial ^{2}T_{ij}}{\partial x_{i}\partial x_{j}}},\quad (*)}
  
where

  
    
      
        
          T
          
            i
            j
          
        
        =
        ?
        
          v
          
            i
          
        
        
          v
          
            j
          
        
        ?
        
          ?
          
            i
            j
          
        
        +
        (
        p
        ?
        
          c
          
            0
          
          
            2
          
        
        ?
        )
        
          ?
          
            i
            j
          
        
        ,
      
    
    {\displaystyle T_{ij}=\rho v_{i}v_{j}-\sigma _{ij}+(p-c_{0}^{2}\rho )\delta _{ij},}
  
and 
  
    
      
        
          ?
          
            i
            j
          
        
      
    
    {\displaystyle \delta _{ij}}
   is the Kronecker delta. Each of the acoustic source terms, i.e. terms in 
  
    
      
        
          T
          
            i
            j
          
        
      
    
    {\displaystyle T_{ij}}
  , may play a significant role in the generation of noise depending upon flow conditions considered. 
  
    
      
        ?
        
          v
          
            i
          
        
        
          v
          
            j
          
        
      
    
    {\displaystyle \rho v_{i}v_{j}}
   describes unsteady convection of flow (or Reynolds' Stress, developed by Osborne Reynolds), 
  
    
      
        
          ?
          
            i
            j
          
        
      
    
    {\displaystyle \sigma _{ij}}
   describes sound generated by shear, and 
  
    
      
        (
        p
        ?
        
          c
          
            0
          
          
            2
          
        
        ?
        )
        
          ?
          
            i
            j
          
        
      
    
    {\displaystyle (p-c_{0}^{2}\rho )\delta _{ij}}
   describes non-linear acoustic generation processes.
In practice, it is customary to neglect the effects of viscosity on the fluid, i.e. one takes 
  
    
      
        ?
        =
        0
      
    
    {\displaystyle \sigma =0}
  , because it is generally accepted that the effects of the latter on noise generation, in most situations, are orders of magnitude smaller than those due to the other terms. Lighthill provides an in-depth discussion of this matter.
In aeroacoustic studies, both theoretical and computational efforts are made to solve for the acoustic source terms in Lighthill's equation in order to make statements regarding the relevant aerodynamic noise generation mechanisms present.
Finally, it is important to realize that Lighthill's equation is exact in the sense that no approximations of any kind have been made in its derivation.

Related model equations
In their classical text on fluid mechanics, Landau and Lifshitz derive an aeroacoustic equation analogous to Lighthill's (i.e., an equation for sound generated by ""turbulent"" fluid motion), but for the incompressible flow of an inviscid fluid. The inhomogeneous wave equation that they obtain is for the pressure 
  
    
      
        p
      
    
    {\displaystyle p}
   rather than for the density 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   of the fluid. Furthermore, unlike Lighthill's equation, Landau and Lifshitz's equation is not exact; it is an approximation.
If one is to allow for approximations to be made, a simpler way (without necessarily assuming the fluid is incompressible) to obtain an approximation to Lighthill's equation is to assume that 
  
    
      
        p
        ?
        
          p
          
            0
          
        
        =
        
          c
          
            0
          
          
            2
          
        
        (
        ?
        ?
        
          ?
          
            0
          
        
        )
      
    
    {\displaystyle p-p_{0}=c_{0}^{2}(\rho -\rho _{0})}
  , where 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
   and 
  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p_{0}}
   are the (characteristic) density and pressure of the fluid in its equilibrium state. Then, upon substitution the assumed relation between pressure and density into 
  
    
      
        (
        ?
        )
        
      
    
    {\displaystyle (*)\,}
   we obtain the equation (for an inviscid fluid, ? = 0)

  
    
      
        
          
            1
            
              c
              
                0
              
              
                2
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              p
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        ?
        
          ?
          
            2
          
        
        p
        =
        
          
            
              
                ?
                
                  2
                
              
              
                
                  
                    
                      T
                      ~
                    
                  
                
                
                  i
                  j
                
              
            
            
              ?
              
                x
                
                  i
                
              
              ?
              
                x
                
                  j
                
              
            
          
        
        ,
        
        
          where
        
        
        
          
            
              
                T
                ~
              
            
          
          
            i
            j
          
        
        =
        ?
        
          v
          
            i
          
        
        
          v
          
            j
          
        
        .
      
    
    {\displaystyle {\frac {1}{c_{0}^{2}}}{\frac {\partial ^{2}p}{\partial t^{2}}}-\nabla ^{2}p={\frac {\partial ^{2}{\tilde {T}}_{ij}}{\partial x_{i}\partial x_{j}}},\quad {\text{where}}\quad {\tilde {T}}_{ij}=\rho v_{i}v_{j}.}
  
And for the case when the fluid is indeed incompressible, i.e. 
  
    
      
        ?
        =
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho =\rho _{0}}
   (for some positive constant 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
  ) everywhere, then we obtain exactly the equation given in Landau and Lifshitz, namely

  
    
      
        
          
            1
            
              c
              
                0
              
              
                2
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              p
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        ?
        
          ?
          
            2
          
        
        p
        =
        
          ?
          
            0
          
        
        
          
            
              
                ?
                
                  2
                
              
              
                
                  
                    
                      T
                      ^
                    
                  
                
                
                  i
                  j
                
              
            
            
              ?
              
                x
                
                  i
                
              
              ?
              
                x
                
                  j
                
              
            
          
        
        ,
        
        
          where
        
        
        
          
            
              
                T
                ^
              
            
          
          
            i
            j
          
        
        =
        
          v
          
            i
          
        
        
          v
          
            j
          
        
        .
      
    
    {\displaystyle {\frac {1}{c_{0}^{2}}}{\frac {\partial ^{2}p}{\partial t^{2}}}-\nabla ^{2}p=\rho _{0}{\frac {\partial ^{2}{\hat {T}}_{ij}}{\partial x_{i}\partial x_{j}}},\quad {\text{where}}\quad {\hat {T}}_{ij}=v_{i}v_{j}.}
  
A similar approximation [in the context of equation 
  
    
      
        (
        ?
        )
        
      
    
    {\displaystyle (*)\,}
  ], namely 
  
    
      
        T
        ?
        
          ?
          
            0
          
        
        
          
            
              T
              ^
            
          
        
      
    
    {\displaystyle T\approx \rho _{0}{\hat {T}}}
  , is suggested by Lighthill [see Eq. (7) in the latter paper].
Of course, one might wonder whether we are justified in assuming that 
  
    
      
        p
        ?
        
          p
          
            0
          
        
        =
        
          c
          
            0
          
          
            2
          
        
        (
        ?
        ?
        
          ?
          
            0
          
        
        )
      
    
    {\displaystyle p-p_{0}=c_{0}^{2}(\rho -\rho _{0})}
  . The answer is affirmative, if the flow satisfies certain basic assumptions. In particular, if 
  
    
      
        ?
        ?
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho \ll \rho _{0}}
   and 
  
    
      
        p
        ?
        
          p
          
            0
          
        
      
    
    {\displaystyle p\ll p_{0}}
  , then the assumed relation follows directly from the linear theory of sound waves (see, e.g., the linearized Euler equations and the acoustic wave equation). In fact, the approximate relation between 
  
    
      
        p
      
    
    {\displaystyle p}
   and 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   that we assumed is just a linear approximation to the generic barotropic equation of state of the fluid.
However, even after the above deliberations, it is still not clear whether one is justified in using an inherently linear relation to simplify a nonlinear wave equation. Nevertheless, it is a very common practice in nonlinear acoustics as the textbooks on the subject show: e.g., Naugolnykh and Ostrovsky and Hamilton and Morfey.

See also
Acoustic theory
Aeolian harp
Computational aeroacoustics

References
External links
M. J. Lighthill, ""On Sound Generated Aerodynamically. I. General Theory,"" Proc. R. Soc. Lond. A 211 (1952) pp. 564–587. This article on JSTOR.
M. J. Lighthill, ""On Sound Generated Aerodynamically. II. Turbulence as a Source of Sound,"" Proc. R. Soc. Lond. A 222 (1954) pp. 1–32. This article on JSTOR.
L. D. Landau and E. M. Lifshitz, Fluid Mechanics 2ed., Course of Theoretical Physics vol. 6, Butterworth-Heinemann (1987) §75. ISBN 0-7506-2767-0, Preview from Amazon.
K. Naugolnykh and L. Ostrovsky, Nonlinear Wave Processes in Acoustics, Cambridge Texts in Applied Mathematics vol. 9, Cambridge University Press (1998) chap. 1. ISBN 0-521-39984-X, Preview from Google.
M. F. Hamilton and C. L. Morfey, ""Model Equations,"" Nonlinear Acoustics, eds. M. F. Hamilton and D. T. Blackstock, Academic Press (1998) chap. 3. ISBN 0-12-321860-8, Preview from Google.
Aeroacoustics at the University of Mississippi
Aeroacoustics at the University of Leuven
International Journal of Aeroacoustics
Examples in Aeroacoustics from NASA
Aeroacoustics.info",Category:Fluid dynamics,1
66,67,Reverberation,"Reverberation, in psychoacoustics and acoustics, is the persistence of sound after a sound is produced. A reverberation, or reverb, is created when a sound or signal is reflected causing a large number of reflections to build up and then decay as the sound is absorbed by the surfaces of objects in the space – which could include furniture, people, and air. This is most noticeable when the sound source stops but the reflections continue, decreasing in amplitude, until they reach zero amplitude.
Reverberation is frequency dependent: the length of the decay, or reverberation time, receives special consideration in the architectural design of spaces which need to have specific reverberation times to achieve optimum performance for their intended activity. In comparison to a distinct echo that is a minimum of 50 to 100 ms after the initial sound, reverberation is the occurrence of reflections that arrive in less than approximately 50 ms. As time passes, the amplitude of the reflections gradually reduces to zero. Reverberation is not limited to indoor spaces as it exists in forests and other outdoor environments where reflection exists.
Reverberation occurs naturally when a person sings, talks, or plays an instrument acoustically in a hall or performance space with sound-reflective surfaces. The sound of reverberation is often electronically added to the vocals of singers and to musical instruments. This is done in both live sound systems and sound recordings by using effects units. Effects units that are specialized in the generation of the reverberation effect are commonly called reverbs.

Reverberation time
The time it takes for a signal to drop by 60 dB is the reverberation time.
RT60 is the time required for reflections of a direct sound to decay 60 dB. Reverberation time is frequently stated as a single value, if measured as a wide band signal (20 Hz to 20 kHz). However, being frequency dependent, it can be more precisely described in terms of frequency bands (one octave, 1/3 octave, 1/6 octave, etc.). Being frequency dependent, the reverberation time measured in narrow bands will differ depending on the frequency band being measured. For precision, it is important to know what ranges of frequencies are being described by a reverberation time measurement.
In the late 19th century, Wallace Clement Sabine started experiments at Harvard University to investigate the impact of absorption on the reverberation time. Using a portable wind chest and organ pipes as a sound source, a stopwatch and his ears, he measured the time from interruption of the source to inaudibility (a difference of roughly 60 dB). He found that the reverberation time is proportional to room dimensions and inversely proportional to the amount of absorption present.
The optimum reverberation time for a space in which music is played depends on the type of music that is to be played in the space. Rooms used for speech typically need a shorter reverberation time so that speech can be understood more clearly. If the reflected sound from one syllable is still heard when the next syllable is spoken, it may be difficult to understand what was said. ""Cat"", ""Cab"", and ""Cap"" may all sound very similar. If on the other hand the reverberation time is too short, tonal balance and loudness may suffer. Reverberation effects are often used in studios to add depth to sounds. Reverberation changes the perceived spectral structure of a sound but does not alter the pitch.
Basic factors that affect a room's reverberation time include the size and shape of the enclosure as well as the materials used in the construction of the room. Every object placed within the enclosure can also affect this reverberation time, including people and their belongings.

Sabine equation
Sabine's reverberation equation was developed in the late 1890s in an empirical fashion. He established a relationship between the RT60 of a room, its volume, and its total absorption (in sabins). This is given by the equation:

  
    
      
        R
        
          T
          
            60
          
        
        =
        
          
            
              24
              ln
              ?
              
                10
                
                  1
                
              
            
            
              c
              
                20
              
            
          
        
        
          
            V
            
              S
              a
            
          
        
        ?
        0.1611
        
        
          s
        
        
          
            m
          
          
            ?
            1
          
        
        
          
            V
            
              S
              a
            
          
        
      
    
    {\displaystyle RT_{60}={\frac {24\ln 10^{1}}{c_{20}}}{\frac {V}{Sa}}\approx 0.1611\,\mathrm {s} \mathrm {m} ^{-1}{\frac {V}{Sa}}}
  .
where 
  
    
      
        
          c
          
            20
          
        
      
    
    {\displaystyle c_{20}}
   is the speed of sound in the room (for 20 degrees Celsius), 
  
    
      
        V
      
    
    {\displaystyle V}
   is the volume of the room in m³, 
  
    
      
        S
      
    
    {\displaystyle S}
   total surface area of room in m², 
  
    
      
        a
      
    
    {\displaystyle a}
   is the average absorption coefficient of room surfaces, and the product 
  
    
      
        S
        a
      
    
    {\displaystyle Sa}
   is the total absorption in sabins.
The total absorption in sabins (and hence reverberation time) generally changes depending on frequency (which is defined by the acoustic properties of the space). The equation does not take into account room shape or losses from the sound travelling through the air (important in larger spaces). Most rooms absorb less sound energy in the lower frequency ranges resulting in longer reverb times at lower frequencies.
Sabine concluded that the reverberation time depends upon the reflectivity of sound from various surfaces available inside the hall. If the reflection is coherent, the reverberation time of the hall will be longer; the sound will take more time to die out.
The reverberation time RT60 and the volume V of the room have great influence on the critical distance dc (conditional equation):

  
    
      
        
          d
          
            
              c
            
          
        
        ?
        0
        
          .
        
        057
        ?
        
          
            
              V
              
                R
                
                  T
                  
                    60
                  
                
              
            
          
        
      
    
    {\displaystyle d_{\mathrm {c} }\approx 0{.}057\cdot {\sqrt {\frac {V}{RT_{60}}}}}
  
where critical distance 
  
    
      
        
          d
          
            c
          
        
      
    
    {\displaystyle d_{c}}
   is measured in meters, volume 
  
    
      
        V
      
    
    {\displaystyle V}
   is measured in m³, and reverberation time 
  
    
      
        R
        
          T
          
            60
          
        
      
    
    {\displaystyle RT_{60}}
   is measured in seconds.

Absorption Coefficient
The absorption coefficient of a material is a number between 0 and 1 which indicates the proportion of sound which is absorbed by the surface compared to the proportion which is reflected back into the room. A large, fully open window would offer no reflection as any sound reaching it would pass straight out and no sound would be reflected. This would have an absorption coefficient of 1. Conversely, a thick, smooth painted concrete ceiling would be the acoustic equivalent of a mirror and have an absorption coefficient very close to 0.

Measurement of reverberation time
Historically, reverberation time could only be measured using a level recorder (a plotting device which graphs the noise level against time on a ribbon of moving paper). A loud noise is produced, and as the sound dies away the trace on the level recorder will show a distinct slope. Analysis of this slope reveals the measured reverberation time. Some modern digital sound level meters can carry out this analysis automatically.
Several methods exist for measuring reverb time. An impulse can be measured by creating a sufficiently loud noise (which must have a defined cut-off point). Impulse noise sources such as a blank pistol shot or balloon burst may be used to measure the impulse response of a room.
Alternatively, a random noise signal such as pink noise or white noise may be generated through a loudspeaker, and then turned off. This is known as the interrupted method, and the measured result is known as the interrupted response.
A two-port measurement system can also be used to measure noise introduced into a space and compare it to what is subsequently measured in the space. Consider sound reproduced by a loudspeaker into a room. A recording of the sound in the room can be made and compared to what was sent to the loudspeaker. The two signals can be compared mathematically. This two port measurement system utilizes a Fourier transform to mathematically derive the impulse response of the room. From the impulse response, the reverberation time can be calculated. Using a two port system allows reverberation time to be measured with signals other than loud impulses. Music or recordings of other sound can be used. This allows measurements to be taken in a room after the audience is present.
Reverberation time is usually stated as a decay time and is measured in seconds. There may or may not be any statement of the frequency band used in the measurement. Decay time is the time it takes the signal to diminish 60 dB below the original sound. It is often difficult to inject enough sound into the room to measure a decay of 60 dB, particularly at lower frequencies. If the decay is linear, it is sufficient to measure a drop of 20 dB and multiply the time by 3, or a drop of 30 dB and multiply the time by 2. These are the so-called T20 and T30 measurement methods.
The concept of Reverberation Time implicitly supposes that the decay rate of the sound is exponential, so that the sound level diminishes regularly, at a rate of so many dB per second. It is not often the case in real rooms, depending on the disposition of reflective, dispersive and absorbing surfaces. Moreover, successive measurement of the sound level often yields very different results, as differences in phase in the exciting sound build up in notably different sound waves. In 1965, Manfred R. Schroeder published ""A new method of Measuring Reverberation Time"" in the Journal of the Acoustical Society of America. He proposed to measure, not the power of the sound, but the energy, by integrating it. This made it possible to show the variation in the rate of decay, and to free acousticians from the necessity of averaging many measurements.

Creating reverberation effects
A performer or a producer of live or recorded music often induces reverberation in a work. Several systems have been developed to produce or to simulate reverberation.

Chamber reverberators
The first reverb effects created for recordings used a real physical space as a natural echo chamber. A loudspeaker would play the sound, and then a microphone would pick it up again, including the effects of reverb. Although this is still a common technique, it requires a dedicated soundproofed room, and varying the reverb time is difficult.

Plate reverberators
A plate reverb system uses an electromechanical transducer, similar to the driver in a loudspeaker, to create vibrations in a large plate of sheet metal. The plate’s motion is picked up by one or more contact microphones whose output is an audio signal which may be added to the original ""dry"" signal. In the late 1950s, Elektro-Mess-Technik (EMT) introduced the EMT 140; a 600-pound (270 kg) model popular in recording studios, contributing to many hit records such as Beatles and Pink Floyd albums recorded at Abbey Road Studios in the 1960s, and others recorded by Bill Porter in Nashville's RCA Studio B. Early units had one pickup for mono output, and later models featured two pickups for stereo use. The reverb time can be adjusted by a damping pad, made from framed acoustic tiles. The closer the damping pad, the shorter the reverb time. However, the pad never touches the plate. Some units also featured a remote control.

Spring reverberators
A spring reverb system uses a transducer at one end of a spring and a pickup at the other, similar to those used in plate reverbs, to create and capture vibrations within a metal spring. Laurens Hammond was granted a patent on a spring-based mechanical reverberation system in 1939. Guitar amplifiers frequently incorporate spring reverbs due to their compact construction and low cost. Spring reverberators were once widely used in semi-professional recording due to their modest cost and small size.
Many musicians have made use of spring reverb units by rocking them back and forth, creating a thundering, crashing sound caused by the springs colliding with each other. The Hammond Organ included a built-in spring reverberator, making this a popular effect when used in a rock band.

Digital reverberators
Digital reverberators use various signal processing algorithms in order to create the reverb effect. Since reverberation is essentially caused by a very large number of echoes, simple reverberation algorithms use several feedback delay circuits to create a large, decaying series of echoes. More advanced digital reverb generators can simulate the time and frequency domain response of a specific room (using room dimensions, absorption and other properties). In a music hall, the direct sound always arrives at the listener's ear first because it follows the shortest path. Shortly after the direct sound, the reverberant sound arrives. The time between the two is called the ""pre-delay.""
Reverberation, or informally, ""reverb,"" is one of the most universally used audio effects and is often found in guitar pedals, synthesizers, effects units, digital audio workstations (DAWs) and VST plug-ins.

Convolution reverb
Convolution reverb is a process used for digitally simulating reverberation. It uses the mathematical convolution operation, a pre-recorded audio sample of the impulse response of the space being modeled, and the sound to be echoed, to produce the effect. The impulse-response recording is first stored in a digital signal-processing system. This is then convolved with the incoming audio signal to be processed.

See also
Acoustic resonance
Audio mixing
Exponential decay
Reverberation room

References
External links
Reverberation - Hyperphysics
A database of measured room impulse responses to generate realistic reverberation effects
Spring Reverb Tanks Explained and Compared
Care and Feeding of Spring Reverb Tanks",Category:Acoustics,1
67,68,Acoustic lubrication,"Acoustic or sonic lubrication occurs when sound (measurable in a vacuum by placing a microphone on one element of the sliding system) permits vibration to introduce separation between the sliding faces. This could happen between two plates or between a series of particles. The frequency of sound required to induce optimal vibration, and thus cause sonic lubrication, varies with the size of the particles (high frequencies will have the desired, or undesired, effect on sand and lower frequencies will have this effect on boulders).

Examples
If there is a dynamic coefficient of friction between two objects of 0.20, and vibration causes them to be in contact only half of the time, that would be equivalent to a constant coefficient of friction of 0.10. This substantial reduction in friction can have a profound effect on the system. According to anecdote, World War II Panzer tank treads may have been lubricated by their own squeak providing a serendipitous example of acoustic lubrication.
Another example occurs during landslides. Most landslides do not involve this effect, but occasionally the frequency of vibrations caused by the landslide is optimal to cause the boulders to vibrate. In this case, feedback causes the boulders to slide much farther and more quickly than typical, which can pose an increased danger to those in their path. One notable feature of such a landslide is that it appears to resemble flowing water, or mud, and not the dry sliding rocks that they were seconds earlier.

Applications
Besides the study of landslides, there could be many other applications for acoustic lubrication, particularly where variable friction is required or traditional lubricants can't be used. One case might be drilling wells (for water, oil, etc.) through sand. The optimal pitch of the sound (measurement of frequency) could reduce the friction between the drill bit and sand considerably. New razors with a vibrating head may also be an example.

In fiction
The protagonist in the videogame Shadow Complex can acquire a ""friction dampener"" that uses acoustic lubrication; this enables him to run at very high speeds.

See also
Friction


== References ==",Category:Mechanics,1
68,69,Visual reinforcement audiometry,,Category:Acoustics,1
69,70,Formant,"A formant, as defined by James Jeans, is a harmonic of a note that is augmented by a resonance. Speech researcher, Gunnar Fant, defines formants as ""the spectral peaks of the sound spectrum |P(f)|"". In acoustics generally, a very similar definition is widely used: the Acoustical Society of America defines a formant as: ""a range of frequencies [of a complex sound] in which there is an absolute or relative maximum in the sound spectrum"". In speech science and phonetics, however, a formant is also sometimes used to mean acoustic resonance of the human vocal tract. Thus, in phonetics, formant can mean either a resonance or the spectral maximum that the resonance produces. Formants are often measured as amplitude peaks in the frequency spectrum of the sound, using a spectrogram (in the figure) or a spectrum analyzer and, in the case of the voice, this gives an estimate of the vocal tract resonances. In vowels spoken with a high fundamental frequency, as in a female or child voice, however, the frequency of the resonance may lie between the widely spaced harmonics and hence no corresponding peak is visible.
A room can be said to have formants characteristic of that particular room, due to the way sound reflects from its walls and objects. Room formants of this nature reinforce themselves by emphasizing specific frequencies and absorbing others, as exploited, for example, by Alvin Lucier in his piece I Am Sitting in a Room.

History
From an acoustic point of view, phonetics had a serious problem with the idea that length of vocal tract changed vowels. It was unclear how they could depend on frequencies when everyone from bass to soprano can make the same vowels. There had to be some way to normalize the frequencies. Hermann suggested a solution to this problem in 1894, coining the term ""formant"". ""A vowel, according to him, is a special acoustic phenomenon, depending on the intermittent production of a special partial, or ""formant"", or ""characteristique"". The pitch of the ""formant"" may vary a little without altering the character of the vowel. For a, for example, the ""formant"" may vary from fa4 to la4 even in the same person.""

Phonetics
Formants are distinctive frequency components of the acoustic signal produced by speech or singing. The information that humans require to distinguish between speech sounds can be represented purely quantitatively by specifying peaks in the amplitude/frequency spectrum. Most of these formants are produced by tube and chamber resonance, but a few whistle tones derive from periodic collapse of Venturi effect low-pressure zones. The formant with the lowest frequency is called F1, the second F2, and the third F3. Most often the two first formants, F1 and F2, are enough to disambiguate the vowel. The relationship between the perceived vowel quality and the first two formant frequencies can be appreciated by listening to ""artificial vowels"" that are generated by passing a click train (to simulate the glottal pulse train) through a pair of bandpass filters (to simulate vocal tract resonances). An interactive demonstration of this can be found here.
Nasal consonants usually have an additional formant around 2500 Hz. The liquid [l] usually has an extra formant at 1500 Hz, whereas the English ""r"" sound ([?]) is distinguished by a very low third formant (well below 2000 Hz).
Plosives (and, to some degree, fricatives) modify the placement of formants in the surrounding vowels. Bilabial sounds (such as /b/ and /p/ in ""ball"" or ""sap"") cause a lowering of the formants; velar sounds (/k/ and /?/ in English) almost always show F2 and F3 coming together in a 'velar pinch' before the velar and separating from the same 'pinch' as the velar is released; alveolar sounds (English /t/ and /d/) cause fewer systematic changes in neighbouring vowel formants, depending partially on exactly which vowel is present. The time course of these changes in vowel formant frequencies are referred to as 'formant transitions'.
If the fundamental frequency of the underlying vibration is higher than a resonance frequency of the system, then the formant usually imparted by that resonance will be mostly lost. This is most apparent in the example of soprano opera singers, who sing high enough that their vowels become very hard to distinguish.
Control of resonances is an essential component of the vocal technique known as overtone singing, in which the performer sings a low fundamental tone, and creates sharp resonances to select upper harmonics, giving the impression of several tones being sung at once.
Spectrograms are used to visualise formants. In spectrograms, it can be hard to distinguish formants from naturally occurring harmonics when one sings. However, one can hear the natural formants in a vowel shape through atonal techniques such as vocal fry.

Formant plots
The first two formants are important in determining the quality of vowels, and are frequently said to correspond to the open/close and front/back dimensions (which have traditionally, though not entirely accurately, been associated with the shape and position of the tongue). Thus the first formant F1 has a higher frequency for an open vowel (such as [a]) and a lower frequency for a close vowel (such as [i] or [u]); and the second formant F2 has a higher frequency for a front vowel (such as [i]) and a lower frequency for a back vowel (such as [u]). as can be seen in Fig. 1.

Vowels will almost always have four or more distinguishable formants; sometimes there are more than six. However, the first two formants are most important in determining vowel quality, and this is often displayed in terms of a plot of the first formant against the second formant, though this is not sufficient to capture some aspects of vowel quality, such as rounding. An example of how the vowels of a language or dialect may be plotted on a traditional auditory vowel chart and also on a formant plot may be seen in the case of Norwegian.
Many writers have addressed the problem of finding an optimal alignment of the positions of vowels on formant plots with those on the conventional vowel quadrilateral. The pioneering work of Ladefoged  used the Mel scale because this scale was claimed to correspond more closely to the auditory scale of pitch than to the acoustic measure of fundamental frequency expressed in Hertz as in Fig. 1. Two alternatives to the Mel scale are the Bark scale and the ERB-rate scale. A comparison of these three scales is shown by Hayward, p. 141, and formant plots based on the Hertz scale and on the Bark scale are compared on p. 153. Another strategy for improving formant plots that has been widely adopted is to plot on the horizontal axis not the value of F2 but the difference between F1 and F2 for a given vowel.

Singer's formant
Studies of the frequency spectrum of trained singers, especially male singers, indicate a clear formant around 3000 Hz (between 2800 and 3400 Hz) that is absent in speech or in the spectra of untrained singers. It is thought to be associated with one or more of the higher resonances of the vocal tract. It is this increase in energy at 3000 Hz which allows singers to be heard and understood over an orchestra. This formant is actively developed through vocal training, for instance through so-called voce di strega or ""witch's voice"" exercises and is caused by a part of the vocal tract acting as a resonator. In classical music and vocal pedagogy, this phenomenon is also known as squillo.

See also
Formant synthesis
Human voice
Linear predictive coding
Praat
Timbre
Vocoder

References
External links
Formants for fun and profit
Formants and wah-wah pedals
What is a formant? A discussion of the three different meanings of the word 'formant'
Formant tuning by soprano singers from the University of New South Wales
The acoustics of harmonic or overtone singing from the University of New South Wales
Materials for measuring and plotting vowel formants
Acoustics of the Vowel A discussion of possible formant variations without affection of the phoneme identity.",Category:Sound synthesis types,1
70,71,Acoustic transmission,"Acoustic transmission is the transmission of sounds through and between materials, including air, wall, and musical instruments.
The degree to which sound is transferred between two materials depends on how well their acoustical impedances match.

In musical instrument design
Musical instruments are generally designed to radiate sound effectively. A high-impedance part of the instrument, such as a string, transmits vibrations through a bridge (intermediate impedance) to a sound board (lower impedance). The soundboard then moves the still lower-impedance air. Without bridge and soundboard, the instrument does not transmit enough sound to the air, and is too quiet to be performed with. An electric guitar has no soundboard; it uses a microphone pick-up and artificial amplification. Without amplification, electric guitars are very quiet.

Stethoscope
Stethoscopes roughly match the acoustical impedance of the human body, so they transmit sounds from a patient's chest to the doctor's ear much more effectively than the air does. Putting an ear to someone's chest would have a similar effect.

In building design
Acoustic transmission in building design refers to a number of processes by which sound can be transferred from one part of a building to another. Typically these are:
Airborne transmission - a noise source in one room sends air pressure waves which induce vibration to one side of a wall or element of structure setting it moving such that the other face of the wall vibrates in an adjacent room. Structural isolation therefore becomes an important consideration in the acoustic design of buildings. Highly sensitive areas of buildings, for example recording studios, may be almost entirely isolated from the rest of a structure by constructing the studios as effective boxes supported by springs. Air tightness also becomes an important control technique. A tightly sealed door might have reasonable sound reduction properties, but if it is left open only a few millimeters its effectiveness is reduced to practically nothing. The most important acoustic control method is adding mass into the structure, such as a heavy dividing wall, which will usually reduce airborne sound transmission better than a light one.
Impact transmission - a noise source in one room results from an impact of an object onto a separating surface, such as a floor and transmits the sound to an adjacent room. A typical example would be the sound of footsteps in a room being heard in a room below. Acoustic control measures usually include attempts to isolate the source of the impact, or cushioning it. For example carpets will perform significantly better than hard floors.
Flanking transmission - a more complex form of noise transmission, where the resultant vibrations from a noise source are transmitted to other rooms of the building usually by elements of structure within the building. For example, in a steel framed building, once the frame itself is set into motion the effective transmission can be pronounced.

References
Carl Hopkins. Sound insulation. Elsevier. Imprint: Butterworth-Heinemann. 2007. ISBN 978-0-7506-6526-1.
Tomas Ficker. Handbook of building thermal technology, acoustics and daylighting. CERM. 2004 ISBN 80-214-2670-5

See also
Acoustic absorption
Acoustic attenuation
Architectural acoustics
Attenuation coefficient
Noise pollution
Soundproofing
Sound pressure
Sound reflection
Sound transmission class",Category:Building engineering,1
71,72,Lumped element model,"The lumped element model (also called lumped parameter model, or lumped component model) simplifies the description of the behaviour of spatially distributed physical systems into a topology consisting of discrete entities that approximate the behaviour of the distributed system under certain assumptions. It is useful in electrical systems (including electronics), mechanical multibody systems, heat transfer, acoustics, etc.
Mathematically speaking, the simplification reduces the state space of the system to a finite dimension, and the partial differential equations (PDEs) of the continuous (infinite-dimensional) time and space model of the physical system into ordinary differential equations (ODEs) with a finite number of parameters.

Electrical systems
Lumped matter discipline
The lumped matter discipline is a set of imposed assumptions in electrical engineering that provides the foundation for lumped circuit abstraction used in network analysis. The self-imposed constraints are:
1. The change of the magnetic flux in time outside a conductor is zero.

  
    
      
        
          
            
              ?
              
                ?
                
                  B
                
              
            
            
              ?
              t
            
          
        
        =
        0
      
    
    {\displaystyle {\frac {\partial \phi _{B}}{\partial t}}=0}
  

2. The change of the charge in time inside conducting elements is zero.

  
    
      
        
          
            
              ?
              q
            
            
              ?
              t
            
          
        
        =
        0
      
    
    {\displaystyle {\frac {\partial q}{\partial t}}=0}
  

3. Signal timescales of interest are much larger than propagation delay of electromagnetic waves across the lumped element.
The first two assumptions result in Kirchhoff's circuit laws when applied to Maxwell's equations and are only applicable when the circuit is in steady state. The third assumption is the basis of the lumped element model used in network analysis. Less severe assumptions result in the distributed element model, while still not requiring the direct application of the full Maxwell equations.

Lumped element model
The lumped element model of electronic circuits makes the simplifying assumption that the attributes of the circuit, resistance, capacitance, inductance, and gain, are concentrated into idealized electrical components; resistors, capacitors, and inductors, etc. joined by a network of perfectly conducting wires.
The lumped element model is valid whenever 
  
    
      
        
          L
          
            c
          
        
        ?
        ?
      
    
    {\displaystyle L_{c}\ll \lambda }
  , where 
  
    
      
        
          L
          
            c
          
        
      
    
    {\displaystyle L_{c}}
   denotes the circuit's characteristic length, and 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   denotes the circuit's operating wavelength. Otherwise, when the circuit length is on the order of a wavelength, we must consider more general models, such as the distributed element model (including transmission lines), whose dynamic behaviour is described by Maxwell's equations. Another way of viewing the validity of the lumped element model is to note that this model ignores the finite time it takes signals to propagate around a circuit. Whenever this propagation time is not significant to the application the lumped element model can be used. This is the case when the propagation time is much less than the period of the signal involved. However, with increasing propagation time there will be an increasing error between the assumed and actual phase of the signal which in turn results in an error in the assumed amplitude of the signal. The exact point at which the lumped element model can no longer be used depends to a certain extent on how accurately the signal needs to be known in a given application.
Real-world components exhibit non-ideal characteristics which are, in reality, distributed elements but are often represented to a first-order approximation by lumped elements. To account for leakage in capacitors for example, we can model the non-ideal capacitor as having a large lumped resistor connected in parallel even though the leakage is, in reality distributed throughout the dielectric. Similarly a wire-wound resistor has significant inductance as well as resistance distributed along its length but we can model this as a lumped inductor in series with the ideal resistor.

Thermal systems
A lumped capacitance model, also called lumped system analysis, reduces a thermal system to a number of discrete “lumps” and assumes that the temperature difference inside each lump is negligible. This approximation is useful to simplify otherwise complex differential heat equations. It was developed as a mathematical analog of electrical capacitance, although it also includes thermal analogs of electrical resistance as well.
The lumped capacitance model is a common approximation in transient conduction, which may be used whenever heat conduction within an object is much faster than heat transfer across the boundary of the object. The method of approximation then suitably reduces one aspect of the transient conduction system (spatial temperature variation within the object) to a more mathematically tractable form (that is, it is assumed that the temperature within the object is completely uniform in space, although this spatially uniform temperature value changes over time). The rising uniform temperature within the object or part of a system, can then be treated like a capacitative reservoir which absorbs heat until it reaches a steady thermal state in time (after which temperature does not change within it).
An early-discovered example of a lumped-capacitance system which exhibits mathematically simple behavior due to such physical simplifications, are systems which conform to Newton's law of cooling. This law simply states that the temperature of a hot (or cold) object progresses toward the temperature of its environment in a simple exponential fashion. Objects follow this law strictly only if the rate of heat conduction within them is much larger than the heat flow into or out of them. In such cases it makes sense to talk of a single ""object temperature"" at any given time (since there is no spatial temperature variation within the object) and also the uniform temperatures within the object allow its total thermal energy excess or deficit to vary proportionally to its surface temperature, thus setting up the Newton's law of cooling requirement that the rate of temperature decrease is proportional to difference between the object and the environment. This in turn leads to simple exponential heating or cooling behavior (details below).

Method
To determine the number of lumps, the Biot number (Bi), a dimensionless parameter of the system, is used. Bi is defined as the ratio of the conductive heat resistance within the object to the convective heat transfer resistance across the object's boundary with a uniform bath of different temperature. When the thermal resistance to heat transferred into the object is larger than the resistance to heat being diffused completely within the object, the Biot number is less than 1. In this case, particularly for Biot numbers which are even smaller, the approximation of spatially uniform temperature within the object can begin to be used, since it can be presumed that heat transferred into the object has time to uniformly distribute itself, due to the lower resistance to doing so, as compared with the resistance to heat entering the object.
If the Biot number is less than 0.1 for a solid object, then the entire material will be nearly the same temperature with the dominant temperature difference will be at the surface. It may be regarded as being ""thermally thin"". The Biot number must generally be less than 0.1 for usefully accurate approximation and heat transfer analysis. The mathematical solution to the lumped system approximation gives Newton's law of cooling.
A Biot number greater than 0.1 (a ""thermally thick"" substance) indicates that one cannot make this assumption, and more complicated heat transfer equations for ""transient heat conduction"" will be required to describe the time-varying and non-spatially-uniform temperature field within the material body.
The single capacitance approach can be expanded to involve many resistive and capacitive elements, with Bi < 0.1 for each lump. As the Biot number is calculated based upon a characteristic length of the system, the system can often be broken into a sufficient number of sections, or lumps, so that the Biot number is acceptably small.
Some characteristic lengths of thermal systems are:
Plate: thickness
Fin: thickness/2
Long cylinder: diameter/4
Sphere: diameter/6
For arbitrary shapes, it may be useful to consider the characteristic length to be volume / surface area.

Thermal purely resistive circuits
A useful concept used in heat transfer applications once the condition of steady state heat conduction has been reached, is the representation of thermal transfer by what is known as thermal circuits. A thermal circuit is the representation of the resistance to heat flow in each element of a circuit, as though it were an electrical resistor. The heat transferred is analogous to the electric current and the thermal resistance is analogous to the electrical resistor. The values of the thermal resistance for the different modes of heat transfer are then calculated as the denominators of the developed equations. The thermal resistances of the different modes of heat transfer are used in analyzing combined modes of heat transfer. The lack of ""capacitative"" elements in the following purely resistive example, means that no section of the circuit is absorbing energy or changing in distribution of temperature. This is equivalent to demanding that a state of steady state heat conduction (or transfer, as in radiation) has already been established.
The equations describing the three heat transfer modes and their thermal resistances in steady state conditions, as discussed previously, are summarized in the table below:

In cases where there is heat transfer through different media (for example, through a composite material), the equivalent resistance is the sum of the resistances of the components that make up the composite. Likely, in cases where there are different heat transfer modes, the total resistance is the sum of the resistances of the different modes. Using the thermal circuit concept, the amount of heat transferred through any medium is the quotient of the temperature change and the total thermal resistance of the medium.
As an example, consider a composite wall of cross-sectional area 
  
    
      
        A
      
    
    {\displaystyle A}
  . The composite is made of an 
  
    
      
        
          L
          
            1
          
        
      
    
    {\displaystyle L_{1}}
   long cement plaster with a thermal coefficient 
  
    
      
        
          k
          
            1
          
        
      
    
    {\displaystyle k_{1}}
   and 
  
    
      
        
          L
          
            2
          
        
      
    
    {\displaystyle L_{2}}
   long paper faced fiber glass, with thermal coefficient 
  
    
      
        
          k
          
            2
          
        
      
    
    {\displaystyle k_{2}}
  . The left surface of the wall is at 
  
    
      
        
          T
          
            i
          
        
      
    
    {\displaystyle T_{i}}
   and exposed to air with a convective coefficient of 
  
    
      
        
          h
          
            i
          
        
      
    
    {\displaystyle h_{i}}
  . The right surface of the wall is at 
  
    
      
        
          T
          
            o
          
        
      
    
    {\displaystyle T_{o}}
   and exposed to air with convective coefficient 
  
    
      
        
          h
          
            o
          
        
      
    
    {\displaystyle h_{o}}
  .

Using the thermal resistance concept, heat flow through the composite is as follows:

  
    
      
        
          
            
              Q
              ?
            
          
        
        =
        
          
            
              
                T
                
                  i
                
              
              ?
              
                T
                
                  o
                
              
            
            
              
                R
                
                  i
                
              
              +
              
                R
                
                  1
                
              
              +
              
                R
                
                  2
                
              
              +
              
                R
                
                  o
                
              
            
          
        
        =
        
          
            
              
                T
                
                  i
                
              
              ?
              
                T
                
                  1
                
              
            
            
              R
              
                i
              
            
          
        
        =
        
          
            
              
                T
                
                  i
                
              
              ?
              
                T
                
                  2
                
              
            
            
              
                R
                
                  i
                
              
              +
              
                R
                
                  1
                
              
            
          
        
        =
        
          
            
              
                T
                
                  i
                
              
              ?
              
                T
                
                  3
                
              
            
            
              
                R
                
                  i
                
              
              +
              
                R
                
                  1
                
              
              +
              
                R
                
                  2
                
              
            
          
        
        =
        
          
            
              
                T
                
                  1
                
              
              ?
              
                T
                
                  2
                
              
            
            
              R
              
                1
              
            
          
        
        =
        
          
            
              
                T
                
                  3
                
              
              ?
              
                T
                
                  o
                
              
            
            
              R
              
                0
              
            
          
        
      
    
    {\displaystyle {\dot {Q}}={\frac {T_{i}-T_{o}}{R_{i}+R_{1}+R_{2}+R_{o}}}={\frac {T_{i}-T_{1}}{R_{i}}}={\frac {T_{i}-T_{2}}{R_{i}+R_{1}}}={\frac {T_{i}-T_{3}}{R_{i}+R_{1}+R_{2}}}={\frac {T_{1}-T_{2}}{R_{1}}}={\frac {T_{3}-T_{o}}{R_{0}}}}
  
where

  
    
      
        
          R
          
            i
          
        
        =
        
          
            1
            
              
                h
                
                  i
                
              
              A
            
          
        
      
    
    {\displaystyle R_{i}={\frac {1}{h_{i}A}}}
  , 
  
    
      
        
          R
          
            o
          
        
        =
        
          
            1
            
              
                h
                
                  o
                
              
              A
            
          
        
      
    
    {\displaystyle R_{o}={\frac {1}{h_{o}A}}}
  , 
  
    
      
        
          R
          
            1
          
        
        =
        
          
            
              L
              
                1
              
            
            
              
                k
                
                  1
                
              
              A
            
          
        
      
    
    {\displaystyle R_{1}={\frac {L_{1}}{k_{1}A}}}
  , and 
  
    
      
        
          R
          
            2
          
        
        =
        
          
            
              L
              
                2
              
            
            
              
                k
                
                  2
                
              
              A
            
          
        
      
    
    {\displaystyle R_{2}={\frac {L_{2}}{k_{2}A}}}

Newton's law of cooling
Newton's law of cooling is an empirical relationship attributed to English physicist Sir Isaac Newton (1642 - 1727). This law stated in non-mathematical form is the following:

The rate of heat loss of a body is proportional to the temperature difference between the body and its surroundings.

Or, using symbols:

  
    
      
        
          Rate of cooling
        
        ?
        
        
        ?
        T
      
    
    {\displaystyle {\text{Rate of cooling}}\sim \!\,\Delta T}
  
An object at a different temperature from its surroundings will ultimately come to a common temperature with its surroundings. A relatively hot object cools as it warms its surroundings; a cool object is warmed by its surroundings. When considering how quickly (or slowly) something cools, we speak of its rate of cooling - how many degrees' change in temperature per unit of time.
The rate of cooling of an object depends on how much hotter the object is than its surroundings. The temperature change per minute of a hot apple pie will be more if the pie is put in a cold freezer than if it is placed on the kitchen table. When the pie cools in the freezer, the temperature difference between it and its surroundings is greater. On a cold day, a warm home will leak heat to the outside at a greater rate when there is a large difference between the inside and outside temperatures. Keeping the inside of a home at high temperature on a cold day is thus more costly than keeping it at a lower temperature. If the temperature difference is kept small, the rate of cooling will be correspondingly low.
As Newton's law of cooling states, the rate of cooling of an object - whether by conduction, convection, or radiation - is approximately proportional to the temperature difference ?T. Frozen food will warm up faster in a warm room than in a cold room. Note that the rate of cooling experienced on a cold day can be increased by the added convection effect of the wind. This is referred to as wind chill. For example, a wind chill of -20 °C means that heat is being lost at the same rate as if the temperature were -20 °C without wind.

Applicable situations
This law describes many situations in which an object has a large thermal capacity and large conductivity, and is suddenly immersed in a uniform bath which conducts heat relatively poorly. It is an example of a thermal circuit with one resistive and one capacitative element. For the law to be correct, the temperatures at all points inside the body must be approximately the same at each time point, including the temperature at its surface. Thus, the temperature difference between the body and surroundings does not depend on which part of the body is chosen, since all parts of the body have effectively the same temperature. In these situations, the material of the body does not act to ""insulate"" other parts of the body from heat flow, and all of the significant insulation (or ""thermal resistance"") controlling the rate of heat flow in the situation resides in the area of contact between the body and its surroundings. Across this boundary, the temperature-value jumps in a discontinuous fashion.
In such situations, heat can be transferred from the exterior to the interior of a body, across the insulating boundary, by convection, conduction, or diffusion, so long as the boundary serves as a relatively poor conductor with regard to the object's interior. The presence of a physical insulator is not required, so long as the process which serves to pass heat across the boundary is ""slow"" in comparison to the conductive transfer of heat inside the body (or inside the region of interest—the ""lump"" described above).
In such a situation, the object acts as the ""capacitative"" circuit element, and the resistance of the thermal contact at the boundary acts as the (single) thermal resistor. In electrical circuits, such a combination would charge or discharge toward the input voltage, according to a simple exponential law in time. In the thermal circuit, this configuration results in the same behavior in temperature: an exponential approach of the object temperature to the bath temperature.

Mathematical statement
Newton's law is mathematically stated by the simple first-order differential equation:

  
    
      
        
          
            
              d
              Q
            
            
              d
              t
            
          
        
        =
        ?
        h
        ?
        A
        (
        T
        (
        t
        )
        ?
        
          T
          
            env
          
        
        )
        =
        ?
        h
        ?
        A
        ?
        T
        (
        t
        )
        
      
    
    {\displaystyle {\frac {dQ}{dt}}=-h\cdot A(T(t)-T_{\text{env}})=-h\cdot A\Delta T(t)\quad }
  
where
Q is thermal energy in joules
h is the heat transfer coefficient between the surface and the fluid
A is the surface area of the heat being transferred
T is the temperature of the object's surface and interior (since these are the same in this approximation)
Tenv is the temperature of the environment
?T(t) = T(t) - Tenv is the time-dependent thermal gradient between environment and object
Putting heat transfers into this form is sometimes not a very good approximation, depending on ratios of heat conductances in the system. If the differences are not large, an accurate formulation of heat transfers in the system may require analysis of heat flow based on the (transient) heat transfer equation in nonhomogeneous or poorly conductive media.

Solution in terms of object heat capacity
If the entire body is treated as lumped capacitance heat reservoir, with total heat content which is proportional to simple total heat capacity 
  
    
      
        C
      
    
    {\displaystyle C}
  , and 
  
    
      
        T
      
    
    {\displaystyle T}
  , the temperature of the body, or 
  
    
      
        Q
        =
        C
        T
      
    
    {\displaystyle Q=CT}
  . It is expected that the system will experience exponential decay with time in the temperature of a body.
From the definition of heat capacity 
  
    
      
        C
      
    
    {\displaystyle C}
   comes the relation 
  
    
      
        C
        =
        d
        Q
        
          /
        
        d
        T
      
    
    {\displaystyle C=dQ/dT}
  . Differentiating this equation with regard to time gives the identity (valid so long as temperatures in the object are uniform at any given time): 
  
    
      
        d
        Q
        
          /
        
        d
        t
        =
        C
        (
        d
        T
        
          /
        
        d
        t
        )
      
    
    {\displaystyle dQ/dt=C(dT/dt)}
  . This expression may be used to replace 
  
    
      
        d
        Q
        
          /
        
        d
        t
      
    
    {\displaystyle dQ/dt}
   in the first equation which begins this section, above. Then, if 
  
    
      
        T
        (
        t
        )
      
    
    {\displaystyle T(t)}
   is the temperature of such a body at time 
  
    
      
        t
      
    
    {\displaystyle t}
  , and 
  
    
      
        
          T
          
            e
            n
            v
          
        
      
    
    {\displaystyle T_{env}}
   is the temperature of the environment around the body:

  
    
      
        
          
            
              d
              T
              (
              t
              )
            
            
              d
              t
            
          
        
        =
        ?
        r
        (
        T
        (
        t
        )
        ?
        
          T
          
            
              e
              n
              v
            
          
        
        )
        =
        ?
        r
        ?
        T
        (
        t
        )
        
      
    
    {\displaystyle {\frac {dT(t)}{dt}}=-r(T(t)-T_{\mathrm {env} })=-r\Delta T(t)\quad }
  
where

  
    
      
        r
        =
        h
        A
        
          /
        
        C
      
    
    {\displaystyle r=hA/C}
   is a positive constant characteristic of the system, which must be in units of 
  
    
      
        
          s
          
            ?
            1
          
        
      
    
    {\displaystyle s^{-1}}
  , and is therefore sometimes expressed in terms of a characteristic time constant 
  
    
      
        
          t
          
            0
          
        
      
    
    {\displaystyle t_{0}}
   given by: 
  
    
      
        r
        =
        1
        
          /
        
        
          t
          
            0
          
        
        =
        ?
        T
        
          /
        
        (
        d
        T
        (
        t
        )
        
          /
        
        d
        t
        )
      
    
    {\displaystyle r=1/t_{0}=\Delta T/(dT(t)/dt)}
  . Thus, in thermal systems, 
  
    
      
        
          t
          
            0
          
        
        =
        C
        
          /
        
        h
        A
      
    
    {\displaystyle t_{0}=C/hA}
  . (The total heat capacity 
  
    
      
        C
      
    
    {\displaystyle C}
   of a system may be further represented by its mass-specific heat capacity 
  
    
      
        
          c
          
            p
          
        
      
    
    {\displaystyle c_{p}}
   multiplied by its mass 
  
    
      
        m
      
    
    {\displaystyle m}
  , so that the time constant 
  
    
      
        
          t
          
            0
          
        
      
    
    {\displaystyle t_{0}}
   is also given by 
  
    
      
        m
        
          c
          
            p
          
        
        
          /
        
        h
        A
      
    
    {\displaystyle mc_{p}/hA}
  ).
The solution of this differential equation, by standard methods of integration and substitution of boundary conditions, gives:

  
    
      
        T
        (
        t
        )
        =
        
          T
          
            
              e
              n
              v
            
          
        
        +
        (
        T
        (
        0
        )
        ?
        
          T
          
            
              e
              n
              v
            
          
        
        )
         
        
          e
          
            ?
            r
            t
          
        
        .
        
      
    
    {\displaystyle T(t)=T_{\mathrm {env} }+(T(0)-T_{\mathrm {env} })\ e^{-rt}.\quad }
  
If:

  
    
      
        ?
        T
        (
        t
        )
        
      
    
    {\displaystyle \Delta T(t)\quad }
   is defined as : 
  
    
      
        T
        (
        t
        )
        ?
        
          T
          
            
              e
              n
              v
            
          
        
         
        ,
        
      
    
    {\displaystyle T(t)-T_{\mathrm {env} }\ ,\quad }
   where 
  
    
      
        ?
        T
        (
        0
        )
        
      
    
    {\displaystyle \Delta T(0)\quad }
   is the initial temperature difference at time 0,
then the Newtonian solution is written as:

  
    
      
        ?
        T
        (
        t
        )
        =
        ?
        T
        (
        0
        )
         
        
          e
          
            ?
            r
            t
          
        
        =
        ?
        T
        (
        0
        )
         
        
          e
          
            ?
            t
            
              /
            
            
              t
              
                0
              
            
          
        
        .
        
      
    
    {\displaystyle \Delta T(t)=\Delta T(0)\ e^{-rt}=\Delta T(0)\ e^{-t/t_{0}}.\quad }
  
This same solution is almost immediately apparent if the initial differential equation is written in terms of 
  
    
      
        ?
        T
        (
        t
        )
      
    
    {\displaystyle \Delta T(t)}
  , as the single function to be solved for. '

  
    
      
        
          
            
              d
              T
              (
              t
              )
            
            
              d
              t
            
          
        
        =
        
          
            
              d
              ?
              T
              (
              t
              )
            
            
              d
              t
            
          
        
        =
        ?
        
          
            1
            
              t
              
                0
              
            
          
        
        ?
        T
        (
        t
        )
        
      
    
    {\displaystyle {\frac {dT(t)}{dt}}={\frac {d\Delta T(t)}{dt}}=-{\frac {1}{t_{0}}}\Delta T(t)\quad }

Applications
This mode of analysis has been applied to forensic sciences to analyze the time of death of humans. Also, it can be applied to HVAC (heating, ventilating and air-conditioning, which can be referred to as ""building climate control""), to ensure more nearly instantaneous effects of a change in comfort level setting.

Mechanical systems
The simplifying assumptions in this domain are:
all objects are rigid bodies;
all interactions between rigid bodies take place via kinematic pairs (joints), springs and dampers.

Acoustics
In this context, the lumped component model extends the distributed concepts of Acoustic theory subject to approximation. In the acoustical lumped component model, certain physical components with acoustical properties may be approximated as behaving similarly to standard electronic components or simple combinations of components.
A rigid-walled cavity containing air (or similar compressible fluid) may be approximated as a capacitor whose value is proportional to the volume of the cavity. The validity of this approximation relies on the shortest wavelength of interest being significantly (much) larger than the longest dimension of the cavity.
A reflex port may be approximated as an inductor whose value is proportional to the effective length of the port divided by its cross-sectional area. The effective length is the actual length plus an end correction. This approximation relies on the shortest wavelength of interest being significantly larger than the longest dimension of the port.
Certain types of damping material can be approximated as a resistor. The value depends on the properties and dimensions of the material. The approximation relies in the wavelengths being long enough and on the properties of the material itself.
A loudspeaker drive unit (typically a woofer or subwoofer drive unit) may be approximated as a series connection of a zero-impedance voltage source, a resistor, a capacitor and an inductor. The values depend on the specifications of the unit and the wavelength of interest.

Heat transfer for buildings
The simplifying assumption in this domain are",Category:Components,1
72,73,Acousto-optic deflector,"An acousto-optic deflector (AOD) spatially controls the optical beam. In the operation of an acousto-optic deflector the power driving the acoustic transducer is kept on, at a constant level, while the acoustic frequency is varied to deflect the beam to different angular positions. The acousto-optic deflector makes use of the acoustic frequency dependent diffraction angle, where a change in the angle 
  
    
      
        ?
        
          ?
          
            d
          
        
      
    
    {\displaystyle \Delta \theta _{d}}
   as a function of the change in frequency 
  
    
      
        ?
        f
      
    
    {\displaystyle \Delta f}
   given as,

  
    
      
        (
        12
        )
         
        ?
        
          ?
          
            d
          
        
        =
        
          
            ?
            ?
          
        
        ?
        f
      
    
    {\displaystyle (12)\ \Delta \theta _{d}={\frac {\lambda }{\nu }}\Delta f}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   is the optical wavelength and 
  
    
      
        ?
      
    
    {\displaystyle \nu }
   is the velocity of the acoustic wave.
AOD technology has made practical the Bose–Einstein condensation for which the 2001 Nobel Prize in Physics was awarded to Eric A. Cornell, Wolfgang Ketterle and Carl E. Wieman. Another application of acoustic-optical deflection is optical trapping of small molecules.
AODs are essentially the same as acousto-optic modulators (AOMs). In an AOM, only the amplitude of the sound wave is modulated (to modulate the intensity of the diffracted laser beam), whereas in an AOD, both the amplitude and frequency are adjusted, making the engineering requirements tighter for an AOD than an AOM.

See also
Acousto-optic modulator
Acousto-optics
Nonlinear optics
Sonoluminescence

References


== External links ==",Category:Acoustics,1
73,74,Waveguide (acoustics),"This page is about waveguides for acoustics and sound, for other types of waveguide, see Waveguide
An acoustic waveguide is a physical structure for guiding sound waves.

Examples
One example might be a speaking tube used aboard ships for communication between decks. Other examples include the rear passage in a transmission line loudspeaker enclosure, the ear canal or a device like a stethoscope. The term also applies to guided waves in solids.
A duct for sound propagation also behaves like a transmission line (e.g. air conditioning duct, car muffler, etc.). The duct contains some medium, such as air, that supports sound propagation. Its length is typically around quarter of the wavelength which is intended to be guided, but the dimensions of its cross section are smaller than this. Sound is introduced at one end of the tube by forcing the pressure to vary in the direction of propagation, which causes a pressure gradient to travel perpendicular to the cross section at the speed of sound. When the wave reaches the end of the transmission line, its behaviour depends on what is present at the end of the line. There are three generalized scenarios:
A low impedance load (e.g. leaving the end open in free air) will cause a reflected wave in which the sign of the pressure variation reverses, but the direction of the pressure wave remains the same.
A load that matches the characteristic impedance (defined below) will completely absorb the wave and the energy associated with it. No reflection will occur.
A high impedance load (e.g. by plugging the end of the line) will cause a reflected wave in which the direction of the pressure wave is reversed but the sign of the pressure remains the same.
Since a transmission line behaves like a four terminal model, one cannot really define or measure the impedance of a transmission line component. One can however measure its input or output impedance. It depends on the cross-sectional area and length of the line, the sound frequency, as well as the characteristic impedance of the sound propagating medium within the duct. Only in the exceptional case of a closed end tube (to be compared with electrical short circuit), the input impedance could be regarded as a component impedance.
Where a transmission line of finite length is mismatched at both ends, there is the potential for a wave to bounce back and forth many times until it is absorbed. This phenomenon is a kind of resonance and will tend to attenuate any signal fed into the line.
When this resonance effect is combined with some sort of active feedback mechanism and power input, it is possible to set up an oscillation which can be used to generate periodic acoustic signals such as musical notes (e.g. in an organ pipe).
The application of transmission line theory is however seldom used in acoustics. An equivalent four terminal model which splits the downstream and upstream waves is used. This eases the introduction of physically measurable acoustic characteristics, reflection coefficients, material constants of insulation material, the influence of air velocity on wavelength (Mach number), etc. This approach also circumvents impractical theoretical concepts, such as acoustic impedance of a tube, which is not measurable because of its inherent interaction with the sound source and the load of the acoustic component.

Notes
See also
Acoustic transmission line - a type of technology used in some loudspeakers, and based on the principles of an acoustic waveguide.

References
Bibliography
Morse, P.M., Vibration and sound, McGraw Hill, 1948, NYC, NY.
Pierce, A.D., Acoustics: An Introduction to its Physical Principles and Applications, McGraw Hill, 1981, NYC, NY.",Category:Acoustics,1
74,75,Absolute threshold of hearing,"The absolute threshold of hearing (ATH) is the minimum sound level of a pure tone that an average human ear with normal hearing can hear with no other sound present. The absolute threshold relates to the sound that can just be heard by the organism. The absolute threshold is not a discrete point, and is therefore classed as the point at which a sound elicits a response a specified percentage of the time. This is also known as the auditory threshold.
The threshold of hearing is generally reported as the RMS sound pressure of 20 micropascals, corresponding to a sound intensity of 0.98 pW/m2 at 1 atmosphere and 25 °C. It is approximately the quietest sound a young human with undamaged hearing can detect at 1,000 Hz. The threshold of hearing is frequency-dependent and it has been shown that the ear's sensitivity is best at frequencies between 2 kHz and 5 kHz, where the threshold reaches as low as ?9 dB SPL.

Psychophysical methods for measuring thresholds
Measurement of the absolute hearing threshold provides some basic information about our auditory system. The tools used to collect such information are called psychophysical methods. Through these, the perception of a physical stimulus (sound) and our psychological response to the sound is measured.
Several psychophysical methods can measure absolute threshold. These vary, but certain aspects are identical. Firstly, the test defines the stimulus and specifies the manner in which the subject should respond. The test presents the sound to the listener and manipulates the stimulus level in a predetermined pattern. The absolute threshold is defined statistically, often as an average of all obtained hearing thresholds.
Some procedures use a series of trials, with each trial using the ‘single-interval “yes”/”no” paradigm’. This means that sound may be present or absent in the single interval, and the listener has to say whether he thought the stimulus was there. When the interval does not contain a stimulus, it is called a ""catch trial"".

Classical methods
Classical methods date back to the 19th century and were first described by Gustav Theodor Fechner in his work Elements of Psychophysics. Three methods are traditionally used for testing a subject's perception of a stimulus: the method of limits, the method of constant stimuli, and the method of adjustment.
Method of limits
In the method of limits, the tester controls the level of the stimuli. Single-interval yes/no paradigm’ is used, but there are no catch trials.
The trial uses several series of descending and ascending runs.
The trial starts with the descending run, where a stimulus is presented at a level well above the expected threshold. When the subject responds correctly to the stimulus, the level of intensity of the sound is decreased by a specific amount and presented again. The same pattern is repeated until the subject stops responding to the stimuli, at which point the descending run is finished.
In the ascending run, which comes after, the stimulus is first presented well below the threshold and then gradually increased in two decibel (dB) steps until the subject responds.

As there are no clear margins to ‘hearing’ and ‘not hearing’, the threshold for each run is determined as the midpoint between the last audible and first inaudible level.
The subject's absolute hearing threshold is calculated as the mean of all obtained thresholds in both ascending and descending runs.
There are several issues related to the method of limits. First is anticipation, which is caused by the subject's awareness that the turn-points determine a change in response. Anticipation produces better ascending thresholds and worse descending thresholds.
Habituation creates completely opposite effect, and occurs when the subject becomes accustomed to responding either “yes” in the descending runs and/or “no” in the ascending runs. For this reason, thresholds are raised in ascending runs and improved in descending runs.
Another problem may be related to step size. Too large a step compromises accuracy of the measurement as the actual threshold may be just between two stimulus levels.
Finally, since the tone is always present, “yes” is always the correct answer.
Method of constant stimuli
In the method of constant stimuli, the tester sets the level of stimuli and presents them at completely random order.

Thus, there are no ascending or descending trials.
The subject responds “yes”/”no” after each presentation.
The stimuli are presented many times at each level and the threshold is defined as the stimulus level at which the subject scored 50% correct. “Catch” trials may be included in this method.
Method of constant stimuli has several advantages over the method of limits. Firstly, the random order of stimuli means that the correct answer cannot be predicted by the listener. Secondarily, as the tone may be absent (catch trial), “yes” is not always the correct answer. Finally, catch trials help to detect the amount of a listener's guessing.
The main disadvantage lies in the large number of trials needed to obtain the data, and therefore time required to complete the test.
Method of adjustment
Method of adjustment shares some features with the method of limits, but differs in others. There are descending and ascending runs and the listener knows that the stimulus is always present.

However, unlike in the method of limits, here the stimulus is controlled by the listener. The subject reduces the level of the tone until it cannot be detected anymore, or increases until it can be heard again.
The stimulus level is varied continuously via a dial and the stimulus level is measured by the tester at the end. The threshold is the mean of the just audible and just inaudible levels.
Also this method can produce several biases. To avoid giving cues about the actual stimulus level, the dial must be unlabeled. Apart from already mentioned anticipation and habituation, stimulus persistence (preservation) could influence the result from the method of adjustment.
In the descending runs, the subject may continue to reduce the level of the sound as if the sound was still audible, even though the stimulus is already well below the actual hearing threshold.
In contrast, in the ascending runs, the subject may have persistence of the absence of the stimulus until the hearing threshold is passed by certain amount.

Modified classical methods
Forced-choice methods
Two intervals are presented to a listener, one with a tone and one without a tone. Listener must decide which interval had the tone in it. The number of the intervals can be increased, but this may cause problems to the listener who has to remember which interval contained the tone.

Adaptive methods
Unlike the classical methods, where the pattern for changing the stimuli is preset, in adaptive methods the subject's response to the previous stimuli determines the level at which a subsequent stimulus is presented.

Staircase’ methods (up-down methods)
The simple ‘1-down-1-up’ method consists of series of descending and ascending trials runs and turning points (reversals). The stimulus level is increased if the subject does not respond and decreased when a response occurs.
Similarly, as in the method of limits, the stimuli are adjusted in predetermined steps. After obtaining from six to eight reversals, the first one is discarded and the threshold is defined as the average of the midpoints of the remaining runs. Experiments showed that this method provides only 50% accuracy.
To produce more accurate results, this simple method can be further modified by increasing the size of steps in the descending runs, e.g. ‘2-down-1-up method’, ‘3-down-1-up methods’.

Bekesy's tracking method
Bekesy's method contains some aspects of classical methods and staircase methods. The level of the stimulus is automatically varied at a fixed rate. The subject is asked to press a button when the stimulus is detectable.

Once the button is pressed, the level is automatically decreased by the motor-driven attenuator and increased when the button is not pushed. The threshold is thus tracked by the listeners, and calculated as the mean of the midpoints of the runs as recorded by the automat.

Hysteresis effect
Hysteresis can be defined roughly as ‘the lagging of an effect behind its cause’. When measuring hearing thresholds it is always easier for the subject to follow a tone that is audible and decreasing in amplitude than to detect a tone that was previously inaudible.
This is because ‘top-down’ influences mean that the subject expects to hear the sound and is, therefore, more motivated with higher levels of concentration.
The ‘bottom-up’ theory explains that unwanted external (from the environment) and internal (e.g., heartbeat) noise results in the subject only responding to the sound if the signal to noise ratio is above a certain point.
In practice this means that when measuring threshold with sounds decreasing in amplitude, the point at which the sound becomes inaudible is always lower than the point at which it returns to audibility. This phenomenon is known as the ‘hysteresis effect’.

Psychometric function of absolute hearing threshold
Psychometric function ‘represents the probability of a certain listener's response as a function of the magnitude of the particular sound characteristic being studied’.
To give an example, this could be the probability curve of the subject detecting a sound being presented as a function of the sound level. When the stimulus is presented to the listener one would expect that the sound would either be audible or inaudible, resulting in a 'doorstep' function. In reality a grey area exists where the listener is uncertain as to whether they have actually heard the sound or not, so their responses are inconsistent, resulting in a psychometric function.
The psychometric function is a sigmoid function characterised by being ‘s’ shaped in its graphical representation.

Minimal audible field (MAF) vs minimal audible pressure (MAP)
Two methods can be used to measure the minimal audible stimulus and therefore the absolute threshold of hearing. Minimal audible field involves the subject sitting in a sound field and stimulus being presented via a loudspeaker. The sound level is then measured at the position of the subjects head with the subject not in the sound field. Minimal audible pressure involves presenting stimuli via headphones or earphones and measuring sound pressure in the subject's ear canal using a very small probe microphone. The two different methods produce different thresholds and minimal audible field thresholds are often 6 to 10 dB better than minimal audible pressure thresholds. It is thought that this difference is due to:
monaural vs binaural hearing. With minimal audible field both ears are able to detect the stimuli but with minimal audible pressure only one ear is able to detect the stimuli. Binaural hearing is more sensitive than monaural hearing/
physiological noises heard when ear is occluded by an earphone during minimal audible pressure measurements. When the ear is covered the subject hears body noises, such as heart beat, and these may have a masking effect.
Minimal audible field and minimal audible pressure are important when considering calibration issues and they also illustrate that the human hearing is most sensitive in the 2–5 kHz range.

Temporal summation
Temporal summation is the relationship between stimulus duration and intensity when the presentation time is less than 1 second. Auditory sensitivity changes when the duration of a sound becomes less than 1 second. The threshold intensity decreases by about 10 dB when the duration of a tone burst is increased from 20 to 200 ms.
For example, suppose that the quietest sound a subject can hear is 16 dB SPL if the sound is presented at a duration of 200 ms. If the same sound is then presented for a duration of only 20 ms, the quietest sound that can now be heard by the subject goes up to 26 dB SPL. In other words, if a signal is shortened by a factor of 10 then the level of that signal must be increased by as much as 10 dB to be heard by the subject.
The ear operates as an energy detector that samples the amount of energy present within a certain time frame. A certain amount of energy is needed within a time frame to reach the threshold. This can be done by using a higher intensity for less time or by using a lower intensity for more time. Sensitivity to sound improves as the signal duration increases up to about 200 to 300 ms, after that the threshold remains constant.
The timpani of the ear operates more as a sound pressure sensor. Also a microphone works the same way and is not sensitive to sound intensity.

See also
dB(A)
Equal-loudness contour
Hearing range
Loudness
Phon
Psychoacoustics
Psychophysics
Signal detection theory
Sone

References
Fechner, G., 1860. Elements of psychophysics. New York: Holt, Rinehart and Winston. Citation from the book available on: http://psychclassics.yorku.ca/Fechner/.
Katz J. (Ed). United States of America: Lippencott, Williams & Wilkins
Levitt H., 1971. ""Transformed up-down methods in psychoacoustics"". J. Acoust. Soc. Amer. 49, 467-477. Available to download from: http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal&id=JASMAN00004900002B000467000001&idtype=cvips&gifs=yes. (Accessed 1 March 2007).
www.thefreedictionary.com. Accessed 28 February 2007

External links
A comparison of threshold estimation methods in children 6–11 years of age
A Concise Vocabulary of Audiology and allied topics
Fundamental aspects of hearing
Equal loudness contours and audiometry - Test your own hearing
Online Hearing Threshold Test - An alternate audiometric test, with calibrated levels and results expressed in dBHL
Fundamentals of psychoacoustics
Minimising boredom by maximising likelihood-an efficient estimation of masked thresholds
On Minimum Audible Sound Fields
Psychometric Functions for Children's Detection of Tones in Noise
Psychophysical methods
Reference levels for objective audiometry
Response bias in psychophysics
Sensitivity of Human Ear
The psychoacoustics of multichannel audio
Three Models of Temporal Summation Evaluated Using Normal-Hearing and Hearing-Impaired Subjects
Threshold
Threshold of Hearing - equation and graph",Category:Hearing,1
75,76,Rijke tube,"Rijke's tube turns heat into sound, by creating a self-amplifying standing wave. It is an entertaining phenomenon in acoustics and is an excellent example of resonance.

Discovery
P. L. Rijke was a professor of physics at the Leiden University in the Netherlands when, in 1859, he discovered a way of using heat to sustain a sound in a cylindrical tube open at both ends. He used a glass tube, about 0.8 m long and 3.5 cm in diameter. Inside it, about 20 cm from one end, he placed a disc of wire gauze as shown in the figure on right. Friction with the walls of the tube is sufficient to keep the gauze in position. With the tube vertical and the gauze in the lower half, he heated the gauze with a flame until it was glowing red hot. Upon removing the flame, he obtained a loud sound from the tube which lasted until the gauze cooled down (about 10s). It is safer in modern reproductions of this experiment to use a borosilicate glass tube or, better still, one made of metal.
Instead of heating the gauze with a flame, Rijke also tried electrical heating. Making the gauze with electrical resistance wire causes it to glow red when a sufficiently large current is passed. With the heat being continuously supplied, the sound is also continuous and rather loud. Rijke seems to have received complaints from his university colleagues because he reports that the sound could be easily heard three rooms away from his laboratory. The electrical power required to achieve this is about 1 kW.
Lord Rayleigh, who wrote the definitive textbook on sound in 1877, recommends this as a very effective lecture demonstration. He used a cast iron pipe 1.5 m long and 12 cm diameter with two layers of gauze made from iron wire inserted about quarter of the way up the tube. The extra gauze is to retain more heat, which makes the sound longer lasting. He reports in his book that the sound rises to such intensity as to shake the room!
A ""reverse"" Rijke effect — namely, that a Rijke tube will also produce audio oscillations if hot air flows through a cold screen — was first observed by Rijke's assistant Johannes Bosscha and subsequently investigated by German physicist Peter Theophil Rieß.

Mechanism
The sound comes from a standing wave whose wavelength is about twice the length of the tube, giving the fundamental frequency. Lord Rayleigh, in his book, gave the correct explanation of how the sound is stimulated. The flow of air past the gauze is a combination of two motions. There is a uniform upwards motion of the air due to a convection current resulting from the gauze heating up the air. Superimposed on this is the motion due to the sound wave.
For half the vibration cycle, the air flows into the tube from both ends until the pressure reaches a maximum. During the other half cycle, the flow of air is outwards until the minimum pressure is reached. All air flowing past the gauze is heated to the temperature of the gauze and any transfer of heat to the air will increase its pressure according to the gas law. As the air flows upwards past the gauze most of it will already be hot because it has just come downwards past the gauze during the previous half cycle. However, just before the pressure maximum, a small quantity of cool air comes into contact with the gauze and its pressure is suddenly increased. This increases the pressure maximum, so reinforcing the vibration. During the other half cycle, when the pressure is decreasing, the air above the gauze is forced downwards past the gauze again. Since it is already hot, no pressure change due to the gauze takes place, since there is no transfer of heat. The sound wave is therefore reinforced once every vibration cycle, and it quickly builds up to a very large amplitude.
This explains why there is no sound when the flame is heating the gauze: all air flowing through the tube is heated by the flame, so when it reaches the gauze, it is already hot and no pressure increase takes place.
When the gauze is in the upper half of the tube, there is no sound. In this case, the cool air brought in from the bottom by the convection current reaches the gauze towards the end of the outward vibration movement. This is immediately before the pressure minimum, so a sudden increase in pressure due to the heat transfer tends to cancel out the sound wave instead of reinforcing it.
The position of the gauze in the tube is not critical as long as it is in the lower half. To work out its best position, there are two things to consider. Most heat will be transferred to the air where the displacement of the wave is a maximum, i.e. at the end of the tube. However, the effect of increasing the pressure is greatest where there is the greatest pressure variation, i.e. in the middle of the tube. Placing the gauze midway between these two positions (one quarter of the way in from the bottom end) is a simple way to come close to the optimal placement.
The Rijke tube is considered to be a standing wave form of thermoacoustic devices known as ""heat engines"" or ""prime movers"".

Sondhauss tube
The Rijke tube operates with both ends open. However, a tube with one end closed will also generate sound from heat, if the closed end is very hot. Such a device is called a “Sondhauss tube”. The phenomenon was first observed by glassblowers and was first described in 1850 by the German physicist Karl Friedrich Julius Sondhauss (1815–1886). Lord Rayleigh first explained the operation of the Sondhauss tube.
The Sondhauss tube operates in a way that is basically similar to the Rijke tube: Initially, air moves towards the hot, closed end of the tube, where it's heated, so that the pressure at that end increases. The hot, higher-pressure air then flows from the closed end towards the cooler, open end of the tube. The air transfers its heat to the tube and cools. The air surges slightly beyond the open end of the tube, briefly compressing the atmosphere; the compression propagates through the atmosphere as a sound wave. The atmosphere then pushes the air back into the tube, and the cycle repeats. Unlike the Rijke tube, the Sondhauss tube does not require a steady flow of air through it, and whereas the Rijke tube acts as a half-wave resonator, the Sondhauss tube acts as a quarter-wave resonator.
Like the Rijke tube, it was discovered that placing a porous heater — as well as a ""stack"" (a ""plug"" that is porous) — in the tube greatly increased the power and efficiency of the Sondhauss tube. (In demonstration models, the tube can be heated externally and steel wool can serve as a stack.)

See also
Pyrophone

References
Further information
K. T. Feldman, Jr. (1968) ""Review of literature on Rijke thermoacoustic phenomena,"" Journal of Sound and Vibration, vol. 7, no. 1, pages 83–89.
In German: Rijke-Rohr [Rijke tube]: http://wundersamessammelsurium.info/akustisches/rijke_rohr/index.html . Includes links to original articles by Rijke, Riess, etc.
R. E. Evans and A. A. Putnam (1966) ""Rijke Tube Apparatus,"" American Journal of Physics, vol. 34, no. 4, pages 360-361.
Julius Sumner Miller, ""Sounding Pipes"": https://www.youtube.com/watch?v=7dxkW5bsUgs . Demonstrations of Rijke tubes.",Category:Toy instruments and noisemakers,1
76,77,Rubens' tube,"A Rubens' tube, also known as a standing wave flame tube, or simply flame tube, is an antique physics apparatus for demonstrating acoustic standing waves in a tube. Invented by German physicist Heinrich Rubens in 1905, it graphically shows the relationship between sound waves and sound pressure, as a primitive oscilloscope. Today, it is used only occasionally, typically as a demonstration in physics education.

Overview
A length of pipe is perforated along the top and sealed at both ends - one seal is attached to a small speaker or frequency generator, the other to a supply of a flammable gas (propane tank). The pipe is filled with the gas, and the gas leaking from the perforations is lit. If a suitable constant frequency is used, a standing wave can form within the tube. When the speaker is turned on, the standing wave will create points with oscillating (higher and lower) pressure and points with constant pressure (pressure nodes) along the tube. Where there is oscillating pressure due to the sound waves, less gas will escape from the perforations in the tube, and the flames will be lower at those points. At the pressure nodes, the flames are higher. At the end of the tube gas molecule velocity is zero and oscillating pressure is maximal, thus low flames are observed. It is possible to determine the wavelength from the flame minimum and maximum by simply measuring with a ruler.

Explanation
Since the time averaged pressure is equal at all points of the tube, it is not straightforward to explain the different flame heights. The flame height is proportional to the gas flow as shown in the figure. Based on Bernoulli's principle, the gas flow is proportional to the square root of the pressure difference between the inside and outside of the tube. This is shown in the figure for a tube without standing sound wave. Based on this argument, the flame height depends non-linearly on the local, time-dependent pressure. The time average of the flow is reduced at the points with oscillating pressure and thus flames are lower.

History
Heinrich Rubens was a German physicist born in 1865. Though he allegedly worked with better remembered physicists such as Max Planck at the University of Berlin on some of the ground work for quantum physicists, he is best known for his flame tube, which was demonstrated in 1905. This original Rubens' tube was a four-meter section of pipe with 200 holes spaced evenly along its length.
When the ends of the pipe are sealed and a flammable gas is pumped into the device, the escaping gas can be lit to form a row of flames of roughly equal size. When sound is applied from one end by means of a loudspeaker, internal pressure will change along the length of the tube. If the sound is of a frequency that produces standing waves, the wavelength will be visible in the series of flames, with the tallest flames occurring at pressure nodes, and the lowest flames occurring at pressure antinodes. The pressure antinodes correspond to the locations with the highest amount of compression and rarefaction.

Public displays
A Rubens' tube was on display at The Exploratory in Bristol, England until it closed in 1999. A similar exhibit using polystyrene beads instead of flames featured in the At-Bristol science centre until 2009. Students make models of rubens' tube at their school science exhibition.
This display is also found in physics departments at a number of universities. A number of physics shows also have one, such as: Rino Foundation  (The Netherlands), Fysikshow Aarhus (Denmark), Fizika Ekspres (Croatia) and ÅA Physics show (Finland).
The MythBusters also included a demonstration on their ""Voice Flame Extinguisher"" episode in 2007. The Daily Planet's The Greatest Show Ever, ran a competition whereby five Canadian science centres competed for the best science centre's experiment/display. Edmonton's Science Centre (Telus World of Science) utilized a Rubens' tube, and won the competition. The special was filmed on October 10, 2010. Tim Shaw on the show Street Genius on National Geographic Channel also featured one in Episode 18 ""Wave of fire"".
The artist Emer O'Brien used Rubens tubes as the basis for the sound sculpture featured in her 2012 exhibition Return to Normal at the Wapping Project in London.

2D Rubens' Tube
Overview
A 2D Rubens’ tube, also known as a pyro board, is a plane of Bunsen burners that can demonstrate an acoustic standing wave in two dimensions. Similar to its predecessor, the one dimensional Rubens' tube, this standing wave is caused by a multitude of factors. Pressure variation caused by the inflow of propane gas interfering with the input of sound waves into the plane causes changes in the height and color of the flames. The 2D Rubens’ tube was made famous by a Danish science demonstrator group in Denmark called Fysikshow.

Explanation
A 2D Rubens’ tube is made up of a lot different parts. The main part itself is the rectangular steel box that outputs the propane gas. Steel is generally used for the plane on pyro boards because the compound can generally withstand immense amounts of heat and still be able to maintain its structure. Holes are drilled on the top of the steel plane to output the propane gas that is being constantly and slowly pumped into the steel box. Instead of having a complete steel box, some pyro boards designs have wooden sides to support the steel plane on top. In wooden-style pyro boards, the interior of the box is usually covered with some sort of heat-resistant membrane that prevents the propane inside the box from leaking.
On the sides of the steel box are speakers that input a sound into the contained medium. The rate at which the propane gas escapes through the holes on the top of the pyro board is dependent on the intensity of the inputted sound. This relationship is directly proportional, meaning as the intensity of the sound increases, the rate at which the propane gas increases.
Since the medium inside the steel box is kept at a constant volume, a standing wave has the ability to be produced. The frequency at which the standing wave can be produced is largely dependent on the physical dimensions of the box and the wavelength of the wave. Since pyro boards range in sizes, each board has its own unique frequencies at which a standing wave can be produced.

Public displays
In 2014, Danish science demonstrator Sune Nielsen, a member of Fysikshow, teamed up with science blogger Derek Muller in a YouTube video showing off the pyro board in action. Derek Muller, also known as Veritasium on YouTube, explains the science behind how the 1D and 2D Rubens’ Tubes work.

References
External links
Baroque hoedown for 6 Ruben's tubes by Mathew Kneebone and Yuri Suzuki
Detailed Video including sound board and microphone
Experiment notes, video & detailed analysis
Flame tube setup and explanation of effects
Brief Setup Guide
Classroom setup guide
Information on Rubens' original design in .doc format
Image showing setup
General information
Experiment setup - under ""Links"" heading and photo illustrating this experiment
Video various tones and music being played
Rubens' Tube performance by Alyce Santoro
DCC capstone proposal
Fysikshow YouTube demonstration",Category:Wave mechanics,1
77,78,Sound from ultrasound,"Sound from ultrasound is the name given here to the generation of audible sound from modulated ultrasound without using an active receiver. This happens when the modulated ultrasound passes through a nonlinear medium which acts, intentionally or unintentionally, as a demodulator.

Parametric array
Since the early 1960s, researchers have been experimenting with creating directive low-frequency sound from nonlinear interaction of an aimed beam of ultrasound waves produced by a parametric array using heterodyning. Ultrasound has much shorter wavelengths than audible sound, so that it propagates in a much narrower beam than any normal loudspeaker system using audio frequencies. Most of the work was performed in liquids (for underwater sound use).
The first modern device for air acoustic use was created in 1998, and is now known by the trademark name ""Audio Spotlight"", a term first coined in 1983 by the Japanese researchers who abandoned the technology as infeasible in the mid-1980s.
A transducer can be made to project a narrow beam of modulated ultrasound that is powerful enough, at 100 to 110 dBSPL, to substantially change the speed of sound in the air that it passes through. The air within the beam behaves nonlinearly and extracts the modulation signal from the ultrasound, resulting in sound that can be heard only along the path of the beam, or that appears to radiate from any surface that the beam strikes. This technology allows a beam of sound to be projected over a long distance to be heard only in a small well-defined area; for a listener outside the beam the Sound pressure decreases substantially. This effect cannot be achieved with conventional loudspeakers, because sound at audible frequencies cannot be focused into such a narrow beam.
There are some limitations with this approach. Anything that interrupts the beam will prevent the ultrasound from propagating, like interrupting a spotlight's beam. For this reason, most systems are mounted overhead, like lighting.

Applications
Military
There has been speculation about military sonic weapons that emit highly-directional high-intensity sound; however, these devices do not use ultrasound, although sometimes thought to do so. Wikileaks has published technical specifications of such sound weapons.

Commercial advertising
A sound signal can be aimed so that only a particular passer-by, or somebody very close, can hear it. In commercial applications, it can target sound to a single person without the peripheral sound and related noise of a loudspeaker.

Personal audio
It can be used for personal audio, either to have sounds audible to only one person, or that which a group wants to listen to. The navigation instructions for example are only interesting for the driver in a car, not for the passengers. Another possibility are future applications for true stereo sound, where one ear does not hear what the other is hearing.

Train Signaling Device
Directional audio train signaling may be accomplished through the use of an ultrasonic beam which will warn of the approach of a train while avoiding the nuisance of loud train signals on surrounding homes and businesses.

History
This technology was originally developed by the US Navy and Soviet Navy for underwater sonar in the mid-1960s, and was briefly investigated by Japanese researchers in the early 1980s, but these efforts were abandoned due to extremely poor sound quality (high distortion) and substantial system cost. These problems went unsolved until a paper published by Dr. F. Joseph Pompei of the Massachusetts Institute of Technology in 1998 fully described a working device that reduced audible distortion essentially to that of a traditional loudspeaker.

Products
As of 2014 there were known to be five devices which have been marketed that use ultrasound to create an audible beam of sound.

Audio Spotlight
F. Joseph Pompei of MIT developed technology he calls the ""Audio Spotlight"", and made it commercially available in 2000 by his company Holosonics, which according to their website claims to have sold ""thousands"" of their ""Audio Spotlight"" systems. Disney was amongst the first major corporations to adopt it for use at the Epcot Center, and many other application examples are shown on the Holosonics website.
Audio Spotlight is a narrow beam of sound that can be controlled with similar precision to light from a spotlight. It uses a beam of ultrasound as a ""virtual acoustic source"", enabling control of sound distribution. The ultrasound has wavelengths only a few millimeters long which are much smaller than the source, and therefore naturally travel in an extremely narrow beam. The ultrasound, which contains frequencies far outside the range of human hearing, is completely inaudible. But as the ultrasonic beam travels through the air, the inherent properties of the air cause the ultrasound to change shape in a predictable way. This gives rise to frequency components in the audible band, which can be predicted and controlled.

HyperSonic Sound
Elwood ""Woody"" Norris, founder and Chairman of American Technology Corporation (ATC), announced he had successfully created a device which achieved ultrasound transmission of sound in 1996. This device used piezoelectric transducers to send two ultrasonic waves of differing frequencies toward a point, giving the illusion that the audible sound from their interference pattern was originating at that point. ATC named and trademarked their device as ""HyperSonic Sound"" (HSS). In December 1997, HSS was one of the items in the Best of What's New issue of Popular Science. In December 2002, Popular Science named HyperSonic Sound the best invention of 2002. Norris received the 2005 Lemelson-MIT Prize for his invention of a ""hypersonic sound"". ATC (now named LRAD Corporation) spun off the technology to Parametric Sound Corporation in September 2010 to focus on their Long Range Acoustic Device products (LRAD), according to their quarterly reports, press releases and executive statements.

Mitsubishi Electric Engineering Corporation
Mitsubishi apparently offers a sound from ultrasound product named the ""MSP-50E"" but commercial availability has not been confirmed.

AudioBeam
German audio company Sennheiser Electronic once listed their ""AudioBeam"" product for about $4,500. There is no indication that the product has been used in any public applications. The product has since been discontinued.

Soundlazer
Started as a Kickstarter project in 2012, Richard Haberkern developed the Soundlazer SL-01 for use by the general public. The SL-01 is currently available for sale on the company's website as the consumer version or as a developer's kit. A new model, named the Soundlazer Snap, is expected to deliver in early 2015, following a crowdfunding campaign on Kickstarter. The Snap was designed primarily for hobbyists and requires some assembly.

Literature survey
The first experimental systems were built over 30 years ago, although these first versions only played simple tones. It was not until much later (see above) that the systems were built for practical listening use.

Experimental ultrasonic nonlinear acoustics
A chronological summary of the experimental approaches taken to examine Audio Spotlight systems in the past will be presented here. At the turn of the millennium working versions of an Audio Spotlight capable of reproducing speech and music could be bought from Holosonics, a company founded on Dr. Pompei's work in the MIT Media Lab.
Related topics were researched almost 40 years earlier in the context of underwater acoustics.
The first article consisted of a theoretical formulation of the half pressure angle of the demodulated signal.
The second article provided an experimental comparison to the theoretical predictions.
Both articles were supported by the U.S. Office of Naval Research, specifically for the use of the phenomenon for underwater sonar pulses. The goal of these systems was not high directivity per se, but rather higher usable bandwidth of a typically band-limited transducer.
The 1970s saw some activity in experimental airborne systems, both in air and underwater. Again supported by the U.S. Office of Naval Research, the primary aim of the underwater experiments was to determine the range limitations of sonar pulse propagation due to nonlinear distortion. The airborne experiments were aimed at recording quantitative data about the directivity and propagation loss of both the ultrasonic carrier and demodulated waves, rather than developing the capability to reproduce an audio signal.
In 1983 the idea was again revisited experimentally but this time with the firm intent to analyze the use of the system in air to form a more complex base band signal in a highly directional manner. The signal processing used to achieve this was simple DSB-AM with no precompensation, and because of the lack of precompensation applied to the input signal, the THD Total harmonic distortion levels of this system would have probably been satisfactory for speech reproduction, but prohibitive for the reproduction of music. An interesting feature of the experimental set up used in was the use of 547 ultrasonic transducers to produce a 40 kHz ultrasonic sound source of over 130db at 4 m, which would demand significant safety considerations. Even though this experiment clearly demonstrated the potential to reproduce audio signals using an ultrasonic system, it also showed that the system suffered from heavy distortion, especially when no precompensation was used.

Theoretical ultrasonic nonlinear acoustics
The equations that govern nonlinear acoustics are quite complicated and unfortunately they do not have general analytical solutions. They usually require the use of a computer simulation. However, as early as 1965, Berktay performed an analysis under some simplifying assumptions that allowed the demodulated SPL to be written in terms of the amplitude modulated ultrasonic carrier wave pressure Pc and various physical parameters. Note that the demodulation process is extremely lossy, with a minimum loss in the order of 60 dB from the ultrasonic SPL to the audible wave SPL. A precompensation scheme can be based from Berktay's expression, shown in Equation 1, by taking the square root of the base band signal envelope E and then integrating twice to invert the effect of the double partial time derivative. The analogue electronic circuit equivalents of a square root function is simply an op-amp with feedback, and an equalizer is analogous to an integration function. However these topic areas lie outside the scope of this project.

  
    
      
        
          p
          
            2
          
        
        (
        x
        ,
        t
        )
        =
        K
        ?
        
          P
          
            c
          
          
            2
          
        
        ?
        
          
            
              ?
              
                2
              
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        
          E
          
            2
          
        
        (
        x
        ,
        t
        )
      
    
    {\displaystyle p_{2}(x,t)=K\cdot P_{c}^{2}\cdot {\frac {\partial ^{2}}{\partial t^{2}}}E^{2}(x,t)}
  
Where

  
    
      
        
          p
          
            2
          
        
        (
        x
        ,
        t
        )
        =
        
      
    
    {\displaystyle p_{2}(x,t)=\,}
   Audible secondary pressure wave

  
    
      
        K
        =
        
      
    
    {\displaystyle K=\,}
   misc. physical parameters

  
    
      
        
          P
          
            c
          
        
        =
        
      
    
    {\displaystyle P_{c}=\,}
   SPL of the ultrasonic carrier wave

  
    
      
        E
        (
        x
        ,
        t
        )
        =
        
      
    
    {\displaystyle E(x,t)=\,}
   Envelope function (such as DSB-AM)
This equation says that the audible demodulated ultrasonic pressure wave (output signal) is proportional to the twice differentiated, squared version of the envelope function (input signal). Precompensation refers to the trick of anticipating these transforms and applying the inverse transforms on the input, hoping that the output is then closer to the untransformed input.
By the 1990s, it was well known that the Audio Spotlight could work but suffered from heavy distortion. It was also known that the precompensation schemes placed an added demand on the frequency response of the ultrasonic transducers. In effect the transducers needed to keep up with what the digital precompensation demanded of them, namely a broader frequency response. In 1998 the negative effects on THD of an insufficiently broad frequency response of the ultrasonic transducers was quantified with computer simulations by using a precompensation scheme based on Berktay's expression. In 1999 Pompei's article discussed how a new prototype transducer met the increased frequency response demands placed on the ultrasonic transducers by the precompensation scheme, which was once again based on Berktay's expression. In addition impressive reductions in the THD of the output when the precompensation scheme was employed were graphed against the case of using no precompensation.
In summary, the technology that originated with underwater sonar 40 years ago has been made practical for reproduction of audible sound in air by Pompei's paper and device, which, according to his AES paper (1998), demonstrated that distortion had been reduced to levels comparable to traditional loudspeaker systems.

Modulation scheme
The nonlinear interaction mixes ultrasonic tones in air to produce sum and difference frequencies. A DSB-AM modulation scheme with an appropriately large baseband DC offset, to produce the demodulating tone superimposed on the modulated audio spectra, is one way to generate the signal that encodes the desired baseband audio spectra. This technique suffers from extremely heavy distortion as not only the demodulating tone interferes, but also all other frequencies present interfere with one another. The modulated spectra is convolved with itself, doubling its bandwidth by the length property of the convolution. The baseband distortion in the bandwidth of the original audio spectra is inversely proportional to the magnitude of the DC offset (demodulation tone) superimposed on the signal. A larger tone results in less distortion.
Further distortion is introduced by the second order differentiation property of the demodulation process. The result is a multiplication of the desired signal by the function -?² in frequency. This distortion may be equalized out with the use of preemphasis filtering (increase amplitude of high frequency signal).
By the time convolution property of the fourier transform, multiplication in the time domain is a convolution in the frequency domain. Convolution between a baseband signal and a unity gain pure carrier frequency shifts the baseband spectra in frequency and halves its magnitude, though no energy is lost. One half-scale copy of the replica resides on each half of the frequency axis. This is consistent with Parseval's theorem.
The modulation depth m is a convenient experimental parameter when assessing the total harmonic distortion in the demodulated signal. It is inversely proportional to the magnitude of the DC offset. THD increases proportionally with m1².
These distorting effects may be better mitigated by using another modulation scheme that takes advantage of the differential squaring device nature of the nonlinear acoustic effect. Modulation of the second integral of the square root of the desired baseband audio signal, without adding a DC offset, results in convolution in frequency of the modulated square-root spectra, half the bandwidth of the original signal, with itself due to the nonlinear channel effects. This convolution in frequency is a multiplication in time of the signal by itself, or a squaring. This again doubles the bandwidth of the spectra, reproducing the second time integral of the input audio spectra. The double integration corrects for the -?² filtering characteristic associated with the nonlinear acoustic effect. This recovers the scaled original spectra at baseband.
The harmonic distortion process has to do with the high frequency replicas associated with each squaring demodulation, for either modulation scheme. These iteratively demodulate and self-modulate, adding a spectrally smeared out and time exponentiated copy of the original signal to baseband and twice the original center frequency each time, with one iteration corresponding to one traversal of the space between the emitter and target. Only sound with parallel collinear phase velocity vectors interfere to produce this nonlinear effect. Even-numbered iterations will produce their modulation products, baseband and high frequency, as reflected emissions from the target. Odd-numbered iterations will produce their modulation products as reflected emissions off the emitter.
This effect still holds when the emitter and the reflector are not parallel, though due to diffraction effects the baseband products of each iteration will originate from a different location each time, with the originating location corresponding to the path of the reflected high frequency self-modulation products.
These harmonic copies are largely attenuated by the natural losses at those higher frequencies when propagating through air.

Attenuation of ultrasound in air
The Figure provided in provided an estimation of the attenuation that the ultrasound would suffer as it propagated through air. The figures from this graph correspond to completely linear propagation, and the exact effect of the nonlinear demodulation phenomena on the attenuation of the ultrasonic carrier waves in air was not considered. There is an interesting dependence on humidity. Nevertheless, a 50 kHz wave can be seen to suffer an attenuation level in the order of 1 dB per meter at one atmosphere of pressure.

Safe use of high-intensity ultrasound
For the nonlinear effect to occur, relatively high intensity ultrasonics are required. The SPL involved was typically greater than 100 dB of ultrasound at a nominal distance of 1 m from the face of the ultrasonic transducer. Exposure to more intense ultrasound over 140 dB near the audible range (20–40 kHz) can lead to a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness and fatigue, but this is around 100 times the 100 dB level cited above, and is generally not a concern. Dr Joseph Pompei of Audio Spotlight has published data showing that their product generates ultrasonic sound pressure levels around 130 dB (at 60 kHz) measured at 3 meters.
The UK's independent Advisory Group on Non-ionising Radiation (AGNIR) produced a 180-page report on the health effects of human exposure to ultrasound and infrasound in 2010. The UK Health Protection Agency (HPA) published their report, which recommended an exposure limit for the general public to airborne ultrasound sound pressure levels (SPL) of 100 dB (at 25 kHz and above).
OSHA specifies a safe ceiling value of ultrasound as 145 dB SPL exposure at the frequency range used by commercial systems in air, as long as there is no possibility of contact with the transducer surface or coupling medium (i.e. submerged). This is several times the highest levels used by commercial Audio Spotlight systems, so there is a significant margin for safety. In a review of international acceptable exposure limits Howard et al. (2005) noted the general agreement amongst standards organizations, but expressed concern with the decision by United States of America’s Occupational Safety and Health Administration (OSHA) to increase the exposure limit by an additional 30 dB under some conditions (equivalent to a factor of 1000 in intensity).
For frequencies of ultrasound from 25 to 50 kHz, a guideline of 110 dB has been recommended by Canada, Japan, the USSR, and the International Radiation Protection Agency, and 115 dB by Sweden in the late 1970s to early 1980s, but these were primarily based on subjective effects. The more recent OSHA guidelines above are based on ACGIH (American Conference of Governmental Industrial Hygienists) research from 1987.
Lawton(2001) reviewed international guidelines for airborne ultrasound in a report published by the United Kingdom’s Health and Safety Executive, this included a discussion of the guidelines issued by the American Conference of Governmental Industrial Hygienists (ACGIH), 1988. Lawton states “This reviewer believes that the ACGIH has pushed its acceptable exposure limits to the very edge of potentially injurious exposure"". The ACGIH document also mentioned the possible need for hearing protection.

See also
Directional sound
Infrasound
Parametric array

Further resources
USS Patent 6778672 filed on 17 August 2004 describes an HSS system for using ultrasound to:-
Direct distinct 'in-car entertainment' directly to passengers in different positions.
Shape the airwaves in the vehicle to deaden unwanted noises.

References


== External links ==",Category:Webarchive template wayback links,1
78,79,Target strength,"The target strength or acoustic size is a measure of the reflection coefficient of a sonar target. This is usually quantified as a number of negative decibels. For fish such as salmon, the target size varies with the length of the fish and a 5 cm fish could have a target strength of about -50 dB.
Target strength (TS) is equal to 10 log10(?bs), where ?bs is the differential backscattering cross section. Backscattering cross section is 4??bs.

References
Further reading
""Introduction to the use of sonar systems for estimating fish biomass, FAO Fisheries Technical Paper No. 191, Revision 1, FAO 1982""
Fisheries Acoustics Simmonds, E John and MacLennan, David N (2005) Blackwell Publishing. ISBN 978-0-632-05994-2
C. S. Clay & H. Medwin, Acoustical Oceanography (Wiley, New York, 1977)

target strength formula:
TS=10log((R^2)/4) FOR Circular
TS=10log((RL^2)/(2*landa)*(SIN(B)/B)^2*COS(U)^2) B=KL*SIN(U) K=2pi/landa",Category:Acoustics,1
79,80,Diaphragm (acoustics),"In the field of acoustics, a diaphragm is a transducer intended to inter-convert mechanical vibrations to sounds, or vice versa. It is commonly constructed of a thin membrane or sheet of various materials, suspended at its edges. The varying air pressure of sound waves imparts mechanical vibrations to the diaphragm which can then be converted to some other type of signal; examples of this type of diaphragm are found in microphones and the human eardrum. Conversely a diaphragm vibrated by a source of energy beats against the air, creating sound waves. Examples of this type of diaphragm are loudspeaker cones and earphone diaphragms and are found in air horns.

Loudspeaker
In a dynamic loudspeaker, a diaphragm is the thin, semi-rigid membrane attached to the voice coil, which moves in a magnetic gap, vibrating the diaphragm, and producing sound. It can also be called a cone, though not all speaker diaphragms are cone-shaped. Diaphragms are also found in headphones.
Quality midrange and bass drivers are usually made from paper, paper composites and laminates, plastic materials such as polypropylene, or mineral/fiber filled polypropylene. Such materials have very high strength/weight ratios (paper being even higher than metals) and tend to be relatively immune from flexing during large excursions. This allows the driver to react quickly during transitions in music (i.e. fast changing transient impulses) and minimizes acoustical output distortion.
If properly designed in terms of mass, stiffness, and damping, paper woofer/midrange cones can outperform many exotic drivers made from more expensive materials. Other materials used for diaphragms include polypropylene (PP), polyetheretherketone (PEEK) polycarbonate (PC), Mylar (PET), silk, glassfibre, carbon fibre, titanium, aluminium, aluminium-magnesium alloy, nickel, and beryllium. A 12-inch-diameter (300 mm) paper woofer with a peak-to-peak excursion of 0.5 inches at 60 Hz undergoes a maximum acceleration of 92 ""g""s.
Paper-based cones account for approximately 85% of the cones sold worldwide. The ability of paper (cellulose) to be easily modified by chemical or mechanical means gives it a practical processing advantage not found in other common cone materials.
The purpose of the cone/surround assembly is to accurately reproduce the voice coil signal waveform. Inaccurate reproduction of the voice coil signal results in acoustical distortion. The ideal for a cone/surround assembly is an extended range of linearity or ""pistonic"" motion characterized by i) minimal acoustical breakup of the cone material, ii) minimal standing wave patterns in the cone, and iii) linearity of the surrounds force-deflection curve. The cone stiffness/damping plus the surround's linearity/damping play a crucial role in accuracy of the reproduced voice coil signal waveform. This is the crux of high-fidelity stereo.
The surround may be resin-treated cloth, resin-treated non-wovens, polymeric foams, or thermoplastic elastomers over-molded onto the cone body. An ideal surround has a linear force-deflection curve with sufficient damping to fully absorb vibrational transmissions from the cone/surround interface, and the ""toughness"" to withstand long-term vibration-induced fatigue. Sometimes the conical part and the outer surround are molded in one step and are one piece as commonly used for a Guitar speaker.
Other types of speakers (such as electrostatic loudspeakers) may use a thin membrane instead of a cone.

Microphone
Microphones can be thought of as speakers in reverse. The sound waves strike the thin diaphragm, causing it to vibrate. Microphone diaphragms, unlike speaker diaphragms, tend to be thin and flexible, since they need to absorb as much sound as possible. In a condenser microphone, the diaphragm is placed in front of a plate and is charged.  In a dynamic microphone, the diaphragm is glued to a magnetic coil, similar to the one in a dynamic loudspeaker. (In fact, a dynamic speaker can be used as a rudimentary microphone, and vice versa.) 
The diaphragm in a microphone works similarly to the human eardrum.

Other uses
In a phonograph reproducer, the diaphragm is a flat disk of typically mica isinglass that converts the mechanical vibration imparted on the buttress from the recorded groove into sound. In the case of acoustic recording the reproducer converts the sound into the motion of the needle that scribes the groove on the recording media.

See also
Acoustic membrane


== References ==",Category:Loudspeakers,1
80,81,Isolation booth,"An isolation booth is a device used to prevent a person or people from seeing or hearing certain events. On game shows and Beauty pageants, the isolation booth might be used to prevent contestants from hearing the other player's answers (such as on Twenty One, Family Feud, Win Ben Stein's Money, 50 Grand Slam, The $64,000 Challenge, and the CBS version of Double Dare), from hearing the audience (The $64,000 Question, The $1,000,000 Chance of a Lifetime), from seeing moves or plays made by a player (Whew!, Solitary) or sometimes for comedic effect only (Idiot Savants). On The Money List, the players in a booth are only isolated when the booth is red.
Isolation booths are also frequently used in audio recordings, with non-reflective walls, lined with acoustic foam that eliminate potential reverberations.

See also
Recording studio",Category:All articles lacking sources,1
81,82,Ambience (sound recording),"In filmmaking, ambience (also known as atmosphere, atmos, or background) consists of the sounds of a given location or space. It is the opposite of ""silence."" This term is often confused with presence.
Every location has distinct and subtle sounds created by its environment. These sound sources can include wildlife, wind, music, rain, running water, thunder, rustling leaves, distant traffic, aircraft and machinery noise, the sound of distant human movement and speech, creaks from thermal contraction, air conditioning and plumbing noises, fan and motor noises, and harmonics of mains power.
Reverberation will further distort these already faint sounds, often beyond recognition, by introducing complex patterns of peaks and nulls in their frequency spectrum, and blurring their temporal characteristics. Finally, sound absorption can cause high frequencies to be rolled off, dulling the sound further.
Ambience is normally recorded in stereo by the sound department during the production stage of filmmaking. It is used to provide a movie location with sonic space and normally occupies a separate track in the sound edit.

See also
Presence (sound recording)
Environmental noise
Filmmaking
Ambient noise level
Acoustic signature


== References ==",Category:Acoustics,1
82,83,Minimum audibility curve,"Minimum audibility curve is a standardized graph of the threshold of hearing frequency for an average human, and is used as the reference level when measuring hearing loss with an audiometer as shown on an audiogram.
Audiograms are produced using a piece of test equipment called an audiometer, and this allows different frequencies to be presented to the subject, usually over calibrated headphones, at any specified level. The levels are, however, not absolute, but weighted with frequency relative to a standard graph known as the minimum audibility curve which is intended to represent 'normal' hearing. This is not the best threshold found for all subjects, under ideal test conditions, which is represented by around 0 phon or the threshold of hearing on the equal-loudness contours, but is standardised in an ANSI standard to a level somewhat higher at 1 kHz [1]. There are several definitions of the minimal audibility curve, defined in different international standards, and they differ significantly, giving rise to differences in audiograms according to the audiometer used. The ASA-1951 standard for example used a level of 16.5 dB SPL at 1 kHz whereas the later ANSI-1969/ISO-1963 standard uses 6.5 dB SPL, and it is common to allow a 10 dB correction for the older standard.

See also
Audiogram
Hearing range
Equal-loudness contour
Articulation index
Pure tone audiometry
Psychoacoustics
Hearing (sense)
Audiology
Audiometry
A-weighting

External links
Hearing Loss by Robert Thayer Sataloff",Category:Acoustics,1
83,84,Smaart,"Smaart is a software suite of audio and acoustic measurement tools, a software application for the analysis portion of acoustical measurements and instrumentation. Introduced in 1996 by JBL's pro audio division, it was designed to help the live sound engineer optimize the linearity of sound reinforcement systems during the public performance unlike most earlier analysis systems which required specific test signals sent to the sound system, ones which would be unpleasant for the audience to hear. It was also intended to assist audio engineers in analyzing the output of loudspeakers, audio amplifiers and other audio gear, as well as helping the acoustician analyze room acoustics. The software product has been known as JBL-Smaart, SIA-Smaart Pro, EAW Smaart, SmaartLive, and simply Smaart, while the acoustician version has been offered as Smaart Acoustic Tools. Smaart is an acronym which stands for Sound Measurement Acoustical Analysis Real Time tool.
Smaart has three modes: transfer function, real-time analyzer (RTA) and impulse response. The first two modes use dual- and single-fast Fourier transform (FFT), and the impulse response mode uses either FFT or maximum length sequence (MLS) analysis.
Smaart has been licensed and owned by several companies since JBL, and is now owned and developed by Rational Acoustics. First written as a native Windows 3.1 application to work within Windows 95 on IBM-compatible computers, in 2006 a version was introduced that was compatible on both Windows and Apple Macintosh operating systems. In March 2016 Smaart was in its 8th version.

Use
Smaart is based on real-time fast Fourier transform (FFT) analysis, including dual-FFT audio signal comparison, called ""transfer function"", and single-FFT spectrum analyzer. It includes maximum length sequence (MLS) analysis as a choice for impulse response, for the measurement of room acoustics. The FFT implementation of Smaart is a multi-time window (MTW) solution in which the FFT, rather than being a fixed length, is made increasingly shorter as the frequency increases. This feature allows the software to 'ignore' later signal reflections from walls and other surfaces, increasing in coherence as the audio frequency increases.
The latest version of Smaart 8 runs under Windows 7 or newer, and Mac OSX 10.7 or newer, including 32- and 64-bit versions. A computer having a dual-core processor with a clock rate of at least 2 GHz is recommended. Smaart can be set to sample rates of 44.1 kHz, 48 kHz or 96 kHz, and to bit depths of 16 or 24. The software works with computer audio protocols ASIO, Core Audio, WAV or WDM audio drivers.

Transfer function
Smaart's transfer function requires a stereo input to the computer because it analyzes two channels of audio signal. Using its dual-FFT mode, Smaart compares one channel with the other to show the difference. This is used by live sound engineers to set up concert sound systems before a show and to monitor and adjust these systems during the performance. The first channel of audio undergoing analysis is connected directly from one of the main outputs of the mixing console and the second channel is connected to a microphone placed in the audience listening area, usually an omnidirectional test microphone with a flat, neutral pickup characteristic. The direct mixing console audio output is compared with the microphone input to determine how the sound is changed by the sound system elements such as loudspeakers and amplifiers, and by the room acoustics indoors or by the weather conditions and acoustic environment outdoors. Smaart displays the difference between the intended sound from the mixer and the received sound at the microphone, and this real-time display informs the audio engineer's decisions regarding delay times, equalization and other sound system adjustment parameters.
Before the audience arrives, random or pseudo-random noise is used as a stimulus signal, usually pink noise. Such noise signals provide equal energy at each octave of the audio frequency range that the sound system can reproduce, unlike a stimulus signal of music or speech which emphasizes some frequencies over others. Once the audience enters the performance venue, the main output of the mixing console is substituted as the stimulus signal.
Transfer mode can also be used to examine the frequency response of audio equipment, including individual amplifiers, loudspeakers and digital signal processors such as audio crossovers and equalizers. It can be used to compare a known neutral-response test microphone with another microphone in order to better understand its frequency response and, by changing the angle of the microphone under test, its polar response.
Transfer mode can be used to adjust audio crossover settings for multi-way loudspeakers; similarly, it can be used to adjust only the subwoofer-to-top box crossover characteristics in a sound system where the main, non-subwoofer loudspeakers are flown or rigged but the subwoofers are placed on the ground. One of the traces in the Smaart display shows phase response. To properly align adjacent frequency bands through a crossover, the two phase responses should be adjusted until they are seen in Smaart to be parallel through the crossover frequency.
The transfer mode can be used to measure frequency-related electrical impedance, one of the electrical characteristics of dynamic loudspeakers. Grateful Dead sound system engineer ""Dr. Don"" Pearson worked out the method in 2000, using Smaart to compare the voltage drop through a simple resistor between a loudspeaker and a random noise generator.

Real time analyzer
In Spectrograph mode, Smaart displays a real-time spectrum analysis, showing the relative strength of audio frequencies for one audio signal. Needing only one channel of audio input, this capability can be used for a variety of purposes. With Smaart's input connected to the mixing console's pre-fade listen (PFL) or cue bus, Spectrograph mode can display the frequency response of individual channels, several selected channels, or various mixes. Spectrograph mode can be used to display room resonances: pink noise is applied to the room's sound system, and the signal from a test microphone in the room is displayed on Smaart. When the pink noise is muted, the display shows the lingering tails of noise frequencies that are resonating.

Impulse response
Smaart can be used to find the delay time between two signals, in which case the computer needs two input channels and the software uses dual-FFT mode. Called ""Delay Locator"", the software calculates the impulse responses of two continuous audio signals, finding the similarities in the signals and measuring how much time has elapsed between them. This is used to set delay times for delay towers at large outdoor sound systems, and it is used to set delay times for other loudspeaker zones in smaller systems. Veteran Van Halen touring sound engineer Jim Yakabuski calls such delay locator programs as Smaart a ""must have"" item, useful for quickly aligning sound system elements when setup time is limited.

Market
Smaart is primarily aimed at sound system operators to assist them in setting up and tuning sound systems. Other users include audio equipment designers and architectural acousticians. Author and sound engineer Bob McCarthy wrote in 2007 that because of Smaart's widespread acceptance at all levels of live sound mixing, the paradigm has reversed from the 1980s one of surprise at finding scientific tools in the concert sound scene to one of surprise if the observer finds that such tools are not being used to tune a sound system.
Smaart has been compared to other software-based sound system measurement tools such as SIM by Meyer Sound Laboratories and IASYS by Audio Control, both of which offer delay finder tools. Smaart has been described as ""a newer, slimmer and much cheaper—but not necessarily better—version of the Meyer SIM system."" MLSSA, developed by DRA Laboratories in 1987, and TEF, a time delay spectrometry product by Gold Line, are other products predating Smaart that are used to tune loudspeakers such as studio monitors. A software tool that reached Mac users in 1997 was named SpectraFoo, by Metric Halo. At the same time, some early Smaart users found that after tweaking their MIDI drivers they could get Smaart to work on an Apple computer, the software running inside an x86 emulator such as SoftWindows ""with varying results"".

History
As early as 1978, field analysis of rock concert audio was undertaken by Don Pearson, known by his nickname ""Dr. Don"", while working on sound systems used by the Grateful Dead. Pearson published articles about impulse response measurements taken during setup and testing of concert sound systems, and recommended the Dead buy an expensive Brüel & Kjær 2032 Dual Channel FFT analyzer, made for industrial engineering. Along with Dead soundman Dan Healy, Pearson developed methods of working with this system to set up sound systems on tour, and he assisted Meyer engineers working on a more suitable source-independent measurement system which was to become their SIM product. As well, Pearson had an ""intimate involvement"" with the engineers who were creating Smaart, including a meeting with Jamie Anderson.
Smaart was developed by Sam Berkow in association with Alexander ""Thorny"" Yuill-Thornton II, touring sound engineer with Luciano Pavarotti and The Three Tenors. In 1995, Berkow and Thorny founded SIA Software Company, produced Smaart and licensed the product to JBL. First exhibited in New York City at the Audio Engineering Society's 99th convention in October 1995 and described the next month in Billboard magazine, in May 1996 the software product was introduced at the price of $695, the equivalent of $1,084 in today's currency. Studio Sound magazine described Smaart in 1996 as ""the most talked about new product"" at the 100th AES convention in Copenhagen, exemplifying a new trend in software audio measurement. Calvert Dayton joined SIA Software in 1996 as graphic designer, technical writer and website programmer.
Smaart was unusual because it helped audio professionals such as theatrical sound designers do what was previously possible only with highly sophisticated and expensive measurement devices. Audio system engineers from Clair Brothers used Smaart to tune the sound system at each stop during U2's PopMart Tour 1997–1998. As it increased in popularity, engineers who used Smaart found mixed results: touring veteran Doug Fowler wrote that ""misuse was rampant"" when the software first started appearing in the field. He warned users against faulty interpretation, saying ""I still see bad decisions based on bad data, or bad decisions based on a fundamental lack of understanding of the issues at hand."" Nevertheless, Clive Young, editor of Pro Sound News, wrote in 2005 that the introduction of Smaart in 1995 was the start of ""the modern era of sound reinforcement system analysis software"".
In 1998, JBL Smaart Pro won the TEC Awards category for computer software and peripherals. Eastern Acoustic Works (EAW) bought SIA Software, and brought in Jamie Anderson to manage the division. Version 3 was introduced under EAW's ownership, with the additional capability of accepting optional plug-ins which could be used to apply sound system adjustments, as measured by Smaart, to digital signal processing (DSP) equipment. The external third party DSP would perform the corrections indicated by Smaart.

Versions 4 and 5 were built upon the foundation of version 3, but with each major release, the application was getting more and more difficult to write, and further improvements appeared practically impossible to implement. For version 6, the designers decided to tear Smaart back down to its basics and rebuild it on a flexible multi-tasking, multi-platform framework which would allow it to be used on Mac OS X and Windows machines. Writing it took two years, and it was released in a package which included the earlier version 5 because there was not enough time to incorporate all elements of the existing feature set. Anderson said in 2007, ""we released Version 6 without all of the features of 5, but we are adding those features back in."" Smaart 6 was nominated for a TEC Award in 2007 but did not win.
EAW developed a digital mixing console prototype in 2005, the UMX.96; a console which incorporated SmaartLive 5 internally. Any selected channel on the mixer could be used as a source for Smaart analysis, displaying, for instance, the real-time results of channel equalization. The console could be configured to send multiple microphone inputs to Smaart, and it offered constant metering of sound pressure level in decibels. When it was put into production in 2007, band engineer Don Dodge took the mixer out on a world tour with Foreigner, the first concert mixed in March 2007. With its 15-inch touchscreen able to serve both audio control and Smaart analysis functions, Dodge continued to mix Foreigner on it throughout 2007 and 2008.
Rational Acoustics was incorporated on April 1, 2008. On November 9, 2009, under the leadership of Jamie and Karen Anderson, programmer Adam Black and technical chief Calvert Dayton, Rational Acoustics became the full owner of the Smaart brand. Rational released Smaart 7 on April 14, 2010; a version which uses less processing power than v5 and v6 because of efficiencies brought about in the redesigned code. Smaart 7 was written using a new object-oriented code architecture, it was given improved data acquisition. Other new features include graphic user interface changes and delay tracking. Users can run simultaneously displayed real-time measurements in multiple windows, as many as their computer hardware will allow. Smaart 7 was nominated in 2010 for a TEC Award but did not win. In April 2011, Smaart 7 was named one of four Live Design Sound Products of the Year 2010–2011.

Version history
May 1996 – JBL-Smaart 1.0
March 1997 – JBL-Smaart 1.4

1998 – SIA-Smaart Pro 2
April 1999 – SIA-Smaart Pro 3
2000 – SIA SmaartLive 4
October 2000 – SIA SmaartLive 4.1
April 2001 – SIA SmaartLive 4.5
September 2001 – SIA SmaartLive 4.6

June 2002 – SIA SmaartLive 5
October 2003 – SIA SmaartLive 5.3

2006 – EAW Smaart 6
April 2010 – Smaart 7
October 2010 – Smaart 7.1
April 2011 – Smaart 7.2
July 2011 – Smaart 7.3
August 2012 - Smaart 7.4
April 2014 - Smaart 7.5

March 2016 - Smaart 8.0
November 2016 – Smaart 8.1
December 2017 – Smaart 8.2

References
External links
Rational Acoustics Home Page
Smaart Basics: Example System Overview, video with Jamie Anderson
Sam Berkow NAMM Oral History Interview (2011)",Category:Pages with citations lacking titles,1
84,85,Dummy head recording,"In acoustics, the dummy head recording (also known as artificial head, Kunstkopf or Head and Torso Simulator) is a method of recording used to generate binaural recordings. The tracks are then listened to through headphones allowing for the listener to hear from the dummy’s perspective. The dummy head is designed to record multiple sounds at the same time enabling it to be exceptional at recording music as well as in other industries where multiple sound sources are involved.
The dummy head is designed to replicate average sized human head and depending on the manufacturer may have a nose and mouth too. Each dummy head is equipped with pinnae and ear canals in which small microphones are placed, one in each ear. The leading manufacturers in Dummy Head design are: Neumann, Brüel & Kjær, Head Acoustics GmBH, and Knowles Electronics.

Technical
The human perception of direction is complex:
The sound information arriving at the left and right ears causes inter-aural time differences and interaural level differences. These small variations allow the brain and auditory system to calculate the direction and distance of the sound sources from the listener. [See Interaural time difference & Sound localization]
With percussive sounds, the impact can be noticed on the skin (usually torso). The strongest and earliest sensory stimulus comes from the skin regions which are aligned perpendicular to the sound source direction.
The human head imprints frequency-dependent distortions of phase and amplitude on sounds as they reach the eardrums. The frequency-dependent level differences as well as these distortions vary with the direction of the sound source. This is caused by the geometry and sound-transmitting characteristics of the sinus, throat cavities, eustachian tubes, inner ear, external ears and other tissues of the head and upper body. [See: Head Related Transfer Function, “HRTF”]
Conventional music recording is produced for stereo playback which makes use of only Left and Right playback for speakers and headphones. The implementation of Dummy Head allows the recording artist to make use of three dimensional sound reproduction. This is because through playback via headphones the listener perceives sound as if they were in the position of the dummy. The recording is perceived through the pinnae of the dummy head.

Methods
There are two main methods used to create a binaural effect:
Dummy head recording The dummy head or Head and Torso Simulator (HATS) are based upon the average dimensions of a human head and torso. They consist of acoustic materials fitted with ear and mouth simulators as well as two microphones inserted within each ear canal, typically at the ear drum.
Simulated dummy head recording Takes place through the digital signal processing (DSP) where the signal is sent through a complex mathematical algorithm imprinting limited HRTF information creating the binaural effect. This process is called HRTF-based binaural algorithm.

Limitations
The main focus of recording with a dummy head is to achieve a perfect binaural playback that is suited to all listeners. The problem arises that each human head has different shaped and sized features. Due to the diversity in HRTFs it is impossible to create a binaural effect compatible for everyone’s ears. Therefore the simulated dummy head recording algorithm uses average HRTFs to create a moderate binaural effect for everyone.

History
The dummy head recording is associated with the use of the physical synthetic head called the “Kunstkopf”. The Kunstkopf would be placed in concert halls during the recording of a live orchestra or in the film industry actors could stand around the head whilst recording their dialogue. The dummy head could also be used to imprint positional information on prerecorded sound effects by playing sounds through a loudspeaker in a suitable orientation to the head. For example thunder and birdsong sounds to be played above the dummy head.
During the 1990s, electronic devices which used Digital signal processing (DSP) to reproduce HRTFs were made commercially available. These devices would allow the sound engineer to use dialled parameters to adjust the apparent direction of real time sounds. They were unusual and expensive, but would allow the sound engineer to alter special effects of prerecorded sounds quickly and conveniently. Through the manipulation of the parameters, sound engineers could take a monophonic recording of a passing car and make it sound as if it were passing behind them in real time. Recording with an actual dummy head for the same outcome would require a recording booth and a moving speaker, or an array of speakers as well as multiple panning or switching devices.

Applications
The dummy head manufacturers design their products differently to one another catering for specific situations. The Neumann dummy’s are designed as the head with pinnae and a nose, whereas the Brüel & Kjær design includes soft moulded pinnae, nose, mouth and torso. Both the dummy head and the HATS can be used to record audio of the same nature but are specifically designed to accomplish different tasks. A new manufacturer to the market for recording binaural is 3Dio with the purpose of recording on a smaller scale. The 3Dio microphones are situated in the ears at an average head distance apart, however the model does not include the full head or torso.
The main applications for the above manufacturers include:
Neumann KU-100 product line: Radio drama recording, Musical instruments, Room acoustics, Industrial noise influence
Brüel & Kjær 4128D/C products: Telephone handset testing, Audio conference testing, Hearing aids, Hearing protectors
3Dio Free Space (Pro) microphones: Instruments with high frequency dependence, Pro studio recording, Binaural capture (ASMR), Nature recording
Within the film industry Demolition was the first radio drama recorded using a dummy head.
In 1974 Virgin Records issued the first solo album by Tangerine Dream's leader Edgar Froese, titled Aqua. The brief sleeve notes inform listeners that side 2 of the disc - i.e. the tracks NGC 891 and Upland - were recorded using the artificial head system developed by Gunther Brunschen. Listeners were advised to optimise their listening by using stereo headphones for that side of the album.
Although Edgar was keen to continue to use and promote this system for subsequent recordings, it was abandoned, due to the fact that, although it worked well through headphones, the improved sound quality did not translate adequately through a hi-fi speaker system.
In 2005, Aqua was remixed for limited edition reissue in Germany and Japan, with an additional track Upland Dawn appended to the end of the CD.
In 2015, Singaporean singer-songwriter JJ Lin released his debut experimental album From M.E. to Myself, using dummy head recording. This is also the first album in pop music industry using this technology.

See also
Head-related transfer function
Binaural
Autonomous Sensory Meridian Response (ASMR)
Interaural time difference
Sound localization


== References ==",Category:CS1 German-language sources (de),1
85,86,Sound localization,"Sound localization is a listener's ability to identify the location or origin of a detected sound in direction and distance. It may also refer to the methods in acoustical engineering to simulate the placement of an auditory cue in a virtual 3D space (see binaural recording, wave field synthesis).
The sound localization mechanisms of the mammalian auditory system have been extensively studied. The auditory system uses several cues for sound source localization, including time- and level-differences (or intensity-difference) between both ears, spectral information, timing analysis, correlation analysis, and pattern matching.
These cues are also used by other animals, but there may be differences in usage, and there are also localization cues which are absent in the human auditory system, such as the effects of ear movements. Animals with the ability to localize sound have a clear evolutionary advantage.

How sound reaches the brain
Sound is the perceptual result of mechanical vibrations traveling through a medium such as air or water. Through the mechanisms of compression and rarefaction, sound waves travel through the air, bounce off the pinna and concha of the exterior ear, and enter the ear canal. The sound waves vibrate the tympanic membrane (ear drum), causing the three bones of the middle ear to vibrate, which then sends the energy through the oval window and into the cochlea where it is changed into a chemical signal by hair cells in the organ of corti, which synapse onto spiral ganglion fibers that travel through the cochlear nerve into the brain.

Neural interactions
In vertebrates, inter-aural time differences are known to be calculated in the superior olivary nucleus of the brainstem. According to Jeffress, this calculation relies on delay lines: neurons in the superior olive which accept innervation from each ear with different connecting axon lengths. Some cells are more directly connected to one ear than the other, thus they are specific for a particular inter-aural time difference. This theory is equivalent to the mathematical procedure of cross-correlation. However, because Jeffress' theory is unable to account for the precedence effect, in which only the first of multiple identical sounds is used to determine the sounds' location (thus avoiding confusion caused by echoes), it cannot be entirely used to explain the response. Furthermore, a number of recent physiological observations made in the midbrain and brainstem of small mammals have shed considerable doubt on the validity of Jeffress' original ideas 
Neurons sensitive to inter-aural level differences (ILDs) are excited by stimulation of one ear and inhibited by stimulation of the other ear, such that the response magnitude of the cell depends on the relative strengths of the two inputs, which in turn, depends on the sound intensities at the ears.
In the auditory midbrain nucleus, the inferior colliculus (IC), many ILD sensitive neurons have response functions that decline steeply from maximum to zero spikes as a function of ILD. However, there are also many neurons with much more shallow response functions that do not decline to zero spikes.

The cone of confusion
Most mammals are adept at resolving the location of a sound source using interaural time differences and interaural level differences. However, no such time or level differences exist for sounds originating along the circumference of circular conical slices, where the cone's axis lies along the line between the two ears.
Consequently, sound waves originating at any point along a given circumference slant height will have ambiguous perceptual coordinates. That is to say, the listener will be incapable of determining whether the sound originated from the back, front, top, bottom or anywhere else along the circumference at the base of a cone at any given distance from the ear. Of course, the importance of these ambiguities are vanishingly small for sound sources very close to or very far away from the subject, but it is these intermediate distances that are most important in terms of fitness.
These ambiguities can be removed by tilting the head, which can introduce a shift in both the amplitude and phase of sound waves arriving at each ear. This translates the vertical orientation of the interaural axis horizontally, thereby leveraging the mechanism of localization on the horizontal plane. Moreover, even with no alternation in the angle of the interaural axis (i.e. without tilting one's head) the hearing system can capitalize on interference patterns generated by pinnae, the torso, and even the temporary re-purposing of a hand as extension of the pinna (e.g., cupping one's hand around the ear).
As with other sensory stimuli, perceptual disambiguation is also accomplished through integration of multiple sensory inputs, especially visual cues. Having localized a sound within the circumference of a circle at some perceived distance, visual cues serve to fix the location of the sound. Moreover, prior knowledge of the location of the sound generating agent will assist in resolving its current location.

Sound localization by the human auditory system
Sound localization is the process of determining the location of a sound source. Objectively speaking, the major goal of sound localization is to simulate a specific sound field, including the acoustic sources, the listener, the media and environments of sound propagation. The brain utilizes subtle differences in intensity, spectral, and timing cues to allow us to localize sound sources. In this section, to more deeply understand the human auditory mechanism, we will briefly discuss about human ear localization theory.

General Introduction
Localization can be described in terms of three-dimensional position: the azimuth or horizontal angle, the elevation or vertical angle, and the distance (for static sounds) or velocity (for moving sounds).
The azimuth of a sound is signaled by the difference in arrival times between the ears, by the relative amplitude of high-frequency sounds (the shadow effect), and by the asymmetrical spectral reflections from various parts of our bodies, including torso, shoulders, and pinnae.
The distance cues are the loss of amplitude, the loss of high frequencies, and the ratio of the direct signal to the reverberated signal.
Depending on where the source is located, our head acts as a barrier to change the timbre, intensity, and spectral qualities of the sound, helping the brain orient where the sound emanated from. These minute differences between the two ears are known as interaural cues.
Lower frequencies, with longer wavelengths, diffract the sound around the head forcing the brain to focus only on the phasing cues from the source.
Helmut Haas discovered that we can discern the sound source despite additional reflections at 10 decibels louder than the original wave front, using the earliest arriving wave front. This principle is known as the Haas effect, a specific version of the precedence effect. Haas measured down to even a 1 millisecond difference in timing between the original sound and reflected sound increased the spaciousness, allowing the brain to discern the true location of the original sound. The nervous system combines all early reflections into a single perceptual whole allowing the brain to process multiple different sounds at once. The nervous system will combine reflections that are within about 35 milliseconds of each other and that have a similar intensity.

Duplex Theory
To determine the lateral input direction (left, front, right), the auditory system analyzes the following ear signal information:

Duplex Theory
In 1907, Lord Rayleigh utilized tuning forks to generate monophonic excitation and studied the lateral sound localization theory on a human head model without auricle. He first presented the interural clue difference based sound localization theory, which is known as Duplex Theory. Human ears are on the different sides of the head, thus they have different coordinates in space. As shown in fig. 2, since the distances between the acoustic source and ears are different, there are time difference and intensity difference between the sound signals of two ears. We call those kinds of differences as Interaural Time Difference (ITD) and Interaural Intensity Difference (IID) respectively.

ITD and IID
From fig.2 we can see that no matter for source B1 or source B2, there will be a propagation delay between two ears, which will generate the ITD. Simultaneously, human head and ears may have shadowing effect on high frequency signals, which will generate IID.
Interaural Time Difference (ITD) Sound from the right side reaches the right ear earlier than the left ear. The auditory system evaluates interaural time differences from: (a) Phase delays at low frequencies and (b) group delays at high frequencies.
Massive experiments demonstrate that ITD relates to the signal frequency f. Suppose the angular position of the acoustic source is ?, the head radius is r and the acoustic velocity is c, the function of ITD is given by:
  
    
      
        I
        T
        D
        =
        
          
            {
            
              
                
                  300
                  ×
                  
                    r
                  
                  ×
                  sin
                  ?
                  ?
                  
                    /
                  
                  
                    c
                  
                  ,
                
                
                  
                    if 
                  
                  f
                  ?
                  
                    4000Hz 
                  
                
              
              
                
                  200
                  ×
                  
                    r
                  
                  ×
                  sin
                  ?
                  ?
                  
                    /
                  
                  
                    c
                  
                  ,
                
                
                  
                    if 
                  
                  f
                  >
                  
                     4000Hz
                  
                
              
            
            
          
        
      
    
    {\displaystyle ITD={\begin{cases}300\times {\text{r}}\times \sin \theta /{\text{c}},&{\text{if }}f\leq {\text{4000Hz }}\\200\times {\text{r}}\times \sin \theta /{\text{c}},&{\text{if }}f>{\text{ 4000Hz}}\end{cases}}}
  . In above closed form, we assumed that the 0 degree is in the right ahead of the head and counter-clockwise is positive.
Interaural Intensity Difference (IID) or Interaural Level Difference (ILD) Sound from the right side has a higher level at the right ear than at the left ear, because the head shadows the left ear. These level differences are highly frequency dependent and they increase with increasing frequency. Massive theoretical researches demonstrate that IID relates to the signal frequency f and the angular position of the acoustic source ?. The function of IID is given by: 
  
    
      
        I
        I
        D
        =
        1.0
        +
        (
        f
        
          /
        
        1000
        
          )
          
            0.8
          
        
        ×
        sin
        ?
        ?
      
    
    {\displaystyle IID=1.0+(f/1000)^{0.8}\times \sin \theta }
  
For frequencies below 1000 Hz, mainly ITDs are evaluated (phase delays), for frequencies above 1500 Hz mainly IIDs are evaluated. Between 1000 Hz and 1500 Hz there is a transition zone, where both mechanisms play a role.
Localization accuracy is 1 degree for sources in front of the listener and 15 degrees for sources to the sides. Humans can discern interaural time differences of 10 microseconds or less.

Evaluation for low frequencies
For frequencies below 800 Hz, the dimensions of the head (ear distance 21.5 cm, corresponding to an interaural time delay of 625 µs) are smaller than the half wavelength of the sound waves. So the auditory system can determine phase delays between both ears without confusion. Interaural level differences are very low in this frequency range, especially below about 200 Hz, so a precise evaluation of the input direction is nearly impossible on the basis of level differences alone. As the frequency drops below 80 Hz it becomes difficult or impossible to use either time difference or level difference to determine a sound's lateral source, because the phase difference between the ears becomes too small for a directional evaluation.

Evaluation for high frequencies
For frequencies above 1600 Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.

Pinna Filtering Effect Theory
Motivations
Duplex theory clearly points out that ITD and IID play significant roles in sound localization but they can only deal with lateral localizing problems. For example, based on duplex theory, if two acoustic sources are symmetrically located on the right front and right back of the human head, they will generate equal ITDs and IIDs, which is called as cone model effect. However, human ears can actually distinguish this set of sources. Besides that, in natural sense of hearing, only one ear, which means no ITD or IID, can distinguish the sources with a high accuracy. Due to the disadvantages of duplex theory, researchers proposed the pinna filtering effect theory. The shape of human pinna is very special. It is concave with complex folds and asymmetrical no matter horizontally or vertically. The reflected waves and the direct waves will generate a frequency spectrum on the eardrum, which is related to the acoustic sources. Then auditory nerves localize the sources by this frequency spectrum. Therefore, a corresponding theory was proposed and called as pinna filtering effect theory.

Math Model
These spectrum clue generated by pinna filtering effect can be presented as Head-Related Transfer Functions (HRTF). The corresponding time domain expressions are called as Head-Related Impulse Response (HRIR). HRTF is also called as the transfer function from the free field to a specific point in the ear canal. We usually recognize HRTFs as LTI systems:

  
    
      
        
          H
          
            L
          
        
        =
        
          H
          
            L
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        =
        
          P
          
            L
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        
          /
        
        
          P
          
            0
          
        
        (
        r
        ,
        ?
        )
      
    
    {\displaystyle H_{L}=H_{L}(r,\theta ,\varphi ,\omega ,\alpha )=P_{L}(r,\theta ,\varphi ,\omega ,\alpha )/P_{0}(r,\omega )}
  

  
    
      
        
          H
          
            R
          
        
        =
        
          H
          
            R
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        =
        
          P
          
            R
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        
          /
        
        
          P
          
            0
          
        
        (
        r
        ,
        ?
        )
      
    
    {\displaystyle H_{R}=H_{R}(r,\theta ,\varphi ,\omega ,\alpha )=P_{R}(r,\theta ,\varphi ,\omega ,\alpha )/P_{0}(r,\omega )}
  ,
where L and R represent the left ear and right ear respectively. 
  
    
      
        
          P
          
            L
          
        
      
    
    {\displaystyle P_{L}}
   and 
  
    
      
        
          P
          
            R
          
        
      
    
    {\displaystyle P_{R}}
   represent the amplitude of sound pressure at entrances of left and right ear canal. 
  
    
      
        
          P
          
            0
          
        
      
    
    {\displaystyle P_{0}}
   is the amplitude of sound pressure at the center of the head coordinate when listener does not exist. In general, HRTFs 
  
    
      
        
          H
          
            L
          
        
      
    
    {\displaystyle H_{L}}
   and 
  
    
      
        
          H
          
            R
          
        
      
    
    {\displaystyle H_{R}}
   are functions of source angular position 
  
    
      
        ?
      
    
    {\displaystyle \theta }
  , elevation angle 
  
    
      
        ?
      
    
    {\displaystyle \varphi }
  , distance between source and center of the head 
  
    
      
        r
      
    
    {\displaystyle r}
  , the angular velocity 
  
    
      
        ?
      
    
    {\displaystyle \omega }
   and the equivalent dimension of the head 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
  .

HRTF Database
At present, the main institutes that work on measuring HRTF database includes CIPIC International Lab, MIT Media Lab, The Graduate School in Psychoacoustics at the University of Oldenburg, Neurophysiology Lab in University of Wisconsin-Madison and Ames Lab of NASA. They carefully measures the HRIRs from both humans and animals and share the results on Internet for people who want to study.

Other Cues for 3D Space Localization
Monaural cues
The human outer ear, i.e. the structures of the pinna and the external ear canal, form direction-selective filters. Depending on the sound input direction in the median plane, different filter resonances become active. These resonances implant direction-specific patterns into the frequency responses of the ears, which can be evaluated by the auditory system (directional bands) for vertical sound localization. Together with other direction-selective reflections at the head, shoulders and torso, they form the outer ear transfer functions. These patterns in the ear's frequency responses are highly individual, depending on the shape and size of the outer ear. If sound is presented through headphones, and has been recorded via another head with different-shaped outer ear surfaces, the directional patterns differ from the listener's own, and problems will appear when trying to evaluate directions in the median plane with these foreign ears. As a consequence, front–back permutations or inside-the-head-localization can appear when listening to dummy head recordings, or otherwise referred to as binaural recordings. It has been shown that human subjects can monaurally localize high frequency sound but not low frequency sound. Binaural localization, however, was possible with lower frequencies. This is likely due to the pinna being small enough to only interact with sound waves of high frequency. It seems that people can only accurately localize the elevation of sounds that are complex and include frequencies above 7,000 Hz, and a pinna must be present.

Dynamic binaural cues
When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound’s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.

Distance of the sound source
The human auditory system has only limited possibilities to determine the distance of a sound source. In the close-up-range there are some indications for distance determination, such as extreme level differences (e.g. when whispering into one ear) or specific pinna (the visible part of the ear) resonances in the close-up range.
The auditory system uses these clues to estimate the distance to a sound source:
Direct/ Reflection ratio: In enclosed rooms, two types of sound are arriving at a listener: The direct sound arrives at the listener's ears without being reflected at a wall. Reflected sound has been reflected at least one time at a wall before arriving at the listener. The ratio between direct sound and reflected sound can give an indication about the distance of the sound source.
Loudness: Distant sound sources have a lower loudness than close ones. This aspect can be evaluated especially for well-known sound sources.
Sound spectrum : High frequencies are more quickly damped by the air than low frequencies. Therefore, a distant sound source sounds more muffled than a close one, because the high frequencies are attenuated. For sound with a known spectrum (e.g. speech) the distance can be estimated roughly with the help of the perceived sound.
ITDG: The Initial Time Delay Gap describes the time difference between arrival of the direct wave and first strong reflection at the listener. Nearby sources create a relatively large ITDG, with the first reflections having a longer path to take, possibly many times longer. When the source is far away, the direct and the reflected sound waves have similar path lengths.
Movement: Similar to the visual system there is also the phenomenon of motion parallax in acoustical perception. For a moving listener nearby sound sources are passing faster than distant sound sources.
Level Difference: Very close sound sources cause a different level between the ears.

Signal processing
Sound processing of the human auditory system is performed in so-called critical bands. The hearing range is segmented into 24 critical bands, each with a width of 1 Bark or 100 Mel. For a directional analysis the signals inside the critical band are analyzed together.
The auditory system can extract the sound of a desired sound source out of interfering noise. This allows the listener to concentrate on only one speaker if other speakers are also talking (the cocktail party effect). With the help of the cocktail party effect sound from interfering directions is perceived attenuated compared to the sound from the desired direction. The auditory system can increase the signal-to-noise ratio by up to 15 dB, which means that interfering sound is perceived to be attenuated to half (or less) of its actual loudness.

Localization in enclosed rooms
In enclosed rooms not only the direct sound from a sound source is arriving at the listener's ears, but also sound which has been reflected at the walls. The auditory system analyses only the direct sound, which is arriving first, for sound localization, but not the reflected sound, which is arriving later (law of the first wave front). So sound localization remains possible even in an echoic environment. This echo cancellation occurs in the Dorsal Nucleus of the Lateral Lemniscus (DNLL).
In order to determine the time periods, where the direct sound prevails and which can be used for directional evaluation, the auditory system analyzes loudness changes in different critical bands and also the stability of the perceived direction. If there is a strong attack of the loudness in several critical bands and if the perceived direction is stable, this attack is in all probability caused by the direct sound of a sound source, which is entering newly or which is changing its signal characteristics. This short time period is used by the auditory system for directional and loudness analysis of this sound. When reflections arrive a little bit later, they do not enhance the loudness inside the critical bands in such a strong way, but the directional cues become unstable, because there is a mix of sound of several reflection directions. As a result, no new directional analysis is triggered by the auditory system.
This first detected direction from the direct sound is taken as the found sound source direction, until other strong loudness attacks, combined with stable directional information, indicate that a new directional analysis is possible. (see Franssen effect)

Specific techniques with applications
Auditory transmission stereo system
This kind of sound localization technique provides us the real virtual stereo system. It utilizes ""smart"" manikins, such as KEMAR, to glean signals or use DSP methods to simulate the transmission process from sources to ears. After amplifying, recording and transmitting, the two channels of received signals will be reproduced through earphones or speakers. This localization approach uses electroacoustic methods to obtain the spatial information of the original sound field by transferring the listener's auditory apparatus to the original sound field. The most considerable advantages of it would be that its acoustic images are lively and natural. Also, it only needs two independent transmitted signal to reproduce the acoustic image of a 3D system.

3D para-virtualization stereo system
The representatives of this kind of system are SRS Audio Sandbox, Spatializer Audio Lab and Qsound Qxpander. They use HRTF to simulate the received acoustic signals at the ears from different directions with common binary-channel stereo reproduction. Therefore, they can simulate reflected sound waves and improve subjective sense of space and envelopment. Since they are para-virtualization stereo systems, the major goal of them is to simulate stereo sound information. Traditional stereo systems use sensors that are quite different from human ears. Although those sensors can receive the acoustic information from different directions, they do not have the same frequency response of human auditory system. Therefore, when binary-channel mode is applied, human auditory systems still cannot feel the 3D sound effect field. However, the 3D para-virtualization stereo system overcome such disadvantages. It uses HRTF principles to glean acoustic information from the original sound field then produce a lively 3D sound field through common earphones or speakers.

Multichannel stereo virtual reproduction
Since the multichannel stereo systems require lots of reproduction channels, some researchers adopted the HRTF simulation technologies to reduce the number of reproduction channels. They use only two speakers to simulate multiple speakers in a multichannel system. This process is called as virtual reproduction. Essentially, such approach uses both interaural difference principle and pinna filtering effect theory. Unfortunately, this kind of approach cannot perfectly substitute the traditional multichannel stereo system, such as 5.1 surround sound system. That is because when the listening zone is relatively larger, simulation reproduction through HRTFs may cause invert acoustic images at symmetric positions.

Animals
Since most animals have two ears, many of the effects of the human auditory system can also be found in other animals. Therefore, interaural time differences (interaural phase differences) and interaural level differences play a role for the hearing of many animals. But the influences on localization of these effects are dependent on head sizes, ear distances, the ear positions and the orientation of the ears.

Lateral information (left, ahead, right)
If the ears are located at the side of the head, similar lateral localization cues as for the human auditory system can be used. This means: evaluation of interaural time differences (interaural phase differences) for lower frequencies and evaluation of interaural level differences for higher frequencies. The evaluation of interaural phase differences is useful, as long as it gives unambiguous results. This is the case, as long as ear distance is smaller than half the length (maximal one wavelength) of the sound waves. For animals with a larger head than humans the evaluation range for interaural phase differences is shifted towards lower frequencies, for animals with a smaller head, this range is shifted towards higher frequencies.
The lowest frequency which can be localized depends on the ear distance. Animals with a greater ear distance can localize lower frequencies than humans can. For animals with a smaller ear distance the lowest localizable frequency is higher than for humans.
If the ears are located at the side of the head, interaural level differences appear for higher frequencies and can be evaluated for localization tasks. For animals with ears at the top of the head, no shadowing by the head will appear and therefore there will be much less interaural level differences, which could be evaluated. Many of these animals can move their ears, and these ear movements can be used as a lateral localization cue.

Odontocetes
Dolphins (and other odontocetes) rely on echolocation to aid in detecting, identifying, localizing, and capturing prey. Dolphin sonar signals are well suited for localizing multiple, small targets in a three-dimensional aquatic environment by utilizing highly directional (3 dB beamwidth of about 10 deg), broadband (3 dB bandwidth typically of about 40 kHz; peak frequencies between 40 kHz and 120 kHz), short duration clicks (about 40 ?s). Dolphins can localize sounds both passively and actively (echolocation) with a resolution of about 1 deg. Cross-modal matching (between vision and echolocation) suggests dolphins perceive the spatial structure of complex objects interrogated through echolocation, a feat that likely requires spatially resolving individual object features and integration into a holistic representation of object shape. Although dolphins are sensitive to small, binaural intensity and time differences, mounting evidence suggests dolphins employ position-dependent spectral cues derived from well-developed head-related transfer functions, for sound localization in both the horizontal and vertical planes. A very small temporal integration time (264 ?s) allows localization of multiple targets at varying distances. Localization adaptations include pronounc",Category:Sound,1
86,87,Acoustic impedance,"Acoustic impedance and specific acoustic impedance are measures of the opposition that a system presents to the acoustic flow resulting of an acoustic pressure applied to the system. The SI unit of acoustic impedance is the pascal second per cubic metre (Pa·s/m3) or the rayl per square metre (rayl/m2), while that of specific acoustic impedance is the pascal second per metre (Pa·s/m) or the rayl. In this article the symbol rayl denotes the MKS rayl. There is a close analogy with electrical impedance, which measures the opposition that a system presents to the electrical flow resulting of an electrical voltage applied to the system.

Mathematical definitions
Acoustic impedance
For a linear time-invariant system, the relationship between the acoustic pressure applied to the system and the resulting acoustic volume flow rate through a surface perpendicular to the direction of that pressure at its point of application is given by

  
    
      
        p
        (
        t
        )
        =
        [
        R
        ?
        Q
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle p(t)=[R*Q](t),}
  
or equivalently by

  
    
      
        Q
        (
        t
        )
        =
        [
        G
        ?
        p
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle Q(t)=[G*p](t),}
  
where
p is the acoustic pressure;
Q is the acoustic volume flow rate;

  
    
      
        ?
      
    
    {\displaystyle *}
   is the convolution operator;
R is the acoustic resistance in the time domain;
G = R ?1 is the acoustic conductance in the time domain (R ?1 is the convolution inverse of R).
Acoustic impedance, denoted Z, is the Laplace transform, or the Fourier transform, or the analytic representation of time domain acoustic resistance:

  
    
      
        Z
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        R
        ]
        (
        s
        )
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              Q
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle Z(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[R](s)={\frac {{\mathcal {L}}[p](s)}{{\mathcal {L}}[Q](s)}},}
  

  
    
      
        Z
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        R
        ]
        (
        ?
        )
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              Q
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle Z(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[R](\omega )={\frac {{\mathcal {F}}[p](\omega )}{{\mathcal {F}}[Q](\omega )}},}
  

  
    
      
        Z
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          R
          
            
              a
            
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                Q
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle Z(t){\stackrel {\mathrm {def} }{{}={}}}R_{\mathrm {a} }(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left(Q^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where

  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
   is the Laplace transform operator;

  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is the Fourier transform operator;
subscript ""a"" is the analytic representation operator;
Q ?1 is the convolution inverse of Q.
Acoustic resistance, denoted R, and acoustic reactance, denoted X, are the real part and imaginary part of acoustic impedance respectively:

  
    
      
        Z
        (
        s
        )
        =
        R
        (
        s
        )
        +
        i
        X
        (
        s
        )
        ,
      
    
    {\displaystyle Z(s)=R(s)+iX(s),}
  

  
    
      
        Z
        (
        ?
        )
        =
        R
        (
        ?
        )
        +
        i
        X
        (
        ?
        )
        ,
      
    
    {\displaystyle Z(\omega )=R(\omega )+iX(\omega ),}
  

  
    
      
        Z
        (
        t
        )
        =
        R
        (
        t
        )
        +
        i
        X
        (
        t
        )
        ,
      
    
    {\displaystyle Z(t)=R(t)+iX(t),}
  
where
i is the imaginary unit;
in Z(s), R(s) is not the Laplace transform of the time domain acoustic resistance R(t), Z(s) is;
in Z(?), R(?) is not the Fourier transform of the time domain acoustic resistance R(t), Z(?) is;
in Z(t), R(t) is the time domain acoustic resistance and X(t) is the Hilbert transform of the time domain acoustic resistance R(t), according to the definition of the analytic representation.
Inductive acoustic reactance, denoted XL, and capacitive acoustic reactance, denoted XC, are the positive part and negative part of acoustic reactance respectively:

  
    
      
        X
        (
        s
        )
        =
        
          X
          
            L
          
        
        (
        s
        )
        ?
        
          X
          
            C
          
        
        (
        s
        )
        ,
      
    
    {\displaystyle X(s)=X_{L}(s)-X_{C}(s),}
  

  
    
      
        X
        (
        ?
        )
        =
        
          X
          
            L
          
        
        (
        ?
        )
        ?
        
          X
          
            C
          
        
        (
        ?
        )
        ,
      
    
    {\displaystyle X(\omega )=X_{L}(\omega )-X_{C}(\omega ),}
  

  
    
      
        X
        (
        t
        )
        =
        
          X
          
            L
          
        
        (
        t
        )
        ?
        
          X
          
            C
          
        
        (
        t
        )
        .
      
    
    {\displaystyle X(t)=X_{L}(t)-X_{C}(t).}
  
Acoustic admittance, denoted Y, is the Laplace transform, or the Fourier transform, or the analytic representation of time domain acoustic conductance:

  
    
      
        Y
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        G
        ]
        (
        s
        )
        =
        
          
            1
            
              Z
              (
              s
              )
            
          
        
        =
        
          
            
              
                
                  L
                
              
              [
              Q
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle Y(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[G](s)={\frac {1}{Z(s)}}={\frac {{\mathcal {L}}[Q](s)}{{\mathcal {L}}[p](s)}},}
  

  
    
      
        Y
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        G
        ]
        (
        ?
        )
        =
        
          
            1
            
              Z
              (
              ?
              )
            
          
        
        =
        
          
            
              
                
                  F
                
              
              [
              Q
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle Y(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[G](\omega )={\frac {1}{Z(\omega )}}={\frac {{\mathcal {F}}[Q](\omega )}{{\mathcal {F}}[p](\omega )}},}
  

  
    
      
        Y
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          G
          
            
              a
            
          
        
        (
        t
        )
        =
        
          Z
          
            ?
            1
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            Q
            
              
                a
              
            
          
          ?
          
            
              (
              
                p
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle Y(t){\stackrel {\mathrm {def} }{{}={}}}G_{\mathrm {a} }(t)=Z^{-1}(t)={\frac {1}{2}}\!\left[Q_{\mathrm {a} }*\left(p^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where
Z ?1 is the convolution inverse of Z;
p ?1 is the convolution inverse of p.
Acoustic conductance, denoted G, and acoustic susceptance, denoted B, are the real part and imaginary part of acoustic admittance respectively:

  
    
      
        Y
        (
        s
        )
        =
        G
        (
        s
        )
        +
        i
        B
        (
        s
        )
        ,
      
    
    {\displaystyle Y(s)=G(s)+iB(s),}
  

  
    
      
        Y
        (
        ?
        )
        =
        G
        (
        ?
        )
        +
        i
        B
        (
        ?
        )
        ,
      
    
    {\displaystyle Y(\omega )=G(\omega )+iB(\omega ),}
  

  
    
      
        Y
        (
        t
        )
        =
        G
        (
        t
        )
        +
        i
        B
        (
        t
        )
        ,
      
    
    {\displaystyle Y(t)=G(t)+iB(t),}
  
where
in Y(s), G(s) is not the Laplace transform of the time domain acoustic conductance G(t), Y(s) is;
in Y(?), G(?) is not the Fourier transform of the time domain acoustic conductance G(t), Y(?) is;
in Y(t), G(t) is the time domain acoustic conductance and B(t) is the Hilbert transform of the time domain acoustic conductance G(t), according to the definition of the analytic representation.
Acoustic resistance represents the energy transfer of an acoustic wave. The pressure and motion are in phase, so work is done on the medium ahead of the wave.
Acoustic reactance represents, as well, the pressure that is out of phase with the motion and causes no average energy transfer. For example, a closed bulb connected to an organ pipe will have air moving into it and pressure, but they are out of phase so no net energy is transmitted into it. While the pressure rises, air moves in, and while it falls, it moves out, but the average pressure when the air moves in is the same as that when it moves out, so the power flows back and forth but with no time averaged energy transfer. The electrical analogy for this is a capacitor connected across a power line. Current flows through the capacitor but it is out of phase with the voltage, so no net power is transmitted into it.

Specific acoustic impedance
For a linear time-invariant system, the relationship between the acoustic pressure applied to the system and the resulting particle velocity in the direction of that pressure at its point of application is given by

  
    
      
        p
        (
        t
        )
        =
        [
        r
        ?
        v
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle p(t)=[r*v](t),}
  
or equivalently by:

  
    
      
        v
        (
        t
        )
        =
        [
        g
        ?
        p
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle v(t)=[g*p](t),}
  
where
p is the acoustic pressure;
v is the particle velocity;
r is the specific acoustic resistance in the time domain;
g = r ?1 is the specific acoustic conductance in the time domain (r ?1 is the convolution inverse of r).
Specific acoustic impedance, denoted z is the Laplace transform, or the Fourier transform, or the analytic representation of time domain specific acoustic resistance:

  
    
      
        z
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        r
        ]
        (
        s
        )
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              v
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle z(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[r](s)={\frac {{\mathcal {L}}[p](s)}{{\mathcal {L}}[v](s)}},}
  

  
    
      
        z
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        r
        ]
        (
        ?
        )
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              v
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle z(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[r](\omega )={\frac {{\mathcal {F}}[p](\omega )}{{\mathcal {F}}[v](\omega )}},}
  

  
    
      
        z
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          r
          
            
              a
            
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                v
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle z(t){\stackrel {\mathrm {def} }{{}={}}}r_{\mathrm {a} }(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left(v^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where v ?1 is the convolution inverse of v.
Specific acoustic resistance, denoted r, and specific acoustic reactance, denoted x, are the real part and imaginary part of specific acoustic impedance respectively:

  
    
      
        z
        (
        s
        )
        =
        r
        (
        s
        )
        +
        i
        x
        (
        s
        )
        ,
      
    
    {\displaystyle z(s)=r(s)+ix(s),}
  

  
    
      
        z
        (
        ?
        )
        =
        r
        (
        ?
        )
        +
        i
        x
        (
        ?
        )
        ,
      
    
    {\displaystyle z(\omega )=r(\omega )+ix(\omega ),}
  

  
    
      
        z
        (
        t
        )
        =
        r
        (
        t
        )
        +
        i
        x
        (
        t
        )
        ,
      
    
    {\displaystyle z(t)=r(t)+ix(t),}
  
where
in z(s), r(s) is not the Laplace transform of the time domain specific acoustic resistance r(t), z(s) is;
in z(?), r(?) is not the Fourier transform of the time domain specific acoustic resistance r(t), z(?) is;
in z(t), r(t) is the time domain specific acoustic resistance and x(t) is the Hilbert transform of the time domain specific acoustic resistance r(t), according to the definition of the analytic representation.
Specific inductive acoustic reactance, denoted xL, and specific capacitive acoustic reactance, denoted xC, are the positive part and negative part of specific acoustic reactance respectively:

  
    
      
        x
        (
        s
        )
        =
        
          x
          
            L
          
        
        (
        s
        )
        ?
        
          x
          
            C
          
        
        (
        s
        )
        ,
      
    
    {\displaystyle x(s)=x_{L}(s)-x_{C}(s),}
  

  
    
      
        x
        (
        ?
        )
        =
        
          x
          
            L
          
        
        (
        ?
        )
        ?
        
          x
          
            C
          
        
        (
        ?
        )
        ,
      
    
    {\displaystyle x(\omega )=x_{L}(\omega )-x_{C}(\omega ),}
  

  
    
      
        x
        (
        t
        )
        =
        
          x
          
            L
          
        
        (
        t
        )
        ?
        
          x
          
            C
          
        
        (
        t
        )
        .
      
    
    {\displaystyle x(t)=x_{L}(t)-x_{C}(t).}
  
Specific acoustic admittance, denoted y, is the Laplace transform, or the Fourier transform, or the analytic representation of time domain specific acoustic conductance:

  
    
      
        y
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        g
        ]
        (
        s
        )
        =
        
          
            1
            
              z
              (
              s
              )
            
          
        
        =
        
          
            
              
                
                  L
                
              
              [
              v
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle y(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[g](s)={\frac {1}{z(s)}}={\frac {{\mathcal {L}}[v](s)}{{\mathcal {L}}[p](s)}},}
  

  
    
      
        y
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        g
        ]
        (
        ?
        )
        =
        
          
            1
            
              z
              (
              ?
              )
            
          
        
        =
        
          
            
              
                
                  F
                
              
              [
              v
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle y(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[g](\omega )={\frac {1}{z(\omega )}}={\frac {{\mathcal {F}}[v](\omega )}{{\mathcal {F}}[p](\omega )}},}
  

  
    
      
        y
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          g
          
            
              a
            
          
        
        (
        t
        )
        =
        
          z
          
            ?
            1
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            v
            
              
                a
              
            
          
          ?
          
            
              (
              
                p
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle y(t){\stackrel {\mathrm {def} }{{}={}}}g_{\mathrm {a} }(t)=z^{-1}(t)={\frac {1}{2}}\!\left[v_{\mathrm {a} }*\left(p^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where
z ?1 is the convolution inverse of z;
p ?1 is the convolution inverse of p.
Specific acoustic conductance, denoted g, and specific acoustic susceptance, denoted b, are the real part and imaginary part of specific acoustic admittance respectively:

  
    
      
        y
        (
        s
        )
        =
        g
        (
        s
        )
        +
        i
        b
        (
        s
        )
        ,
      
    
    {\displaystyle y(s)=g(s)+ib(s),}
  

  
    
      
        y
        (
        ?
        )
        =
        g
        (
        ?
        )
        +
        i
        b
        (
        ?
        )
        ,
      
    
    {\displaystyle y(\omega )=g(\omega )+ib(\omega ),}
  

  
    
      
        y
        (
        t
        )
        =
        g
        (
        t
        )
        +
        i
        b
        (
        t
        )
        ,
      
    
    {\displaystyle y(t)=g(t)+ib(t),}
  
where
in y(s), g(s) is not the Laplace transform of the time domain acoustic conductance g(t), y(s) is;
in y(?), g(?) is not the Fourier transform of the time domain acoustic conductance g(t), y(?) is;
in y(t), g(t) is the time domain acoustic conductance and b(t) is the Hilbert transform of the time domain acoustic conductance g(t), according to the definition of the analytic representation.
Specific acoustic impedance z is an intensive property of a particular medium: for instance, the z of air or of water can be specified. Whereas acoustic impedance Z is an extensive property of a particular medium and geometry: for instance, the Z of a particular duct filled with air can be discussed.

Relationship
A one dimensional wave passing through an aperture with area A is now considered. The acoustic volume flow rate Q is the volume of medium passing per second through the aperture. If the acoustic flow moves a distance dx = v dt, then the volume of medium passing through is dV = A dx, so

  
    
      
        Q
        =
        
          
            
              
                d
              
              V
            
            
              
                d
              
              t
            
          
        
        =
        A
        
          
            
              
                d
              
              x
            
            
              
                d
              
              t
            
          
        
        =
        A
        v
        .
      
    
    {\displaystyle Q={\frac {\mathrm {d} V}{\mathrm {d} t}}=A{\frac {\mathrm {d} x}{\mathrm {d} t}}=Av.}
  
Provided that the wave is only one-dimensional, it yields

  
    
      
        Z
        (
        s
        )
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              Q
              ]
              (
              s
              )
            
          
        
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              A
              
                
                  L
                
              
              [
              v
              ]
              (
              s
              )
            
          
        
        =
        
          
            
              z
              (
              s
              )
            
            A
          
        
        ,
      
    
    {\displaystyle Z(s)={\frac {{\mathcal {L}}[p](s)}{{\mathcal {L}}[Q](s)}}={\frac {{\mathcal {L}}[p](s)}{A{\mathcal {L}}[v](s)}}={\frac {z(s)}{A}},}
  

  
    
      
        Z
        (
        ?
        )
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              Q
              ]
              (
              ?
              )
            
          
        
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              A
              
                
                  F
                
              
              [
              v
              ]
              (
              ?
              )
            
          
        
        =
        
          
            
              z
              (
              ?
              )
            
            A
          
        
        ,
      
    
    {\displaystyle Z(\omega )={\frac {{\mathcal {F}}[p](\omega )}{{\mathcal {F}}[Q](\omega )}}={\frac {{\mathcal {F}}[p](\omega )}{A{\mathcal {F}}[v](\omega )}}={\frac {z(\omega )}{A}},}
  

  
    
      
        Z
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                Q
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                
                  
                    v
                    
                      ?
                      1
                    
                  
                  A
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        =
        
          
            
              z
              (
              t
              )
            
            A
          
        
        .
      
    
    {\displaystyle Z(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left(Q^{-1}\right)_{\mathrm {a} }\right]\!(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left({\frac {v^{-1}}{A}}\right)_{\mathrm {a} }\right]\!(t)={\frac {z(t)}{A}}.}

Characteristic acoustic impe",Category:Physical quantities,1
87,88,International Commission for Acoustics,"The purpose of the International Commission for Acoustics (ICA) is to promote international development and collaboration in all fields of acoustics including research, development, education, and standardisation.
The ICA is a Scientific Associate of the International Council for Science (ICSU), an Affiliated Commission of the International Union of Pure and Applied Physics (IUPAP), and an Affiliated Organization of the International Union of Theoretical and Applied Mechanics (IUTAM).
The ICA has a membership of over 40 national acoustical societies, and has affiliate members including the European Acoustics Association (EAA), the IberoAmerican Federation of Acoustics (FIA), the International Institute of Noise Control Engineering (I-INCE), the International Institute of Acoustics and Vibration (IIAV), the Western Pacific Acoustics Commission (WESPAC) and the International Congress on Ultrasonics (ICU).
The ICA convenes the triennial International Congresses on Acoustics, sponsors specialty symposia in acoustics, and coordinates the main international meetings within acoustics.

History
The International Commission on Acoustics (ICA) was instituted in 1951 as a subcommittee to the International Union of Pure and Applied Physics (IUPAP). New Statutes were adopted by the International Commission on Acoustics in Antwerp 1996 March 31 and were approved by the IUPAP General Assembly in Uppsala Sweden 1996 September 20. An information letter explaining the proposed changes to the status of the commission was circulated to acoustical societies 1996 May 10. The new ICA held its first General Assembly 1998 June 25 during the 16th Congress in Seattle where the By-laws of the new organization were adopted by the Member Societies. The Commission then became known as the International Commission for Acoustics. The ICA has also applied to become an Affiliated Commission of the International Union of Theoretical and Applied Mechanics (IUTAM). A motion in favor of Affiliation was carried unanimously at the IUTAM General Assembly Meeting held at the University of Stuttgart 1998 August 28–30. The ICA became a Scientific Associate of the International Council of Scientific Unions ICSU in 2006.

The International Congress on Acoustics
The International Congress on Acoustics is organized every three years by the International Commission for Acoustics. It is the world's biggest congress in the field of acoustics.
Dates and locations of past congresses of the International Commission for Acoustics:
1st ICA 1953 Delft, Netherlands
2nd ICA 1956 Cambridge, USA
3rd ICA 1959 Stuttgart, Germany
4th ICA 1962 Copenhagen, Denmark
5th ICA 1965 Liege, Belgium
6th ICA 1968 Tokyo, Japan
7th ICA 1971 Budapest, Hungary
8th ICA 1974 London, England
9th ICA 1977 Madrid, Spain
10th ICA 1980 Sydney, Australia
11th ICA 1983 Paris, France
12th ICA 1986 Toronto, Canada
13th ICA 1989 Belgrade, Yugoslavia
14th ICA 1992 Beijing, China
15th ICA 1995 Trondheim, Norway
16th ICA 1998 Seattle, USA
17th ICA 2001 Rome, Italy
18th ICA 2004 Kyoto, Japan
19th ICA 2007 Madrid, Spain
20th ICA 2010 Sydney, Australia
21st ICA 2013 Montreal, Canada
22nd ICA 2016 Buenos Aires, Argentine

The ICA Early Career Award
The ICA Award, officially known as International Commission for Acoustics Early Career Award, or ICA Early Career Award, is a prize awarded at the Triennial International Congress on Acoustics (ICA) to an individual who is relatively early in his/her professional career, and who has been active in the affairs of Acoustics and has contributed substantially, through outstanding published papers, to the advancement of theoretical or applied acoustics or both. The award can be withheld if candidates do not meet the requirements of excellence but is normally awarded to one or more scientists in the same year. The Award consists of a Certificate with citation, an engraved Medal, and a Cash Prize. The prestige is comparable to that of the Fields Medal for Mathematics.
There exist other awards, German Acoustical Society Awards, Institute of Acoustics Society Awards and Acoustical Society of America Awards in acoustics which are usually focussing either on scientists from one country or in one particular field in acoustics or on members of one acoustical organization only and which are normally intended as lifetime achievement awards and/or awards for service to a particular society.
List of past recipients:
2016 Frank A. Russo, Ryerson University, Canadian citizen
2013 Tapio Lokki, Aalto University School of Science, Finish citizen
2010 Torsten Dau, Technical University of Denmark, German citizen
2007 Nico F. Declercq, Georgia Institute of Technology, Belgian citizen
2004 Timothy Leighton, University of Southampton, UK citizen
2004 Oleg Sapozhnikov, Moscow State University, Russian citizen

References
External links
Official website",Category:Acoustics,1
88,89,Frequency,"Frequency is the number of occurrences of a repeating event per unit of time. It is also referred to as temporal frequency, which emphasizes the contrast to spatial frequency and angular frequency. The period is the duration of time of one cycle in a repeating event, so the period is the reciprocal of the frequency. For example, if a newborn baby's heart beats at a frequency of 120 times a minute, its period—the time interval between beats—is half a second (that is, 60 seconds divided by 120 beats). Frequency is an important parameter used in science and engineering to specify the rate of oscillatory and vibratory phenomena, such as mechanical vibrations, audio (sound) signals, radio waves, and light.

Definitions
For cyclical processes, such as rotation, oscillations, or waves, frequency is defined as a number of cycles per unit time. In physics and engineering disciplines, such as optics, acoustics, and radio, frequency is usually denoted by a Latin letter f or by the Greek letter 
  
    
      
        ?
      
    
    {\displaystyle \nu }
   or ? (nu) (see e.g. Planck's formula).
The relation between the frequency and the period 
  
    
      
        T
      
    
    {\displaystyle T}
   of a repeating event or oscillation is given by

  
    
      
        f
        =
        
          
            1
            T
          
        
        .
      
    
    {\displaystyle f={\frac {1}{T}}.}

Units
The SI unit of frequency is the hertz (Hz), named after the German physicist Heinrich Hertz; one hertz means that an event repeats once per second. A previous name for this unit was cycles per second (cps). The SI unit for period is the second.
A traditional unit of measure used with rotating mechanical devices is revolutions per minute, abbreviated r/min or rpm. 60 rpm equals one hertz.

Period versus frequency
As a matter of convenience, longer and slower waves, such as ocean surface waves, tend to be described by wave period rather than frequency. Short and fast waves, like audio and radio, are usually described by their frequency instead of period. These commonly used conversions are listed below:

Related types of frequency
Angular frequency, usually denoted by the Greek letter ? (omega), is defined as the rate of change of angular displacement, ?, (during rotation), or the rate of change of the phase of a sinusoidal waveform (e.g. in oscillations and waves), or as the rate of change of the argument to the sine function:

  
    
      
        y
        (
        t
        )
        =
        sin
        ?
        
          (
          
            ?
            (
            t
            )
          
          )
        
        =
        sin
        ?
        (
        ?
        t
        )
        =
        sin
        ?
        (
        2
        
          ?
        
        f
        t
        )
      
    
    {\displaystyle y(t)=\sin \left(\theta (t)\right)=\sin(\omega t)=\sin(2\mathrm {\pi } ft)}
  

  
    
      
        
          
            
              
                d
              
              ?
            
            
              
                d
              
              t
            
          
        
        =
        ?
        =
        2
        
          ?
        
        f
      
    
    {\displaystyle {\frac {\mathrm {d} \theta }{\mathrm {d} t}}=\omega =2\mathrm {\pi } f}
  

Angular frequency is commonly measured in radians per second (rad/s) but, for discrete-time signals, can also be expressed as radians per sample time, which is a dimensionless quantity. Angular frequency (in radians) is larger than regular frequency (in Hz) by a factor of 2?.
Spatial frequency is analogous to temporal frequency, but the time axis is replaced by one or more spatial displacement axes. E.g.:

  
    
      
        y
        (
        t
        )
        =
        sin
        ?
        
          (
          
            ?
            (
            t
            ,
            x
            )
          
          )
        
        =
        sin
        ?
        (
        ?
        t
        +
        k
        x
        )
      
    
    {\displaystyle y(t)=\sin \left(\theta (t,x)\right)=\sin(\omega t+kx)}
  

  
    
      
        
          
            
              
                d
              
              ?
            
            
              
                d
              
              x
            
          
        
        =
        k
      
    
    {\displaystyle {\frac {\mathrm {d} \theta }{\mathrm {d} x}}=k}
  

Wavenumber, k, is the spatial frequency analogue of angular temporal frequency and is measured in radians per meter. In the case of more than one spatial dimension, wavenumber is a vector quantity.

In wave propagation
For periodic waves in nondispersive media (that is, media in which the wave speed is independent of frequency), frequency has an inverse relationship to the wavelength, ? (lambda). Even in dispersive media, the frequency f of a sinusoidal wave is equal to the phase velocity v of the wave divided by the wavelength ? of the wave:

  
    
      
        f
        =
        
          
            v
            ?
          
        
        .
      
    
    {\displaystyle f={\frac {v}{\lambda }}.}
  
In the special case of electromagnetic waves moving through a vacuum, then v = c, where c is the speed of light in a vacuum, and this expression becomes:

  
    
      
        f
        =
        
          
            c
            ?
          
        
        .
      
    
    {\displaystyle f={\frac {c}{\lambda }}.}
  
When waves from a monochrome source travel from one medium to another, their frequency remains the same—only their wavelength and speed change.

Measurement
Measurement of frequency can done in the following ways,

Counting
Calculating the frequency of a repeating event is accomplished by counting the number of times that event occurs within a specific time period, then dividing the count by the length of the time period. For example, if 71 events occur within 15 seconds the frequency is:

  
    
      
        f
        =
        
          
            71
            
              15
              
              
                s
              
            
          
        
        ?
        4.7
        
        
          Hz
        
      
    
    {\displaystyle f={\frac {71}{15\,{\text{s}}}}\approx 4.7\,{\text{Hz}}}
  
If the number of counts is not very large, it is more accurate to measure the time interval for a predetermined number of occurrences, rather than the number of occurrences within a specified time. The latter method introduces a random error into the count of between zero and one count, so on average half a count. This is called gating error and causes an average error in the calculated frequency of ?f = 1/(2 Tm), or a fractional error of ?f / f = 1/(2 f Tm) where Tm is the timing interval and f is the measured frequency. This error decreases with frequency, so it is a problem at low frequencies where the number of counts N is small.

Stroboscope
An older method of measuring the frequency of rotating or vibrating objects is to use a stroboscope. This is an intense repetitively flashing light (strobe light) whose frequency can be adjusted with a calibrated timing circuit. The strobe light is pointed at the rotating object and the frequency adjusted up and down. When the frequency of the strobe equals the frequency of the rotating or vibrating object, the object completes one cycle of oscillation and returns to its original position between the flashes of light, so when illuminated by the strobe the object appears stationary. Then the frequency can be read from the calibrated readout on the stroboscope. A downside of this method is that an object rotating at an integral multiple of the strobing frequency will also appear stationary.

Frequency counter
Higher frequencies are usually measured with a frequency counter. This is an electronic instrument which measures the frequency of an applied repetitive electronic signal and displays the result in hertz on a digital display. It uses digital logic to count the number of cycles during a time interval established by a precision quartz time base. Cyclic processes that are not electrical in nature, such as the rotation rate of a shaft, mechanical vibrations, or sound waves, can be converted to a repetitive electronic signal by transducers and the signal applied to a frequency counter. Frequency counters can currently cover the range up to about 100 GHz. This represents the limit of direct counting methods; frequencies above this must be measured by indirect methods.

Heterodyne methods
Above the range of frequency counters, frequencies of electromagnetic signals are often measured indirectly by means of heterodyning (frequency conversion). A reference signal of a known frequency near the unknown frequency is mixed with the unknown frequency in a nonlinear mixing device such as a diode. This creates a heterodyne or ""beat"" signal at the difference between the two frequencies. If the two signals are close together in frequency the heterodyne is low enough to be measured by a frequency counter. This process only measures the difference between the unknown frequency and the reference frequency. To reach higher frequencies, several stages of heterodyning can be used. Current research is extending this method to infrared and light frequencies (optical heterodyne detection).

Examples
Light
Visible light is an electromagnetic wave, consisting of oscillating electric and magnetic fields traveling through space. The frequency of the wave determines its color: 4×1014 Hz is red light, 8×1014 Hz is violet light, and between these (in the range 4-8×1014 Hz) are all the other colors of the visible spectrum. An electromagnetic wave can have a frequency less than 4×1014 Hz, but it will be invisible to the human eye; such waves are called infrared (IR) radiation. At even lower frequency, the wave is called a microwave, and at still lower frequencies it is called a radio wave. Likewise, an electromagnetic wave can have a frequency higher than 8×1014 Hz, but it will be invisible to the human eye; such waves are called ultraviolet (UV) radiation. Even higher-frequency waves are called X-rays, and higher still are gamma rays.
All of these waves, from the lowest-frequency radio waves to the highest-frequency gamma rays, are fundamentally the same, and they are all called electromagnetic radiation. They all travel through a vacuum at the same speed (the speed of light), giving them wavelengths inversely proportional to their frequencies.

  
    
      
        
          c
          =
          f
          ?
        
      
    
    {\displaystyle \displaystyle c=f\lambda }
  
where c is the speed of light (c in a vacuum, or less in other media), f is the frequency and ? is the wavelength.
In dispersive media, such as glass, the speed depends somewhat on frequency, so the wavelength is not quite inversely proportional to frequency.

Sound
Sound propagates as mechanical vibration waves of pressure and displacement, in air or other substances. Frequency is the property of sound that most determines pitch.
The frequencies an ear can hear are limited to a specific range of frequencies. The audible frequency range for humans is typically given as being between about 20 Hz and 20,000 Hz (20 kHz), though the high frequency limit usually reduces with age. Other species have different hearing ranges. For example, some dog breeds can perceive vibrations up to 60,000 Hz.
In many media, such as air, the speed of sound is approximately independent of frequency, so the wavelength of the sound waves (distance between repetitions) is approximately inversely proportional to frequency.

Line current
In Europe, Africa, Australia, Southern South America, most of Asia, and Russia, the frequency of the alternating current in household electrical outlets is 50 Hz (close to the tone G), whereas in North America and Northern South America, the frequency of the alternating current in household electrical outlets is 60 Hz (between the tones B? and B; that is, a minor third above the European frequency). The frequency of the 'hum' in an audio recording can show where the recording was made, in countries using a European, or an American, grid frequency.

See also
Notes and references
Further reading
Giancoli, D.C. (1988). Physics for Scientists and Engineers (2nd ed.). Prentice Hall. ISBN 0-13-669201-X.

External links
Conversion: frequency to wavelength and back
Conversion: period, cycle duration, periodic time to frequency
Keyboard frequencies = naming of notes - The English and American system versus the German system
Teaching resource for 14-16yrs on sound including frequency
A simple tutorial on how to build a frequency meter
Frequency - diracdelta.co.uk – JavaScript calculation.
A frequency generator with sound, useful for hearing tests",Category:Articles needing additional references from February 2015,1
89,90,Category:Acoustics journals,,Category:Physics journals,1
90,91,Environmental noise,"Environmental noise is the summary of noise pollution from outside, caused by transport, industrial and recreational activities.
Noise is frequently described as 'unwanted sound', and, within this context, environmental noise is generally present in some form in all areas of human activity. The effects in humans of exposure to environmental noise may vary from emotional to physiological and psychological.
Noise at low levels is not necessarily harmful; environmental noise can also convey a sense of liveliness in an area, and is not then always considered 'unwanted'. However, the adverse effects of noise exposure (i.e. noise pollution) could include: interference with speech or other 'desired' sounds, annoyance, sleep disturbance, anxiety, hearing damage and stress-related cardiovascular health problems.

As a result, environmental noise is studied, regulated and monitored by many governments and institutions. This creates a number of different occupations. The basis of all decisions is supported by the objective and accurate measurement of noise. Noise is measured in decibels (dB) using a pattern-approved sound level meter. The measurements are typically taken over a period of weeks, in all weather conditions.

Environmental noise emission
Noise from transportation is typically emitted by the machinery (e.g. the engine or exhaust) and aerodynamic noise (see aerodynamics and aircraft noise) caused by the compression and friction in the air around the vessel during motion.
Industrial and recreational noise could be generated by a multitude of different sources and processes.
Sound propagation outdoors is subject to meteorological effects (e.g. wind, temperature) that affect the distance, speed, and direction with which environmental noise travels from a source to a listener.

Environmental noise policy and regulation
European Union
The European Union has a special definition based on the European directive 2002/49/EC article 10.1. This directive gives a definition for environmental noise. The main target is an integrated noise management.
The implementation is divided into phases: In the first phase, the member states shall inform about major roads with more than six million vehicles a year, major railways with more than 60,000 trains per year, major airports with more than 50,000 movements per year and metropolitan areas with more than 250,000 inhabitants. In the second phase, these numbers are halved; only the criteria for airports remains unchanged. In the third and the following phases, the methods for calculation of the noise levels will change while the criteria remain unchanged. Each phase consists of three steps: the collection of the data from the main sources of noise, strategic noise maps and action plans.

Austria
In Austria the institution which is responsible for the noise sources is also responsible for the noise maps concerning these sources. This means that the Federation is responsible for the federal roads and each state is responsible for the country's roads.

France
France reported 24 metropolitan areas, Paris was the biggest with 9.6 million inhabitants and 272 square kilometres.

Germany
Germany implemented national regulations in 2005 and 2006 and reported 27 metropolitan areas in the first phase: Berlin was the biggest with 3.39 million inhabitants and 889 square kilometres, Hamburg the largest with 1,045 square kilometres and 2 million inhabitants. The smallest was Gelsenkirchen with 270,000 inhabitants and 105 square kilometres. In the national legislation, noise resulting from recreational activities like sports and leisure is not considered as environmental noise.

United Kingdom
The United Kingdom reported a total of 28 metropolitan areas, where London is the largest with 8.3 million inhabitants. The majority of metropolitan areas are located in England; in Scotland and Wales there are each two, in Northern Ireland only the capital Belfast.

See also
Acoustical engineering
Noise control
Noise calculation
Ambience (sound recording)
Buy Quiet
General:
Health effects from noise
Noise pollution
Environmental health

Notes
References
Report from the Commission to the European Parliament and the Council concerning Directive 2002/49/EC
Directive 2002/49/EC of the European Parliament and of the Council of 25 June 2002 relating to the assessment and management of environmental noise

External links
Noise effects. Beyond annoyance
Noise Observation and Information Service for Europe
Information: Lärm – ein Problem in Europa / Noise – a problem in Europe
NIOSH Buy Quiet Topic Page",Category:Noise pollution,1
91,92,Virtual hammock,,Category:Acoustics,1
92,93,Particle acceleration,"In a compressible sound transmission medium - mainly air - air particles get an accelerated motion: the particle acceleration or sound acceleration with the symbol a in metre/second2. In acoustics or physics, acceleration (symbol: a) is defined as the rate of change (or time derivative) of velocity. It is thus a vector quantity with dimension length/time2. In SI units, this is m/s2.
To accelerate an object (air particle) is to change its velocity over a period. Acceleration is defined technically as ""the rate of change of velocity of an object with respect to time"" and is given by the equation

  
    
      
        
          a
        
        =
        
          
            
              d
              
                v
              
            
            
              d
              t
            
          
        
      
    
    {\displaystyle \mathbf {a} ={d\mathbf {v}  \over dt}}
  
where
a is the acceleration vector
v is the velocity vector expressed in m/s
t is time expressed in seconds.
This equation gives a the units of m/(s·s), or m/s2 (read as ""metres per second per second"", or ""metres per second squared"").
An alternative equation is:

  
    
      
        
          
            
              a
              ¯
            
          
        
        =
        
          
            
              
                v
              
              ?
              
                u
              
            
            t
          
        
      
    
    {\displaystyle \mathbf {\bar {a}} ={\mathbf {v} -\mathbf {u}  \over t}}
  
where

  
    
      
        
          
            
              a
              ¯
            
          
        
      
    
    {\displaystyle \mathbf {\bar {a}} }
   is the average acceleration (m/s2)

  
    
      
        
          u
        
      
    
    {\displaystyle \mathbf {u} }
   is the initial velocity (m/s)

  
    
      
        
          v
        
      
    
    {\displaystyle \mathbf {v} }
   is the final velocity (m/s)

  
    
      
        t
      
    
    {\displaystyle t}
   is the time interval (s)
Transverse acceleration (perpendicular to velocity) causes change in direction. If it is constant in magnitude and changing in direction with the velocity, we get a circular motion. For this centripetal acceleration we have

  
    
      
        
          a
        
        =
        ?
        
          
            
              v
              
                2
              
            
            r
          
        
        
          
            
              r
            
            r
          
        
        =
        ?
        
          ?
          
            2
          
        
        
          r
        
      
    
    {\displaystyle \mathbf {a} =-{\frac {v^{2}}{r}}{\frac {\mathbf {r} }{r}}=-\omega ^{2}\mathbf {r} }
  
One common unit of acceleration is g-force, one g being the acceleration caused by the gravity of Earth.
In classical mechanics, acceleration 
  
    
      
        a
         
      
    
    {\displaystyle a\ }
   is related to force 
  
    
      
        F
         
      
    
    {\displaystyle F\ }
   and mass 
  
    
      
        m
         
      
    
    {\displaystyle m\ }
   (assumed to be constant) by way of Newton's second law:

  
    
      
        F
        =
        m
        ?
        a
      
    
    {\displaystyle F=m\cdot a}

Equations in terms of other measurements
The Particle acceleration of the air particles a in m/s2 of a plain sound wave is:

  
    
      
        a
        =
        ?
        ?
        
          ?
          
            2
          
        
        =
        v
        ?
        ?
        =
        
          
            
              p
              ?
              ?
            
            Z
          
        
        =
        ?
        
          
            
              J
              Z
            
          
        
        =
        ?
        
          
            
              E
              ?
            
          
        
        =
        ?
        
          
            
              
                P
                
                  a
                  c
                
              
              
                Z
                ?
                A
              
            
          
        
      
    
    {\displaystyle a=\delta \cdot \omega ^{2}=v\cdot \omega ={\frac {p\cdot \omega }{Z}}=\omega {\sqrt {\frac {J}{Z}}}=\omega {\sqrt {\frac {E}{\rho }}}=\omega {\sqrt {\frac {P_{ac}}{Z\cdot A}}}}

See also
Sound
Sound particle
Particle displacement
Particle velocity

External links
Relationships of acoustic quantities associated with a plane progressive acoustic sound wave - pdf",Category:All articles lacking sources,1
93,94,Category:Acoustic measurement,,Category:Acoustics,1
94,95,Sound generator,,Category:Articles needing additional references from September 2009,1
95,96,Robinson–Dadson curves,"The Robinson–Dadson curves are one of many sets of equal-loudness contours for the human ear, determined experimentally by D. W. Robinson and R. S. Dadson, and reported in a paper entitled ""A re-determination of the equal-loudness relations for pure tones"" in Br. J. Appl. Phys. 7, 166-181 (1956).
Until recently, it was common to see the term 'Fletcher–Munson' used to refer to equal-loudness contours generally, even though the re-determination carried out by Robinson and Dadson in 1956, became the basis for an ISO standard ISO 226 which was only revised recently.
It is now better to use the term 'Equal-loudness contours' as the generic term, especially as a recent survey by ISO redefined the curves in a new standard, ISO 226 :2003.
According to the ISO report, the Robinson-Dadson results were the odd one out, differing more from the current standard than did the Fletcher–Munson curves. It comments that it is fortunate that the 40-Phon Fletcher-Munson curve on which the A-weighting standard was based turns out to have been in good agreement with modern determinations.
The article also comments on the large differences apparent in the low-frequency region, which remain unexplained. Possible explanations are:
The equipment used was not properly calibrated.
The criteria used for judging equal loudness (which is tricky) differed.
Different races actually vary greatly in this respect (possible, and most recent determinations were by the Japanese).
Subjects were not properly rested for days in advance, or were exposed to loud noise in travelling to the tests which tensed the tensor timpani and stapedius muscles controlling low-frequency mechanical coupling.

See also
dB(A)
CCIR (ITU) 468 Noise Weighting

External links
ISO Standard
Fletcher–Munson is not Robinson–Dadson
Full Revision of International Standards for Equal-Loudness Level Contours (ISO 226)
Hearing curves and on-line hearing test
Equal-loudness contours by Robinson and Dadson",Category:Audio engineering,1
96,97,Semantic audio,"Semantic audio is the extraction of symbols or meaning from an audio stream. Speech recognition is an important semantic audio application. But for speech, other semantic operations include language, speaker or gender identification. For more general audio or music, it includes identifying a piece of music (e.g. Shazam (service)) or a movie soundtrack.
Areas of research in semantic audio include the ability to label an audio waveform with where the harmonies change and what they are and where material is repeated and what instruments are playing.

External links
AES 42nd Conference on Semantic Audio
AES 53rd Conference on Semantic Audio",Category:Audio engineering,1
97,98,Loudness,"In acoustics, loudness is the subjective perception of sound pressure. More formally, it is defined as, ""That attribute of auditory sensation in terms of which sounds can be ordered on a scale extending from quiet to loud."" The relation of physical attributes of sound to perceived loudness consists of physical, physiological and psychological components. The study of apparent loudness is included in the topic of psychoacoustics and employs methods of psychophysics.
In different industries, loudness may have different meanings and different measurement standards. Some definitions such as LKFS refer to relative loudness of different segments of electronically reproduced sounds such as for broadcasting and cinema. Others, such as ISO 532A (Stevens loudness, measured in sones), ISO 532B (Zwicker loudness), DIN 45631 and ASA/ANSI S3.4, have a more general scope and are often used to characterize loudness of environmental noise.
Loudness, a subjective measure, often confused with physical measures of sound strength such as sound pressure, sound pressure level (in decibels), sound intensity or sound power. Filters such as A-weighting and ITU-R BS.1770 attempt to compensate measurements to correspond to loudness as perceived by the typical human.

Explanation
The perception of loudness is related to sound pressure level (SPL), frequency content and duration of a sound. The human auditory system averages the effects of SPL over a 600–1000 ms interval. A sound of constant SPL will be perceived to increase in loudness as samples of duration 20, 50, 100, 200 ms are heard, up to a duration of about 1 second at which point the perception of loudness will stabilize. For sounds of duration greater than 1 second, the moment-by-moment perception of loudness will be related to the average loudness during the preceding 600–1000 ms.
For sounds having a duration longer than 1 second, the relationship between SPL and loudness of a single tone can be approximated by Stevens' power law in which SPL has an exponent of 0.6. More precise measurements indicate that loudness increases with a higher exponent at low and high levels and with a lower exponent at moderate levels.

The sensitivity of the human ear changes as a function of frequency, as shown in the equal-loudness graph. Each line on this graph shows the SPL required for frequencies to be perceived as equally loud, and different curves pertain to different sound pressure levels. It also shows that humans with normal hearing are most sensitive to sounds around 2–4 kHz, with sensitivity declining to either side of this region. A complete model of the perception of loudness will include the integration of SPL by frequency.
Historically, loudness was measured using an ""ear-balance"" audiometer in which the amplitude of a sine wave was adjusted by the user to equal the perceived loudness of the sound being evaluated. Contemporary standards for measurement of loudness are based on summation of energy in critical bands as described in IEC 532, DIN 45631 and ASA/ANSI S3.4. A distinction is made between stationary loudness (sounds that remain sensibly constant) and non-stationary (sound sources that move in space or change amplitude over time.)

Hearing loss
When sensorineural hearing loss (damage to the cochlea or in the brain) is present, the perception of loudness is altered. Sounds at low levels (often perceived by those without hearing loss as relatively quiet) are no longer audible to the hearing impaired, but sounds at high levels often are perceived as having the same loudness as they would for an unimpaired listener. This phenomenon can be explained by two theories: loudness grows more rapidly for these listeners than normal listeners with changes in level. This theory is called ""loudness recruitment"" and has been accepted as the classical explanation. More recently, it has been proposed that some listeners with sensorineural hearing loss may in fact exhibit a normal rate of loudness growth, but instead have an elevated loudness at their threshold. That is, the softest sound that is audible to these listeners is louder than the softest sound audible to normal listeners. This theory is called ""softness imperception"", a term coined by Mary Florentine.

Compensation
The ""loudness"" control on some consumer stereos alters the frequency response curve to correspond roughly with the equal loudness characteristic of the ear. Loudness compensation is intended to make the recorded music sound more natural when played at a lower levels by boosting low frequencies, to which the ear is less sensitive at lower sound pressure levels.

Normalization
Loudness normalization is a specific type of audio normalization that equalizes perceived level such that, for instance, commercials do not sound louder than television programs. Loudness normalization schemes exist for a number of audio applications.

Broadcast
Commercial Advertisement Loudness Mitigation Act
European Broadcasting Union R128

Movie and home theaters
Dialnorm

Music playback
Sound Check in iTunes
ReplayGain

Measurement
Historically Sone (loudness N) and Phon (loudness level L) units have been used to measure loudness.
A-weighting follows human sensitivity to sound and describes relative perceived loudness for at quiet to moderate speech levels, around 40 phons. However, physiological loudness perception is a much more complex process than can be captured with a single correction curve. Not only do equal-loudness contours vary with intensity, but perceived loudness of a complex sound depends on whether its spectral components are closely or widely spaced in frequency. When generating neural impulses in response to sounds of one frequency, the ear is less sensitive to nearby frequencies, which are said to be in the same critical band. Sounds containing spectral components in many critical bands are perceived as louder even if the total sound pressure remains constant.
Relative loudness monitoring in production is measured in accordance with ITU-R BS.1770 in units of LKFS.
Work began on ITU-R BS.1770 in 2001 after 0 dBFS+ level distortion in converters and lossy codecs had become evident; and the original Leq(RLB) loudness metric was proposed by Gilbert Soloudre in 2003.
Based on data from subjective listening tests, Leq(RLB) was compared against numerous other algorithms where it did remarkably well. After modification of the frequency weighting, the measurement was made multi-channel (monaural to 5.1 surround sound). CBC, Dolby and TC Electronics and numerous broadcasters contributed to the listening tests.
To make the loudness metric cross-genre friendly, a relative measurement gate was added. This work was carried out in 2008 by the EBU. The improvements were brought back into BS.1770-2. ITU subsequently updated the true-peak metric (BS.1770-3) and added provision for more audio channels, for instance 22.2 surround sound (BS.1770-4).

See also
Loudness war
Sending loudness rating
Volume in acoustics is related to:
Amplitude
Sound pressure
Dynamics

Notes


== References ==",Category:Acoustics,1
98,99,Spectral splatter,"In radio electronics or acoustics, spectral splatter (also called switch noise) refers to spurious emissions that result from an abrupt change in the transmitted signal, usually when transmission is started or stopped.
For example, a device transmitting a sine wave produces a single peak in the frequency spectrum; however, if the device abruptly starts or stops transmitting this sine wave, it will emit noise at frequencies other than the frequency of the sine wave. This noise is known as spectral splatter.
When the signal is represented in the time domain, an abrupt change may not be visually apparent; in the frequency domain, however, the abrupt change causes the appearance of spikes at various frequencies.
A sharper change in the time domain usually results in more spikes or stronger spikes in the frequency domain. Spectral splatter can thus be reduced by making the change more smooth. Controlling the power ramp shape (i.e. the way in which the signal increases (""power-on ramp"") or falls off (""power-down ramp"")) can help reduce the splatter. In some cases one can use a filter to remove unwanted emissions. Note that a completely abrupt change (in the mathematical sense) is not possible in physical reality; the change is always somewhat smoothed naturally, for example due to the capacitance (in electronics) or inertia (in acoustics) of the components involved.
In radio electronics, the need to minimize spectral splatter arises because signals are usually required by government regulations to be contained in a particular frequency band, defined by a spectral mask. Spectral splatter can cause emissions that violate this mask.

See also
Gibbs phenomenon",Category:All articles lacking sources,1
99,100,Acousto-electric effect,"Acousto-electric effect is a nonlinear phenomenon of generation of electric current in a piezo-electric semiconductor by a propagating acoustic wave. The generated electric current is proportional to the intensity of the acoustic wave and to the value of its electron-induced attenuation. The effect was theoretically predicted in 1953 by Parmenter. Its first experimental observation was reported in 1957 by Weinreich and White.

See also
Physical acoustics
Semiconductors
Piezoelectricity
Elastic waves

References

R.H. Parmenter, The acousto-electric effect, Physical Review, vol.89, pp. 990–998 (1953).
G. Weinreich and H.G. White, Observation of the acoustoelectric effect, Physical Review, vol. 106, pp. 1104–1106 (1957).",Category:Acoustics,1
100,101,Acoustic dispersion,"Acoustic dispersion is the phenomenon of a sound wave separating into its component frequencies as it passes through a material. The phase velocity of the sound wave is viewed as a function of frequency. Hence, separation of component frequencies is measured by the rate of change in phase velocities as the radiated waves pass through a given medium.

Broadband transmission method
A widely used technique for determining acoustic dispersion is a broadband transmission method. This technique was originally introduced in 1978 and has been employed to study the dispersion properties of metal (1978), epoxy resin (1986), paper materials (1993) ultra-sound contrast agent (1998). In 1990 and 1993 this method confirmed the Kramers–Kronig relation for acoustic waves.
Application of this method requires the measurements of a reference velocity to obtain values for the acoustic dispersion. This is accomplished by determining (usually) the speed of the sound in water, the thickness of the specimen, and the phase spectrum of each of the two transmitted ultrasound pulses.

Dispersive attenuation
Acoustic attenuation

See also
Dispersion (optics)


== References ==",Category:Acoustics,1
101,102,Acoustic phonon,"In physics, a phonon is a collective excitation in a periodic, elastic arrangement of atoms or molecules in condensed matter, like solids and some liquids. Often designated a quasiparticle, it represents an excited state in the quantum mechanical quantization of the modes of vibrations of elastic structures of interacting particles.
Phonons play a major role in many of the physical properties of condensed matter, like thermal conductivity and electrical conductivity. The study of phonons is an important part of condensed matter physics.
The concept of phonons was introduced in 1932 by Soviet physicist Igor Tamm. The name phonon comes from the Greek word ???? (phon?), which translates to sound or voice because long-wavelength phonons give rise to sound. Shorter-wavelength higher-frequency phonons are responsible for the majority of the thermal capacity of solids.

Definition
A phonon is a quantum mechanical description of an elementary vibrational motion in which a lattice of atoms or molecules uniformly oscillates at a single frequency. In classical mechanics this designates a normal mode of vibration. Normal modes are important because any arbitrary lattice vibration can be considered to be a superposition of these elementary vibration modes (cf. Fourier analysis). While normal modes are wave-like phenomena in classical mechanics, phonons have particle-like properties too, in a way related to the wave–particle duality of quantum mechanics.

Lattice dynamics
The equations in this section do not use axioms of quantum mechanics but instead use relations for which there exists a direct correspondence in classical mechanics.
For example: a rigid regular, crystalline (not amorphous), lattice is composed of N particles. These particles may be atoms or molecules. N is a large number, say of the order of 1023, or on the order of Avogadro's number for a typical sample of a solid. Since the lattice is rigid, the atoms must be exerting forces on one another to keep each atom near its equilibrium position. These forces may be Van der Waals forces, covalent bonds, electrostatic attractions, and others, all of which are ultimately due to the electric force. Magnetic and gravitational forces are generally negligible. The forces between each pair of atoms may be characterized by a potential energy function V that depends on the distance of separation of the atoms. The potential energy of the entire lattice is the sum of all pairwise potential energies:

  
    
      
        
          ?
          
            i
            ?
            j
          
        
        V
        
          (
          
            r
            
              i
            
          
          ?
          
            r
            
              j
            
          
          )
        
      
    
    {\displaystyle \sum _{i\neq j}V\left(r_{i}-r_{j}\right)}
  
where ri is the position of the ith atom, and V is the potential energy between two atoms.
It is difficult to solve this many-body problem explicitly in either classical or quantum mechanics. In order to simplify the task, two important approximations are usually imposed. First, the sum is only performed over neighboring atoms. Although the electric forces in real solids extend to infinity, this approximation is still valid because the fields produced by distant atoms are effectively screened. Secondly, the potentials V are treated as harmonic potentials. This is permissible as long as the atoms remain close to their equilibrium positions. Formally, this is accomplished by Taylor expanding V about its equilibrium value to quadratic order, giving V proportional to the displacement x2 and the elastic force simply proportional to x. The error in ignoring higher order terms remains small if x remains close to the equilibrium position.
The resulting lattice may be visualized as a system of balls connected by springs. The following figure shows a cubic lattice, which is a good model for many types of crystalline solid. Other lattices include a linear chain, which is a very simple lattice which we will shortly use for modeling phonons. (For other common lattices, see crystal structure.)

The potential energy of the lattice may now be written as

  
    
      
        
          ?
          
            {
            i
            j
            }
            (
            
              n
              n
            
            )
          
        
        
          
            
              1
              2
            
          
        
        m
        
          ?
          
            2
          
        
        
          
            (
            
              R
              
                i
              
            
            ?
            
              R
              
                j
              
            
            )
          
          
            2
          
        
        .
      
    
    {\displaystyle \sum _{\{ij\}(\mathrm {nn} )}{\tfrac {1}{2}}m\omega ^{2}\left(R_{i}-R_{j}\right)^{2}.}
  
Here, ? is the natural frequency of the harmonic potentials, which are assumed to be the same since the lattice is regular. Ri is the position coordinate of the ith atom, which we now measure from its equilibrium position. The sum over nearest neighbors is denoted (nn).

Lattice waves
Due to the connections between atoms, the displacement of one or more atoms from their equilibrium positions give rise to a set of vibration waves propagating through the lattice. One such wave is shown in the figure to the right. The amplitude of the wave is given by the displacements of the atoms from their equilibrium positions. The wavelength ? is marked.
There is a minimum possible wavelength, given by twice the equilibrium separation a between atoms. Any wavelength shorter than this can be mapped onto a wavelength longer than 2a, due to the periodicity of the lattice. This can be thought as one consequence of Nyquist–Shannon sampling theorem, the lattice points are viewed as the ""sampling points"" of a continuous wave.
Not every possible lattice vibration has a well-defined wavelength and frequency. However, the normal modes do possess well-defined wavelengths and frequencies.

One-dimensional lattice
In order to simplify the analysis needed for a 3-dimensional lattice of atoms, it is convenient to model a 1-dimensional lattice or linear chain. This model is complex enough to display the salient features of phonons.

Classical treatment
The forces between the atoms are assumed to be linear and nearest-neighbour, and they are represented by an elastic spring. Each atom is assumed to be a point particle and the nucleus and electrons move in step (adiabatic approximation):

n ? 1   n   n + 1   ?   a   ?

···o++++++o++++++o++++++o++++++o++++++o++++++o++++++o++++++o++++++o···

?? ? ???
un ? 1 un un + 1

where n labels the nth atom out of a total of N, a is the distance between atoms when the chain is in equilibrium, and un the displacement of the nth atom from its equilibrium position.
If C is the elastic constant of the spring and m the mass of the atom, then the equation of motion of the nth atom is

  
    
      
        ?
        2
        C
        
          u
          
            n
          
        
        +
        C
        
          (
          
            u
            
              n
              +
              1
            
          
          +
          
            u
            
              n
              ?
              1
            
          
          )
        
        =
        m
        
          
            
              
                d
                
                  2
                
              
              
                u
                
                  n
                
              
            
            
              d
              
                t
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle -2Cu_{n}+C\left(u_{n+1}+u_{n-1}\right)=m{\frac {d^{2}u_{n}}{dt^{2}}}.}
  
This is a set of coupled equations. Since the solutions are expected to be oscillatory, new coordinates are defined by a discrete Fourier transform, in order to decouple them.
Put

  
    
      
        
          u
          
            n
          
        
        =
        
          ?
          
            N
            a
            k
            
              /
            
            2
            ?
            =
            1
          
          
            N
          
        
        
          Q
          
            k
          
        
        
          e
          
            i
            k
            n
            a
          
        
      
    
    {\displaystyle u_{n}=\sum _{Nak/2\pi =1}^{N}Q_{k}e^{ikna}}
  
Here, na corresponds and devolves to the continuous variable x of scalar field theory. The Qk are known as the normal coordinates, continuum field modes ?k. Substitution into the equation of motion produces the following decoupled equations (this requires a significant manipulation using the orthonormality and completeness relations of the discrete Fourier transform,

  
    
      
        2
        C
        (
        cos
        ?
        
          k
          a
          ?
          1
        
        )
        
          Q
          
            k
          
        
        =
        m
        
          
            
              
                d
                
                  2
                
              
              
                Q
                
                  k
                
              
            
            
              d
              
                t
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle 2C(\cos {ka-1})Q_{k}=m{\frac {d^{2}Q_{k}}{dt^{2}}}.}
  
These are the equations for harmonic oscillators which have the solution

  
    
      
        
          Q
          
            k
          
        
        =
        
          A
          
            k
          
        
        
          e
          
            i
            
              ?
              
                k
              
            
            t
          
        
        ;
        
        
          ?
          
            k
          
        
        =
        
          
            
              
                
                  2
                  C
                
                m
              
            
            (
            1
            ?
            cos
            ?
            
              k
              a
            
            )
          
        
      
    
    {\displaystyle Q_{k}=A_{k}e^{i\omega _{k}t};\qquad \omega _{k}={\sqrt {{\frac {2C}{m}}(1-\cos {ka})}}}
  
Each normal coordinate Qk represents an independent vibrational mode of the lattice with wavenumber k which is known as a normal mode.
The second equation, for ?k, is known as the dispersion relation between the angular frequency and the wavenumber. In the continuum limit, a?0, N??, with Na held fixed, un ? ?(x), a scalar field, and 
  
    
      
        ?
        (
        k
        )
        ?
        k
        a
      
    
    {\displaystyle \omega (k)\propto ka}
  . This amounts to free scalar classical field theory.

Quantum treatment
A one-dimensional quantum mechanical harmonic chain consists of N identical atoms. This is the simplest quantum mechanical model of a lattice that allows phonons to arise from it. The formalism for this model is readily generalizable to two and three dimensions.
In some contrast to the previous section, the positions of the masses are not denoted by ui, but, instead, by x1, x2…, as measured from their equilibrium positions (i.e. xi = 0 if particle i is at its equilibrium position.) In two or more dimensions, the xi are vector quantities. The Hamiltonian for this system is

  
    
      
        
          
            H
          
        
        =
        
          ?
          
            i
            =
            1
          
          
            N
          
        
        
          
            
              p
              
                i
              
              
                2
              
            
            
              2
              m
            
          
        
        +
        
          
            1
            2
          
        
        m
        
          ?
          
            2
          
        
        
          ?
          
            {
            i
            j
            }
            (
            
              n
              n
            
            )
          
        
        
          
            (
            
              x
              
                i
              
            
            ?
            
              x
              
                j
              
            
            )
          
          
            2
          
        
      
    
    {\displaystyle {\mathcal {H}}=\sum _{i=1}^{N}{\frac {p_{i}^{2}}{2m}}+{\frac {1}{2}}m\omega ^{2}\sum _{\{ij\}(\mathrm {nn} )}\left(x_{i}-x_{j}\right)^{2}}
  
where m is the mass of each atom (assuming it is equal for all), and xi and pi are the position and momentum operators, respectively, for the ith atom and the sum is made over the nearest neighbors (nn). However one expects that in a lattice there could also appear waves that behave like particles. It is customary to deal with waves in Fourier space which uses normal modes of the wavevector as variables instead coordinates of particles. The number of normal modes is same as the number of particles. However, the Fourier space is very useful given the periodicity of the system.
A set of N ""normal coordinates"" Qk may be introduced, defined as the discrete Fourier transforms of the xk and N ""conjugate momenta"" ?k defined as the Fourier transforms of the pk:

  
    
      
        
          
            
              
                
                  Q
                  
                    k
                  
                
              
              
                
                =
                
                  
                    1
                    
                      N
                    
                  
                
                
                  ?
                  
                    l
                  
                
                
                  e
                  
                    i
                    k
                    a
                    l
                  
                
                
                  x
                  
                    l
                  
                
              
            
            
              
                
                  ?
                  
                    k
                  
                
              
              
                
                =
                
                  
                    1
                    
                      N
                    
                  
                
                
                  ?
                  
                    l
                  
                
                
                  e
                  
                    ?
                    i
                    k
                    a
                    l
                  
                
                
                  p
                  
                    l
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}Q_{k}&={\frac {1}{\sqrt {N}}}\sum _{l}e^{ikal}x_{l}\\\Pi _{k}&={\frac {1}{\sqrt {N}}}\sum _{l}e^{-ikal}p_{l}.\end{aligned}}}
  
The quantity kn turns out to be the wavenumber of the phonon, i.e. 2? divided by the wavelength.
This choice retains the desired commutation relations in either real space or wavevector space

  
    
      
        
          
            
              
                
                  [
                  
                    x
                    
                      l
                    
                  
                  ,
                  
                    p
                    
                      m
                    
                  
                  ]
                
              
              
                
                =
                i
                ?
                
                  ?
                  
                    l
                    ,
                    m
                  
                
              
            
            
              
                
                  [
                  
                    Q
                    
                      k
                    
                  
                  ,
                  
                    ?
                    
                      
                        k
                        ?
                      
                    
                  
                  ]
                
              
              
                
                =
                
                  
                    1
                    N
                  
                
                
                  ?
                  
                    l
                    ,
                    m
                  
                
                
                  e
                  
                    i
                    k
                    a
                    l
                  
                
                
                  e
                  
                    ?
                    i
                    
                      k
                      ?
                    
                    a
                    m
                  
                
                
                  [
                  
                    x
                    
                      l
                    
                  
                  ,
                  
                    p
                    
                      m
                    
                  
                  ]
                
              
            
            
              
              
                
                =
                
                  
                    
                      i
                      ?
                    
                    N
                  
                
                
                  ?
                  
                    l
                  
                
                
                  e
                  
                    i
                    a
                    l
                    
                      (
                      k
                      ?
                      
                        k
                        ?
                      
                      )
                    
                  
                
                =
                i
                ?
                
                  ?
                  
                    k
                    ,
                    
                      k
                      ?
                    
                  
                
              
            
            
              
                
                  [
                  
                    Q
                    
                      k
                    
                  
                  ,
                  
                    Q
                    
                      
                        k
                        ?
                      
                    
                  
                  ]
                
              
              
                
                =
                
                  [
                  
                    ?
                    
                      k
                    
                  
                  ,
                  
                    ?
                    
                      
                        k
                        ?
                      
                    
                  
                  ]
                
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\left[x_{l},p_{m}\right]&=i\hbar \delta _{l,m}\\\left[Q_{k},\Pi _{k'}\right]&={\frac {1}{N}}\sum _{l,m}e^{ikal}e^{-ik'am}\left[x_{l},p_{m}\right]\\&={\frac {i\hbar }{N}}\sum _{l}e^{ial\left(k-k'\right)}=i\hbar \delta _{k,k'}\\\left[Q_{k},Q_{k'}\right]&=\left[\Pi _{k},\Pi _{k'}\right]=0\end{aligned}}}
  
From the general result

  
    
      
        
          
            
              
                
                  ?
                  
                    l
                  
                
                
                  x
                  
                    l
                  
                
                
                  x
                  
                    l
                    +
                    m
                  
                
              
              
                
                =
                
                  
                    1
                    N
                  
                
                
                  ?
                  
                    k
                    
                      k
                      ?
                    
                  
                
                
                  Q
                  
                    k
                  
                
                
                  Q
                  
                    
                      k
                      ?
                    
                  
                
                
                  ?
                  
                    l
                  
                
                
                  e
                  
                    i
                    a
                    l
                    
                      (
                      k
                      +
                      
                        k
                        ?
                      
                      )
                    
                  
                
                
                  e
                  
                    i
                    a
                    m
                    
                      k
                      ?
                    
                  
                
                =
                
                  ?
                  
                    k
                  
                
                
                  Q
                  
                    k
                  
                
                
                  Q
                  
                    ?
                    k
                  
                
                
                  e
                  
                    i
                    a
                    m
                    k
                  
                
              
            
            
              
                
                  ?
                  
                    l
                  
                
                
                  
                    
                      p
                      
                        l
                      
                    
                  
                  
                    2
                  
                
              
              
                
                =
                
                  ?
                  
                    k
                  
                
                
                  ?
                  
                    k
                  
                
                
                  ?
                  
                    ?
                    k
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\sum _{l}x_{l}x_{l+m}&={\frac {1}{N}}\sum _{kk'}Q_{k}Q_{k'}\sum _{l}e^{ial\left(k+k'\right)}e^{iamk'}=\sum _{k}Q_{k}Q_{-k}e^{iamk}\\\sum _{l}{p_{l}}^{2}&=\sum _{k}\Pi _{k}\Pi _{-k}\end{aligned}}}
  
The potential energy term is

  
    
      
        
          
            
              1
              2
            
          
        
        m
        
          ?
          
            2
          
        
        
          ?
          
            j
          
        
        
          
            (
            
              x
              
                j
              
            
            ?
            
              x
              
                j
                +
                1
              
            
            )
          
          
            2
          
        
        =
        
          
            
              1
              2
            
          
        
        m
        
          ?
          
            2
          
        
        
          ?
          
            k
          
        
        
          Q
          
            k
          
        
        
          Q
          
            ?
            k
          
        
        (
        2
        ?
        
          e
          
            i
            k
            a
          
        
        ?
        
          e
          
            ?
            i
            k
            a
          
        
        )
        =
        
          
            
              1
              2
            
          
        
        
          ?
          
            k
          
        
        m
        
          
            
              ?
              
                k
              
            
          
          
            2
          
        
        
          Q
          
            k
          
        
        
          Q
          
            ?
            k
          
        
      
    
    {\displaystyle {\tfrac {1}{2}}m\omega ^{2}\sum _{j}\left(x_{j}-x_{j+1}\right)^{2}={\tfrac {1}{2}}m\omega ^{2}\sum _{k}Q_{k}Q_{-k}(2-e^{ika}-e^{-ika})={\tfrac {1}{2}}\sum _{k}m{\omega _{k}}^{2}Q_{k}Q_{-k}}
  
where

  
    
      
        
          ?
          
            k
          
        
        =
        
          
            2
            
              ?
              
                2
              
            
            
              (
              1
              ?
              cos
              ?
              
                k
                a
              
              )
            
          
        
        =
        2
        ?
        
          |
          sin
          ?
          
            
              
                k
                a
              
              2
            
          
          |
        
      
    
    {\displaystyle \omega _{k}={\sqrt {2\omega ^{2}\left(1-\cos {ka}\right)}}=2\omega \left|\sin {\frac {ka}{2}}\right|}
  
The Hamiltonian may be written in wavevector space as

  
    
      
        
          
            H
          
        
        =
        
          
            1
            
              2
              m
            
          
        
        
          ?
          
            k
          
        
        
          (
          
            ?
            
              k
            
          
          
            ?
            
              ?
              k
            
          
          +
          
            m
            
              2
            
          
          
            ?
            
              k
            
            
              2
            
          
          
            Q
            
              k
            
          
          
            Q
            
              ?
              k
            
          
          )
        
      
    
    {\displaystyle {\mathcal {H}}={\frac {1}{2m}}\sum _{k}\left(\Pi _{k}\Pi _{-k}+m^{2}\omega _{k}^{2}Q_{k}Q_{-k}\right)}
  
The couplings between the position variables have been transformed away; if the Q and ? were Hermitian (which they are not), the transformed Hamiltonian would describe N uncoupled harmonic oscillators.
The form of the quantization depends on the choice of boundary conditions; for simplicity, periodic boundary conditions are imposed, defining the (N + 1)th atom as equivalent to the first atom. Physically, this corresponds to joining the chain at its ends. The resulting quantization is

  
    
      
        k
        =
        
          k
          
            n
          
        
        =
        
          
            
              2
              ?
              n
            
            
              N
              a
            
          
        
        
        
          for 
        
        n
        =
        0
        ,
        ±
        1
        ,
        ±
        2
        ,
        …
        ±
        
          
            N
            2
          
        
        .
         
      
    
    {\displaystyle k=k_{n}={\frac {2\pi n}{Na}}\quad {\mbox{for }}n=0,\pm 1,\pm 2,\ldots \pm {\frac {N}{2}}.\ }
  
The upper bound to n comes from the minimum wavelength, which is twice the lattice spacing a, as discussed above.
The harmonic oscillator eigenvalues or energy levels for the mode ?k are:

  
    
      
        
          E
          
            n
          
        
        =
        
          (
          
            
              
                1
                2
              
            
          
          +
          n
          )
        
        ?
        
          ?
          
            k
          
        
        
        n
        =
        0
        ,
        1
        ,
        2
        ,
        3
        …
      
    
    {\displaystyle E_{n}=\left({\tfrac {1}{2}}+n\right)\hbar \omega _{k}\qquad n=0,1,2,3\ldots }
  
The levels are evenly spaced at:

  
    
      
        
          
            
              1
              2
            
          
        
        ?
        ?
        ,
         
        
          
            
              3
              2
            
          
        
        ?
        ?
        ,
         
        
          
            
              5
              2
            
          
        
        ?
        ?
         
        ?
      
    
    {\displaystyle {\tfrac {1}{2}}\hbar \omega ,\ {\tfrac {3}{2}}\hbar \omega ,\ {\tfrac {5}{2}}\hbar \omega \ \cdots }
  
where 1/2?? is the zero-point energy of a quantum harmonic oscillator.
An exact amount of energy ?? must be supplied to the harmonic oscillator lattice to push it to the next energy level. In comparison to the photon case when the electromagnetic field is quantized, the quantum of vibrational energy is called a phonon.
All quantum systems show wavelike and particlelike properties simultaneously. The particle-like properties of the phonon are best understood using the methods of second quantization and operator techniques described later.

Three-dimensional lattice
This may be generalized to a three-dimensional lattice. The wavenumber k is replaced by a three-dimensional wavevector k. Furthermore, each k is now associated with three normal coordinates.
The new indices s = 1, 2, 3 label the polarization of the phonons. In the one-dimensional model, the atoms were restricted to moving along the line, so the phonons corresponded to longitudinal waves. In three dimensions, vibration is not restricted to the direction of propagation, and can also occur in the perpendicular planes, like transverse waves. This gives rise to the additional normal coordinates, which, as the form of the Hamiltonian indicates, we may view as independent species of phonons.

Dispersion relation
For a one-dimensional alternating array of two types of ion or atom of mass m1, m2 repeated periodically at a distance a, connected by springs of spring constant K, two modes of vibration result:

  
    
      
        
          ?
          
            ±
          
          
            2
          
        
        =
        K
        
          (
          
            
              1
              
                m
                
                  1
                
              
            
          
          +
          
            
              1
           ",Category:Bosons,1
102,103,Plane wave tube,,Category:Acoustics,1
103,104,Acoustics,"Acoustics is the branch of physics that deals with the study of all mechanical waves in gases, liquids, and solids including topics such as vibration, sound, ultrasound and infrasound. A scientist who works in the field of acoustics is an acoustician while someone working in the field of acoustics technology may be called an acoustical engineer. The application of acoustics is present in almost all aspects of modern society with the most obvious being the audio and noise control industries.
Hearing is one of the most crucial means of survival in the animal world, and speech is one of the most distinctive characteristics of human development and culture. Accordingly, the science of acoustics spreads across many facets of human society—music, medicine, architecture, industrial production, warfare and more. Likewise, animal species such as songbirds and frogs use sound and hearing as a key element of mating rituals or marking territories. Art, craft, science and technology have provoked one another to advance the whole, as in many other fields of knowledge. Robert Bruce Lindsay's 'Wheel of Acoustics' is a well accepted overview of the various fields in acoustics.
The word ""acoustic"" is derived from the Greek word ?????????? (akoustikos), meaning ""of or for hearing, ready to hear"" and that from ???????? (akoustos), ""heard, audible"", which in turn derives from the verb ????? (akouo), ""I hear"".
The Latin synonym is ""sonic"", after which the term sonics used to be a synonym for acoustics and later a branch of acoustics. Frequencies above and below the audible range are called ""ultrasonic"" and ""infrasonic"", respectively.

History
Early research in acoustics
In the 6th century BC, the ancient Greek philosopher Pythagoras wanted to know why some combinations of musical sounds seemed more beautiful than others, and he found answers in terms of numerical ratios representing the harmonic overtone series on a string. He is reputed to have observed that when the lengths of vibrating strings are expressible as ratios of integers (e.g. 2 to 3, 3 to 4), the tones produced will be harmonious, and the smaller the integers the more harmonious the sounds. If, for example, a string of a certain length would sound particularly harmonious with a string of twice the length (other factors being equal). In modern parlance, if a string sounds the note C when plucked, a string twice as long will sound a C an octave lower. In one system of musical tuning, the tones in between are then given by 16:9 for D, 8:5 for E, 3:2 for F, 4:3 for G, 6:5 for A, and 16:15 for B, in ascending order.
Aristotle (384-322 BC) understood that sound consisted of compressions and rarefactions of air which ""falls upon and strikes the air which is next to it..."", a very good expression of the nature of wave motion.
In about 20 BC, the Roman architect and engineer Vitruvius wrote a treatise on the acoustic properties of theaters including discussion of interference, echoes, and reverberation—the beginnings of architectural acoustics. In Book V of his De architectura (The Ten Books of Architecture) Vitruvius describes sound as a wave comparable to a water wave extended to three dimensions, which, when interrupted by obstructions, would flow back and break up following waves. He described the ascending seats in ancient theaters as designed to prevent this deterioration of sound and also recommended bronze vessels of appropriate sizes be placed in theaters to resonate with the fourth, fifth and so on, up to the double octave, in order to resonate with the more desirable, harmonious notes.

The physical understanding of acoustical processes advanced rapidly during and after the Scientific Revolution. Mainly Galileo Galilei (1564–1642) but also Marin Mersenne (1588–1648), independently, discovered the complete laws of vibrating strings (completing what Pythagoras and Pythagoreans had started 2000 years earlier). Galileo wrote ""Waves are produced by the vibrations of a sonorous body, which spread through the air, bringing to the tympanum of the ear a stimulus which the mind interprets as sound"", a remarkable statement that points to the beginnings of physiological and psychological acoustics. Experimental measurements of the speed of sound in air were carried out successfully between 1630 and 1680 by a number of investigators, prominently Mersenne. Meanwhile, Newton (1642–1727) derived the relationship for wave velocity in solids, a cornerstone of physical acoustics (Principia, 1687).

Age of Enlightenment and onward
The eighteenth century saw major advances in acoustics as mathematicians applied the new techniques of calculus to elaborate theories of sound wave propagation. In the nineteenth century the major figures of mathematical acoustics were Helmholtz in Germany, who consolidated the field of physiological acoustics, and Lord Rayleigh in England, who combined the previous knowledge with his own copious contributions to the field in his monumental work The Theory of Sound (1877). Also in the 19th century, Wheatstone, Ohm, and Henry developed the analogy between electricity and acoustics.
The twentieth century saw a burgeoning of technological applications of the large body of scientific knowledge that was by then in place. The first such application was Sabine’s groundbreaking work in architectural acoustics, and many others followed. Underwater acoustics was used for detecting submarines in the first World War. Sound recording and the telephone played important roles in a global transformation of society. Sound measurement and analysis reached new levels of accuracy and sophistication through the use of electronics and computing. The ultrasonic frequency range enabled wholly new kinds of application in medicine and industry. New kinds of transducers (generators and receivers of acoustic energy) were invented and put to use.

Fundamental concepts of acoustics
Definition
Acoustics is defined by ANSI/ASA S1.1-2013 as ""(a) Science of sound, including its production, transmission, and effects, including biological and psychological effects. (b) Those qualities of a room that, together, determine its character with respect to auditory effects.""
The study of acoustics revolves around the generation, propagation and reception of mechanical waves and vibrations.

The steps shown in the above diagram can be found in any acoustical event or process. There are many kinds of cause, both natural and volitional. There are many kinds of transduction process that convert energy from some other form into sonic energy, producing a sound wave. There is one fundamental equation that describes sound wave propagation, the acoustic wave equation, but the phenomena that emerge from it are varied and often complex. The wave carries energy throughout the propagating medium. Eventually this energy is transduced again into other forms, in ways that again may be natural and/or volitionally contrived. The final effect may be purely physical or it may reach far into the biological or volitional domains. The five basic steps are found equally well whether we are talking about an earthquake, a submarine using sonar to locate its foe, or a band playing in a rock concert.
The central stage in the acoustical process is wave propagation. This falls within the domain of physical acoustics. In fluids, sound propagates primarily as a pressure wave. In solids, mechanical waves can take many forms including longitudinal waves, transverse waves and surface waves.
Acoustics looks first at the pressure levels and frequencies in the sound wave and how the wave interacts with the environment. This interaction can be described as either a diffraction, interference or a reflection or a mix of the three. If several media are present, a refraction can also occur. Transduction processes are also of special importance to acoustics.

Wave propagation: pressure levels
In fluids such as air and water, sound waves propagate as disturbances in the ambient pressure level. While this disturbance is usually small, it is still noticeable to the human ear. The smallest sound that a person can hear, known as the threshold of hearing, is nine orders of magnitude smaller than the ambient pressure. The loudness of these disturbances is related to the sound pressure level (SPL) which is measured on a logarithmic scale in decibels.

Wave propagation: frequency
Physicists and acoustic engineers tend to discuss sound pressure levels in terms of frequencies, partly because this is how our ears interpret sound. What we experience as ""higher pitched"" or ""lower pitched"" sounds are pressure vibrations having a higher or lower number of cycles per second. In a common technique of acoustic measurement, acoustic signals are sampled in time, and then presented in more meaningful forms such as octave bands or time frequency plots. Both of these popular methods are used to analyze sound and better understand the acoustic phenomenon.
The entire spectrum can be divided into three sections: audio, ultrasonic, and infrasonic. The audio range falls between 20 Hz and 20,000 Hz. This range is important because its frequencies can be detected by the human ear. This range has a number of applications, including speech communication and music. The ultrasonic range refers to the very high frequencies: 20,000 Hz and higher. This range has shorter wavelengths which allow better resolution in imaging technologies. Medical applications such as ultrasonography and elastography rely on the ultrasonic frequency range. On the other end of the spectrum, the lowest frequencies are known as the infrasonic range. These frequencies can be used to study geological phenomena such as earthquakes.
Analytic instruments such as the spectrum analyzer facilitate visualization and measurement of acoustic signals and their properties. The spectrogram produced by such an instrument is a graphical display of the time varying pressure level and frequency profiles which give a specific acoustic signal its defining character.

Transduction in acoustics
A transducer is a device for converting one form of energy into another. In an electroacoustic context, this means converting sound energy into electrical energy (or vice versa). Electroacoustic transducers include loudspeakers, microphones, hydrophones and sonar projectors. These devices convert a sound pressure wave to or from an electric signal. The most widely used transduction principles are electromagnetism, electrostatics and piezoelectricity.
The transducers in most common loudspeakers (e.g. woofers and tweeters), are electromagnetic devices that generate waves using a suspended diaphragm driven by an electromagnetic voice coil, sending off pressure waves. Electret microphones and condenser microphones employ electrostatics—as the sound wave strikes the microphone's diaphragm, it moves and induces a voltage change. The ultrasonic systems used in medical ultrasonography employ piezoelectric transducers. These are made from special ceramics in which mechanical vibrations and electrical fields are interlinked through a property of the material itself.

Acoustician
An acoustician is an expert in the science of sound.

Education
There are many types of acoustician, but they usually have a Bachelor's degree or higher qualification. Some possess a degree in acoustics, while others enter the discipline via studies in fields such as physics or engineering. Much work in acoustics requires a good grounding in Mathematics and science. Many acoustic scientists work in research and development. Some conduct basic research to advance our knowledge of the perception (e.g. hearing, psychoacoustics or neurophysiology) of speech, music and noise. Other acoustic scientists advance understanding of how sound is affected as it moves through environments, e.g. Underwater acoustics, Architectural acoustics or Structural acoustics. Others areas of work are listed under subdisciplines below. Acoustic scientists work in government, university and private industry laboratories. Many go on to work in Acoustical Engineering. Some positions, such as Faculty (academic staff) require a Doctor of Philosophy.

Subdisciplines
These subdisciplines are a slightly modified list from the PACS (Physics and Astronomy Classification Scheme) coding used by the Acoustical Society of America.

Archaeoacoustics
Archaeoacoustics is the study of sound within archaeology. This typically involves studying the acoustics of archaeological sites and artefacts.

Aeroacoustics
Aeroacoustics is the study of noise generated by air movement, for instance via turbulence, and the movement of sound through the fluid air. This knowledge is applied in acoustical engineering to study how to quieten aircraft. Aeroacoustics is important to understanding how wind musical instruments work.

Acoustic signal processing
Acoustic signal processing is the electronic manipulation of acoustic signals. Applications include: active noise control; design for hearing aids or cochlear implants; echo cancellation; music information retrieval, and perceptual coding (e.g. MP3 or Opus).

Architectural acoustics
Architectural acoustics (also known as building acoustics) involves the scientific understanding of how to achieve a good sound within a building. It typically involves the study of speech intelligibility, speech privacy, music quality, and vibration reduction in the built environment.

Bioacoustics
Bioacoustics is the scientific study of the hearing and calls of animal calls, as well as how animals are affected by the acoustic and sounds of their habitat.

Electroacoustics
This subdiscipline is concerned with the recording, manipulation and reproduction of audio using electronics. This might include products such as mobile phones, large scale public address systems or virtual reality systems in research laboratories.

Environmental noise and soundscapes
Environmental acoustics is concerned with noise and vibration caused by railways, road traffic, aircraft, industrial equipment and recreational activities. The main aim of these studies is to reduce levels of environmental noise and vibration. Research work now also has a focus on the positive use of sound in urban environments: soundscapes and tranquility.

Musical acoustics
Musical acoustics is the study of the physics of acoustic instruments; the audio signal processing used in electronic music; the computer analysis of music and composition, and the perception and cognitive neuroscience of music.

Psychoacoustics
Psychoacoustics explains how humans respond to sounds.

Speech
Acousticians study the production, processing and perception of speech. Speech recognition and Speech synthesis are two important areas of speech processing using computers. The subject also overlaps with the disciplines of physics, physiology, psychology, and linguistics.

Ultrasonics
Ultrasonics deals with sounds at frequencies too high to be heard by humans. Specialisms include medical ultrasonics (including medical ultrasonography), sonochemistry, material characterisation and underwater acoustics (Sonar).

Underwater acoustics
Underwater acoustics is the scientific study of natural and man-made sounds underwater. Applications include sonar to locate submarines, underwater communication by whales, climate change monitoring by measuring sea temperatures acoustically, sonic weapons, and marine bioacoustics.

Vibration and dynamics
This is the study of how mechanical systems vibrate and interact with their surroundings. Applications might include: ground vibrations from railways; vibration isolation to reduce vibration in operating theatres; studying how vibration can damage health (vibration white finger); vibration control to protect a building from earthquakes, or measuring how structure-borne sound moves through buildings.

Professional societies
The Acoustical Society Of America (ASA)
The European Acoustics Association (EAA)
Institute of Electrical and Electronics Engineers (IEEE)
Institute of Acoustics (IoA UK)
The Audio Engineering Society (AES)
American Society of Mechanical Engineers, Noise Control and Acoustics Division (ASME-NCAD)
International Commission for Acoustics (ICA)
American Institute of Aeronautics and Astronautics, Aeroacoustics (AIAA)
International Computer Music Association (ICMA)

Academic journals
Acta Acustica united with Acustica
Applied Acoustics
Journal of the Acoustical Society of America (JASA)
Journal of the Acoustical Society of America, Express Letters (JASA-EL)
Journal of the Audio Engineering Society
Journal of Sound and Vibration (JSV)
Journal of Vibration and Acoustics American Society of Mechanical Engineers
Ultrasonics (journal)

See also
Notes and references
Further reading
Benade, Arthur H (1976). Fundamentals of Musical Acoustics. New York: Oxford University Press. OCLC 2270137. 
S.V. Biryukov, Y.V. Gulyaev, V.V. Krylov and V.P. Plessky (1995). Surface Acoustic Waves in Inhomogeneous Media, Springer.
M. Crocker (editor), 1994. Encyclopedia of Acoustics (Interscience).
Falkovich, G. (2011). Fluid Mechanics, a short course for physicists. Cambridge University Press. ISBN 978-1-107-00575-4. 
F. Fahy and P. Gardonio (2007). Sound and Structural Vibration: Radiation, Transmission and Response, 2nd Edition, Academic Press.
M.C. Junger and D. Feit (1986). Sound, Structures and Their Interaction, 2nd Edition, MIT Press.
L. E. Kinsler, A. R. Frey, A. B. Coppens, and J. V. Sanders, 1999. Fundamentals of Acoustics, fourth edition (Wiley).
Mason W.P., Thurston R.N. Physical Acoustics (1981)
Philip M. Morse and K. Uno Ingard, 1986. Theoretical Acoustics (Princeton University Press). ISBN 0-691-08425-4
Allan D. Pierce, 1989. Acoustics: An Introduction to its Physical Principles and Applications (Acoustical Society of America). ISBN 0-88318-612-8
D. R. Raichel, 2006. The Science and Applications of Acoustics, second edition (Springer). ISBN 0-387-30089-9
Rayleigh, J. W. S. (1894). The Theory of Sound. New York: Dover. ISBN 0-8446-3028-4. 
E. Skudrzyk, 1971. The Foundations of Acoustics: Basic Mathematics and Basic Acoustics (Springer).
Stephens, R. W. B.; Bate, A. E. (1966). Acoustics and Vibrational Physics (2nd ed.). London: Edward Arnold. 
Wilson, Charles E. (2006). Noise Control (Revised ed.). Malabar, FL: Krieger Publishing Company. ISBN 1-57524-237-0. OCLC 59223706.

External links
Acoustical Society of America
Institute of Acoustic in UK
National Council of Acoustical Consultants
International Commission for Acoustics
Institute of Noise Control Engineers
Laboratoire d'Acoustique de l'Université du Maine",Category:Acoustics,1
104,105,Category:Harmonic series,,Category:Acoustics,1
105,106,Acoustic theory,"Acoustic theory is a scientific field that relates to the description of sound waves. It derives from fluid dynamics. See acoustics for the engineering approach.
Propagation of sound waves in a fluid (such as water) can be modeled by an equation of continuity (conservation of mass) and an equation of motion (conservation of momentum) . With some simplifications, in particular constant density, they can be given as follows:

  
    
      
        
          
            
              
                
                  
                    
                      ?
                      p
                    
                    
                      ?
                      t
                    
                  
                
                +
                ?
                 
                ?
                ?
                
                  u
                
              
              
                
                =
                0
                
                
                  (Mass balance)
                
              
            
            
              
                
                  ?
                  
                    0
                  
                
                
                  
                    
                      ?
                      
                        u
                      
                    
                    
                      ?
                      t
                    
                  
                
                +
                ?
                p
              
              
                
                =
                0
                
                
                  (Momentum balance)
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial p}{\partial t}}+\kappa ~\nabla \cdot \mathbf {u} &=0\qquad {\text{(Mass balance)}}\\\rho _{0}{\frac {\partial \mathbf {u} }{\partial t}}+\nabla p&=0\qquad {\text{(Momentum balance)}}\end{aligned}}}
  
where 
  
    
      
        p
        (
        
          x
        
        ,
        t
        )
      
    
    {\displaystyle p(\mathbf {x} ,t)}
   is the acoustic pressure and 
  
    
      
        
          u
        
        (
        
          x
        
        ,
        t
        )
      
    
    {\displaystyle \mathbf {u} (\mathbf {x} ,t)}
   is the flow velocity vector, 
  
    
      
        
          x
        
      
    
    {\displaystyle \mathbf {x} }
   is the vector of spatial coordinates 
  
    
      
        x
        ,
        y
        ,
        z
      
    
    {\displaystyle x,y,z}
  , 
  
    
      
        t
      
    
    {\displaystyle t}
   is the time, 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
   is the static mass density of the medium and 
  
    
      
        ?
      
    
    {\displaystyle \kappa }
   is the bulk modulus of the medium. The bulk modulus can be expressed in terms of the density and the speed of sound in the medium (
  
    
      
        
          c
          
            0
          
        
      
    
    {\displaystyle c_{0}}
  ) as

  
    
      
        ?
        =
        
          ?
          
            0
          
        
        
          c
          
            0
          
          
            2
          
        
         
        .
      
    
    {\displaystyle \kappa =\rho _{0}c_{0}^{2}~.}
  
If the flow velocity field is irrotational, 
  
    
      
        ?
        ×
        
          u
        
        =
        
          0
        
      
    
    {\displaystyle \nabla \times \mathbf {u} =\mathbf {0} }
  , then the acoustic wave equation is a combination of these two sets of balance equations and can be expressed as

  
    
      
        
          
            
              
                
              
              
                
                  
                    ?
                    
                      2
                    
                  
                  
                    u
                  
                
              
            
            
              
                
              
              
                
                  ?
                  
                    t
                    
                      2
                    
                  
                
              
            
          
        
        ?
        
          c
          
            0
          
          
            2
          
        
         
        
          ?
          
            2
          
        
        
          u
        
        =
        0
        
        
          or
        
        
        
          
            
              
                
              
              
                
                  
                    ?
                    
                      2
                    
                  
                  p
                
              
            
            
              
                
              
              
                
                  ?
                  
                    t
                    
                      2
                    
                  
                
              
            
          
        
        ?
        
          c
          
            0
          
          
            2
          
        
         
        
          ?
          
            2
          
        
        p
        =
        0
        ,
      
    
    {\displaystyle {\cfrac {\partial ^{2}\mathbf {u} }{\partial t^{2}}}-c_{0}^{2}~\nabla ^{2}\mathbf {u} =0\qquad {\text{or}}\qquad {\cfrac {\partial ^{2}p}{\partial t^{2}}}-c_{0}^{2}~\nabla ^{2}p=0,}
  
where we have used the vector Laplacian, 
  
    
      
        
          ?
          
            2
          
        
        
          u
        
        =
        ?
        (
        ?
        ?
        
          u
        
        )
        ?
        ?
        ×
        (
        ?
        ×
        
          u
        
        )
      
    
    {\displaystyle \nabla ^{2}\mathbf {u} =\nabla (\nabla \cdot \mathbf {u} )-\nabla \times (\nabla \times \mathbf {u} )}
   . The acoustic wave equation (and the mass and momentum balance equations) are often expressed in terms of a scalar potential 
  
    
      
        ?
      
    
    {\displaystyle \varphi }
   where 
  
    
      
        
          u
        
        =
        ?
        ?
      
    
    {\displaystyle \mathbf {u} =\nabla \varphi }
  . In that case the acoustic wave equation is written as

  
    
      
        
          
            
              
                
              
              
                
                  
                    ?
                    
                      2
                    
                  
                  ?
                
              
            
            
              
                
              
              
                
                  ?
                  
                    t
                    
                      2
                    
                  
                
              
            
          
        
        ?
        
          c
          
            0
          
          
            2
          
        
         
        
          ?
          
            2
          
        
        ?
        =
        0
      
    
    {\displaystyle {\cfrac {\partial ^{2}\varphi }{\partial t^{2}}}-c_{0}^{2}~\nabla ^{2}\varphi =0}
  
and the momentum balance and mass balance are expressed as

  
    
      
        p
        +
        
          ?
          
            0
          
        
         
        
          
            
              
                
              
              
                
                  ?
                  ?
                
              
            
            
              
                
              
              
                
                  ?
                  t
                
              
            
          
        
        =
        0
         
        ;
         
         
        ?
        +
        
          
            
              
                
              
              
                
                  
                    ?
                    
                      0
                    
                  
                
              
            
            
              
                
              
              
                
                  
                    c
                    
                      0
                    
                    
                      2
                    
                  
                
              
            
          
        
         
        
          
            
              
                
              
              
                
                  ?
                  ?
                
              
            
            
              
                
              
              
                
                  ?
                  t
                
              
            
          
        
        =
        0
         
        .
      
    
    {\displaystyle p+\rho _{0}~{\cfrac {\partial \varphi }{\partial t}}=0~;~~\rho +{\cfrac {\rho _{0}}{c_{0}^{2}}}~{\cfrac {\partial \varphi }{\partial t}}=0~.}

Derivation of the governing equations
The derivations of the above equations for waves in an acoustic medium are given below.

Conservation of momentum
The equations for the conservation of linear momentum for a fluid medium are

  
    
      
        ?
        
          (
          
            
              
                ?
                
                  u
                
              
              
                ?
                t
              
            
          
          +
          
            u
          
          ?
          ?
          
            u
          
          )
        
        =
        ?
        ?
        p
        +
        ?
        ?
        
          ?
        
        +
        ?
        
          g
        
      
    
    {\displaystyle \rho \left({\frac {\partial \mathbf {u} }{\partial t}}+\mathbf {u} \cdot \nabla \mathbf {u} \right)=-\nabla p+\nabla \cdot {\boldsymbol {\tau }}+\rho \mathbf {g} }
  
where 
  
    
      
        
          g
        
      
    
    {\displaystyle \mathbf {g} }
   is the body force per unit mass, 
  
    
      
        p
      
    
    {\displaystyle p}
   is the pressure, and 
  
    
      
        
          ?
        
      
    
    {\displaystyle {\boldsymbol {\tau }}}
   is the deviatoric stress. If 
  
    
      
        
          ?
        
      
    
    {\displaystyle {\boldsymbol {\sigma }}}
   is the Cauchy stress, then

  
    
      
        p
        :=
        ?
        
          
            
              1
              3
            
          
        
         
        
          tr
        
        (
        
          ?
        
        )
         
        ;
         
         
        
          ?
        
        :=
        ?
        p
        
          I
        
        +
        
          ?
        
      
    
    {\displaystyle p:=-{\tfrac {1}{3}}~{\text{tr}}({\boldsymbol {\sigma }})~;~~{\boldsymbol {\sigma }}:=-p{\boldsymbol {I}}+{\boldsymbol {\tau }}}
  
where 
  
    
      
        
          I
        
      
    
    {\displaystyle {\boldsymbol {I}}}
   is the rank-2 identity tensor.
We make several assumptions to derive the momentum balance equation for an acoustic medium. These assumptions and the resulting forms of the momentum equations are outlined below.

Assumption 1: Newtonian fluid
In acoustics, the fluid medium is assumed to be Newtonian. For a Newtonian fluid, the deviatoric stress tensor is related to the flow velocity by

  
    
      
        
          ?
        
        =
        ?
         
        
          [
          ?
          
            u
          
          +
          (
          ?
          
            u
          
          
            )
            
              T
            
          
          ]
        
        +
        ?
         
        (
        ?
        ?
        
          u
        
        )
         
        
          I
        
      
    
    {\displaystyle {\boldsymbol {\tau }}=\mu ~\left[\nabla \mathbf {u} +(\nabla \mathbf {u} )^{T}\right]+\lambda ~(\nabla \cdot \mathbf {u} )~{\boldsymbol {I}}}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \mu }
   is the shear viscosity and 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   is the bulk viscosity.
Therefore, the divergence of 
  
    
      
        
          ?
        
      
    
    {\displaystyle {\boldsymbol {\tau }}}
   is given by

  
    
      
        
          
            
              
                ?
                ?
                
                  ?
                
                ?
                
                  
                    
                      
                        
                      
                      
                        
                          ?
                          
                            s
                            
                              i
                              j
                            
                          
                        
                      
                    
                    
                      
                        
                      
                      
                        
                          ?
                          
                            x
                            
                              i
                            
                          
                        
                      
                    
                  
                
              
              
                
                =
                ?
                
                  [
                  
                    
                      
                        
                          
                        
                        
                          
                            ?
                          
                        
                      
                      
                        
                          
                        
                        
                          
                            ?
                            
                              x
                              
                                i
                              
                            
                          
                        
                      
                    
                  
                  
                    (
                    
                      
                        
                          
                            
                          
                          
                            
                              ?
                              
                                u
                                
                                  i
                                
                              
                            
                          
                        
                        
                          
                            
                          
                          
                            
                              ?
                              
                                x
                                
                                  j
                                
                              
                            
                          
                        
                      
                    
                    +
                    
                      
                        
                          
                            
                          
                          
                            
                              ?
                              
                                u
                                
                                  j
                                
                              
                            
                          
                        
                        
                          
                            
                          
                          
                            
                              ?
                              
                                x
                                
                                  i
                                
                              
                            
                          
                        
                      
                    
                    )
                  
                  ]
                
                +
                ?
                 
                
                  [
                  
                    
                      
                        
                          
                        
                        
                          
                            ?
                          
                        
                      
                      
                        
                          
                        
                        
                          
                            ?
                            
                              x
                              
                                i
                              
                            
                          
                        
                      
                    
                  
                  
                    (
                    
                      
                        
                          
                            
                          
                          
                            
                              ?
                              
                                u
                                
                                  k
                                
                              
                            
                          
                        
                        
                          
                            
                          
                          
                            
                              ?
                              
                                x
                                
                                  k
                                
                              
                            
                          
                        
                      
                    
                    )
                  
                  ]
                
                
                  ?
                  
                    i
                    j
                  
                
              
            
            
              
              
                
                =
                ?
                 
                
                  
                    
                      
                        
                      
                      
                        
                          
                            ?
                            
                              2
                            
                          
                          
                            u
                            
                              i
                            
                          
                        
                      
                    
                    
                      
                        
                      
                      
                        
                          ?
                          
                            x
                            
                              i
                            
                          
                          ?
                          
                            x
                            
                              j
                            
                          
                        
                      
                    
                  
                
                +
                ?
                 
                
                  
                    
                      
                        
                      
                      
                        
                          
                            ?
                            
                              2
                            
                          
                          
                            u
                            
                              j
                            
                          
                        
                      
                    
                    
                      
                        
                      
                      
                        
                          ?
                          
                            x
                            
                              i
                            
                          
                          ?
                          
                            x
                            
                              i
                            
                          
                        
                      
                    
                  
                
                +
                ?
                 
                
                  
                    
                      
                        
                      
                      
                        
                          
                            ?
                            
                              2
                            
                          
                          
                            u
                            
                              k
                            
                          
                        
                      
                    
                    
                      
                        
                      
                      
                        
                          ?
                          
                            x
                            
                              k
                            
                          
                          ?
                          
                            x
                            
                              j
                            
                          
                        
                      
                    
                  
                
              
            
            
              
              
                
                =
                (
                ?
                +
                ?
                )
                 
                
                  
                    
                      
                        
                      
                      
                        
                          
                            ?
                            
                              2
                            
                          
                          
                            u
                            
                              i
                            
                          
                        
                      
                    
                    
                      
                        
                      
                      
                        
                          ?
                          
                            x
                            
                              i
                            
                          
                          ?
                          
                            x
                            
                              j
                            
                          
                        
                      
                    
                  
                
                +
                ?
                 
                
                  
                    
                      
                        
                      
                      
                        
                          
                            ?
                            
                              2
                            
                          
                          
                            u
                            
                              j
                            
                          
                        
                      
                    
                    
                      
                        
                      
                      
                        
                          ?
                          
                            x
                            
                              i
                            
                            
                              2
                            
                          
                        
                      
                    
                  
                
              
            
            
              
              
                
                ?
                (
                ?
                +
                ?
                )
                 
                ?
                (
                ?
                ?
                
                  u
                
                )
                +
                ?
                 
                
                  ?
                  
                    2
                  
                
                
                  u
                
                 
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\nabla \cdot {\boldsymbol {\tau }}\equiv {\cfrac {\partial s_{ij}}{\partial x_{i}}}&=\mu \left[{\cfrac {\partial }{\partial x_{i}}}\left({\cfrac {\partial u_{i}}{\partial x_{j}}}+{\cfrac {\partial u_{j}}{\partial x_{i}}}\right)\right]+\lambda ~\left[{\cfrac {\partial }{\partial x_{i}}}\left({\cfrac {\partial u_{k}}{\partial x_{k}}}\right)\right]\delta _{ij}\\&=\mu ~{\cfrac {\partial ^{2}u_{i}}{\partial x_{i}\partial x_{j}}}+\mu ~{\cfrac {\partial ^{2}u_{j}}{\partial x_{i}\partial x_{i}}}+\lambda ~{\cfrac {\partial ^{2}u_{k}}{\partial x_{k}\partial x_{j}}}\\&=(\mu +\lambda )~{\cfrac {\partial ^{2}u_{i}}{\partial x_{i}\partial x_{j}}}+\mu ~{\cfrac {\partial ^{2}u_{j}}{\partial x_{i}^{2}}}\\&\equiv (\mu +\lambda )~\nabla (\nabla \cdot \mathbf {u} )+\mu ~\nabla ^{2}\mathbf {u} ~.\end{aligned}}}
  
Using the identity 
  
    
      
        
          ?
          
            2
          
        
        
          u
        
        =
        ?
        (
        ?
        ?
        
          u
        
        )
        ?
        ?
        ×
        ?
        ×
        
          u
        
      
    
    {\displaystyle \nabla ^{2}\mathbf {u} =\nabla (\nabla \cdot \mathbf {u} )-\nabla \times \nabla \times \mathbf {u} }
  , we have

  
    
      
        ?
        ?
        
          ?
        
        =
        (
        2
        ?
        +
        ?
        )
         
        ?
        (
        ?
        ?
        
          u
        
        )
        ?
        ?
         
        ?
        ×
        ?
        ×
        
          u
        
         
        .
      
    
    {\displaystyle \nabla \cdot {\boldsymbol {\tau }}=(2\mu +\lambda )~\nabla (\nabla \cdot \mathbf {u} )-\mu ~\nabla \times \nabla \times \mathbf {u} ~.}
  
The equations for the conservation of momentum may then be written as

  
    
      
        ?
        
          (
          
            
              
                ?
                
                  u
                
              
              
                ?
                t
              
            
          
          +
          
            u
          
          ?
          ?
          
            u
          
          )
        
        =
        ?
        ?
        p
        +
        (
        2
        ?
        +
        ?
        )
         
        ?
        (
        ?
        ?
        
          u
        
        )
        ?
        ?
         
        ?
        ×
        ?
        ×
        
          u
        
        +
        ?
        
          g
        
      
    
    {\displaystyle \rho \left({\frac {\partial \mathbf {u} }{\partial t}}+\mathbf {u} \cdot \nabla \mathbf {u} \right)=-\nabla p+(2\mu +\lambda )~\nabla (\nabla \cdot \mathbf {u} )-\mu ~\nabla \times \nabla \times \mathbf {u} +\rho \mathbf {g} }

Assumption 2: Irrotational flow
For most acoustics problems we assume that the flow is irrotational, that is, the vorticity is zero. In that case

  
    
      
        ?
        ×
        
          u
        
        =
        0
      
    
    {\displaystyle \nabla \times \mathbf {u} =0}
  
and the momentum equation reduces to

  
    
      
        ?
        
          (
          
            
              
                ?
                
                  u
                
              
              
                ?
                t
              
            
          
          +
          
            u
          
          ?
          ?
          
            u
          
          )
        
        =
        ?
        ?
        p
        +
        (
        2
        ?
        +
        ?
        )
         
        ?
        (
        ?
        ?
        
          u
        
        )
        +
        ?
        
          g
        
      
    
    {\displaystyle \rho \left({\frac {\partial \mathbf {u} }{\partial t}}+\mathbf {u} \cdot \nabla \mathbf {u} \right)=-\nabla p+(2\mu +\lambda )~\nabla (\nabla \cdot \mathbf {u} )+\rho \mathbf {g} }

Assumption 3: No body forces
Another frequently made assumption is that effect of body forces on the fluid medium is negligible. The momentum equation then further simplifies to

  
    
      
        ?
        
          (
          
            
              
                ?
                
                  u
                
              
              
                ?
                t
              
            
          
          +
          
            u
          
          ?
          ?
          
            u
          
          )
        
        =
        ?
        ?
        p
        +
        (
        2
        ?
        +
        ?
        )
         
        ?
        (
        ?
        ?
        
          u
        
        )
      
    
    {\displaystyle \rho \left({\frac {\partial \mathbf {u} }{\partial t}}+\mathbf {u} \cdot \nabla \mathbf {u} \right)=-\nabla p+(2\mu +\lambda )~\nabla (\nabla \cdot \mathbf {u} )}

Assumption 4: No viscous forces
Additionally, if we assume that there are no viscous forces in the medium (the bulk and shear viscosities are zero), the momentum equation takes the form

  
    
      
        ?
        
          (
          
            
              
                ?
                
                  u
                
              
              
                ?
                t
              
            
          
          +
          
            u
          
          ?
          ?
          
            u
          
          )
        
        =
        ?
        ?
        p
      
    
    {\displaystyle \rho \left({\frac {\partial \mathbf {u} }{\partial t}}+\mathbf {u} \cdot \nabla \mathbf {u} \right)=-\nabla p}

Assumption 5: Small disturbances
An important simplifying assumption for acoustic waves is that the amplitude of the disturbance of the field quantities is small. This assumption leads to the linear or small ",Category:Acoustics,1
106,107,Micro perforated plate,"A micro perforated plate (MPP) is a device used to absorb sound, reducing its intensity. It consists of a thin flat plate, made from one of several different materials, with small holes punched in it. An MPP offers an alternative to traditional sound absorbers made from porous materials.

Structure
An MPP is normally 0.5 - 2 mm thick. The holes typically cover 0.5 to 2% of the plate, depending on the application and the environment in which the MPP is to be mounted. Hole diameter is usually less than 1 millimeter, typically 0.05 to 0.5 mm. They are usually made using the microperforation process.

Operating principle
The goal of a sound absorber is to convert acoustical energy into heat. In a traditional absorber, the sound wave propagates into the absorber. Because of the proximity of the porous material, the oscillating air molecules inside the absorber lose their acoustical energy due to friction.
A MPP works in almost the same way. When the oscillating air molecules penetrate the MPP, the friction between the air in motion and the surface of the MPP dissipates the acoustical energy.

Comparison with other materials
Traditional sound absorbers are porous materials such as mineral wool, glass or polyester fibres. It is not possible to use these materials in harsh environments such as engine compartments. Traditional absorbers have many drawbacks, including pollution, the risk of fire, and problems with the useful lifetime of the absorbing material.
The main reason why Micro Perforates have become so popular among acousticians is that they have a good absorption performance but without the disadvantages of a porous material. Furthermore, an MPP is also preferable from an aesthetic point of view.

History
For a while, perforated metal panels with holes in the 1 – 10 mm range have been used as a cage for sound-absorbing glass-fiber bats where large holes let the sound waves reach into the absorbent fiber. Another use has been the creation of narrowband Helmholtz absorbers which can be tuned by hole size and the dimensions of the hole distance and air gap behind the panel. However, when the hole dimensions are in the region of 0.05 - 0.5 mm, the narrow absorption peaks become much wider, making the additional fiber absorber more or less unnecessary, while still maintaining a very high absorption factor. By varying geometrical and material parameters, the acoustical performance can be tailored to meet a multitude of specifications in various applications.
One early contributor to the theory of micro perforated plates as sound absorbers was Professor Daa-You Maa. Further possibilities aiming to improve the accuracy of Maa’s original model are currently being investigated. One other major phenomenon that currently being investigated is the nonlinear effect i.e. an MPP behaves differently depending on the magnitude of the incident sound wave.

References
External links
Acoustical Society of America",Category:Sound,1
107,108,Prefix (acoustics),"In acoustics, the prefix of a sound is an initial phase, the onset of a sound quite dissimilar to the ensuing lasting vibration.
The term was coined by J. F. Schouten (1968, 42), who called it one of at least five major acoustic parameters that determine the elusive attributes of timbre.

See also
Onset (audio)
Timbre#Attributes
Synthesizer#ADSR envelope
Transient (acoustics)

References
Schouten, J. F. (1968). ""The Perception of Timbre"". In Reports of the 6th International Congress on Acoustics, Tokyo, GP-6-2, 6 vols., edited by Y. Kohasi, 6:35–44, 90. Tokyo: Maruzen; Amsterdam: Elsevier.",Category:Music stubs,1
108,109,Aeroacoustic analogy,"Acoustic analogies are applied mostly in numerical aeroacoustics to reduce aeroacoustic sound sources to simple emitter types. They are therefore often also referred to as aeroacoustic analogies.
In general, aeroacoustic analogies are derived from the compressible Navier–Stokes equations (NSE). The compressible NSE are rearranged into various forms of the inhomogeneous acoustic wave equation. Within these equations, source terms describe the acoustic sources. They consist of pressure and speed fluctuation as well as stress tensor and force terms.
Approximations are introduced to make the source terms independent of the acoustic variables. In this way, linearized equations are derived which describe the propagation of the acoustic waves in a homogeneous, resting medium. The latter is excited by the acoustic source terms, which are determined from the turbulent fluctuations. Since the aeroacoustics are described by the equations of classical acoustics, the methods are called aeroacoustic analogies.
The Lighthill analogy considers a free flow, as for example with an engine jet. The nonstationary fluctuations of the stream are represented by a distribution of quadrupole sources in the same volume.
The Curle analogy is a formal solution of the Lighthill analogy, which takes hard surfaces into consideration.
The Ffowcs Williams–Hawkings analogy is valid for aeroacoustic sources in relative motion with respect to a hard surface, as is the case in many technical applications for example in the automotive industry or in air travel. The calculation involves quadrupole, dipole and monopole terms.

References
Further reading
Blumrich, R.: Berechnungsmethoden für die Aeroakustik von Fahrzeugen. Tagungsband der ATZ/MTZ-Konferenz Akustik 2006, Stuttgart, 17–18.5.2006..
Contribution of the Technical University of Dresden to the modeling of flow sound sources with elementary emitters.
Contribution of the Technical University of Dresden to the history of aeroacoustics.",Category:Computational fluid dynamics,1
109,110,Category:Acoustic equations,,Category:Acoustics,1
110,111,Acoustic resonance,"Acoustic resonance is a phenomenon where acoustic systems amplify sound waves whose frequency matches one of its own natural frequencies of vibration (its resonance frequencies).
The term ""acoustic resonance"" is sometimes used to narrow mechanical resonance to the frequency range of human hearing, but since acoustics is defined in general terms concerning vibrational waves in matter, acoustic resonance can occur at frequencies outside the range of human hearing.
An acoustically resonant object usually has more than one resonance frequency, especially at harmonics of the strongest resonance. It will easily vibrate at those frequencies, and vibrate less strongly at other frequencies. It will ""pick out"" its resonance frequency from a complex excitation, such as an impulse or a wideband noise excitation. In effect, it is filtering out all frequencies other than its resonance.
Acoustic resonance is an important consideration for instrument builders, as most acoustic instruments use resonators, such as the strings and body of a violin, the length of tube in a flute, and the shape of a drum membrane. Acoustic resonance is also important for hearing. For example, resonance of a stiff structural element, called the basilar membrane within the cochlea of the inner ear allows hair cells on the membrane to detect sound. (For mammals the membrane has tapering resonances across its length so that high frequencies are concentrated on one end and low frequencies on the other.)
Like mechanical resonance, acoustic resonance can result in catastrophic failure of the vibrator. The classic example of this is breaking a wine glass with sound at the precise resonant frequency of the glass; although this is difficult in practice.

Vibrating string
In musical instruments, strings under tension, as in lutes, harps, guitars, pianos, violins and so forth, have resonant frequencies directly related to the mass, length, and tension of the string. The wavelength that will create the first resonance on the string is equal to twice the length of the string. Higher resonances correspond to wavelengths that are integer divisions of the fundamental wavelength. The corresponding frequencies are related to the speed v of a wave traveling down the string by the equation

  
    
      
        f
        =
        
          
            
              n
              v
            
            
              2
              L
            
          
        
      
    
    {\displaystyle f={nv \over 2L}}
  
where L is the length of the string (for a string fixed at both ends) and n = 1, 2, 3...(Harmonic in an open end pipe (that is, both ends of the pipe are open)). The speed of a wave through a string or wire is related to its tension T and the mass per unit length ?:

  
    
      
        v
        =
        
          
            
              T
              ?
            
          
        
      
    
    {\displaystyle v={\sqrt {T \over \rho }}}
  
So the frequency is related to the properties of the string by the equation

  
    
      
        f
        =
        
          
            
              n
              
                
                  
                    T
                    ?
                  
                
              
            
            
              2
              L
            
          
        
        =
        
          
            
              n
              
                
                  
                    T
                    
                      m
                      
                        /
                      
                      L
                    
                  
                
              
            
            
              2
              L
            
          
        
      
    
    {\displaystyle f={n{\sqrt {T \over \rho }} \over 2L}={n{\sqrt {T \over m/L}} \over 2L}}
  
where T is the tension, ? is the mass per unit length, and m is the total mass.
Higher tension and shorter lengths increase the resonant frequencies. When the string is excited with an impulsive function (a finger pluck or a strike by a hammer), the string vibrates at all the frequencies present in the impulse (an impulsive function theoretically contains 'all' frequencies). Those frequencies that are not one of the resonances are quickly filtered out—they are attenuated—and all that is left is the harmonic vibrations that we hear as a musical note.

String resonance in music instruments
String resonance occurs on string instruments. Strings or parts of strings may resonate at their fundamental or overtone frequencies when other strings are sounded. For example, an A string at 440 Hz will cause an E string at 330 Hz to resonate, because they share an overtone of 1320 Hz (3rd overtone of A and 4th overtone of E).

Resonance of a tube of air
The resonance of a tube of air is related to the length of the tube, its shape, and whether it has closed or open ends. Musically useful tube shapes are conical and cylindrical (see bore). A pipe that is closed at one end is said to be stopped while an open pipe is open at both ends. Modern orchestral flutes behave as open cylindrical pipes; clarinets and lip-reed instruments (brass instruments) behave as closed cylindrical pipes; and saxophones, oboes, and bassoons as closed conical pipes. Vibrating air columns also have resonances at harmonics, like strings.

Cylinders
Any cylinder resonates at multiple frequencies, producing multiple musical pitches. The lowest frequency is called the fundamental frequency or the first harmonic. Cylinders used as musical instruments are generally open, either at both ends, like a flute, or at one end, like some organ pipes. However, a cylinder closed at both ends can also be used to create or visualize sound waves, as in a Rubens Tube.
The resonance properties of a cylinder may be understood by considering the behavior of a sound wave in air. Sound travels as a longitudinal compression wave, causing air molecules to move back and forth along the direction of travel. Within a tube, a standing wave is formed, whose length depends on the length of the tube. At the closed end of the tube, air molecules cannot move much, so this end of the tube is a displacement node in the standing wave. At the open end of the tube, air molecules can move freely, producing a displacement antinode. Where the molecules are unable to move freely, pressure builds up. Thus, the closed end of a pipe is a pressure node as well as a displacement antinode.

Closed at both ends
The table below shows the displacement waves in a cylinder closed at both ends. Note that the air molecules near the closed ends cannot move, whereas the molecules near the center of the pipe move freely. In the first harmonic, the closed tube contains exactly half of a standing wave (node-antinode-node).

Open at both ends
In cylinders with both ends open, air molecules near the end move freely in and out of the tube. This movement produces displacement antinodes in the standing wave. Nodes tend to form inside the cylinder, away from the ends. In the first harmonic, the open tube contains exactly half of a standing wave (antinode-node-antinode). Thus the harmonics of the open cylinder are calculated in the same way as the harmonics of a closed/closed cylinder.
The physics of a pipe open at both ends are explained in Physics Classroom. Note that the diagrams in this reference show displacement waves, similar to the ones shown above. These stand in sharp contrast to the pressure waves shown near the end of the present article.
By overblowing an open tube, a note can be obtained that is an octave above the fundamental frequency or note of the tube. For example, if the fundamental note of an open pipe is C1, then overblowing the pipe gives C2, which is an octave above C1.
Open cylindrical tubes resonate at the approximate frequencies:

  
    
      
        f
        =
        
          
            
              n
              v
            
            
              2
              L
            
          
        
      
    
    {\displaystyle f={nv \over 2L}}
  
where n is a positive integer (1, 2, 3...) representing the resonance node, L is the length of the tube and v is the speed of sound in air (which is approximately 343 metres per second [770 mph] at 20 °C [68 °F] and at sea level).
A more accurate equation considering an end correction is given below:

  
    
      
        f
        =
        
          
            
              n
              v
            
            
              2
              (
              L
              +
              0.8
              d
              )
            
          
        
      
    
    {\displaystyle f={nv \over 2(L+0.8d)}}
  
where d is the diameter of the resonance tube. This equation compensates for the fact that the exact point at which a sound wave is reflecting at an open end is not perfectly at the end section of the tube, but a small distance outside the tube.
The reflection ratio is slightly less than 1; the open end does not behave like an infinitesimal acoustic impedance; rather, it has a finite value, called radiation impedance, which is dependent on the diameter of the tube, the wavelength, and the type of reflection board possibly present around the opening of the tube.
So when n is 1:

  
    
      
        f
        =
        
          
            v
            
              2
              (
              L
              +
              0.8
              d
              )
            
          
        
      
    
    {\displaystyle f={v \over 2(L+0.8d)}}
  

  
    
      
        
          f
          (
          2
          (
          L
          +
          0.8
          d
          )
          )
        
        =
        v
      
    
    {\displaystyle {f(2(L+0.8d))}=v}
  

  
    
      
        
          f
          ?
        
        =
        v
      
    
    {\displaystyle {f\lambda }=v}
  

  
    
      
        ?
        =
        
          2
          (
          L
          +
          0.8
          d
          )
        
      
    
    {\displaystyle \lambda ={2(L+0.8d)}}
  
where v is the speed of sound, L is the length of the resonant tube, d is the diameter of the tube, f is the resonate sound frequency, and ? is the resonant wavelength.

Closed at one end
When used in an organ a tube which is closed at one end is called a ""stopped pipe"". Such cylinders have a fundamental frequency but can be overblown to produce other higher frequencies or notes. These overblown registers can be tuned by using different degrees of conical taper. A closed tube resonates at the same fundamental frequency as an open tube twice its length, with a wavelength equal to four times its length. In a closed tube, a displacement node, or point of no vibration, always appears at the closed end and if the tube is resonating, it will have an antinode, or point greatest vibration at the Phi point (length × 0.618) near the open end.
By overblowing a cylindrical closed tube, a note can be obtained that is approximately a twelfth above the fundamental note of the tube. This is sometimes described as one-fifth above the octave of the fundamental note. For example, if the fundamental note of a closed pipe is C1, then overblowing the pipe gives G2, which is one-twelfth above C1. Alternatively we can say that G2 is one-fifth above C2 — the octave above C1. Adjusting the taper of this cylinder for a decreasing cone can tune the second harmonic or overblown note close to the octave position or 8th. Opening a small ""speaker hole"" at the Phi point, or shared ""wave/node"" position will cancel the fundamental frequency and force the tube to resonate at a 12th above the fundamental. This technique is used in a Recorder by pinching open the dorsal thumb hole. Moving this small hole upwards, closer to the voicing will make it an ""Echo Hole"" (Dolmetsch Recorder Modification) that will give a precise half note above the fundamental when opened. Note: Slight size or diameter adjustment is needed to zero in on the precise half note frequency.
A closed tube will have approximate resonances of:

  
    
      
        f
        =
        
          
            
              n
              v
            
            
              4
              L
            
          
        
      
    
    {\displaystyle f={nv \over 4L}}
  
where ""n"" here is an odd number (1, 3, 5...). This type of tube produces only odd harmonics and has its fundamental frequency an octave lower than that of an open cylinder (that is, half the frequency).
A more accurate equation is given below:

  
    
      
        f
        =
        
          
            
              n
              v
            
            
              4
              (
              L
              +
              0.4
              d
              )
            
          
        
      
    
    {\displaystyle f={nv \over 4(L+0.4d)}}
  .
Again, when n is 1:

  
    
      
        f
        =
        
          
            v
            
              4
              (
              L
              +
              0.4
              d
              )
            
          
        
      
    
    {\displaystyle f={v \over 4(L+0.4d)}}
  

  
    
      
        
          f
          (
          4
          (
          L
          +
          0.4
          d
          )
          )
        
        =
        v
      
    
    {\displaystyle {f(4(L+0.4d))}=v}
  

  
    
      
        
          f
          ?
        
        =
        v
      
    
    {\displaystyle {f\lambda }=v}
  

  
    
      
        ?
        =
        
          4
          (
          L
          +
          0.4
          d
          )
        
      
    
    {\displaystyle \lambda ={4(L+0.4d)}}
  
where v is the speed of sound, L is the length of the resonant tube, d is the diameter of the tube, f is the resonate sound frequency, and ? is the resonant wavelength.

Pressure wave
In the two diagrams below are shown the first three resonances of the pressure wave in a cylindrical tube, with antinodes at the closed end of the pipe. In diagram 1, the tube is open at both ends. In diagram 2, it is closed at one end. The horizontal axis is pressure. Note that in this case, the open end of the pipe is a pressure node while the closed end is a pressure antinode.

Cones
An open conical tube, that is, one in the shape of a frustum of a cone with both ends open, will have resonant frequencies approximately equal to those of an open cylindrical pipe of the same length.
The resonant frequencies of a stopped conical tube — a complete cone or frustum with one end closed — satisfy a more complicated condition:

  
    
      
        k
        L
        =
        n
        ?
        ?
        
          tan
          
            ?
            1
          
        
        ?
        k
        x
      
    
    {\displaystyle kL=n\pi -\tan ^{-1}kx}
  
where the wavenumber k is

  
    
      
        k
        =
        2
        ?
        f
        
          /
        
        v
      
    
    {\displaystyle k=2\pi f/v}
  
and x is the distance from the small end of the frustum to the vertex. When x is small, that is, when the cone is nearly complete, this becomes

  
    
      
        k
        (
        L
        +
        x
        )
        ?
        n
        ?
      
    
    {\displaystyle k(L+x)\approx n\pi }
  
leading to resonant frequencies approximately equal to those of an open cylinder whose length equals L + x. In words, a complete conical pipe behaves approximately like an open cylindrical pipe of the same length, and to first order the behavior does not change if the complete cone is replaced by a closed frustum of that cone.

Closed rectangular box
Sound waves in a rectangular box include such examples as loudspeaker enclosures and buildings. Rectangular building have resonances described as room modes. For a rectangular box, the resonant frequencies are given by

  
    
      
        f
        =
        
          
            v
            2
          
        
        
          
            
              
                (
                
                  
                    ?
                    
                      L
                      
                        x
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    m
                    
                      L
                      
                        y
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    n
                    
                      L
                      
                        z
                      
                    
                  
                
                )
              
              
                2
              
            
          
        
      
    
    {\displaystyle f={v \over 2}{\sqrt {\left({\ell  \over L_{x}}\right)^{2}+\left({m \over L_{y}}\right)^{2}+\left({n \over L_{z}}\right)^{2}}}}
  
where v is the speed of sound, Lx and Ly and Lz are the dimensions of the box. 
  
    
      
        ?
      
    
    {\displaystyle \ell }
  , 
  
    
      
        m
      
    
    {\displaystyle m}
  , and 
  
    
      
        n
      
    
    {\displaystyle n}
   are nonnegative integers that cannot all be zero. If the small loudspeaker box is airtight the frequency low enough and the compression is high enough, the sound pressure (decibel level) inside the box will be the same anywhere inside the box, this is hydraulic pressure.

Resonance of a sphere of air (vented)
The resonant frequency of a rigid cavity of static volume V0 with a necked sound hole of area A and length L is given by the Helmholtz resonance formula

  
    
      
        f
        =
        
          
            v
            
              2
              ?
            
          
        
        
          
            
              A
              
                
                  V
                  
                    0
                  
                
                
                  L
                  
                    e
                    q
                  
                
              
            
          
        
      
    
    {\displaystyle f={\frac {v}{2\pi }}{\sqrt {\frac {A}{V_{0}L_{eq}}}}}
  
where 
  
    
      
        
          L
          
            e
            q
          
        
      
    
    {\displaystyle L_{eq}}
   is the equivalent length of the neck with end correction

  
    
      
        
          L
          
            e
            q
          
        
        =
        L
        +
        0.75
        d
      
    
    {\displaystyle L_{eq}=L+0.75d}
              for an unflanged neck

  
    
      
        
          L
          
            e
            q
          
        
        =
        L
        +
        0.85
        d
      
    
    {\displaystyle L_{eq}=L+0.85d}
              for a flanged neck

For a spherical cavity, the resonant frequency formula becomes

  
    
      
        f
        =
        
          
            
              v
              d
            
            ?
          
        
        
          
            
              3
              
                8
                
                  L
                  
                    e
                    q
                  
                
                
                  D
                  
                    3
                  
                
              
            
          
        
      
    
    {\displaystyle f={\frac {vd}{\pi }}{\sqrt {\frac {3}{8L_{eq}D^{3}}}}}
  
where

D = diameter of sphere
d = diameter of sound hole

For a sphere with just a sound hole, L=0 and the surface of the sphere acts as a flange, so

  
    
      
        f
        =
        
          
            v
            ?
          
        
        
          
            
              
                3
                d
              
              
                8
                (
                0.85
                )
                
                  D
                  
                    3
                  
                
              
            
          
        
      
    
    {\displaystyle f={\frac {v}{\pi }}{\sqrt {\frac {3d}{8(0.85)D^{3}}}}}
  
In dry air at 20 °C, with d and D in metres, f in Hertz, this becomes

  
    
      
        f
        =
        72.6
        
          
            
              d
              
                D
                
                  3
                
              
            
          
        
      
    
    {\displaystyle f=72.6{\sqrt {\frac {d}{D^{3}}}}}

False tones
Some large conical instruments like tubas have a strong and useful resonance that is not in the well-known harmonic series. For example, most large B? tubas have a strong resonance at low E? (E?1, 39 Hz), which is between the fundamental and the second harmonic (an octave higher than the fundamental). These alternative resonances are often known as false tones or privileged tones.
The most convincing explanation for false-tones is that the horn is acting as a 'third of a pipe' rather than as a half-pipe. The bell remains an anti-node, but there would then be a node 1/3 of the way back to the mouthpiece. If so, it seems that the fundamental would be missing entirely, and would only be inferred from the overtones. However, the node and the anti-node collide in the same spot and cancel out the fundamental.

In musical composition
Several composers have begun to make resonance the subject of compositions. Alvin Lucier has used acoustic instruments and sine wave generators to explore the resonance of objects large and small in many of his compositions. The complex inharmonic partials of a swell shaped crescendo and decrescendo on a tamtam or other percussion instrument interact with room resonances in James Tenney's Koan: Having Never Written A Note For Percussion. Pauline Oliveros and Stuart Dempster regularly perform in large reverberant spaces such as the 2-million-US-gallon (7,600 m3) cistern at Fort Worden, WA, which has a reverb with a 45-second decay. Malmö Academy of Music composition professor and composer Kent Olofsson's ""Terpsichord, a piece for percussion and pre-recorded sounds, [uses] the resonances from the acoustic instruments [to] form sonic bridges to the pre-recorded electronic sounds, that, in turn, prolong the resonances, re-shaping them into new sonic gestures.""

See also
Harmony
Music theory
Resonance
Reverberation
Sympathetic string
Reflection phase change

External links
Standing Waves Applet

References

Nederveen, Cornelis Johannes, Acoustical aspects of woodwind instruments. Amsterdam, Frits Knuf, 1969.
Rossing, Thomas D., and Fletcher, Neville H., Principles of Vibration and Sound. New York, Springer-Verlag, 1995.",Category:Acoustics,1
111,112,Sound Retrieval System,"The Sound Retrieval System (SRS) is a patented psychoacoustic 3D audio processing technology originally invented by Arnold Klayman in the early 1980s. (The original SRS patents are US 4866774 , US 4748669  and US 4841572 , which expired between 2006 and 2008. Patents may apply in other countries). The SRS technology applies head-related transfer functions (HRTFs) to create an immersive 3D soundfield using only two speakers, widening the ""sweet spot,"" creating a more spacious sense of ambience, and producing strong localization cues for discrete instruments within an audio mix. SRS is not a Dolby matrix surround decoder but works with normal stereo recordings.
Initially Hughes Aircraft, for whom Klayman was doing acoustic consulting at the time, offered a standalone SRS audio processor, as well as licensing the technology to Sony and Thomson (RCA) for inclusion in their products. In the early 1990s, Hughes sold off its non-aerospace-related holdings, and a group of entrepreneurs formed SRS Labs to acquire the SRS technology.
Many TV sets employ built-in SRS to make their built-in audio systems sound ""bigger."" An article in the November 1994 issue of Consumers Digest magazine tested several SRS-equipped sets from Sony and other manufacturers and concluded that the circuit was essentially a gimmick in these products due to their small, close-set speakers and low-wattage amplifiers. SRS is not a panacea for audio systems that are marginal to begin with; it works best with full-range, high-fidelity sound reproduction.
The company (publicly traded on NASDAQ under the symbol SRSL after a 1996 IPO) since developed or acquired several additional audio technologies, including SRS Headphone, TruSurround XT, TruBass (""psychoacoustic bass enhancement to enable deeper, natural bass of audio source material to be perceived over small speaker drivers""), FOCUS (""sound-image elevation used in combination with SRS to create a large sound image"", originally conceived for in-car listening), SRS Virtual Surround, Circle Surround, SRS Wow (an audio-enhancement suite made up of SRS 3D ""wide stereo imaging and nonfatiguing headphone listening mode"", Focus and TruBass) and SRS Wow HD, Dialog Clarity, and VIP, most using psychoacoustic principles similar to those employed by the SRS technology. SRS or one of its derivatives is offered in products from a wide range of professional and consumer audio manufacturers as well as in SRS's own small stable of products. The company derives most of its revenue from licensing its technologies, which it does in both silicon and software form.
SRS Wow was implemented in many devices such as music players, and in software such as Microsoft's Windows Media Player for playing sound on a computer.
In 2012 DTS, Inc. acquired the business of SRS Labs, including over 1,000 audio patents and trademarks.

See also
QSound

References
External links
Tech tips: How Stereo enhancements work?
Wayback Machine archives of SRS Web site for many dates.
URL of former SRS Labs official site, redirects to DTS",Category:Acoustics,1
112,113,Acoustic metric,"In mathematical physics, a metric describes the arrangement of relative distances within a surface or volume, usually measured by signals passing through the region – essentially describing the intrinsic geometry of the region. An acoustic metric will describe the signal-carrying properties characteristic of a given particulate medium in acoustics, or in fluid dynamics. Other descriptive names such as sonic metric are also sometimes used, interchangeably.

A simple fluid example
For simplicity, we will assume that the underlying background geometry is Euclidean, and that this space is filled with an isotropic inviscid fluid at zero temperature (e.g. a superfluid). This fluid is described by a density field ? and a velocity field 
  
    
      
        
          
            
              v
              ?
            
          
        
      
    
    {\displaystyle {\vec {v}}}
  . The speed of sound at any given point depends upon the compressibility which in turn depends upon the density at that point. This can be specified by the ""speed of sound field"" c. Now, the combination of both isotropy and Galilean covariance tells us that the permissible velocities of the sound waves at a given point x, 
  
    
      
        
          
            
              u
              ?
            
          
        
      
    
    {\displaystyle {\vec {u}}}
   has to satisfy

  
    
      
        (
        
          
            
              u
              ?
            
          
        
        ?
        
          
            
              v
              ?
            
          
        
        (
        x
        )
        
          )
          
            2
          
        
        =
        c
        (
        x
        
          )
          
            2
          
        
      
    
    {\displaystyle ({\vec {u}}-{\vec {v}}(x))^{2}=c(x)^{2}}
  
This restriction can also arise if we imagine that sound is like ""light"" moving though a spacetime described by an effective metric tensor called the acoustic metric.
The acoustic metric

  
    
      
        
          g
        
        =
        
          g
          
            00
          
        
        d
        t
        ?
        d
        t
        +
        2
        
          g
          
            0
            i
          
        
        d
        
          x
          
            i
          
        
        ?
        d
        t
        +
        
          g
          
            i
            j
          
        
        d
        
          x
          
            i
          
        
        ?
        d
        
          x
          
            j
          
        
      
    
    {\displaystyle \mathbf {g} =g_{00}dt\otimes dt+2g_{0i}dx^{i}\otimes dt+g_{ij}dx^{i}\otimes dx^{j}}
  
""Light"" moving with a velocity of 
  
    
      
        
          
            
              u
              ?
            
          
        
      
    
    {\displaystyle {\vec {u}}}
   (NOT the 4-velocity) has to satisfy

  
    
      
        
          g
          
            00
          
        
        +
        2
        
          g
          
            0
            i
          
        
        
          u
          
            i
          
        
        +
        
          g
          
            i
            j
          
        
        
          u
          
            i
          
        
        
          u
          
            j
          
        
        =
        0
      
    
    {\displaystyle g_{00}+2g_{0i}u^{i}+g_{ij}u^{i}u^{j}=0}
  
If

  
    
      
        g
        =
        
          ?
          
            2
          
        
        
          
            (
            
              
                
                  ?
                  (
                  
                    c
                    
                      2
                    
                  
                  ?
                  
                    v
                    
                      2
                    
                  
                  )
                
                
                  ?
                  
                    
                      
                        v
                        ?
                      
                    
                  
                
              
              
                
                  ?
                  
                    
                      
                        v
                        ?
                      
                    
                  
                
                
                  
                    1
                  
                
              
            
            )
          
        
      
    
    {\displaystyle g=\alpha ^{2}{\begin{pmatrix}-(c^{2}-v^{2})&-{\vec {v}}\\-{\vec {v}}&\mathbf {1} \end{pmatrix}}}
  
where ? is some conformal factor which is yet to be determined (see Weyl rescaling), we get the desired velocity restriction. ? may be some function of the density, for example.

Acoustic horizons
An acoustic metric can give rise to ""acoustic horizons"" (also known as ""sonic horizons""), analogous to the event horizons in the spacetime metric of general relativity. However, unlike the spacetime metric, in which the invariant speed is the absolute upper limit on the propagation of all causal effects, the invariant speed in an acoustic metric is not the upper limit on propagation speeds. For example, the speed of sound is less than the speed of light. As a result, the horizons in acoustic metrics are not perfectly analogous to those associated with the spacetime metric. It is possible for certain physical effects to propagate back across an acoustic horizon. Such propagation is sometimes considered to be analogous to Hawking radiation, although the latter arises from quantum field effects in curved spacetime.

Acoustic metrics and quantum gravity
Since acoustic metrics share some statistical behaviours with the way that we expect a future theory of quantum gravity to behave (such as Hawking radiation), these metrics have sometimes been studied in the hope that they might shed light on the statistical mechanics of actual black holes. Some people have suggested that analog models are more than just an analogy and that the actual gravity that we observe is actually an analog theory. But in order for this to hold, since a generic analog model depends upon BOTH the acoustic metric AND the underlying background geometry, the low energy large wavelength limit of the theory has to decouple from the background geometry.

See also
Analog models of gravity
Superfluid vacuum theory
Hawking radiation
Gravastar
Acoustics
Quantum gravity

References
W.G. Unruh, ""Experimental black hole evaporation"" Phys. Rev. Lett. 46 (1981), 1351–1353.
– considers information leakage through a transsonic horizon as an ""analogue"" of Hawking radiation in black hole problems
Matt Visser ""Acoustic black holes: Horizons, ergospheres, and Hawking radiation"" Class. Quant. Grav. 15 (1998), 1767–1791. gr-qc/9712010
– indirect radiation effects in the physics of acoustic horizon explored as a case of Hawking radiation
Carlos Barceló, Stefano Liberati, and Matt Visser, ""Analogue Gravity"" gr-qc/0505065
– huge review article of ""toy models"" of gravitation, 2005, currently on v2, 152 pages, 435 references, alphabetical by author.

External links
Acoustic black holes on arxiv.org",Category:Acoustics,1
113,114,Rarefaction,"Rarefaction is the reduction of an item's density, the opposite of compression. Like compression, which can travel in waves (sound waves, for instance), rarefaction waves also exist in nature. A common rarefaction wave is the area of low relative pressure following a shock wave (see picture).
Rarefaction waves expand with time (much like sea waves spread out as they reach a beach); in most cases rarefaction waves keep the same overall profile ('shape') at all times throughout the wave's movement: it is a self-similar expansion. Each part of the wave travels at the local speed of sound, in the local medium. This expansion behaviour is in contrast to the behaviour of pressure increases, which gets narrower with time, until they steepen into shock waves.

Physical examples
A natural example of rarefaction occurs in the layers of Earth's atmosphere. Because the atmosphere has mass, most atmospheric matter is nearer to the Earth due to the Earth's gravitation. Therefore, air at higher layers of the atmosphere is less dense, or rarefied, relative to air at lower layers. Thus rarefaction can refer either to a reduction in density over space at a single point of time, or a reduction of density over time for one particular area.
Rarefaction can be easily observed by compressing a spring and releasing it. Instead of seeing compressed loops seeming to move through the spring, spaced-out loops move through it: rarefaction waves.

Rarefaction in manufacturing
Modern construction of guitars is an example of using rarefaction in manufacturing. By forcing the reduction of density (loss of oils and other impurities) in the cellular structure of the soundboard, a rarefied guitar top produces a tonal decompression affecting the sound of the instrument, mimicking aged wood.

See also
Longitudinal wave
P-wave
Prandtl-Meyer expansion fan


== Citations ==",Category:Sound,1
114,115,Bore (wind instruments),"In music, the bore of a wind instrument (including woodwind and brass) is its interior chamber. This defines a flow path through which air travels, which is set into vibration to produce sounds. The shape of the bore has a strong influence on the instrument's timbre.

Bore shapes
The cone and the cylinder are the two idealized shapes used to describe the bores of wind instruments. These shapes affect the prominence of harmonics associated with the timbre of the instrument.

Cylindrical bore
The diameter of a cylindrical bore remains constant along its length. The acoustic behavior depends on whether the instrument is stopped (closed at one end and open at the other), or open (at both ends). For an open pipe, the wavelength produced by the first normal mode (the fundamental note) is approximately twice the length of the pipe. The wavelength produced by the second normal mode is half that, that is, the length of the pipe, so its pitch is an octave higher; thus an open cylindrical bore instrument overblows at the octave. This corresponds to the second harmonic, and generally the harmonic spectrum of an open cylindrical bore instrument is strong in both even and odd harmonics. For a stopped pipe, the wavelength produced by the first normal mode is approximately four times the length of the pipe. The wavelength produced by the second normal mode is one third that, i.e. the 4/3 length of the pipe, so its pitch is a twelfth higher; a stopped cylindrical bore instrument overblows at the twelfth. This corresponds to the third harmonic; generally the harmonic spectrum of a stopped cylindrical bore instrument, particularly in its bottom register, is strong in the odd harmonics only.
Instruments having a cylindrical, or mostly cylindrical, bore include:
Chalumeau
Clarinet
Cornamuse
Crumhorn
Flute (Boehm system — open)
Kortholt
Rackett (renaissance)
Sudrophone
Trumpet

Conical bore
The diameter of a conical bore varies linearly with distance from the end of the instrument. A complete conical bore would begin at zero diameter—the cone's vertex. However, actual instrument bores approximate a frustum of a cone. The wavelength produced by the first normal mode is approximately twice the length of the cone measured from the vertex. The wavelength produced by the second normal mode is approximately equal to the length of the cone, so its pitch is an octave higher. Therefore, a conical bore instrument, like one with an open cylindrical bore, overblows at the octave and generally has a harmonic spectrum strong in both even and odd harmonics.
Instruments having a conical, or approximately conical, bore include:
Alphorn
Bassoon
Conch shell
Cornet
Dulcian
Euphonium
Flugelhorn
Flute (pre-Boehm)
Oboe
Rackett (baroque)
Rauschpfeife
Saxhorn
Saxophone
Shawm
Tuba
Uilleann pipes

Woodwinds
Sections of the bores of woodwind instruments deviate from a true cone or a cylinder. For example, although oboes and oboes d'amore are similarly pitched, they have differently shaped terminal bells. Accordingly, the voice of the oboe is described as ""piercing"" as compared to the more ""full"" voice of the oboe d'amore.
Although the bore shape of woodwind instruments generally determines their timbre, the instruments' exterior geometry typically has little effect on their voice. In addition, the exterior shape of woodwind instruments may not overtly match the shape of their bores. For example, while oboes and clarinets may outwardly appear similar, oboes have a conical bore while clarinets have a cylindrical bore.
The bore of a modern recorder has a ""reversed"" taper, being wider at the head and narrower at the foot of the instrument.

Brasses
Brass instruments also are sometimes categorized as conical or cylindrical, though most in fact have cylindrical sections between a conical section (the mouthpiece taper or leadpipe) and a non-conical, non-cylindrical flaring section (the bell). Benade gives the following typical proportions:
These proportions vary as valves or slides are operated; the above numbers are for instruments with the valves open or the slide fully in. Therefore the normal mode frequencies of brass instruments do not correspond to integer multiples of the first mode. However, players of brasses (in contrast to woodwinds) are able to ""lip"" notes up or down substantially, and to make use of certain privileged frequencies in addition to those of the normal modes, to obtain in-tune notes.

See also
Acoustic resonance

Notes
References
Nederveen, Cornelis Johannes, Acoustical aspects of woodwind instruments. Amsterdam, Frits Knuf, 1969.
for waveform and harmonic characteristics, clarinet, and a conical, cylindrical comparison.
The previous author refers to: ""The conical bore in musical acoustics,"" by R. D. Ayers, L. J. Eliason, and D. Mahgerefteh, American Journal of Physics, Vol 53, No. 6, pgs 528-537, (1985).
A short description with waveforms of the bassoon. Also a general discussion of acoustics (with calculations and waveforms) in wind instruments [1] Jan. 18, 2011.",Category:Sound,1
115,116,Acoustic streaming,"Acoustic streaming is a steady flow in a fluid driven by the absorption of high amplitude acoustic oscillations. This phenomenon can be observed near sound emitters, or in the standing waves within a Kundt's tube. It is the less-known opposite of sound generation by a flow.
There are two situations where sound is absorbed in its medium of propagation:
during propagation. The attenuation coefficient is 
  
    
      
        ?
        =
        2
        ?
        
          ?
          
            2
          
        
        
          /
        
        (
        3
        ?
        
          c
          
            3
          
        
        )
      
    
    {\displaystyle \alpha =2\eta \omega ^{2}/(3\rho c^{3})}
  , following Stokes' law (sound attenuation). This effect is more intense at elevated frequencies and is much greater in air (where attenuation occurs on a characteristic distance 
  
    
      
        
          ?
          
            ?
            1
          
        
      
    
    {\displaystyle \alpha ^{-1}}
  ~10 cm at 1 MHz) than in water (
  
    
      
        
          ?
          
            ?
            1
          
        
      
    
    {\displaystyle \alpha ^{-1}}
  ~100 m at 1 MHz). In air it is known as the Quartz wind.
near a boundary. Either when sound reaches a boundary, or when a boundary is vibrating in a still medium. A wall vibrating parallel to itself generates a shear wave, of attenuated amplitude within the Stokes oscillating boundary layer. This effect is localised on an attenuation length of characteristic size 
  
    
      
        ?
        =
        [
        ?
        
          /
        
        (
        ?
        ?
        )
        
          ]
          
            1
            
              /
            
            2
          
        
      
    
    {\displaystyle \delta =[\eta /(\rho \omega )]^{1/2}}
   whose order of magnitude is a few micrometres in both air and water at 1 MHz.

Origin: a body force due to acoustic absorption in the fluid
Acoustic streaming is a non-linear effect.  We can decompose the velocity field in a vibration part and a steady part 
  
    
      
        
          u
        
        =
        v
        +
        
          
            u
            ¯
          
        
      
    
    {\displaystyle {u}=v+{\overline {u}}}
  . The vibration part 
  
    
      
        v
      
    
    {\displaystyle v}
   is due to sound, while the steady part is the acoustic streaming velocity (average velocity). The Navier–Stokes equations implies for the acoustic streaming velocity:

  
    
      
        
          
            ?
            ¯
          
        
        
          
            ?
            
              t
            
          
          
            
              
                u
                ¯
              
            
            
              i
            
          
        
        +
        
          
            ?
            ¯
          
        
        
          
            
              u
              ¯
            
          
          
            j
          
        
        
          
            ?
            
              j
            
          
          
            
              
                u
                ¯
              
            
            
              i
            
          
        
        =
        ?
        
          ?
          
            
              
                p
                ¯
              
            
            
              i
            
          
        
        +
        ?
        
          
            ?
            
              j
            
            
              2
            
          
          
            
              
                u
                ¯
              
            
            
              i
            
          
        
        ?
        
          
            ?
            
              j
            
          
        
        (
        
          
            
              ?
              
                v
                
                  i
                
              
              
                v
                
                  j
                
              
            
            ¯
          
        
        )
        .
      
    
    {\displaystyle {\overline {\rho }}{\partial _{t}{\overline {u}}_{i}}+{\overline {\rho }}{\overline {u}}_{j}{\partial _{j}{\overline {u}}_{i}}=-{\partial {\overline {p}}_{i}}+\eta {\partial _{j}^{2}{\overline {u}}_{i}}-{\partial _{j}}({\overline {\rho v_{i}v_{j}}}).}
  
The steady streaming originates from a steady body force 
  
    
      
        
          f
          
            i
          
        
        =
        ?
        
          ?
        
        (
        
          
            
              ?
              
                v
                
                  i
                
              
              
                v
                
                  j
                
              
            
            ¯
          
        
        )
        
          /
        
        
          ?
          
            x
            
              j
            
          
        
      
    
    {\displaystyle f_{i}=-{\partial }({\overline {\rho v_{i}v_{j}}})/{\partial x_{j}}}
   that appears on the right hand side. This force is a function of what is known as the Reynolds stresses in turbulence 
  
    
      
        ?
        
          
            
              ?
              
                v
                
                  i
                
              
              
                v
                
                  j
                
              
            
            ¯
          
        
      
    
    {\displaystyle -{\overline {\rho v_{i}v_{j}}}}
  . The Reynolds stress depends on the amplitude of sound vibrations, and the body force reflects diminutions in this sound amplitude.
We see that this stress is non-linear (quadratic) in the velocity amplitude. It is non vanishing only where the velocity amplitude varies. If the velocity of the fluid oscillates because of sound as 
  
    
      
        ?
        cos
        ?
        (
        ?
        t
        )
      
    
    {\displaystyle \epsilon \cos(\omega t)}
  , the quadratic non-linearity generates a steady force proportional to 
  
    
      
        
          
            
              
                
                  ?
                  
                    2
                  
                
                
                  cos
                  
                    2
                  
                
                ?
                (
                ?
                t
                )
              
              ¯
            
          
          =
          
            ?
            
              2
            
          
          
            /
          
          2
        
      
    
    {\displaystyle \scriptstyle {\overline {\epsilon ^{2}\cos ^{2}(\omega t)}}=\epsilon ^{2}/2}
  .

Order of magnitude of acoustic streaming velocities
Even if viscosity is responsible for acoustic streaming, the value of viscosity disappears from the resulting streaming velocities in the case of near-boundary acoustic steaming.
The order of magnitude of streaming velocities are:
near a boundary (outside of the boundary layer):

  
    
      
        U
        ?
        ?
        
          3
        
        
          /
        
        
          (
          4
          ?
          )
        
        ×
        
          v
          
            0
          
        
        d
        
          v
          
            0
          
        
        
          /
        
        d
        x
        ,
      
    
    {\displaystyle U\sim -{3}/{(4\omega )}\times v_{0}dv_{0}/dx,}
  
with 
  
    
      
        
          v
          
            0
          
        
      
    
    {\displaystyle v_{0}}
   the sound vibration velocity and 
  
    
      
        x
      
    
    {\displaystyle x}
   along the wall boundary. The flow is directed towards decreasing sound vibrations (vibration nodes).
near a vibrating bubble of rest radius a, whose radius pulsates with relative amplitude 
  
    
      
        ?
        =
        ?
        r
        
          /
        
        a
      
    
    {\displaystyle \epsilon =\delta r/a}
   (or 
  
    
      
        r
        =
        ?
        a
        sin
        ?
        (
        ?
        t
        )
      
    
    {\displaystyle r=\epsilon a\sin(\omega t)}
  ), and whose center of mass also periodically translates with relative amplitude 
  
    
      
        
          ?
          ?
        
        =
        ?
        x
        
          /
        
        a
      
    
    {\displaystyle \epsilon '=\delta x/a}
   (or 
  
    
      
        x
        =
        
          ?
          ?
        
        a
        sin
        ?
        (
        ?
        t
        
          /
        
        ?
        )
      
    
    {\displaystyle x=\epsilon 'a\sin(\omega t/\phi )}
  ). with a phase shift 
  
    
      
        ?
      
    
    {\displaystyle \phi }
  

  
    
      
        
          U
          ?
          ?
          
            ?
            ?
          
          a
          ?
          sin
          ?
          ?
        
      
    
    {\displaystyle \displaystyle U\sim \epsilon \epsilon 'a\omega \sin \phi }
  
far from walls 
  
    
      
        U
        ?
        ?
        P
        
          /
        
        (
        ?
        ?
        c
        )
      
    
    {\displaystyle U\sim \alpha P/(\pi \mu c)}
   far from the origin of the flow ( with 
  
    
      
        P
      
    
    {\displaystyle P}
  the acoustic power, 
  
    
      
        ?
      
    
    {\displaystyle \mu }
   the dynamic viscosity and 
  
    
      
        c
      
    
    {\displaystyle c}
   the celerity of sound). Nearer from the origin of the flow, the velocity scales as the root of 
  
    
      
        P
      
    
    {\displaystyle P}
  .


== References ==",Category:Acoustics,1
116,117,Radio acoustic ranging,"Radio acoustic ranging, occasionally written as ""radio-acoustic ranging"" and sometimes abbreviated RAR, was a method for determining a ship?s precise location at sea by detonating an explosive charge underwater near the ship, detecting the arrival of the underwater sound waves at remote locations, and radioing the time of arrival of the sound waves at the remote stations to the ship, allowing the ship?s crew to use triangulation to determine the ship?s position. Developed by the United States Coast and Geodetic Survey in 1923 and 1924 for use in accurately fixing the position of survey ships during hydrographic survey operations, it was the first navigation technique in human history other than dead reckoning that did not require visual observation of a landmark, marker, light, or celestial body, and the first non-visual means to provide precise positions. First employed operationally in 1924, radio acoustic ranging remained in use until 1944, when new radio navigation techniques developed during World War II rendered it obsolete.

Technique
To fix their position using radio acoustic ranging, a ship?s crew first ascertained the temperature and salinity of sea water in the vicinity of the ship to determine an accurate velocity of sound through the water. The crew then threw a small TNT bomb off the ship?s stern. It exploded at a depth of about 100 feet (30 meters), and a chronograph aboard the ship automatically recorded the time the explosion was heard at the ship. The sound traveled outward from the explosion, eventually reaching hydrophones at known locations – shore stations, anchored manned station ships, or unmanned moored buoys – at a distance from the ship. Each hydrophone was connected to a radio transmitter that automatically sent a signal indicating the time its hydrophone detected the sound. At the distances involved – generally less than 200 nautical miles (370 km) – each of these radio signals arrived at the ship at essentially the same instant that each of the remote hydrophones detected the sound of the explosion. The ship?s chronograph automatically recorded the time each radio signal arrived at the ship. By subtracting the time of the explosion from the time of radio signal reception, the ship?s crew could determine the length of time the sound wave required to travel from the point of the explosion to each remote hydrophone and, knowing the speed of sound in the surrounding sea water, could multiply the sound?s travel time by the velocity of sound in sea water to determine the distance between the explosion and the hydrophone. By determining the distance to at least two remote hydrophones in known locations, the ship?s crew could use triangulation to fix the ship?s position.
In deep waters, such as those that prevailed in the Pacific Ocean along the United States West Coast, the Coast and Geodetic Survey could rely upon shore stations to support radio acoustic ranging because the deep water allowed sound to travel to the coast. Along the United States East Coast, where shallower waters prevailed, sound had greater difficulty in reaching the coast, and the Coast and Geodetic Survey relied more heavily on anchored manned station ships, and later unmanned moored buoys, to support radio acoustic ranging.
Chronographs recorded times to the hundredth of a second, and the crew of a ship using radio acoustic ranging could determine their ship?s distance from the remote hydrophone stations to within 50 feet (15 meters), allowing them to plot their ship?s position with great accuracy for the time. With sound waves traveling from the point of the explosion to the distant hydrophones at about 0.8 nautical mile (1.5 km) per second, ships occasionally used radio acoustic ranging at distances of over 200 nautical miles (370 km) between ship and hydrophone station, and distances of 75 to 100 nautical miles (139 to 185 km) were common.

Development history
Precursors
Radio acoustic ranging had its origins in a growing understanding of underwater acoustics and their practical application during the early decades of the 20th century, and developed in parallel with echo sounding. The first step took place in the early 1900s, when the Submarine Signal Company invented a submarine bell signalling device and a hydrophone that could serve as a receiver of the underwater sounds the bells generated. The crew of a ship equipped with the receiving hydrophone could plot their ship?s distance from the submarine bell mechanism and plot intersecting lines from two or more bells to determine the ship?s position. The bells were installed at lighthouses, aboard lightvessels, and on buoys along the coasts of North America and Europe, and receiving hydrophones were mounted aboard hundreds of ships. It was history?s first practical use of acoustics in an ocean environment.
The sinking of RMS Titanic in 1912 spurred the Canadian inventor Reginald Fessenden (1866–1932) to begin work on a long-distance underwater sound transmission and reception system that could detect hazards in the path of a ship. This led to the invention of the Fessenden oscillator, an electro-acoustic transducer which by 1914 had a proven ability to transmit and receive sound at a distance of 31 miles across Massachusetts Bay and to detect an iceberg ahead of a ship at a range of two miles by bouncing sound off it and detecting the echo, as well as an occasional ability to detect the reflection of sound off the ocean bottom. Further impetus to developing practical applications of underwater acoustics came from World War I, which prompted the Royal Navy, United States Navy, and United States Army Coast Artillery Corps to experiment with sound as a means of detecting submerged submarines. In postwar experiments, the Coast Artillery Corps's Subaqueous Sound Ranging Section conducted experiments in shallow water in Vineyard Sound off Massachusetts in which it detonated explosive charges underwater at the ends of established baselines and measured the amount of time it took for the sound to arrive at hydrophones at the other ends of the baselines in order to establish very accurate measurements of the speed of sound through water. And in 1923, the Submarine Signal Company improved upon its underwater signaling devices by equipping them with radio transmitters that sent signals both to identify the particular device and to indicate to approaching ships that it would generate an acoustic signal at a specific time interval after it sent the radio signal, allowing ships to identify the specific navigational aid they were approaching and to take advantage of a one-way ranging capability that let their crews determine their direction and distance from the navigational aid.

Nicholas Heck
Realizing the potential of these applications of acoustics to hydrographic surveying and navigation, particularly along the United States West Coast, where fog frequently interfered with attempts to fix ship positions accurately, Ernest Lester Jones (1876–1929), then Director of the United States Coast and Geodetic Survey, in consultation with United States Coast and Geodetic Survey Corps officers, decided to investigate the use of acoustics in both depth finding and navigation. Nicholas H. Heck (1882–1953), a Coast and Geodetic Survey Corps officer, had been assigned from 1917 to 1919 to World War I service with the United States Naval Reserve Force, during which he had researched the use of underwater acoustics in antisubmarine warfare. He was the obvious choice to lead the new effort.
By January 1923, the Coast and Geodetic Survey had decided to install a Hayes sonic rangefinder – an early echo sounder – aboard the survey ship USC&GS Guide, which the Coast and Geodetic Survey planned to commission into its fleet later that year; successful operation of the sonic rangefinder would require a precise understanding of the speed of sound through water. When Heck contacted E. A. Stephenson of the U.S. Army Coast Artillery Corps to inform him of this plan and to inquire further about the Vineyard Sound experiments, Stephenson suggested that a system of hydrophones detecting the sound of underwater explosions could allow Coast and Geodetic Survey ships to fix their position while conducting surveys. Heck agreed, but believed that existing navigation aids would not meet the needs of the Coast and Geodetic Survey in terms of the immediacy and accuracy of position fixes. He envisioned improving on the Submarine Signal Company?s system of underwater noise generators and attached radio transmitters, as well as other previous concepts, by creating what would become known as the radio acoustic ranging method. Like echosounding, this method required an accurate calculation of the speed of sound through water.
Heck oversaw tests at Coast and Geodetic Survey headquarters in Washington, D.C., that demonstrated that shipboard recording of the time of an explosion could be performed accurately enough for his concept to work. He worked with Dr. E. A. Eckhardt, a physicist, and M. Keiser, an electrical engineer, of the National Bureau of Standards to develop a hydrophone system that could automatically send a radio signal when it detected the sound of an underwater explosion. When the Coast and Geodetic Survey commissioned Guide in 1923, Heck had her based at New London, Connecticut. Under his direction, Guide both tested her new echo sounder's ability to make accurate depth soundings and conducted radio acoustic ranging experiments in cooperation with the U.S. Army Coast Artillery Corps. Despite many difficulties, testing of both echo sounding and radio acoustic ranging wrapped up successfully in November 1923.

The cruise of the Guide
In late November 1923, with Heck aboard, Guide began a voyage from New London via Puerto Rico and the Panama Canal to San Diego, California, where she would be based in the future, with her route planned to take her over a wide variety of ocean depths so that she could continue to test her echo sounder. Guide made history during the voyage, becoming the first Coast and Geodetic Survey ship to use echo sounding to measure and record the depth of the sea at points along her course; she also measured water temperatures and took water samples so that the Scripps Institution for Biological Research (now the Scripps Institution of Oceanography) at La Jolla, California, could measure salinity levels. She also compared echo sounder soundings with those made by lead lines, discovering that using a single speed of sound through water, as had been the previous practice by those conducting echo sounding experiments, yielded acoustic depth-finding results that did not match the depths found by lead lines. Before she reached San Diego in December 1923, she had accumulated much data beneficial to the study of the movement of sound waves through water and measuring their velocity under varying conditions of salinity, density, and temperature, information essential both to depth-finding and radio acoustic ranging.
Upon arriving in California, Heck and Guide personnel in consultation with the Scripps Institution developed formulas that allowed accurate echo sounding of depths in all but the shallowest waters and installed hydrophones at La Jolla and Oceanside, California, to allow experimentation with radio acoustic ranging. Under Heck's direction, Guide then conducted experiments off the coast of California during the early months of 1924 that demonstrated that accurate echo sounding was possible using the new formulas. Experiments with radio acoustic ranging, despite initial difficulties, demonstrated that the method also was practical, although difficulty with getting some of the explosive charges to detonate hampered some of the experimental program. In April 1924, the Coast and Geodetic Survey concluded that both echo sounding and radio acoustic ranging were fundamentally sound, with no foundational problems left to solve, and that all that remained necessary was continued development and refinement of both techniques during their operational use. Heck turned over continued development of echo sounding and radio acoustic ranging to Guide's commanding officer, Commander Robert Luce, and returned to his duties in Washington, D.C.

Later development
Operating in the Pacific Ocean off Oregon in 1924, Guide became the first ship to employ radio acoustic ranging operationally. Off Oregon that year, she successfully employed the technique at a distance of 206 nautical miles (382 km) between the ranging explosion and the remote hydrophones detecting its sound and in the process achieved the first observed indication of the ocean sound layer that was later called the sound fixing and ranging (SOFAR) channel or deep sound channel (DSC). In 1928, French investigators extended this range, detonating a 30-kg (66-pound) explosive in the Mediterranean Sea between Algiers in French Algeria and Toulon, France, and detecting the sound at a range of 400 nautical miles (741 km).
Initially, Heck and others involved in the development of radio acoustic ranging thought the technique would prove least effective along the coast of the Pacific Northwest, where they assumed that the sound of wave action along the coast and the difficulty of setting up shore stations and cables would reduce the success of radio acoustic ranging; in contrast, they thought that conditions along the United States East Coast would pose no challenges. In fact, the opposite proved true: Among other problems, the relatively shallow water along the U.S. East Coast attenuated the sound of ranging explosions and shoals often blocked the sound from reaching shore at all. To overcome these difficulties, the Coast and Geodetic Survey anchored manned vessels well offshore along the U.S. East Coast to serve as hydrophone stations. In 1931, the Coast and Geodetic Survey proposed replacing the manned station ships with ""radio-sonobuoys"", and in July 1936 it began to place radio-sonobuoys in service. The 700-pound (317.5-kg) unmanned buoys – equipped with subsurface hydrophones, batteries, and radio transmitters that automatically sent a radio signal when their hydrophones detected the sound of a ranging explosion – could be deployed or recovered by Coast and Geodetic Survey ships in five minutes. Use of the buoys spread to the U.S. West Coast as well because they were cheaper to set up and operate than a shore station.
Radio acoustic ranging had limitations and drawbacks. Local peculiarities in the propagation of acoustic waves in the water column could degrade its accuracy, there were problems with maintaining hydrophone stations, and handling explosive charges posed a considerable danger to personnel and ships. On one occasion a Coast and Geodetic Survey Corps ensign on board the survey ship USC&GS Hydrographer inserted a radio acoustic ranging bomb in the mouth of a shark and released the shark, only to watch in horror as it swam back to the ship and exploded next to Hydrographer?s hull; the explosion rocked the ship. Aboard Guide in 1927, tragedy almost struck when a petty officer handling a bomb lit its fuse and then fell when the ship lurched; he dropped the bomb, which rolled into a gutter. The petty officer fell again before finally reaching the bomb and heaving it overboard just in time; it exploded alongside the ship just as it hit the water. The concussion prompted half the crew to rush up from below decks to find out what had happened.
As late as 1942, radio acoustic ranging remained important enough to the Coast and Geodetic Survey for it to devote just over 100 pages of its Hydrographic Manual to it. However, World War II, which by then had been raging for three years, gave impetus to the rapid development of purely radio-based navigation systems to assist bombers in finding their targets in darkness and bad weather. Such radio navigation systems were easier to maintain than hydrophone stations and did not require the handling of explosives and, as the new systems matured, the Coast and Geodetic Survey began to apply them to maritime navigation. Radio acoustic ranging appears not to have been used after 1944, and by 1946, Coast and Geodetic Survey ships had switched over to the new SHORAN electronic navigation technology to fix their positions.

Legacy
The first non-visual method of precise navigation in human history, and the first that could be used at any time of day or night and in any weather conditions, radio acoustic ranging was a major step forward in the development of modern navigation systems. Nicholas Heck revolutionized oceanic surveying through the use of radio electronic ranging to establish ship locations, one of his major contributions to oceanography. His work related to the technique also helped to develop underwater sound velocity tables allowing the establishment of ""true depths"" of up to five miles (8 km) using echo sounding.
Radio acoustic ranging was an early step along the path to modern electronic navigation systems, oceanographic telemetering systems, and the development of marine seismic surveying. The technique also laid the groundwork for the development of sonars capable of looking ahead of and to the sides of vessels.
The Coast and Geodetic Survey's radio-sonobuoys, developed to support radio acoustic ranging, were the ancestors of the sonobuoys used by ships and aircraft in antisubmarine warfare and underwater acoustic research today.

See also
Echo sounding
Hydrographic survey
Nicholas H. Heck
United States Coast and Geodetic Survey

References
External links
NOAA History: The Start of the Acoustic Work of the Coast and Geodetic Survey
NOAA History: Tools of the Trade: Radio Acoustic Ranging
NOAA 200th: Hydrographic Survey Techniques: Acoustic Survey Methods: Radio Acoustic Ranging
NOAA Coast Survey: A Monumental History
Hydro International ""System Without Fixed Points""
EVOLUTION OF THE SONOBUOY.pdf Holler, Roger A., ""The Evolution of the Sonobuoy From World War II to The Cold War,"" U.S. Navy Journal of Underwater Acoustics, January 2014",Category:Acoustics,1
117,118,Parametric array,"In the field of acoustics, a parametric array is a nonlinear transduction mechanism that generates narrow, nearly side lobe-free beams of low frequency sound, through the mixing and interaction of high frequency sound waves, effectively overcoming the diffraction limit (a kind of spatial 'uncertainty principle') associated with linear acoustics. The main side lobe-free beam of low frequency sound is created as a result of nonlinear mixing of two high frequency sound beams at their difference frequency. Parametric arrays can be formed in water, air, and earth materials/rock.

History
Priority for discovery and explanation of the parametric array owes to Peter J. Westervelt, winner of the Lord Rayleigh Medal (currently Professor Emeritus at Brown University), although important experimental work was contemporaneously underway in the former Soviet Union.
According to Muir [16, p. 554] and Albers [17], the concept for the parametric array occurred to Dr. Westervelt while he was stationed at the London, England, branch office of the Office of Naval Research in 1951.
According to Albers [17], he (Westervelt) there first observed an accidental generation of low frequency sound in air by Captain H.J. Round (British pioneer of the superheterodyne receiver) via the parametric array mechanism.
The phenomenon of the parametric array, seen first experimentally by Westervelt in the 1950s, was later explained theoretically in 1960, at a meeting of the Acoustical Society of America. A few years after this, a full paper [2] was published as an extension of Westervelt's classic work on the nonlinear Scattering of Sound by Sound, as described in [8,6,12].

Foundations
The foundation for Westervelt's theory of sound generation and scattering in nonlinear acoustic media owes to an application of Lighthill's equation (see Aeroacoustics) for fluid particle motion.
The application of Lighthill’s theory to the nonlinear acoustic realm yields the Westervelt–Lighthill Equation (WLE). Solutions to this equation have been developed using Green's functions [4,5] and Parabolic Equation (PE) Methods, most notably via the Kokhlov–Zablotskaya–Kuznetzov (KZK) equation.
An alternate mathematical formalism using Fourier operator methods in wavenumber space, was also developed by Westervelt, and generalized in [1] for solving the WLE in a most general manner. The solution method is formulated in Fourier (wavenumber) space in a representation related to the beam patterns of the primary fields generated by linear sources in the medium. This formalism has been applied not only to parametric arrays [15], but also to other nonlinear acoustic effects, such as the absorption of sound by sound and to the equilibrium distribution of sound intensity spectra in cavities [18].

Applications
Practical applications are numerous and include:
underwater sound
sonar
depth sounding
sub-bottom profiling
non-destructive testing
and 'see through walls' sensing
remote ocean sensing

medical ultrasound
and tomography [6]
underground seismic prospecting
active noise control
and directional high-fidelity commercial audio systems (Sound from ultrasound)
Parametric receiving arrays can also be formed for directional reception. In 2005, Elwood Norris won the $500,000 MIT-Lemelson Prize for his application of the parametric array to commercial high-fidelity loudspeakers.

References
Further reading
[1] H.C. Woodsum and P.J. Westervelt, ""A General Theory for the Scattering of Sound by Sound"", Journal of Sound and Vibration (1981), 76(2), 179-186.
[2] Peter J. Westervelt, ""Parametric Acoustic Array"", Journal of the Acoustical Society of America, Vol. 35, No. 4 (535-537), 1963
[4] Mark B. Moffett and Robert H. Mellen, ""Model for Parametric Sources"", J. Acoust. Soc. Am. Vol. 61, No. 2, Feb. 1977
[5] Mark B. Moffett and Robert H. Mellen, ""On Parametric Source Aperture Factors"", J. Acoust. Soc. Am. Vol. 60, No. 3, Sept. 1976
[6] Ronald A. Roy and Junru Wu, ""An Experimental Investigation of the Interaction of Two Non-Collinear Beams of Sound"", Proceedings of the 13th International Symposium on Nonlinear Acoustics, H. Hobaek, Editor, Elsevier Science Ltd., London (1993)
[7] Harvey C. Woodsum, ""Analytical and Numerical Solutions to the 'General Theory for the Scattering of Sound by Sound”, J. Acoust. Soc. Am. Vol. 95, No. 5, Part 2 (2PA14), June, 1994 (Program of the 134th Meeting of the Acoustical Society of America, Cambridge Massachusetts)
[8] Robert T. Beyer, Nonlinear Acoustics, 1st Edition (1974),. Published by the Naval Sea Systems Command.
[9] H.O. Berktay and D.J. Leahy, Journal of the Acoustical Society of America, 55, p. 539 (1974)
[10] M.J. Lighthill, ""On Sound Generated Aerodynamically”, Proc. R. Soc. Lond. A211, 564-687 (1952)
[11] M.J. Lighhill, “On Sound Generated Aerodynamically”, Proc. R. Soc. Lond. A222, 1-32 (1954)
[12] J.S. Bellin and R. T. Beyer, “Scattering of Sound by Sound”, J. Acoust. Soc. Am. 32, 339-341 (1960)
[13] M.J. Lighthill, Math. Revs. 19, 915 (1958)
[14] H.C. Woodsum, Bull. Of Am. Phys. Soc., Fall 1980; “A Boundary Condition Operator for Nonlinear Acoustics”
[15] H.C. Woodsum, Proc. 17th International Conference on Nonlinear Acoustics, AIP Press (NY), 2006; "" Comparison of Nonlinear Acoustic Experiments with a Formal Theory for the Scattering of Sound by Sound"", paper TuAM201.
[16] T.G. Muir, Office of Naval Research Special Report - ""Science, Technology and the Modern Navy, Thirtieth Anniversary (1946-1976), Paper ONR-37, ""Nonlinear Acoustics: A new Dimension in Underwater Sound"", published by the Department of the Navy (1976)
[17] V.M. Albers,""Underwater Sound, Benchmark Papers in Acoustics, p.415; Dowden, Hutchinson and Ross, Inc., Stroudsburg, PA (1972)
[18] M. Cabot and Seth Putterman, ""Renormalized Classical Non-linear Hydrodynamics, Quantum Mode Coupling and Quantum Theory of Interacting Phonons"", Physics Letters Vol. 83A, No. 3, 18 May 1981, pp. 91–94 (North Holland Publishing Company-Amsterdam)
[19] Nonlinear Parameter Imaging Computed Tomography by Parametric Acoustic Array Y. Nakagawa; M. Nakagawa; M. Yoneyama; M. Kikuchi IEEE 1984 Ultrasonics Symposium Volume, Issue, 1984 Page(s):673–676",Category:Sound,1
118,119,Institute of Acoustics,"The Institute of Acoustics (IOA) is a British professional engineering institution founded in 1974. It is licensed by the Engineering Council UK to assess candidates for inclusion on ECUK's Register of professional Engineers. The Institute's address is St Peter's House, 45-49 Victoria Street, St Albans, Herts, AL1 3WZ, United Kingdom. The current president of the IOA is Jo Webb. Past presidents include John Hinton OBE, Colin English, David Weston, Tony Jones, Professor Trevor Cox, William Egan, and Professor Bridget Shield.

History
In 1963 a Society of Acoustic Technology was formed in the UK for those interested in this subject: the President was Elfyn Richards. Because of the interest in establishing a professional body, meetings were held with various societies and institutions, and in 1965 a British Acoustical Society was set up, absorbing the earlier society. In 1974 the British Acoustical Society amalgamated with the Acoustics Group of the Institute of Physics to form the Institute of Acoustics.

Specialist groups
Building acoustics
Electroacoustics
Environmental noise
Measurement and instrumentation
Musical acoustics
Noise and vibration engineering
Physical acoustics
Speech and hearing
Underwater acoustics

Medals and awards
The following prizes are awarded by the Institute
Rayleigh Medal
Tyndall Medal
A B Wood Medal
R W B Stephens Medal
IOA Engineering Medal
Honorary fellowship
Peter Barnett Memorial Award
The Award for Promoting Acoustics to the Public
Award for Services to the Institute
IOA Young Persons' Award for Innovation in Acoustical Engineering
IOA Prize for best diploma student
ANC prize for the best diploma project
ANC prize for the best paper at an IOA conference

References
See also
Chartered engineer
Incorporated engineer
The Association of Noise Consultants

External links
Institute of Acoustics",Category:Acoustics,1
119,120,Kundt's tube,"Kundt's tube is an experimental acoustical apparatus invented in 1866 by German physicist August Kundt for the measurement of the speed of sound in a gas or a solid rod. It is used today only for demonstrating standing waves and acoustical forces.

How it works
The tube is a transparent horizontal pipe which contains a small amount of a fine powder such as cork dust, talc or Lycopodium. At one end of the tube is a source of sound at a single frequency (a pure tone). Kundt used a metal rod resonator that he caused to vibrate or 'ring' by rubbing it, but modern demonstrations usually use a loudspeaker attached to a signal generator producing a sine wave. The other end of the tube is blocked by a movable piston which can be used to adjust the length of the tube.
The sound generator is turned on and the piston is adjusted until the sound from the tube suddenly gets much louder. This indicates that the tube is at resonance. This means the length of the round-trip path of the sound waves, from one end of the tube to the other and back again, is a multiple of the wavelength ? of the sound waves. Therefore the length of the tube is a multiple of half a wavelength. At this point the sound waves in the tube are in the form of standing waves, and the amplitude of vibrations of air are zero at equally spaced intervals along the tube, called the nodes. The powder is caught up in the moving air and settles in little piles or lines at these nodes, because the air is still and quiet there. The distance between the piles is one half wavelength ?/2 of the sound. By measuring the distance between the piles, the wavelength ? of the sound in air can be found. If the frequency f of the sound is known, multiplying it by the wavelength gives the speed of sound c in air:

  
    
      
        c
        =
        ?
        f
        
      
    
    {\displaystyle c=\lambda f\,}
  
The detailed motion of the powder is actually due to an effect called acoustic streaming caused by the interaction of the sound wave with the boundary layer of air at the surface of the tube.

Further experiments
By filling the tube with other gases besides air, and partially evacuating it with a vacuum pump, Kundt was also able to calculate the speed of sound in different gases at different pressures. To create his vibrations, Kundt stopped the other end of the tube with a loose fitting stopper attached to the end of a metal rod projecting into the tube, clamped at its center. When it was rubbed lengthwise with a piece of leather coated with rosin, the rod vibrated longitudinally at its fundamental frequency, giving out a high note. Once the speed of sound in air was known, this allowed Kundt to calculate the speed of sound in the metal of the resonator rod. The length of the rod L was equal to a half wavelength of the sound in metal, and the distance between the piles of powder d was equal to a half wavelength of the sound in air. So the ratio of the two was equal to the ratio of the speed of sound in the two materials:

  
    
      
        
          
            
              c
              
                m
                e
                t
                a
                l
              
            
            
              c
              
                a
                i
                r
              
            
          
        
        =
        
          
            
              f
              
                ?
                
                  m
                  e
                  t
                  a
                  l
                
              
            
            
              f
              
                ?
                
                  a
                  i
                  r
                
              
            
          
        
        =
        
          
            
              ?
              
                m
                e
                t
                a
                l
              
            
            
              ?
              
                a
                i
                r
              
            
          
        
        =
        
          
            L
            d
          
        
        
      
    
    {\displaystyle {\frac {c_{metal}}{c_{air}}}={\frac {f\lambda _{metal}}{f\lambda _{air}}}={\frac {\lambda _{metal}}{\lambda _{air}}}={\frac {L}{d}}\,}

Reason for accuracy
A less accurate method of determining wavelength with a tube, used before Kundt, is simply to measure the length of the tube at resonance, which is approximately equal to a multiple of a half wavelength. The problem with this method is that when a tube of air is driven by a sound source, its length at resonance is not exactly equal to a multiple of the half-wavelength. Because the air at the source end of the tube, next to the speaker's diaphragm, is vibrating, it is not exactly at a node (point of zero amplitude) of the standing wave. The node actually occurs some distance beyond the end of the tube. Kundt's method allowed the actual locations of the nodes to be determined with great accuracy.

See also
Chladni plates, another standing wave visualization technique.
Rubens' tube, demonstrates the relationship between standing sound waves and sound pressure.

References
Further reading
Hortvet, J. (1902). A manual of elementary practical physics. Minneapolis: H.W. Wilson. Page 119+.",Category:CS1 German-language sources (de),1
120,121,Head-related transfer function,"A head-related transfer function (HRTF) also sometimes known as the anatomical transfer function (ATF) is a response that characterizes how an ear receives a sound from a point in space. As sound strikes the listener, the size and shape of the head, ears, ear canal, density of the head, size and shape of nasal and oral cavities, all transform the sound and affect how it is perceived, boosting some frequencies and attenuating others. Generally speaking, the HRTF boosts frequencies from 2 - 5 kHz with a primary resonance of +17 dB at 2,700 Hz. But the response curve is more complex than a single bump, affects a broad frequency spectrum, and varies significantly from person to person.
A pair of HRTFs for two ears can be used to synthesize a binaural sound that seems to come from a particular point in space. It is a transfer function, describing how a sound from a specific point will arrive at the ear (generally at the outer end of the auditory canal). Some consumer home entertainment products designed to reproduce surround sound from stereo (two-speaker) headphones use HRTFs. Some forms of HRTF-processing have also been included in computer software to simulate surround sound playback from loudspeakers.
Humans have just two ears, but can locate sounds in three dimensions – in range (distance), in direction above and below, in front and to the rear, as well as to either side. This is possible because the brain, inner ear and the external ears (pinna) work together to make inferences about location. This ability to localize sound sources may have developed in humans and ancestors as an evolutionary necessity, since the eyes can only see a fraction of the world around a viewer, and vision is hampered in darkness, while the ability to localize a sound source works in all directions, to varying accuracy, regardless of the surrounding light.
Humans estimate the location of a source by taking cues derived from one ear (monaural cues), and by comparing cues received at both ears (difference cues or binaural cues). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the head-related impulse response (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound. 
The HRTF is the Fourier transform of HRIR.
HRTFs for left and right ear (expressed above as HRIRs) describe the filtering of a sound source (x(t)) before it is perceived at the left and right ears as xL(t) and xR(t), respectively.
The HRTF can also be described as the modifications to a sound from a direction in free air to the sound as it arrives at the eardrum. These modifications include the shape of the listener's outer ear, the shape of the listener's head and body, the acoustic characteristics of the space in which the sound is played, and so on. All these characteristics will influence how (or whether) a listener can accurately tell what direction a sound is coming from.
In the AES69-2015 standard, the Audio Engineering Society (AES) has defined the SOFA file format for storing spatially oriented acoustic data like head-related transfer functions (HRTFs). SOFA software libraries and files are collected at the Sofa Conventions website.

How HRTF works
The associated mechanism varies between individuals, as their head and ear shapes differ.
HRTF describes how a given sound wave input (parameterized as frequency and source location) is filtered by the diffraction and reflection properties of the head, pinna, and torso, before the sound reaches the transduction machinery of the eardrum and inner ear (see auditory system). Biologically, the source-location-specific prefiltering effects of these external structures aid in the neural determination of source location, particularly the determination of the source's elevation (see vertical sound localization).

Technical derivation
Linear systems analysis defines the transfer function as the complex ratio between the output signal spectrum and the input signal spectrum as a function of frequency. Blauert (1974; cited in Blauert, 1981) initially defined the transfer function as the free-field transfer function (FFTF). Other terms include free-field to eardrum transfer function and the pressure transformation from the free-field to the eardrum. Less specific descriptions include the pinna transfer function, the outer ear transfer function, the pinna response, or directional transfer function (DTF).
The transfer function H(f) of any linear time-invariant system at frequency f is:
H(f) = Output(f) / Input(f)
One method used to obtain the HRTF from a given source location is therefore to measure the head-related impulse response (HRIR), h(t), at the ear drum for the impulse ?(t) placed at the source. The HRTF H(f) is the Fourier transform of the HRIR h(t).
Even when measured for a ""dummy head"" of idealized geometry, HRTF are complicated functions of frequency and the three spatial variables. For distances greater than 1 m from the head, however, the HRTF can be said to attenuate inversely with range. It is this far field HRTF, H(f, ?, ?), that has most often been measured. At closer range, the difference in level observed between the ears can grow quite large, even in the low-frequency region within which negligible level differences are observed in the far field.
HRTFs are typically measured in an anechoic chamber to minimize the influence of early reflections and reverberation on the measured response. HRTFs are measured at small increments of ? such as 15° or 30° in the horizontal plane, with interpolation used to synthesize HRTFs for arbitrary positions of ?. Even with small increments, however, interpolation can lead to front-back confusion, and optimizing the interpolation procedure is an active area of research.
In order to maximize the signal-to-noise ratio (SNR) in a measured HRTF, it is important that the impulse being generated be of high volume. In practice, however, it can be difficult to generate impulses at high volumes and, if generated, they can be damaging to human ears, so it is more common for HRTFs to be directly calculated in the frequency domain using a frequency-swept sine wave or by using maximum length sequences. User fatigue is still a problem, however, highlighting the need for the ability to interpolate based on fewer measurements.
The head-related transfer function is involved in resolving the Cone of Confusion, a series of points where ITD and ILD are identical for sound sources from many locations around the ""0"" part of the cone. When a sound is received by the ear it can either go straight down the ear into the ear canal or it can be reflected off the pinnae of the ear, into the ear canal a fraction of a second later. The sound will contain many frequencies, so therefore many copies of this signal will go down the ear all at different times depending on their frequency (according to reflection, diffraction, and their interaction with high and low frequencies and the size of the structures of the ear.) These copies overlap each other, and during this, certain signals are enhanced (where the phases of the signals match) while other copies are canceled out (where the phases of the signal do not match). Essentially, the brain is looking for frequency notches in the signal that correspond to particular known directions of sound.
If another person's ears were substituted, the individual would not immediately be able to localize sound, as the patterns of enhancement and cancellation would be different from those patterns the person's auditory system is used to. However, after some weeks, the auditory system would adapt to the new head-related transfer function. The inter-subject variability in the spectra of HRTFs has been studied through cluster analyses.
Assessing the variation through changes between the person's ear, we can limit our perspective with the degrees of freedom of the head and its relation with the spatial domain. Through this, we eliminate the tilt and other co-ordinate parameters that add complexity. For the purpose of calibration we are only concerned with the direction level to our ears, ergo a specific degree of freedom. Some of the ways in which we can deduce an expression to calibrate the HRTF are:
Localization of sound in Virtual Auditory space
HRTF Phase synthesis
HRTF Magnitude synthesis

Localization of sound in virtual auditory space
A basic assumption in the creation of a virtual auditory space is that if the acoustical waveforms present at a listener’s eardrums are the same under headphones as in free field, then the listener’s experience should also be the same.
Typically, sounds generated from headphones appear to originate from within the head. In the virtual auditory space, the headphones should be able to “externalize” the sound. Using the HRTF, sounds can be spatially positioned using the technique described below.
Let x1(t) represent an electrical signal driving a loudspeaker and y1(t) represent the signal received by a microphone inside the listener’s eardrum. Similarly, let x2(t) represent the electrical signal driving a headphone and y2(t) represent the microphone response to the signal. The goal of the virtual auditory space is to choose x2(t) such that y2(t) = y1(t). Applying the Fourier transform to these signals, we come up with the following two equations:
Y1 = X1LFM, and
Y2 = X2HM,
where L is the transfer function of the loudspeaker in the free field, F is the HRTF, M is the microphone transfer function, and H is the headphone-to-eardrum transfer function. Setting Y1 = Y2, and solving for X2 yields
X2 = X1LF/H.
By observation, the desired transfer function is
T= LF/H.
Therefore, theoretically, if x1(t) is passed through this filter and the resulting x2(t) is played on the headphones, it should produce the same signal at the eardrum. Since the filter applies only to a single ear, another one must be derived for the other ear. This process is repeated for many places in the virtual environment to create an array of head-related transfer functions for each position to be recreated while ensuring that the sampling conditions are set by the Nyquist criteria.

HRTF phase synthesis
There is less reliable phase estimation in the very low part of the frequency band, and in the upper frequencies the phase response is affected by the features of the pinna. Earlier studies also show that the HRTF phase response is mostly linear and that listeners are insensitive to the details of the interaural phase spectrum as long as the interaural time delay (ITD) of the combined low-frequency part of the waveform is maintained. This is the modeled phase response of the subject HRTF as a time delay, dependent on the direction and elevation.
A scaling factor is a function of the anthropometric features. For example, a training set of N subjects would consider each HRTF phase and describe a single ITD scaling factor as the average delay of the group. This computed scaling factor can estimate the time delay as function of the direction and elevation for any given individual. Converting the time delay to phase response for the left and the right ears is trivial.
The HRTF phase can be described by the ITD scaling factor. This is in turn is quantified by the anthropometric data of a given individual taken as the source of reference. For a generic case we consider ? as a sparse vector

  
    
      
        ?
        =
        [
        
          ?
          
            1
          
        
        ,
        
          ?
          
            2
          
        
        ,
        …
        ,
        
          ?
          
            N
          
        
        
          ]
          
            T
          
        
      
    
    {\displaystyle \beta =[\beta _{1},\beta _{2},\ldots ,\beta _{N}]^{T}}
  
that represents the subject’s anthropometric features as a linear superposition of the anthropometric features from the training data (y' = ?T X), and then apply the same sparse vector directly on the scaling vector H. We can write this task as a minimization problem, for a non-negative shrinking parameter ?:

  
    
      
        ?
        =
        
          argmin
          
            ?
          
        
        ?
        
          (
          
            
              ?
              
                a
                =
                1
              
              
                A
              
            
            
              (
              
                
                  y
                  
                    a
                  
                
                ?
                
                  ?
                  
                    n
                    =
                    1
                  
                  
                    N
                  
                
                
                  ?
                  
                    n
                  
                
                
                  X
                  
                    n
                  
                  
                    2
                  
                
              
              )
            
            +
            ?
            
              ?
              
                n
                =
                1
              
              
                N
              
            
            
              ?
              
                n
              
            
          
          )
        
      
    
    {\displaystyle \beta =\operatorname {argmin} \limits _{\beta }\left(\sum _{a=1}^{A}\left(y_{a}-\sum _{n=1}^{N}\beta _{n}X_{n}^{2}\right)+\lambda \sum _{n=1}^{N}\beta _{n}\right)}
  
From this, ITD scaling factor value H' is estimated as:

  
    
      
        
          H
          ?
        
        =
        
          ?
          
            n
            =
            1
          
          
            N
          
        
        
          ?
          
            n
          
        
        
          H
          
            n
          
        
        .
      
    
    {\displaystyle H'=\sum _{n=1}^{N}\beta _{n}H_{n}.}
  
where The ITD scaling factors for all persons in the dataset are stacked in a vector H ? RN, so the value Hn corresponds to the scaling factor of the n-th person.

HRTF magnitude synthesis
We solve the above minimization problem using Least Absolute Shrinkage and Selection Operator (LASSO). We assume that the HRTFs are represented by the same relation as the anthropometric features. Therefore, once we learn the sparse vector ? from the anthropometric features, we directly apply it to the HRTF tensor data and the subject’s HRTF values H' given by:

  
    
      
        
          H
          
            d
            ,
            k
          
          ?
        
        =
        
          ?
          
            n
            =
            1
          
          
            N
          
        
        
          ?
          
            n
          
        
        
          H
          
            n
            ,
            d
            ,
            k
          
        
      
    
    {\displaystyle H'_{d,k}=\sum _{n=1}^{N}\beta _{n}H_{n,d,k}}
  
where The HRTFs for each subject are described by a tensor of size D × K, where D is the number of HRTF directions and K is the number of frequency bins. All Hn,d,k corresponds to all the HRTFs of the training set are stacked in a new tensor H ? RN×D×K, so the value Hn,d,k corresponds to the k-th frequency bin for dth HRTF direction of the n-th person. Also H'd,k corresponds to kth frequency for every d-th HRTF direction of the synthesized HRTF.

Recording technology
Recordings processed via an HRTF, such as in a computer gaming environment (see A3D, EAX and OpenAL), which approximates the HRTF of the listener, can be heard through stereo headphones or speakers and interpreted as if they comprise sounds coming from all directions, rather than just two points either side of the head. The perceived accuracy of the result depends on how closely the HRTF data set matches the characteristics of one's own ears.

See also
3D sound reconstruction
A3D
Binaural recording
Dummy head recording
Environmental audio extensions
OpenAL
Sound Retrieval System
Sound localization
Soundbar
Sensaura
Transfer function

References
External links
Spatial Sound Tutorial
CIPIC HRTF Database
Listen HRTF Database
High-resolution HRTF and 3D ear model database (48 subjects)
AIR Database (HRTF database in reverberant environments)
Full Sphere HRIR/HRTF Database of the Neumann KU100
MIT Database (one dataset)
ARI (Acoustics Research Institute) Database (90+ datasets)",Category:Acoustics,1
121,122,Head shadow,"A head shadow or acoustic shadow is a region of reduced amplitude of a sound because it is obstructed by the head. Sound may have to travel through and around the head in order to reach an ear. The obstruction caused by the head can account for a significant attenuation (reduced amplitude) of overall intensity as well as cause a filtering effect. The filtering effects of head shadowing are an essential element of sound localisation—the brain weighs the relative amplitude, timbre, and phase of a sound heard by the two ears and uses the difference to interpret directional information.
The shadowed ear, the ear further from the sound source, receives sound slightly later (up to approximately 0.7 ms later) than the unshadowed ear, and the timbre, or frequency spectrum, of the shadowed sound wave is different because of the obstruction of the head.
The head shadow causes particular difficulty in sound localisation in people suffering from unilateral hearing loss. It is a factor to consider when correcting hearing loss with directional hearing aids.

See also
Interaural intensity difference
Hearing
Ear


== References ==",Category:Acoustics,1
122,123,Outline of acoustics,"The following outline is provided as an overview of and topical guide to acoustics:
Acoustics – interdisciplinary science that deals with the study of all mechanical waves in gases, liquids, and solids including topics such as vibration, sound, ultrasound and infrasound. A scientist who works in the field of acoustics is an acoustician while someone working in the field of acoustics technology may be called an acoustical engineer. The application of acoustics is present in almost all aspects of modern society with the most obvious being the audio and noise control industries.

History of acoustics
Branches of acoustics
Archaeoacoustics – study of sound within archaeology. This typically involves studying the acoustics of archaeological sites and artefacts.
Aeroacoustics – study of noise generated by air movement, for instance via turbulence, and the movement of sound through the fluid air. This knowledge is applied in acoustical engineering to study how to quieten aircraft. Aeroacoustics is important to understanding how wind musical instruments work.
Architectural acoustics – science of how to achieve a good sound within a building. It typically involves the study of speech intelligibility, speech privacy and music quality in the built environment. Also known as building acoustics.
Bioacoustics – scientific study of the hearing and calls of animal calls, as well as how animals are affected by the acoustic and sounds of their habitat.
Electroacoustics – concerned with the recording, manipulation and reproduction of audio using electronics. This might include products such as mobile phones, large scale public address systems or virtual reality systems in research laboratories.
Environmental noise – concerned with noise and vibration caused by railways, road traffic, aircraft, industrial equipment and recreational activities. The main aim of these studies is to reduce levels of environmental noise and vibration. Research work now also has a focus on the positive use of sound in urban environments: soundscapes and tranquility.
Musical acoustics – study of the physics of acoustic instruments; the audio signal processing used in electronic music; the computer analysis of music and composition, and the perception and cognitive neuroscience of music.
Psychoacoustics – study of how humans respond to sounds.
Acoustic signal processing – electronic manipulation of acoustic signals. Applications include: active noise control; design for hearing aids or cochlear implants; echo cancellation; music information retrieval, and perceptual coding (e.g. MP3 or Opus).
Acoustics of speech – acousticians study the production, processing and perception of speech. Speech recognition and Speech synthesis are two important areas of speech processing using computers. The subject also overlaps with the disciplines of physics, physiology, psychology, and linguistics.
Ultrasound – Ultrasonics deals with sounds at frequencies too high to be heard by humans. Specialisms include medical ultrasonics (including medical ultrasonography), sonochemistry, material characterisation and underwater acoustics (Sonar).
Underwater acoustics – scientific study of natural and man-made sounds underwater. Applications include sonar to locate submarines, underwater communication by whales, climate change monitoring by measuring sea temperatures acoustically, sonic weapons, and marine bioacoustics.
Acoustics of vibration – study of how mechanical systems vibrate and interact with their surroundings. Applications might include: ground vibrations from railways; vibration isolation to reduce vibration in operating theatres; studying how vibration can damage health (vibration white finger); vibration control to protect a building from earthquakes, or measuring how structure-borne sound moves through buildings.

Acoustic software
Baudline
Beatmapping
Composers Desktop Project
Diamond Cut Audio Restoration Tools
Enhanced Acoustic Simulator for Engineers
Kyma (sound design language)
NU-Tech
Scratch Live
Unit generator
Vinyl emulation software

Acoustics organizations
Acoustics publications
Applied Acoustics
Journal of Sound and Vibration
Journal of the Acoustical Society of America
Ultrasonics

Influential acousticians
See also
Sound
Wave

References
External links

This outline displayed as a mindmap, at wikimindmap.com
Acoustical Society of America
Institute of Acoustic in UK
National Council of Acoustical Consultants
International Commission for Acoustics
Institute of Noise Control Engineers",Category:All articles with empty sections,1
123,124,Category:Acoustic fingerprinting,,Category:Acoustics,1
124,125,Medical ultrasound,"Medical ultrasound (also known as diagnostic sonography or ultrasonography) is a diagnostic imaging technique based on the application of ultrasound. It is used to see internal body structures such as tendons, muscles, joints,blood vessels and internal organs. Its aim is often to find a source of a disease or to exclude any pathology. The practice of examining pregnant women using ultrasound is called obstetric ultrasound, and is widely used.
Ultrasound is sound waves with frequencies which are higher than those audible to humans (>20,000 Hz). Ultrasonic images also known as sonograms are made by sending pulses of ultrasound into tissue using a probe. The sound echoes off the tissue; with different tissues reflecting varying degrees of sound. These echoes are recorded and displayed as an image to the operator.
Many different types of images can be formed using sonographic instruments. The most well-known type is a B-mode image, which displays the acoustic impedance of a two-dimensional cross-section of tissue. Other types of image can display blood flow, motion of tissue over time, the location of blood, the presence of specific molecules, the stiffness of tissue, or the anatomy of a three-dimensional region.
Compared to other prominent methods of medical imaging, ultrasound has several advantages. It provides images in real-time, it is portable and can be brought to the bedside, it is substantially lower in cost, and it does not use harmful ionizing radiation. Drawbacks of ultrasonography include various limits on its field of view including patient cooperation and physique, difficulty imaging structures behind bone and air, and its dependence on a skilled operator.

By organ or system
Sonography (ultrasonography) is widely used in medicine. It is possible to perform both diagnosis and therapeutic procedures, using ultrasound to guide interventional procedures (for instance biopsies or drainage of fluid collections). Sonographers are medical professionals who perform scans which are then typically interpreted by themselves or the radiologists, physicians who specialize in the application and interpretation of a wide variety of medical imaging modalities, or by cardiologists in the case of cardiac ultrasonography (echocardiography). Sonographers typically use a hand-held probe (called a transducer) that is placed directly on and moved over the patient. Increasingly, clinicians (physicians and other healthcare professionals who provide direct patient care) are using ultrasound in their office and hospital practices.
Sonography is effective for imaging soft tissues of the body. Superficial structures such as muscles, tendons, testes, breast, thyroid and parathyroid glands, and the neonatal brain are imaged at a higher frequency (7–18 MHz), which provides better axial and lateral resolution. Deeper structures such as liver and kidney are imaged at a lower frequency 1–6 MHz with lower axial and lateral resolution but greater penetration.
A general-purpose ultrasound scanner may be used for most imaging purposes. Usually specialty applications may be served only by use of a specialty transducer. Most ultrasound procedures are done using a transducer on the surface of the body, but improved diagnostic confidence is often possible if a transducer can be placed inside the body. For this purpose, specialty transducers, including endovaginal, endorectal, and transesophageal transducers are commonly employed. At the extreme of this, very small transducers can be mounted on small diameter catheters and placed into blood vessels to image the walls and disease of those vessels.

Anesthesiology
In anesthesiology, Ultrasound is commonly used by anesthesiologists to guide injecting needles when placing local anaesthetic solutions near nerves. It is also used for gaining vascular access such as central venous cannulation and difficult arterial cannulation. Transcranial Doppler is frequently used by neuro-anesthesiologists for obtaining information about flow-velocity in the basal cerebral vessels.

Angiology (vascular)
In angiology or vascular medicine, duplex ultrasound (B Mode vessels imaging combined with Doppler flow measurement) is daily used to diagnose arterial and venous disease all over the body. This is particularly important in neurology, where ultrasound is used for assessing blood flow and stenoses in the carotid arteries (Carotid ultrasonography) and the big intracerebral arteries (Transcranial Doppler).
Intravascular ultrasound (IVUS) is a methodology using a specially designed catheter with a miniaturized ultrasound probe attached to the distal end of the catheter. The proximal end of the catheter is attached to computerized ultrasound equipment. It allows the application of ultrasound technology, such as piezoelectric transducer or CMUT, to see from inside blood vessels out through the surrounding blood column, visualizing the endothelium (inner wall) of blood vessels in living individuals.

Cardiology (heart)
Echocardiography is an essential tool in cardiology, to diagnose e.g. dilatation of parts of the heart and function of heart ventricles and valves

Emergency medicine
Point of care emergency ultrasound has many applications in emergency medicine, including the Focused Assessment with Sonography for Trauma (FAST) exam for assessing significant hemoperitoneum or pericardial tamponade after trauma. Ultrasound is occasionally used in the emergency department to expedite the care of people with right upper quadrant abdominal pain who may have gallstones or cholecystitis.

Gastroenterology/Colorectal surgery
Abdominal and endoanal ultrasound are frequently used in gastroenterology and colorectal surgery. In abdominal sonography, the solid organs of the abdomen such as the pancreas, aorta, inferior vena cava, liver, gall bladder, bile ducts, kidneys, and spleen are imaged. Sound waves are blocked by gas in the bowel and attenuated in different degree by fat, therefore there are limited diagnostic capabilities in this area. The appendix can sometimes be seen when inflamed (as in e.g.: appendicitis). Endoanal ultrasound is used particularly in the investigation of anorectal symptoms such as fecal incontinence or obstructed defecation. It images the immediate perianal anatomy and is able to detect occult defects such as tearing of the anal sphincter. Ultrasonography of liver tumors allows for both detection and characterization.

Gynecology and obstetrics
Gynecologic ultrasonography examines female pelvic organs (specifically the uterus, the ovaries, and the Fallopian tubes) as well as the bladder, the adnexa, and the Pouch of Douglas. It commonly uses vaginal ultrasonography. Obstetrical sonography is commonly used during pregnancy to check on the development of the fetus.

Otolaryngology (head and neck)
Most structures of the neck, including the thyroid and parathryoid glands, lymph nodes, and salivary glands, are well-visualized by high-frequency ultrasound with exceptional anatomic detail. Ultrasound is the preferred imaging modality for thyroid tumors and lesions, and ultrasonography is critical in the evaluation, preoperative planning, and postoperative surveillance of patients with thyroid cancer. Many other benign and malignant conditions in the head and neck can be evaluated and managed with the help of diagnostic ultrasound and ultrasound-guided procedures.

Neonatology
In neonatology, transcranial Doppler can be used for basic assessment of intracerebral structural abnormalities, bleeds, ventriculomegaly or hydrocephalus and anoxic insults (Periventricular leukomalacia). The ultrasound can be performed through the soft spots in the skull of a newborn infant (Fontanelle) until these completely close at about 1 year of age and form a virtually impenetrable acoustic barrier for the ultrasound. The most common site for cranial ultrasound is the anterior fontanelle. The smaller the fontanelle, the poorer the quality of the picture.

Ophthalmology (eyes)
In ophthalmology and optometry, there are two major forms of eye exam using ultrasound:
A-scan ultrasound biometry, commonly referred to as an A-scan (short for Amplitude scan). It is an A-mode that provides data on the length of the eye, which is a major determinant in common sight disorders.
B-scan ultrasonography, or B-scan, which is a B-mode scan that produces a cross-sectional view of the eye and the orbit. It is commonly used to see inside the eye when media is hazy due to cataract or any corneal opacity.

Pulmonology (lungs)
In pulmonology, endobronchial Ultrasound (EBUS) probes are applied to standard flexible endoscopic probes and used by pulmonologists to allow for direct visualization of endobronchial lesions and lymph nodes prior to transbronchial needle aspiration. Among its many uses, EBUS aids in lung cancer staging by allowing for lymph node sampling without the need for major surgery.

Urology (urinary)
Ultrasound is routinely used in urology to determine, for example, the amount of fluid retained in a patient's bladder. In a pelvic sonogram, organs of the pelvic region are imaged. This includes the uterus and ovaries or urinary bladder. Males are sometimes given a pelvic sonogram to check on the health of their bladder, the prostate, or their testicles (for example to distinguish epididymitis from testicular torsion). In young males, it is used to distinguish more benign testicular masses (varicocele or hydrocele) from testicular cancer, which is highly curable but which must be treated to preserve health and fertility. There are two methods of performing a pelvic sonography – externally or internally. The internal pelvic sonogram is performed either transvaginally (in a woman) or transrectally (in a man). Sonographic imaging of the pelvic floor can produce important diagnostic information regarding the precise relationship of abnormal structures with other pelvic organs and it represents a useful hint to treat patients with symptoms related to pelvic prolapse, double incontinence and obstructed defecation. It is used to diagnose and, at higher frequencies, to treat (break up) kidney stones or kidney crystals (nephrolithiasis).

Musculoskeletal
Musculoskeletal ultrasound in used to examine tendons, muscles, nerves, ligaments, soft tissue masses, and bone surfaces. Ultrasound is an alternative to x-ray imaging in detecting fractures of the wrist, elbow and shoulder for patients up to 12 years (Fracture sonography).
Quantitative ultrasound is an adjunct musculoskeletal test for myopathic disease in children; estimates of lean body mass in adults; proxy measures of muscle quality (i.e., tissue composition) in older adults with sarcopenia

Nephrology (kidneys)
In nephrology, ultrasonography of the kidneys is essential in the diagnosis and management of kidney-related diseases. The kidneys are easily examined, and most pathological changes in the kidneys are distinguishable with ultrasound. US is an accessible, versatile inexpensive and fast aid for decision-making in patients with renal symptoms and for guidance in renal intervention. Renal ultrasound (US) is a common examination, which has been performed for decades. Using B-mode imaging, assessment of renal anatomy is easily performed, and US is often used as image guidance for renal interventions. Furthermore, novel applications in renal US have been introduced with contrast-enhanced ultrasound (CEUS), elastography and fusion imaging. However, renal US has certain limitations, and other modalities, such as CT and MRI, should always be considered as supplementary imaging modalities in the assessment of renal disease.

From sound to image
The creation of an image from sound is done in three steps – producing a sound wave, receiving echoes, and interpreting those echoes.

Producing a sound wave
A sound wave is typically produced by a piezoelectric transducer encased in a plastic housing. Strong, short electrical pulses from the ultrasound machine drive the transducer at the desired frequency. The frequencies can be anywhere between 1 and 18 MHz, though frequencies up to 50–100 megahertz have been used experimentally in a technique known as biomicroscopy in special regions, such as the anterior chamber of the eye. Older technology transducers focused their beam with physical lenses. Newer technology transducers use phased array techniques to enable the ultrasound machine to change the direction and depth of focus.
The sound is focused either by the shape of the transducer, a lens in front of the transducer, or a complex set of control pulses from the ultrasound scanner, in the (beamforming) technique. This focusing produces an arc-shaped sound wave from the face of the transducer. The wave travels into the body and comes into focus at a desired depth.
Materials on the face of the transducer enable the sound to be transmitted efficiently into the body (often a rubbery coating, a form of impedance matching). In addition, a water-based gel is placed between the patient's skin and the probe.
The sound wave is partially reflected from the layers between different tissues or scattered from smaller structures. Specifically, sound is reflected anywhere where there are acoustic impedance changes in the body: e.g. blood cells in blood plasma, small structures in organs, etc. Some of the reflections return to the transducer.

Receiving the echoes
The return of the sound wave to the transducer results in the same process as sending the sound wave, except in reverse. The returned sound wave vibrates the transducer and the transducer turns the vibrations into electrical pulses that travel to the ultrasonic scanner where they are processed and transformed into a digital image.

Forming the image
To make an image, the ultrasound scanner must determine two things from each received echo:
How long it took the echo to be received from when the sound was transmitted.
How strong the echo was.
Once the ultrasonic scanner determines these two things, it can locate which pixel in the image to light up and to what intensity.
Transforming the received signal into a digital image may be explained by using a blank spreadsheet as an analogy. First picture a long, flat transducer at the top of the sheet. Send pulses down the 'columns' of the spreadsheet (A, B, C, etc.). Listen at each column for any return echoes. When an echo is heard, note how long it took for the echo to return. The longer the wait, the deeper the row (1,2,3, etc.). The strength of the echo determines the brightness setting for that cell (white for a strong echo, black for a weak echo, and varying shades of grey for everything in between.) When all the echoes are recorded on the sheet, we have a greyscale image.

Displaying the image
Images from the ultrasound scanner are transferred and displayed using the DICOM standard. Normally, very little post processing is applied to ultrasound images.

Sound in the body
Ultrasonography (sonography) uses a probe containing multiple acoustic transducers to send pulses of sound into a material. Whenever a sound wave encounters a material with a different density (acoustical impedance), part of the sound wave is reflected back to the probe and is detected as an echo. The time it takes for the echo to travel back to the probe is measured and used to calculate the depth of the tissue interface causing the echo. The greater the difference between acoustic impedances, the larger the echo is. If the pulse hits gases or solids, the density difference is so great that most of the acoustic energy is reflected and it becomes impossible to see deeper.
The frequencies used for medical imaging are generally in the range of 1 to 18 MHz. Higher frequencies have a correspondingly smaller wavelength, and can be used to make sonograms with smaller details. However, the attenuation of the sound wave is increased at higher frequencies, so in order to have better penetration of deeper tissues, a lower frequency (3–5 MHz) is used.
Seeing deep into the body with sonography is very difficult. Some acoustic energy is lost every time an echo is formed, but most of it (approximately 
  
    
      
        
          0.5
          
            
              
                dB
              
              
                
                  
                    cm depth
                  
                
                ?
                
                  
                    MHz
                  
                
              
            
          
        
      
    
    {\displaystyle \textstyle 0.5{\frac {\mbox{dB}}{{\mbox{cm depth}}\cdot {\mbox{MHz}}}}}
  ) is lost from acoustic absorption. (See also Acoustic attenuation for further details on modeling of acoustic attenuation and absorption.)
The speed of sound varies as it travels through different materials, and is dependent on the acoustical impedance of the material. However, the sonographic instrument assumes that the acoustic velocity is constant at 1540 m/s. An effect of this assumption is that in a real body with non-uniform tissues, the beam becomes somewhat de-focused and image resolution is reduced.
To generate a 2D-image, the ultrasonic beam is swept. A transducer may be swept mechanically by rotating or swinging. Or a 1D phased array transducer may be used to sweep the beam electronically. The received data is processed and used to construct the image. The image is then a 2D representation of the slice into the body.
3D images can be generated by acquiring a series of adjacent 2D images. Commonly a specialised probe that mechanically scans a conventional 2D-image transducer is used. However, since the mechanical scanning is slow, it is difficult to make 3D images of moving tissues. Recently, 2D phased array transducers that can sweep the beam in 3D have been developed. These can image faster and can even be used to make live 3D images of a beating heart.
Doppler ultrasonography is used to study blood flow and muscle motion. The different detected speeds are represented in color for ease of interpretation, for example leaky heart valves: the leak shows up as a flash of unique color. Colors may alternatively be used to represent the amplitudes of the received echoes.

Modes
Several modes of ultrasound are used in medical imaging. These are:
A-mode: A-mode (amplitude mode) is the simplest type of ultrasound. A single transducer scans a line through the body with the echoes plotted on screen as a function of depth. Therapeutic ultrasound aimed at a specific tumor or calculus is also A-mode, to allow for pinpoint accurate focus of the destructive wave energy.
B-mode or 2D mode: In B-mode (brightness mode) ultrasound, a linear array of transducers simultaneously scans a plane through the body that can be viewed as a two-dimensional image on screen. More commonly known as 2D mode now.

B-flow is a mode that digitally highlights weak flow reflectors (mainly red blood cells) while suppressing the signals from the surrounding stationary tissue. It can visualize flowing blood and surrounding stationary tissues simultaneously.

C-mode: A C-mode image is formed in a plane normal to a B-mode image. A gate that selects data from a specific depth from an A-mode line is used; then the transducer is moved in the 2D plane to sample the entire region at this fixed depth. When the transducer traverses the area in a spiral, an area of 100 cm2 can be scanned in around 10 seconds.
M-mode: In M-mode (motion mode) ultrasound, pulses are emitted in quick succession – each time, either an A-mode or B-mode image is taken. Over time, this is analogous to recording a video in ultrasound. As the organ boundaries that produce reflections move relative to the probe, this can be used to determine the velocity of specific organ structures.
Doppler mode: This mode makes use of the Doppler effect in measuring and visualizing blood flow
Color Doppler: Velocity information is presented as a color-coded overlay on top of a B-mode image
Continuous wave (CW) Doppler: Doppler information is sampled along a line through the body, and all velocities detected at each time point are presented (on a time line)
Pulsed wave (PW) Doppler: Doppler information is sampled from only a small sample volume (defined in 2D image), and presented on a timeline
Duplex: a common name for the simultaneous presentation of 2D and (usually) PW Doppler information. (Using modern ultrasound machines, color Doppler is almost always also used; hence the alternative name Triplex.)

Pulse inversion mode: In this mode, two successive pulses with opposite sign are emitted and then subtracted from each other. This implies that any linearly responding constituent will disappear while gases with non-linear compressibility stand out. Pulse inversion may also be used in a similar manner as in Harmonic mode; see below:
Harmonic mode: In this mode a deep penetrating fundamental frequency is emitted into the body and a harmonic overtone is detected. This way noise and artifacts due to reverberation and aberration are greatly reduced. Some also believe that penetration depth can be gained with improved lateral resolution; however, this is not well documented.

Expansions
An additional expansion or additional technique of ultrasound is biplanar ultrasound, in which the probe has two 2D planes that are perpendicular to each other, providing more efficient localization and detection. Furthermore, an omniplane probe is one that can rotate 180° to obtain multiple images. In 3D ultrasound, many 2D planes are digitally added together to create a 3-dimensional image of the object.

Doppler ultrasonography
Doppler ultrasonography is employs the Doppler effect to assess whether structures (usually blood) are moving towards or away from the probe, and its relative velocity. By calculating the frequency shift of a particular sample volume, for example flow in an artery or a jet of blood flow over a heart valve, its speed and direction can be determined and visualized. Color Doppler is the measurement of velocity by color scale. Color Doppler images are generally combined with grayscale (B-mode) images to display duplex ultrasonography images. Uses include:
Doppler echocardiography, the use of Doppler ultrasonography to examine the heart. An echocardiogram can, within certain limits, produce accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. Velocity measurements allow assessment of cardiac valve areas and function, any abnormal communications between the left and right side of the heart, any leaking of blood through the valves (valvular regurgitation), calculation of the cardiac output and calculation of E/A ratio (a measure of diastolic dysfunction). Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.
Transcranial Doppler (TCD) and transcranial color Doppler (TCCD), which measure the velocity of blood flow through the brain's blood vessels transcranially (through the cranium). They are used as tests to help diagnose emboli, stenosis, vasospasm from a subarachnoid hemorrhage (bleeding from a ruptured aneurysm), and other problems.
Doppler fetal monitors, although usually not technically -graphy but rather sound-generating, use the Doppler effect to detect the fetal heartbeat for prenatal care. These are hand-held, and some models also display the heart rate in beats per minute (BPM). Use of this monitor is sometimes known as Doppler auscultation. The Doppler fetal monitor is commonly referred to simply as a Doppler or fetal Doppler. Doppler fetal monitors provide information about the fetus similar to that provided by a fetal stethoscope.

Contrast ultrasonography (ultrasound contrast imaging)
A contrast medium for medical ultrasonography is a formulation of encapsulated gaseous microbubbles to increase echogenicity of blood, discovered by Dr Raymond Gramiak in 1968 and named contrast-enhanced ultrasound. This contrast medical imaging modality is clinically used throughout the world, in particular for echocardiography in the United States and for ultrasound radiology in Europe and Asia.
Microbubbles-based contrast media is administrated intravenously in patient blood stream during the medical ultrasonography examination. The microbubbles being too large in diameter, they stay confined in blood vessels and cannot extravasate towards the interstitial fluid. An ultrasound contrast media is therefore purely intravascular, making it an ideal agent to image organ microvascularization for diagnostic purposes. A typical clinical use of contrast ultrasonography is detection of a hypervascular metastatic tumor, which exhibits a contrast uptake (kinetics of microbubbles concentration in blood circulation) faster than healthy biological tissue surrounding the tumor. Other clinical applications using contrast exist, such as in echocardiography to improve delineation of left ventricle for visually checking contractibility of heart after a myocardial infarction. Finally, applications in quantitative perfusion (relative measurement of blood flow ) emerge for identifying early patient response to an anti-cancerous drug treatment (methodology and clinical study by Dr Nathalie Lassau in 2011), enabling to determine the best oncological therapeutic options.

In oncological practice of medical contrast ultrasonography, clinicians use the method of parametric imaging of vascular signatures invented by Dr Nicolas Rognin in 2010. This method is conceived as a cancer aided diagnostic tool, facilitating characterization of a suspicious tumor (malignant versus benign) in an organ. This method is based on medical computational science  to analyze a time sequence of ultrasound contrast images, a digital video recorded in real-time during patient examination. Two consecutive signal processing steps are applied to each pixel of the tumor:
calculation of a vascular signature (contrast uptake difference with respect to healthy tissue surrounding the tumor);
automatic classification of the vascular signature into a unique parameter, this last coded in one of the four following colors:
green for continuous hyper-enhancement (contrast uptake higher than healthy tissue one),
blue for continuous hypo-enhancement (contrast uptake lower than healthy tissue one),
red for fast hyper-enhancement (contrast uptake before healthy tissue one) or
yellow for fast hypo-enhancement (contrast uptake after healthy tissue one).

Once signal processing in each pixel completed, a color spatial map of the parameter is displayed on a computer monitor, summarizing all vascular information of the tumor in a single image called parametric image (see last figure of press article  as clinical examples). This parametric image is interpreted by clinicians based on predominant colorization of the tumor: red indicates a suspicion of malignancy (risk of cancer), green or yellow – a high probability of benignity. In the first case (suspicion of malignant tumor), the clinician typically prescribes a biopsy to confirm the diagnostic or a CT scan examination as a second opinion. In the second case (quasi-certain of benign tumor), only a follow-up is needed with a contrast ultrasonography examination a few months later. The main clinical benefits are to avoid a systematic biopsy (risky invasive procedure) of benign tumors or a CT scan examination exposing the patient to X-ray radiation. The parametric imaging of vascular signatures method proved to be effective in humans for characterization of tumors in the liver. In a cancer screening context, this method might be potentially applicable to other organs such as breast or prostate.

Molecular ultrasonography (ultrasound molecular imaging)
The future of contrast ultrasonography is in molecular imaging with potential clinical applications expected in cancer screening to detect malignant tumors at their earliest stage of appearance. Molecular ultrasonography (or ultrasound molecular imaging) uses targeted microbubbles originally designed by Dr Alexander Klibanov in 1997; such targeted microbubbles specifically bind or adhere to tumoral microvessels by targeting biomolecular cancer expression (overexpression of certain biomolecules occurs during neo-angiogenesis or inflammation processes in malignant tumors). As a result, a few minutes after their injection in blood circulation, the targeted microbubbles accumulate in the malignant tumor; facilitating its localization in a unique ultrasound contrast image. In 2013, the very first exploratory clinical trial in humans for prostate cancer was completed at Amsterdam in the Netherlands by Dr Hessel Wijkstra.
In molecular ultrasonography, the technique of acoustic radiation force (also used for shear wave elastography) is applied in order to literally push the targeted microbubbles towards microvessels wall; firstly demonstrated by Dr Paul Dayton in 1999. This allows to maximize binding to the malignant tumor; the targeted microbubbles being in more direct contact with cancerous biomolecules expressed at the inner surface of tumoral microvessels. At the stage of scientific preclinical research, the technique of acoustic radiation force was implemented as a prototype in clinical ultrasound systems and validated in vivo in 2D and 3D imaging modes.

Elastography (ultrasound elasticity imaging)
Ultrasound is also used for elastography, which is a relatively new imaging modality that maps the elastic properties of soft tissue. This modality emerged in the last two decades. Elastography is useful in medical diagnoses as it can discern healthy from unhealthy tissue for specific organs/growths. For example, cancerous tumors will often be harder than the surrounding tissue, and diseased livers are stiffer than healthy ones.
There are many ultrasound elastography techniques. The most prominent are: Quasistatic Elastography/Strain Imaging, Shear Wave Elasticity Imaging (SWEI), Supersonic Shear Imaging (SSI), Acoustic Radiation Force Impulse imaging (ARFI), and Transient Elastography. The steadily growing clinical use of ultrasound elastography is a result of the implementation of technology in clinical ultrasound machines. Currently, an increase of activities in the field of elastography is observed demonstrating successful application of the technology in various areas of medical diagnostics and treatment monitoring.

Interventional ultrasonography
Interventional ultrasonography involves biopsy, emptying fluids, intrauterine Blood transfusion (Hemolytic disease of the newborn).
Thyroid cysts: The high frequency thyroid ultrasound (HFUS) can be used to treat several gland conditions. The recurrent thyroid cyst that was usually treated in the past with surgery, can be treated effectively by a new procedure called percutaneous ethanol injection, or PEI. With ultrasound guided placement of a 25 gauge needle within the cyst, and after evacuation of the cyst fluid, about 50% of the cyst volume is injected back into the cavity, under strict operator visualization of the needle tip. The procedure is 80% successful in reducing the cyst to minute size.
Metastatic thyroid cancer neck lymph nodes: The other thyroid therapy use for HFUS is to treat metastatic thyroid cancer neck lymph nodes that occur in patients who either refuse surgery, or are no longer a candidate for surgery. Small amounts of ethanol are injected under ultrasound guided needle placement. A blood flow study is done prior to the injection, by power doppler. The blood flow can be destroyed and the node become inactive, although it may still be there. Power doppler visualized blood flow can be eradicated, and there may be a drop in the cancer blood marker test, thyroglobulin, TG, as the node become non-functional. Another interventional use for HFUS is to mark a cancer node one hour prior to surgery to help locate the node cluster at the surgery. A minute amount of methylene dye is injected, under careful ultrasound guided placement of the needle on the anterior surface, but not in the node. The dye will be evident to the thyroid surgeon when he opens the neck. A similar localization procedure with methylene blue, can be done to locate parathyroid adenomas at surgery.

Compression ultrasonography
Compression ultrasonography is a simplified technique used for quick deep vein thrombosis diagnosis. The examination is limited to common femoral vein and popliteal vein only, instead to spend time performing the full examination, lower limbs venous ultrasonography. It is performed using only one test: vein compression.
Compression ultrasono",Category:Articles with dead external links from May 2015,1
125,126,Sound intensity,"Sound intensity also known as acoustic intensity is defined as the power carried by sound waves per unit area in a direction perpendicular to that area. The SI unit of intensity, which includes sound intensity, is the watt per square meter (W/m2). One application is the noise measurement of sound intensity in the air at a listener's location as a sound energy quantity.
Sound intensity is not the same physical quantity as sound pressure. Hearing is directly sensitive to sound pressure which is related to sound intensity. In consumer audio electronics, the level differences are called ""intensity"" differences, but sound intensity is a specifically defined quantity and cannot be sensed by a simple microphone. The rate at which sound energy passes through a unit area held perpendicular to the direction of propagation of sound waves is called intensity of sound.

Mathematical definition
Sound intensity, denoted I, is defined by

  
    
      
        
          I
        
        =
        p
        
          v
        
      
    
    {\displaystyle \mathbf {I} =p\mathbf {v} }
  
where
p is the sound pressure;
v is the particle velocity.
Both I and v are vectors, which means that both have a direction as well as a magnitude. The direction of sound intensity is the average direction in which energy is flowing.
The average sound intensity during time T is given by

  
    
      
        ?
        
          I
        
        ?
        =
        
          
            1
            T
          
        
        
          ?
          
            0
          
          
            T
          
        
        p
        (
        t
        )
        
          v
        
        (
        t
        )
        
        
          d
        
        t
        .
      
    
    {\displaystyle \langle \mathbf {I} \rangle ={\frac {1}{T}}\int _{0}^{T}p(t)\mathbf {v} (t)\,\mathrm {d} t.}
  
Also,
Intensity of Sound = 2?²n²A²?v
Where,
n is frequency of sound, A is the Amplitude of sound wave, v is velocity of sound, and ? is density of medium in which sound is traveling

Inverse-square law
For a spherical sound wave, the intensity in the radial direction as a function of distance r from the centre of the sphere is given by

  
    
      
        I
        (
        r
        )
        =
        
          
            P
            
              A
              (
              r
              )
            
          
        
        =
        
          
            P
            
              4
              ?
              
                r
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle I(r)={\frac {P}{A(r)}}={\frac {P}{4\pi r^{2}}},}
  
where
P is the sound power;
A(r) is the area of a sphere of radius r.
Thus sound intensity decreases as 1/r2 from the centre of the sphere:

  
    
      
        I
        (
        r
        )
        ?
        
          
            1
            
              r
              
                2
              
            
          
        
        .
      
    
    {\displaystyle I(r)\propto {\frac {1}{r^{2}}}.}
  
This relationship is an inverse-square law.

Sound intensity level
Sound intensity level (SIL) or acoustic intensity level is the level (a logarithmic quantity) of the intensity of a sound relative to a reference value.
It is denoted LI, expressed in dB, and defined by

  
    
      
        
          L
          
            I
          
        
        =
        
          
            1
            2
          
        
        ln
        
        
          (
          
            
              I
              
                I
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        
          log
          
            10
          
        
        
        
          (
          
            
              I
              
                I
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              I
              
                I
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{I}={\frac {1}{2}}\ln \!\left({\frac {I}{I_{0}}}\right)\!~\mathrm {Np} =\log _{10}\!\left({\frac {I}{I_{0}}}\right)\!~\mathrm {B} =10\log _{10}\!\left({\frac {I}{I_{0}}}\right)\!~\mathrm {dB} ,}
  
where
I is the sound intensity;
I0 is the reference sound intensity;
1 Np = 1 is the neper;
1 B = (1/2) ln(10) is the bel;
1 dB = (1/20) ln(10) is the decibel.
The commonly used reference sound intensity in air is

  
    
      
        
          I
          
            0
          
        
        =
        1
         
        
          p
          W
          
            /
          
          
            m
            
              2
            
          
        
        .
      
    
    {\displaystyle I_{0}=1~\mathrm {pW/m^{2}} .}
  
The proper notations for sound intensity level using this reference are LI /(1 pW/m2) or LI (re 1 pW/m2), but the notations dB SIL, dB(SIL), dBSIL, or dBSIL are very common, even if they are not accepted by the SI.
The reference sound intensity I0 is defined such that a progressive plane wave has the same value of sound intensity level (SIL) and sound pressure level (SPL), since

  
    
      
        I
        ?
        
          p
          
            2
          
        
        .
      
    
    {\displaystyle I\propto p^{2}.}
  
The equality of SIL and SPL requires that

  
    
      
        
          
            I
            
              I
              
                0
              
            
          
        
        =
        
          
            
              p
              
                2
              
            
            
              p
              
                0
              
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle {\frac {I}{I_{0}}}={\frac {p^{2}}{p_{0}^{2}}},}
  
where p0 = 20 ?Pa is the reference sound pressure.
For a progressive spherical wave,

  
    
      
        
          
            p
            v
          
        
        =
        
          z
          
            0
          
        
        ,
      
    
    {\displaystyle {\frac {p}{v}}=z_{0},}
  
where z0 is the characteristic specific acoustic impedance. Thus,

  
    
      
        
          I
          
            0
          
        
        =
        
          
            
              
                p
                
                  0
                
                
                  2
                
              
              I
            
            
              p
              
                2
              
            
          
        
        =
        
          
            
              
                p
                
                  0
                
                
                  2
                
              
              p
              v
            
            
              p
              
                2
              
            
          
        
        =
        
          
            
              p
              
                0
              
              
                2
              
            
            
              z
              
                0
              
            
          
        
        .
      
    
    {\displaystyle I_{0}={\frac {p_{0}^{2}I}{p^{2}}}={\frac {p_{0}^{2}pv}{p^{2}}}={\frac {p_{0}^{2}}{z_{0}}}.}
  
In air at ambient temperature, z0 = 410 Pa·s/m, hence the reference value I0 = 1 pW/m2.
In an anechoic chamber, which approximates a free field (no reflection), the SIL can be taken as being equal to the SPL. This fact is exploited to measure sound power in anechoic conditions.

Measurement
One method of sound intensity measurement involves the use of two microphones located close to each other, normal to the direction of sound energy flow. A signal analyser is used to compute the crosspower between the measured pressures and the sound intensity is derived from (proportional to) the imaginary part of the crosspower.

References
External links
How Many Decibels Is Twice as Loud? Sound Level Change and the Respective Factor of Sound Pressure or Sound Intensity
Acoustic Intensity
Conversion: Sound Intensity Level to Sound Intensity and Vice Versa
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
Table of Sound Levels. Corresponding Sound Intensity and Sound Pressure
What Is Sound Intensity Measurement and Analysis?",Category:Physical quantities,1
126,127,Acoustic interferometer,"An acoustic interferometer is an instrument, using interferometry, for measuring the physical characteristics of sound waves in a gas or liquid. It may be used to measure velocity, wavelength, absorption, or impedance. A vibrating crystal creates the ultrasonic waves that are radiated into the medium. The waves strike a reflector placed parallel to the crystal. The waves are then reflected back to the source and measured.

See also
Acoustic microscopy

References
Inoue N, Hirai M, Hasegawa T, Matsuzawa K (1986). ""A new ultrasonic interferometer for velocity measurement in liquids using optical diffraction"". J. Phys. D: Appl. Phys. 19 (8): 1439–1447. Bibcode:1986JPhD...19.1439I. doi:10.1088/0022-3727/19/8/012. 
Sukatskas, V.A.; Voleishis, A.P.; Stankyavichyus, É.V.; Armoshka, V.K. (1988). ""Density measurement of a liquid with an ultrasonic interferometer of constant length"". Measurement Techniques. 31 (11): 1126–9. doi:10.1007/BF00864320. 
Gucker, F. T.; Chernick, C. L.; Roy-Chowdhury, P (1966). ""A Frequency-Modulated Ultrasonic Interferometer: Adiabatic Compressibility of Aqueous Solutions of Nacl and Kcl at 25°C"". Proc. Natl. Acad. Sci. U.S.A. 55 (1): 12–19. Bibcode:1966PNAS...55...12G. doi:10.1073/pnas.55.1.12. PMC 285746?. PMID 16578624. 
www.mittalenterprises.com",Category:Acoustics,1
127,128,Whistling kettle,,Category:Acoustics,1
128,129,Anechoic chamber,"An anechoic chamber (an-echoic meaning ""non-reflective, non-echoing, echo-free"") is a room designed to completely absorb reflections of either sound or electromagnetic waves. They are also often isolated from waves entering from their surroundings. This combination means that a person or detector exclusively hears direct sounds (no reverberant sounds), in effect simulating being inside an infinitely large room.
Anechoic chambers, a term coined by American acoustics expert Leo Beranek, were initially exclusively used to refer to acoustic anechoic chambers. Recently, the term has been extended to RF anechoic chambers, which eliminate reflection and external noise caused by electromagnetic waves.
Anechoic chambers range from small compartments the size of household microwave ovens to ones as large as aircraft hangars. The size of the chamber depends on the size of the objects and frequency ranges being tested.

Acoustic anechoic chambers
Anechoic chambers are commonly used in acoustics to conduct experiments in nominally ""free field"" conditions, free-field meaning that there are no reflected signals. All sound energy will be traveling away from the source with almost none reflected back. Common anechoic chamber experiments include measuring the transfer function of a loudspeaker or the directivity of noise radiation from industrial machinery. In general, the interior of an anechoic chamber is very quiet, with typical noise levels in the 10–20 dBA range. In 2005, the best anechoic chamber measured at ?9.4 dBA. In 2015, an anechoic chamber on the campus of Microsoft broke the world record with a measurement of ?20.6 dBA. The human ear can typically detect sounds above 0 dBA, so a human in such a chamber would perceive the surroundings as devoid of sound. Anecdotally, some humans may not like such quietness and can become disoriented.
The mechanism by which anechoic chambers minimize the reflection of sound waves impinging onto their walls is as follows: In the included figure, an incident sound wave I is about to impinge onto a wall of an anechoic chamber. This wall is composed of a series of wedges W with height H. After the impingement, the incident wave I is reflected as a series of waves R which in turn ""bounce up-and-down"" in the gap of air A (bounded by dotted lines) between the wedges W. Such bouncing may produce (at least temporarily) a standing wave pattern in A. During this process, the acoustic energy of the waves R gets dissipated via the air's molecular viscosity, in particular near the corner C. In addition, with the use of foam materials to fabricate the wedges, another dissipation mechanism happens during the wave/wall interactions. As a result, the component of the reflected waves R along the direction of I that escapes the gaps A (and goes back to the source of sound), denoted R', is notably reduced. Even though this explanation is two-dimensional, it is representative and applicable to the actual three-dimensional wedge structures used in anechoic chambers.

Semi-anechoic chambers
Full anechoic chambers aim to absorb energy in all directions. Semi-anechoic chambers have a solid floor that acts as a work surface for supporting heavy items, such as cars, washing machines, or industrial machinery, rather than the mesh floor grille over absorbent tiles found in full anechoic chambers. This floor is damped and floating on absorbent buffers to isolate it from outside vibration or electromagnetic signals. Recording studios are often semi-anechoic.

Radio-frequency anechoic chambers
The internal appearance of the radio frequency (RF) anechoic chamber is sometimes similar to that of an acoustic anechoic chamber, however, the interior surfaces of the RF anechoic chamber are covered with radiation absorbent material (RAM) instead of acoustically absorbent material. Uses for RF anechoic chambers include testing antennae, radars, and is typically used to house the equipment for performing measurements of antenna radiation patterns, electromagnetic interference.
Performance expectations (gain, efficiency, pattern characteristics, etc.) constitute primary challenges in designing stand alone or embedded antennae. Designs are becoming ever more complex with a single device incorporating multiple technologies such as cellular, WiFi, Bluetooth, LTE, MIMO, RFID and GPS.

Radiation-absorbent material
RAM is designed and shaped to absorb incident RF radiation (also known as non-ionising radiation) as effectively as possible, from as many incident directions as possible. The more effective the RAM, the lower the resulting level of reflected RF radiation. Many measurements in electromagnetic compatibility (EMC) and antenna radiation patterns require that spurious signals arising from the test setup, including reflections, are negligible to avoid the risk of causing measurement errors and ambiguities.

Effectiveness over frequency
Waves of higher frequencies have shorter wavelengths and are higher in energy, while waves of lower frequencies have longer wavelengths and are lower in energy, according to the relationship 
  
    
      
        ?
        =
        v
        
          /
        
        f
      
    
    {\displaystyle \lambda =v/f}
   where lambda represents wavelength, v is phase velocity of wave, and 
  
    
      
        f
      
    
    {\displaystyle f}
   is frequency. To shield for a specific wavelength, the cone must be of appropriate size to absorb that wavelength. The performance quality of an RF anechoic chamber is determined by its lowest test frequency of operation, at which measured reflections from the internal surfaces will be the most significant compared to higher frequencies. Pyramidal RAM is at its most absorptive when the incident wave is at normal incidence to the internal chamber surface and the pyramid height is approximately equal to 
  
    
      
        ?
        
          /
        
        4
      
    
    {\displaystyle \lambda /4}
  , where 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   is the free space wavelength. Accordingly, increasing the pyramid height of the RAM for the same (square) base size improves the effectiveness of the chamber at low frequencies but results in increased cost and a reduced unobstructed working volume that is available inside a chamber of defined size.

Installation into a screened room
An RF anechoic chamber is usually built into a screened room, designed using the Faraday cage principle. This is because most of the RF tests that require an anechoic chamber to minimize reflections from the inner surfaces also require the properties of a screened room to attenuate unwanted signals penetrating inwards and causing interference to the equipment under test and prevent leakage from tests penetrating outside.

Chamber size and commissioning
At lower radiated frequencies, far-field measurement can require a large and expensive chamber. Sometimes, for example for radar cross-section measurements, it is possible to scale down the object under test and reduce the chamber size, provided that the wavelength of the test frequency is scaled down in direct proportion by testing at a higher frequency.
RF anechoic chambers are normally designed to meet the electrical requirements of one or more accredited standards. For example, the aircraft industry may test equipment for aircraft according to company specifications or military specifications such as MIL-STD 461E. Once built, acceptance tests are performed during commissioning to verify that the standard(s) are in fact met. Provided they are, a certificate will be issued to that effect. The chamber will need to be periodically retested.

Operational use
Test and supporting equipment configurations to be used within anechoic chambers must expose as few metallic (conductive) surfaces as possible, as these risk causing unwanted reflections. Often this is achieved by using non-conductive plastic or wooden structures for supporting the equipment under test. Where metallic surfaces are unavoidable, they may be covered with pieces of RAM after setting up to minimize such reflection as far as possible.
A careful assessment may be required as to whether the test equipment (as opposed to the equipment under test) should be placed inside or outside the chamber. Typically most of it is located in a separate screened room attached to the main test chamber, in order to shield it from both external interference and from the radiation within the chamber. Mains power and test signal cabling into the test chamber require high quality filtering.
Fiber optic cables are sometimes used for the signal cabling, as they are immune to ordinary RFI and also cause little reflection inside the chamber.

Health and safety risks associated with RF anechoic chamber
The following health and safety risks are associated with RF anechoic chambers:
RF radiation hazard
Fire hazard
Trapped personnel
Personnel are not normally permitted inside the chamber during a measurement as this not only can cause unwanted reflections from the human body but may also be a radiation hazard to the personnel concerned if tests are being performed at high RF powers. Such risks are from RF or non-ionizing radiation and not from the higher energy ionizing radiation.
As RAM is highly absorptive of RF radiation, incident radiation will generate heat within the RAM. If this cannot be dissipated adequately there is a risk that hot spots may develop and the RAM temperature may rise to the point of combustion. This can be a risk if a transmitting antenna inadvertently gets too close to the RAM. Even for quite modest transmitting power levels, high gain antennas can concentrate the power sufficiently to cause high power flux near their apertures. Although recently manufactured RAM is normally treated with a fire retardant to reduce such risks, they are difficult to completely eliminate. Safety regulations normally require the installation of a gaseous fire suppression system including smoke detectors.

See also
Soundproofing
Vibration isolation
Buffer (disambiguation)
Damped wave
Damping
Damper (disambiguation)
Electromagnetic reverberation chamber
Reverberation room
Sensory deprivation
GTEM cell

References
External links
360 video of an anechoic chamber
Pictures and description of an acoustic anechoic chamber
Anechoic Chambers, Past and Present
How RF Anechoic Chambers Work
Video tour of an EMC/RF Test facility. Including the largest anechoic test chamber in the southern hemisphere
Some examples
Antenna Testing For An Anechoic Chamber
Millimeter Wave Inc's Radio/MM Wave anechoic chamber
Bell Labs' Murray Hill anechoic chamber
""Acoustics Anechoic Chamber"". The UK's National Measurement Laboratory. National Physical Laboratory. Archived from the original on 29 September 2007. Retrieved 22 February 2011. CS1 maint: Unfit url (link)
Anechoic chambers at Apple Inc. campus used to test their mobile device products, via WaybackMachine
Photos from building an anechoic chamber in CTU, Prague

Sound examples
The sound of clothes inside an anechoic chamber
Hallucinations in anechoic chambers: the science behind the claim
Listen to a subdued balloon burst in an anechoic chamber",Category:Articles needing additional references from April 2008,1
129,130,Sonochemistry,"In chemistry, the study of sonochemistry is concerned with understanding the effect of ultrasound in forming acoustic cavitation in liquids, resulting in the initiation or enhancement of the chemical activity in the solution. Therefore, the chemical effects of ultrasound do not come from a direct interaction of the ultrasonic sound wave with the molecules in the solution. The simplest explanation for this is that sound waves propagating through a liquid at ultrasonic frequencies do so with a wavelength that is significantly longer than that of the bond length between atoms in the molecule. Therefore, the sound wave cannot affect that vibrational energy of the bond, and can therefore not directly increase the internal energy of a molecule. Instead, sonochemistry arises from acoustic cavitation: the formation, growth, and implosive collapse of bubbles in a liquid. The collapse of these bubbles is an almost adiabatic process, thereby resulting in the massive build-up of energy inside the bubble, resulting in extremely high temperatures and pressures in a microscopic region of the sonicated liquid. The high temperatures and pressures result in the chemical excitation of any matter that was inside of, or in the immediate surroundings of the bubble as it rapidly imploded. A broad variety of outcomes can result from acoustic cavitation, including sonoluminescence, increased chemical activity in the solution due to the formation of primary and secondary radical reactions, and increase chemical activity through the formation of new, relatively stable chemical species that can diffuse further into the solution to create chemical effects (for example, the formation of hydrogen peroxide from the combination of two hydroxyl radicals formed following the dissociation of water vapor inside the collapsing bubbles what water is exposed to ultrasound.
The influence of sonic waves traveling through liquids was first reported by Robert Williams Wood (1868–1955) and Alfred Lee Loomis (1887–1975) in 1927. The experiment was about the frequency of the energy that it took for sonic waves to ""penetrate"" the barrier of water. He came to the conclusion that sound does travel faster in water, but because of the water's density compared to our earth's atmosphere it was incredibly hard to get the sonic waves into the water. After lots of research they decided that the best way to disperse sound into the water was to make loud noises into the water by creating bubbles that were made at the same time as the sound. One of the easier ways that they put sound into the water was they simply yelled. But another road block they ran into was the ratio of the amount of time it took for the lower frequency waves to penetrate the bubbles walls and access the water around the bubble, and then time from that point to the point on the other end of the body of water. But despite the revolutionary ideas of this article it was left mostly unnoticed. Sonochemistry experienced a renaissance in the 1980s with the advent of inexpensive and reliable generators of high-intensity ultrasound.
Upon irradiation with high intensity sound or ultrasound, acoustic cavitation usually occurs. Cavitation – the formation, growth, and implosive collapse of bubbles irradiated with sound — is the impetus for sonochemistry and sonoluminescence. Bubble collapse in liquids produces enormous amounts of energy from the conversion of kinetic energy of the liquid motion into heating the contents of the bubble. The compression of the bubbles during cavitation is more rapid than thermal transport, which generates a short-lived localized hot-spot. Experimental results have shown that these bubbles have temperatures around 5000 K, pressures of roughly 1000 atm, and heating and cooling rates above 1010 K/s. These cavitations can create extreme physical and chemical conditions in otherwise cold liquids.
With liquids containing solids, similar phenomena may occur with exposure to ultrasound. Once cavitation occurs near an extended solid surface, cavity collapse is nonspherical and drives high-speed jets of liquid to the surface. These jets and associated shock waves can damage the now highly heated surface. Liquid-powder suspensions produce high velocity interparticle collisions. These collisions can change the surface morphology, composition, and reactivity.
Three classes of sonochemical reactions exist: homogeneous sonochemistry of liquids, heterogeneous sonochemistry of liquid-liquid or solid–liquid systems, and, overlapping with the aforementioned, sonocatalysis. Sonoluminescence is typically regarded as a special case of homogeneous sonochemistry. The chemical enhancement of reactions by ultrasound has been explored and has beneficial applications in mixed phase synthesis, materials chemistry, and biomedical uses. Because cavitation can only occur in liquids, chemical reactions are not seen in the ultrasonic irradiation of solids or solid–gas systems.
For example, in chemical kinetics, it has been observed that ultrasound can greatly enhance chemical reactivity in a number of systems by as much as a million-fold; effectively acting as a catalyst by exciting the atomic and molecular modes of the system (such as the vibrational, rotational, and translational modes). In addition, in reactions that use solids, ultrasound breaks up the solid pieces from the energy released from the bubbles created by cavitation collapsing through them. This gives the solid reactant a larger surface area for the reaction to proceed over, increasing the observed rate of reaction.
While the application of ultrasound often generates mixtures of products, a paper published in 2007 in the journal Nature described the use of ultrasound to selectively affect a certain cyclobutane ring-opening reaction. Atul Kumar has reported multicomponent reaction Hantzsch ester synthesis in Aqueous Micelles using ultrasound.
Magneto-Sonochemical Dissociation, Hopper, 2015 - proposed the use of magnetic sono-chemical reforming of long chain hydrocarbon molecules. In this technique, magnetic ferrofluid magnetostriction, leading to the dissociation of long chain hydrocarbons, in turn leading to the formation of hydrogen and short chain hydrocarbons; was proposed and used as an alternative to conventional fuel cell fuel reforming. This work employs the imposition of a high frequency ac magnetic field onto a dense block of highly porous ferromagnetic powder while diesel fuel is pumped through it. Coupling of energy via magnetic forces offers longer range and greater dissociative coupling power than traditional ultrasonic based techniques. Electrical energy produced from the fuel cell was fed back to energize the magnetic sono-chemical reforming cell. The work was motivated by the requirement to simplify marine energy production systems towards the use of a single, diesel fuel tank; in turn leading to the ability to a fuel cell based solution to charge a ship's batteries. Work to combine the mechanisms of both magnetic and conventional ultrasonic coupling was proposed as an efficiency improvement direction.
Some water pollutants, especially chlorinated organic compounds, can be destroyed sonochemically.
Sonochemistry can be performed by using a bath (usually used for ultrasonic cleaning) or with a high power probe, called an ultrasonic horn.

See also
Ultrasound
Sonication
Ultrasonics
ultrasonic homogenizer
homogenizer
Homogenization (chemistry)
Sonoelectrochemistry
Kenneth S. Suslick

References
External links
The Chemical and Physical Effects of Ultrasound by Prof. K. S. Suslick
Sonochemistry – Short Review and Recent Literature
Sonochemistry: New Opportunities for Green Chemistry by Gregory Chatel (Université Savoie Mont Blanc, France)",Category:Articles covered by WikiProject Wikify from September 2017,1
130,131,Acoustic radiation pressure,"Acoustic radiation pressure is the apparent pressure difference between the average pressure at a surface moving with the displacement of the wave propagation (the Lagrangian pressure) and the pressure that would have existed in the fluid of the same mean density when at rest. Numerous authors make a distinction between the phenomena of Rayleigh radiation pressure and Langevin radiation pressure.

See also
Radiation pressure
Acoustic levitation

References
RT Beyer (1978). ""Radiation pressure—the history of a mislabeled tensor"" (PDF). The Journal of the Acoustical Society of America. Bibcode:1978ASAJ...63.1025B. doi:10.1121/1.381833. 
Boa?Teh Chu, Apfel RE (December 1982). ""Acoustic radiation pressure produced by a beam of sound"". J. Acoust. Soc. Am. 72 (6): 1673–1687. Bibcode:1982ASAJ...72.1673C. doi:10.1121/1.388660. 
Hasegawa T, Kido T, Iizuka T, Matsuoka C (2000). ""A general theory of Rayleigh and Langevin radiation pressures"". J Acoust Soc Jpn E. 21 (3): 145–152. doi:10.1250/ast.21.145. ISSN 0388-2861. Archived from the original on 2012-02-29.

External links
Tokyo University Researchers Using Ultrasound Technology to Enable Touchable Holograms",Category:Articles lacking in-text citations from March 2012,1
131,132,Denge,"Denge is a former Royal Air Force site near Dungeness, in Kent, England. It is best known for the early experimental acoustic mirrors which remain there.
The acoustic mirrors, known colloquially as 'listening ears', at Denge are located between Greatstone-on-Sea and Lydd airfield, on the banks of a now disused gravel pit. The mirrors were built in the late 1920s and early 1930s as an experimental early warning system for incoming aircraft, developed by Dr William Sansome Tucker. Several were built along the south and east coasts, but the complex at Denge is the best preserved, and are protected as scheduled monuments.

Denge complex
There are three acoustic mirrors in the complex, each consisting of a single concrete hemispherical reflector.
The 200 foot mirror is a near vertical, curved wall, 200 feet (60m) long. It is one of only two similar acoustic mirrors in the world, the other being in Mag?tab, Malta.

The 30 foot mirror is a circular dish, similar to a deeply curved satellite dish, 9 m (30 ft) across, supported on concrete buttresses. This mirror still retains the metal microphone pole at its centre.
The 20 foot mirror is similar to the 30 foot mirror, with a smaller, shallower dish 6 m (20 ft) across. The design is close to that of an acoustic mirror in Kilnsea, East Riding of Yorkshire.

Acoustic mirrors did work, and could effectively be used to detect slow moving enemy aircraft before they came into sight. They worked by concentrating sound waves towards a central point, where a microphone would have been located. However, their use was limited as aircraft became faster. Operators also found it difficult to distinguish between aircraft and seagoing vessels. In any case, they quickly became obsolete due to the invention of radar in 1932. The experiment was abandoned, and the mirrors left to decay. The gravel extraction works caused some undermining of at least one of the structures.

The striking forms of the sound mirrors have attracted artists and photographers. British artist Tacita Dean created a film inspired by the complex. The band Turin Brakes featured the mirrors on some of their album covers. The object appeared in the music video for Blank & Jones' ""A Forest"".The mirrors have also been featured in the music videos for Invaders Must Die by The Prodigy & Young Kato - Something Real.

Restoration
In 2003, English Heritage secured £500,000 from the Aggregates Levy Sustainability Fund and from the EU's Interreg programme under the Historic Fortifications Network, as administered by Kent County Council. This money was spent to restore the damage caused by the gravel works, as well as to install a swing bridge which now is the only means of access, reducing the monument's exposure to vandalism. The mirrors are situated on an island within an RSPB nature reserve, and can only be accessed on open days as the designated site (which has both Site of Special Scientific Interest and Special Protection Area status) is sensitive to disturbance.

References
External links
Greatstone Sound Mirrors
Guided Tours by the Romney Marsh Countryside Project",Category:Warning systems,1
132,133,Standard day,"The term standard day is used throughout meteorology, aviation, and other sciences and disciplines as a way of defining certain properties of the atmosphere in a manner which allows those who use our atmosphere to effectively calculate and communicate its properties at any given time. For example, a temperature deviation of +8 °C means that the air at any given altitude is 8 °C (14 °F) warmer than what standard day conditions and the measurement altitude would predict, and would indicate a higher density altitude. These variations are extremely important to both meteorologists and aviators, as they strongly determine the different properties of the atmosphere.
For example, on a cool day, an airliner might have no problem safely departing a medium-altitude runway, but on a warmer day, the density altitude might require a higher true airspeed, which would require more acceleration, and more runway. The pilot may be forced to reduce fuel or cargo, or even add an intermediate fuel stop, delaying the flight arrival time. In meteorology, departure from standard day conditions is what gives rise to all weather phenomena, including thunderstorms, fronts, clouds, even the heating and cooling of our planet.

Standard day parameters
For Pilots: At sea level, Altimeter:29.92 in/Hg at 15 °C (59 °F) The ""standard day"" model of the atmosphere is defined at sea level, with certain present conditions such as temperature and pressure. But other factors, such as humidity, further alter the nature of the atmosphere, and are also defined under standard day conditions:
Density (?): 1.225 kg/m³ (0.00237 slug/ft3)
Pressure (p): 1013.25 hPa (14.7 lb/ in2)
Temperature (T): 15 °C (59 °F)
Viscosity (?): 17.3 µPa·s (3.62 × 10?7 lb s/ft2)
The first three properties are usually referred as ""standard day"" conditions, which the viscosity aspect is largely ignored throughout the aviation community. However, viscosity, which is affected by humidity levels, plays a key role in aerodynamic drag, which is why it is a key component of standard day conditions. Because it is a key component of drag, it affects the amount of fuel burned per unit of distance travelled.


== References ==",Category:Acoustics,1
133,134,Salford Acoustics,"Salford Acoustics offers Acoustics and Audio Engineering Courses, undertakes public and industrial research in acoustics, carries out commercial testing, and undertakes activities to engage the public in acoustic science and engineering. It is based in two locations: (i) 3 km west of Manchester city centre, UK, in the Newton Building on the Peel Park Campus of the University of Salford, and (ii) on the banks of the Manchester Ship Canal at MediaCityUK.

History and current structure
The first acoustic laboratories were established in Salford in 1965; in the early 1970s the Department of Applied Acoustics was formed. In 1996 the University merged with University College Salford and a Department of Acoustic and Audio Engineering was formed. A couple of years later, this joined with another department to form Acoustic and Electronic Engineering. Finally, when the University greatly reduced the number of schools in the organisation, Salford Acoustics joined the School of Computing, Science and Engineering. Research work comes under the auspices of the Acoustics Research Centre.

Programmes
The Department of Applied Acoustics first taught an undergraduate degree in 1975, namely the BSc (Hons) in Electroacoustics. This was later renamed Beng (Hons) Acoustics. In 1993, Salford Acoustics set up the BEng (Hons) in Audio Technology. These two undergraduate degrees are now taught under a single banner, BEng Audio Acoustics, with two pathways to represent the different interests of the cohort. Salford acoustics has also taught masters in acoustic engineering and audio for many decades, currently offering an MSc in Audio Acoustics and an MSc in Environmental Acoustics. The Acoustics Research Centre offers masters and doctoral research degrees.

Research
Rating
The Acoustics Research Centre achieved the top research rating of 6* in RAE 2001 as part of the Research Institute for the Built and Human Environment's submission to Unit Of Assment 30, Architecture and the Built Environment. In 2008, the RAE submission including the Acoustics Research Centre finished top of Research Fortnight’s ‘Research Power’ table for Architecture & the Built Environment. 90% of the research was graded at international standard and 25% at world-leading.

Sub-disciplines
Research is carried out in the following sub-disciplines of acoustic engineering and science
Archaeoacoustics
Architectural and building acoustics
Audio signal processing
Auralization
Electroacoustics
Environmental noise
Noise control
Outdoor sound propagation
Psychoacoustics
Remote sensing using sound
Sound reproduction
Soundscapes
Surround sound systems
Vibration and dynamics

Public engagement
Examples of public engagement work include:
The search for the Worst Sound in the World (EPSRC GrantRef:EP/D000068/1)
Development of extensive curriculum materials on physics and acoustics for schools (EPSRC GrantRefs:GR/S23919/01, EP/D507030/1, P/D054729/1, EP/E033806/1, EP/G020116/1)
Aeolus sculpture and outreach (EPSRC GrantRefs:EP/G062781/1)
The search for the Sonic Wonders of the World

Laboratories
Most of Salford's Acoustics and Audio Laboratories are based on the Peel Park campus, but some are at MediaCityUK:
Audio production suites
Radio studios
Recording studios
Anechoic chamber
2x Semi-anechoic chambers
Reverberation chamber
Transmission suite
Listening room

Commercial work
Salford Acoustics is a calibration and test house for construction, government, military, audio R&D and the motor industry.

Current Staff
Awards
Notable staff
Trevor Cox, (Professor of Acoustic Engineering and Broadcaster)
Professor Yiu Wai Lam, ex-Editor in Chief of Applied Acoustics
Olga Umnova

Alumni and Former Staff
The following past members of Salford Acoustics have been President of the Institute of Acoustics:

See also
University of Salford

References
External links
Official website",Category:Audio engineering schools,1
134,135,Electromagnetically excited acoustic noise and vibration,"Electromagnetically excited acoustic noise is audible sound directly produced by materials vibrating under the excitation of electromagnetic forces. Some examples of electromagnetically excited acoustic noise include the hum of transformers, the whine of some rotating electric machines, or the buzz of fluorescent lamps. The hissing of high voltage transmission lines is due to corona discharge, not magnetism.
The phenomenon is also called audible magnetic noise, electromagnetic acoustic noise or electromagnetically-induced acoustic noise , more rarely electrical noise , ""coil noise"" or "" coil whine"" depending on the application. The term electromagnetic noise is generally avoided as the term is used in the field of electromagnetic compatibility, dealing with radio frequencies. The term electrical noise describes electrical perturbations occurring in electronic circuits, not sound. The terms electromagnetic vibrations or magnetic vibrations  focusing on the structural phenomenon are less ambiguous.
Acoustic noise and vibrations due to electromagnetic forces can be seen as the reciprocal of microphonics, which describes how a mechanical vibration or acoustic noise can induce an undesired electrical perturbation.

General explanation
Electromagnetic forces can be defined as forces arising from the presence of an electromagnetic field (electrical field only, magnetic field only, or both).
Electromagnetic forces in the presence of a magnetic field include equivalent forces due to Maxwell stress tensor, magnetostriction and Lorentz force (also called Laplace force). Maxwell forces, also called reluctances forces, are concentrated at the interface of high magnetic reluctivity changes, e.g. between air and a ferromagnetic material in electric machines ; they are also responsible of the attraction or repulsion of two magnets facing each other. Magnetostriction forces are concentrated inside the ferromagnetic material itself. Lorentz or Laplace forces act on conductors plunged in an external magnetic field.
Equivalent electromagnetic forces due to the presence of an electrical field can involve electrostatic, electrostrictive and reverse piezoelectric effects.
These phenomena can potentially generate vibrations of the ferromagnetic, conductive parts, coils and permanent magnets of electrical, magnetic and electromechanical device, resulting in an audible sound if the frequency of vibrations lies between 20 Hz and 20 kHz, and if the sound level is high enough to be heard (e.g. large surface of radiation and large vibration levels). Vibration level is increased in case of a mechanical resonance, when electromagnetic forces match with a structural mode natural frequency of the active component (magnetic circuit, electromagnetic coil or electrical circuit) or of its enclosure.
The frequency of the noise depends on the nature of electromagnetic forces (quadratic or linear function of electrical field or magnetic field) and on the frequency content of the electromagnetic field (in particular if a DC component is present or not).

Electromagnetic noise and vibrations in electric machines
Electromagnetic torque, which can be calculated as the average value of the Maxwell stress tensor along the airgap, is one consequence of electromagnetic forces in electric machines. As a static force, it does not create vibrations nor acoustic noise. However torque ripple (also called cogging torque for permanent magnet synchronous machines in open circuit), which represents the harmonic variations of electromagnetic torque, is a dynamic force creating torsional vibrations of both rotor and stator. The torsional deflection of a simple cylinder cannot radiate efficiently acoustic noise, but with particular boundary conditions the stator can radiate acoustic noise under torque ripple excitation. Structure-borne noise can also be generated by torque ripple when rotor shaft line vibrations propagate to the frame and shaft line.
Some tangential magnetic force harmonics can directly create magnetic vibrations and acoustic noise when applied to the stator teeth: tangential forces create a bending moment of the stator teeth, resulting in radial vibrations of the yoke .
Besides tangential force harmonics, Maxwell stress also includes radial force harmonics responsible for radial vibrations of the yoke, which in turn can radiate acoustic noise.

Electromagnetic noise and vibrations in passive components
Inductors
In inductors, also called reactors or chokes, magnetic energy is stored in the airgap of the magnetic circuit, where large Maxwell forces apply. Resulting noise and vibrations depends on airgap material and magnetic circuit geometry .

Transformers
In transformers magnetic noise and vibrations are generated by several phenomena depending on the load case which include Laplace force on the windings, Maxwell forces in the joints of the laminations, and magnetostriction inside the laminated core.

Capacitors
Capacitors are also subject to large electrostatic forces. When the capacitor voltage/current waveform is not constant and contains time harmonics, some harmonic electric forces appear and acoustic noise can be generated. Ferroelectric capacitors also exhibit a piezoelectric effect that can be source of audible noise, this phenomenon is known as the ""singing capacitor"" effect.

Resonance effect in electrical machines
In radial flux rotating electric machines, resonance due to electromagneitc forces is particular as it occurs at two conditions: there must be a match between the exciting Maxwell force and the stator or rotor natural frequency, and between the stator or rotor modal shape and the exciting Maxwell harmonic wavenumber (periodicity of the force along the airgap).

As an example a resonance with the elliptical modal shape of the stator can occur if the force wavenumber is 2. Under resonance conditions, the maxima of the electromagnetic excitation along the airgap and the maxima of the modal shape displacement are in phase.

Numerical simulation
Methodology
The simulation of electromagnetically-excited noise and vibrations is a multiphysic modeling process carried in three steps:
calculation of the electromagnetic forces
calculation of the resulting magnetic vibrations
calculation of the resulting magnetic noise
It is generally considered as a weakly coupled problem: the deformation of the structure under electromagnetic forces is assumed not to change significantly the electromagnetic field distribution and the resulting electromagnetic stress.

Application to electric machines
The assessment of audible magnetic noise in electrical machines can be done using three methods:
using dedicated electromagnetic and vibro-acoustic simulation software (e.g. MANATEE )
using electromagnetic (e.g. Flux, Jmag, Maxwell, Opera), structural (e.g. Ansys Mechanical, Nastran, Optistruct) and acoustic (e.g. Actran, LMS, Sysnoise) numerical software together with dedicated coupling methods
using multiphysics numerical simulation software environment (e.g. Comsol Multiphysics, Ansys Workbench)

Examples of device subject to electromagnetic noise and vibrations
Static device
Static device include electrical systems and components used in electric power storage or power conversion such as
inductors
transformers
power inverters
capacitors
resistors: the braking resistors of electric trains, used to dissipate electrical power when the catenary is not receptive during braking, can make electromagnetically-excited acoustic noise
coils: in magnetic resonance imaging, ""coil noise"" is that part of total system noise attributed to the receiving coil, due to its non-zero temperature.

Rotating device
Rotating device include radial and axial flux rotating electric machines used for electrical to mechanical power conversion such as
induction motors
synchronous motors with permanent magnets or DC wound rotor
switched reluctance motors
In such device, dynamic electromagnetic forces come from variations of magnetic field, which either comes from a steady AC winding or a rotating DC field source (permanent magnet or DC winding).

Sources of magnetic noise and vibrations in electric machines
The harmonic electromagnetic forces responsible for magnetic noise and vibrations in a healthy machine can come from
Pulse-width modulation supply of the machine
slotting effects 
magnetic saturation
In a faulty machine, additional noise and vibrations due to electromagnetic forces can come from
mechanical static and dynamic eccentricities
uneven air-gap
demagnetization
short circuits
missing magnetic wedges
Unbalanced Magnetic Pull (UMP) describes the electromagnetic equivalence of mechanical rotating unbalance: if electromagnetic forces are not balanced, a non-zero net magnetic force appears on stator and rotor. This force can excite the bending mode of the rotor and create additional vibration and noise.

Reduction of electromagnetic noise and vibrations
Reduction of magnetic noise and vibrations in electric machines
NVH mitigation techniques in electrical machines include
reducing the magnitude of electromagnetic excitations, independently of the structural response of the electrical machine
reducing the magnitude of the structural response, independently of the electromagnetic excitations
reducing the resonances occurring between electromagnetic excitations and structural modes
Electromagnetic noise and vibration mitigation techniques in electrical machines include:
choosing the right slot/pole combination and winding design
avoiding resonances match between stator and electromagnetic excitations
skewing the stator or the rotor
implementing pole shaping / pole shifting / pole pairing techniques
implementing harmonic current injection or spread spectrum PWM strategies
using notches / flux barriers on the stator or the rotor
increasing damping

Reduction of ""coil noise""
Coil noise mitigation actions include:
add some glue (e.g. a layer of glue is often added on the top of television coils ; over the years, this glue degrades and the sound level increases)
change the shape of the coil (e.g. change coil shape to a figure eight rather than a traditional coil shape)
isolate the coil from the rest of the device to minimize structure-borne noise
increase damping

Experimental illustrations
A varying electromagnetic force can be produced either by a moving source of DC magnetic field (e.g. rotating permanent magnet or rotating coil supplied with DC current), or by a steady source of AC magnetic field (e.g. a coil fed by a variable current).

Forced vibration by a rotating permanent magnet
This animation illustrates how a ferromagnetic sheet can be deformed due to the magnetic field of a rotating magnet. It corresponds to an ideal one pole pair permanent magnet synchronous machine with a slotless stator.

Acoustic resonance by a variable frequency coil
The resonance effect of magnetic vibration with a structural mode can be illustrated using a tuning fork made of iron. A prong of the tuning fork is wound with a coil fed by a variable frequency power supply. A variable flux density circulates between the two prongs and some dynamic magnetic forces appear between the two prongs at twice the supply frequency. When the exciting force frequency matches the fundamental mode of the tuning fork close to 400 Hz, a strong acoustic resonance occurs.

Examples of audio files
PMSM motor (traction application)
External links
Video of a resonating tuning fork magnetically excited by a variable frequency current on YouTube
Video of a tuning fork magnetically excited by a fixed frequency current on YouTube
Video of a ferromagnetic cylinder deformed by a rotating magnet on YouTube

See also
Magnetostriction
Noise
Mains hum


== References ==",Category:Noise pollution,1
135,136,Computational aeroacoustics,"Computational aeroacoustics is a branch of aeroacoustics that aims to analyze the generation of noise by turbulent flows through numerical methods.

History
The origin of Computational Aeroacoustics can only very likely be dated back to the middle of the 1980s, with a publication of Hardin and Lamkin who claimed, that

""[...] the field of computational fluid mechanics has been advancing rapidly in the past few years and now offers the hope that ""computational aeroacoustics,"" where noise is computed directly from a first principles determination of continuous velocity and vorticity fields, might be possible, [...]""

Later in a publication 1986 the same authors introduced the abbreviation CAA. The term was initially used for a low Mach number approach (Expansion of the acoustic perturbation field about an incompressible flow) as it is described under EIF. Later in the beginning 1990s the growing CAA community picked up the term and extensively used it for any kind of numerical method describing the noise radiation from an aeroacoustic source or the propagation of sound waves in an inhomogeneous flow field. Such numerical methods can be far field integration methods (e.g. FW-H) as well as direct numerical methods optimized for the solutions (e.g.) of a mathematical model describing the aerodynamic noise generation and/or propagation. With the rapid development of the computational resources this field has undergone spectacular progress during the last three decades.

Methods
Direct numerical simulation (DNS) Approach to CAA
The compressible Navier-Stokes equation describes both the flow field, and the aerodynamically generated acoustic field. Thus both may be solved for directly. This requires very high numerical resolution due to the large differences in the length scale present between the acoustic variables and the flow variables. It is computationally very demanding and unsuitable for any commercial use.

Hybrid Approach
In this approach the computational domain is split into different regions, such that the governing acoustic or flow field can be solved with different equations and numerical techniques. This would involve using two different numerical solvers, first a dedicated Computational fluid dynamics (CFD) tool and secondly an acoustic solver. The flow field is then used to calculate the acoustical sources. Both steady state (RANS, SNGR (Stochastic Noise Generation and Radiation), ...) and transient (DNS, LES, DES, URANS, ...) fluid field solutions can be used. These acoustical sources are provided to the second solver which calculates the acoustical propagation. Acoustic propagation can be calculated using one of the following methods :
Integral Methods
Lighthill's analogy
Kirchhoff integral
FW-H

LEE
Pseudospectral
EIF
APE

Integral methods
There are multiple methods, which are based on a known solution of the acoustic wave equation to compute the acoustic far field of a sound source. Because a general solution for wave propagation in the free space can be written as an integral over all sources, these solutions are summarized as integral methods. The acoustic sources have to be known from some different source (e.g. a Finite Element simulation of a moving mechanical system or a fluid dynamic CFD simulation of the sources in a moving medium). The integral is taken over all sources at the retarded time (source time), which is the time at that the source is sent out the signal, which arrives now at a given observer position. Common to all integral methods is, that they cannot account for changes in the speed of sound or the average flow speed between source and observer position as they use a theoretical solution of the wave equation. When applying Lighthill's theory  to the Navier Stokes equations of Fluid mechanics, one obtains volumetric sources, whereas the other two analogies provide the far field information based on a surface integral. Acoustic analogies can be very efficient and fast, as the known solution of the wave equation is used. One far away observer takes as long as one very close observer. Common for the application of all analogies is the integration over a large number of contributions, which can lead to additional numerical problems (addition/subtraction of many large numbers with result close to zero.) Furthermore, when applying an integral method, usually the source domain is limited somehow. While in theory the sources outside have to be zero, the application can not always fulfill this condition. Especially in connection with CFD simulations, this leads to large cut-off errors. By damping the source gradually to zero at the exit of the domain or adding some additional terms to correct this end-effect, these cut-off errors can be minimized.

Lighthill's analogy
Also called 'Acoustic Analogy'. To obtain Lighthill's aeroacoustic analogy the governing Navier-Stokes equations are rearranged. The left hand side is a wave operator, which is applied to the density perturbation or pressure perturbation respectively. The right hand side is identified as the acoustic sources in a fluid flow, then. As Lighthill's analogy follows directly from the Navier-Stokes equations without simplification, all sources are present. Some of the sources are then identified as turbulent or laminar noise. The far-field sound pressure is then given in terms of a volume integral over the domain containing the sound source. The source term always includes physical sources and such sources, which describe the propagation in an inhomogeneous medium.
The wave operator of Lighthill's analogy is limited to constant flow conditions outside the source zone. No variation of density, speed of sound and Mach number is allowed. Different mean flow conditions are identified as strong sources with opposite sign by the analogy, once an acoustic wave passes it. Part of the acoustic wave is removed by one source and a new wave is radiated to fix the different wave speed. This often leads very large volumes with strong sources. Several modifications to Lighthill's original theory have been proposed to account for the sound-flow interaction or other effects. To improve Lighthill's analogy different quantities inside the wave operator as well as different wave operators are considered by following analogies. All of them obtain modified source terms, which sometimes allow a more clear sight on the ""real"" sources. The acoustic analogies of Lilley, Pierce, Howe and Möhring are only some examples for aeroacoustic analogies based on Lighthill's ideas. All acoustic analogies require a volume integration over a source term.
The major difficulty with the acoustic analogy, however, is that the sound source is not compact in supersonic flow. Errors could be encountered in calculating the sound field, unless the computational domain could be extended in the downstream direction beyond the location where the sound source has completely decayed. Furthermore, an accurate account of the retarded time-effect requires keeping a long record of the time-history of the converged solutions of the sound source, which again represents a storage problem. For realistic problems, the required storage can reach the order of 1 terabyte of data.

Kirchhoff integral
Kirchhoff and Helmholtz showed, that the radiation of sound from a limited source region can be described by enclosing this source region by a control surface - the so-called Kichhoff surface. Then the sound field inside or outside the surface, where no sources are allowed and the wave operator on the left hand side applies, can be produced as a superposition of monopoles and dipoles on the surface. The theory follows directly from the wave equation. The source strength of monopoles and dipoles on the surface can be calculated if the normal velocity (for monopoles) and the pressure (for dipoles) on the surface are known respectively. A modification of the method allows even to calculate the pressure on the surface based on the normal velocity only. The normal velocity could be given by a FE-simulation of a moving structure for instance. However, the modification to avid the acoustic pressure on the surface to be known leads to problems, when considering an enclosed volume at its resonant frequencies, which is a major issue of the implementations of their method. The Kirchhoff integral method finds for instance application in Boundary element methods (BEM). A non-zero flow velocity is accounted by considering a moving frame of reference with the outer flow speed, in which the acoustic wave propagation takes place. Repetitive applications of the method can account for obstacles. First the sound field on the surface of the obstacle is calculated and then the obstacle is introduced by adding sources on its surface to cancel the normal velocity on the surface of the obstacle. Variations of the average flow field (speed of sound, density and velocity) can be taken into account by a similar method (e.g. dual reciprocity BEM).

FW-H
The integration method of Ffowcs Williams and Hawkings is based on Lighthill's acoustic analogy. However, by some mathematical modifications under the assumption of a limited source region, which is enclosed by a control surface (FW-H surface), the volume integral is avoided. Surface integrals over monopole and dipole sources remain. Different from the Kirchhoff method, these sources follow directly from the Navier-Stokes equations through Lighthill's analogy. Sources outside the FW-H surface can be accounted by an additional volume integral over quadrupole sources following from the Lighthill Tensor. However, when considering the same assumptions as Kirchhoffs linear theory, the FW-H method equals the Kirchhoff method.

Linearized Euler Equations
Considering small disturbances superimposed on a uniform mean flow of density 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
  , pressure 
  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p_{0}}
   and velocity on x-axis 
  
    
      
        
          u
          
            0
          
        
      
    
    {\displaystyle u_{0}}
  , the Euler equations for a two dimensional model is presented as:

  
    
      
        
          
            
              ?
              
                U
              
            
            
              ?
              t
            
          
        
        +
        
          
            
              ?
              
                F
              
            
            
              ?
              x
            
          
        
        +
        
          
            
              ?
              
                G
              
            
            
              ?
              y
            
          
        
        =
        
          S
        
      
    
    {\displaystyle {\frac {\partial \mathbf {U} }{\partial t}}+{\frac {\partial \mathbf {F} }{\partial x}}+{\frac {\partial \mathbf {G} }{\partial y}}=\mathbf {S} }
  ,
where

  
    
      
        
          U
        
        =
        
          
            [
            
              
                
                  ?
                
              
              
                
                  u
                
              
              
                
                  v
                
              
              
                
                  p
                
              
            
            ]
          
        
         
        ,
         
        
          F
        
        =
        
          
            [
            
              
                
                  
                    ?
                    
                      0
                    
                  
                  u
                  +
                  ?
                  
                    u
                    
                      0
                    
                  
                
              
              
                
                  
                    u
                    
                      0
                    
                  
                  u
                  +
                  p
                  
                    /
                  
                  
                    ?
                    
                      0
                    
                  
                
              
              
                
                  
                    u
                    
                      0
                    
                  
                  v
                
              
              
                
                  
                    u
                    
                      0
                    
                  
                  p
                  +
                  ?
                  
                    p
                    
                      0
                    
                  
                  u
                
              
            
            ]
          
        
         
        ,
         
        
          G
        
        =
        
          
            [
            
              
                
                  
                    ?
                    
                      0
                    
                  
                  v
                
              
              
                
                  0
                
              
              
                
                  p
                  
                    /
                  
                  
                    ?
                    
                      0
                    
                  
                
              
              
                
                  ?
                  
                    p
                    
                      0
                    
                  
                  v
                
              
            
            ]
          
        
        ,
      
    
    {\displaystyle \mathbf {U} ={\begin{bmatrix}\rho \\u\\v\\p\\\end{bmatrix}}\ ,\ \mathbf {F} ={\begin{bmatrix}\rho _{0}u+\rho u_{0}\\u_{0}u+p/\rho _{0}\\u_{0}v\\u_{0}p+\gamma p_{0}u\\\end{bmatrix}}\ ,\ \mathbf {G} ={\begin{bmatrix}\rho _{0}v\\0\\p/\rho _{0}\\\gamma p_{0}v\\\end{bmatrix}},}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \rho }
  , 
  
    
      
        u
      
    
    {\displaystyle u}
  , 
  
    
      
        v
      
    
    {\displaystyle v}
   and 
  
    
      
        p
      
    
    {\displaystyle p}
   are the acoustic field variables, 
  
    
      
        ?
      
    
    {\displaystyle \gamma }
   the ratio of specific heats 
  
    
      
        
          c
          
            p
          
        
        
          /
        
        
          c
          
            v
          
        
      
    
    {\displaystyle c_{p}/c_{v}}
  , for air at 20 °C 
  
    
      
        
          c
          
            p
          
        
        
          /
        
        
          c
          
            v
          
        
        =
        1.4
      
    
    {\displaystyle c_{p}/c_{v}=1.4}
  , and the source term 
  
    
      
        
          S
        
      
    
    {\displaystyle \mathbf {S} }
   on the right-side represents distributed unsteady sources. The application of LEE can be found in engine noise studies.
For high Mach number flows in compressible regimes, the acoustic propagation may be influenced by non-linearities and the LEE may no longer be the appropriate mathematical model.

Pseudospectral
A Fourier pseudospectral time-domain method can be applied to wave propagation problems pertinent to computational aeroacoustics. The original algorithm of the Fourier pseudo spectral time domain method works for periodical problems without the interaction with physical boundaries. A slip wall boundary condition, combined with buffer zone technique to solve some non-periodical aeroacoustic problems has been proposed. Compared to other computational methods, pseudospectral method is preferred for its high-order accuracy.

EIF
Expansion about Incompressible Flow

APE
Acoustic Perturbation Equations
Refer to the paper ""Acoustic Perturbation Equations Based on Flow Decomposition via Source Filtering"" by R.Ewert and W.Schroder.

See also
Aeroacoustics
Acoustic theory

External links
Examples in Aeroacoustics from NASA
Computational Aeroacoustics at the Ecole Centrale de Lyon
Computational Aeroacoustics at the University of Leuven
Computational Aeroacoustics at Technische Universität Berlin
A CAA lecture script of Technische Universität Berlin

References

Lighthill, M. J., ""A General Introduction to Aeroacoustics and Atmospheric Sounds"", ICASE Report 92-52, NASA Langley Research Centre, Hampton, VA, 1992",Category:Wikipedia introduction cleanup from February 2013,1
136,137,Acoustical oceanography,"Acoustical oceanography is the use of underwater sound to study the sea, its boundaries and its contents.

History
Important contributions to acoustical oceanography have been made by:
Leonid Brekhovskikh
Walter Munk
Hank Medwin
John L Spiesberger
C C Leroy
David E. Weston
D. Van Holliday
Charles Greenlaw

Equipment used
The earliest and most widespread use of sound and sonar technology to study the properties of the sea is the use of an rainbow echo sounder to measure water depth. Sounders were the devices used that mapped the many miles of the Santa Barbara Harbor ocean floor until 1993.
Fathometers measure the depth of the waters. It works by electronically sending sounds from ships, therefore also receiving the sound waves that bounces back from the bottom of the ocean. A paper chart moves through the fathometer and is calibrated to record the depth.
As technology advances, the development of high resolution sonars in the second half of the 20th century made it possible to not just detect underwater objects but to classify them and even image them. Electronic sensors are now attached to ROVs since nowadays, ships or robot submarines have Remotely Operated Vehicles (ROVs). There are cameras attached to these devices giving out accurate images. The oceanographers are able to get a clear and precise quality of pictures. The 'pictures' can also be sent from sonars by having sound reflected off ocean surroundings. Oftentimes sound waves reflect off animals, giving information which can be documented into deeper animal behaviour studies.

Theory
See Clay and Medwin.

Measurements
See Clay and Medwin.

Applications
Applications of acoustical oceanography include:
fish population surveys
classification of fish species and other biota
rain rate measurement
wind speed measurement
water depth measurement
seabed classification
ocean acoustic tomography
global thermometry
monitoring of ocean-atmospheric gas exchange

Depth sounding
Marine biology
The study of marine life, from microplankton to the blue whale, uses bioacoustics.

See also
Ocean exploration
Cambridge Interferometer


== References ==",Category:Articles using small message boxes,1
137,138,Sound reduction index,"The sound reduction index is used to measure the level of sound insulation provided by a structure such as a wall, window, door, or ventilator. It is defined in the series of international standards ISO 16283 (parts 1-3) and the older ISO 140 (parts 1-14), or the regional or national variants on these standards. In the United States, the sound transmission class rating is generally used instead. The basic method for both the actual measurements and the mathematical calculations behind both standards is similar, however they diverge to a significant degree in the detail, and in the numerical results produced.
Standardized methods exist for measuring the sound insulation produced by various structures in both laboratory and field (actual functional buildings and building sites) environments. A number of indexes are defined which each offer various benefits for different situations.

Weighted Difference level (Dw)
The most basic index is the Weighted Difference level Dw. This index is defined by measuring in decibels (dB), the noise level produced on each side of a building element under test (e.g. a wall) when noise is produced in a room on one side (or outdoors) and measured both in the room where the noise is produced and in the room on the other side of the element under test.
This measurement may be carried out by measuring the levels in octave bands, or in 1/3 octave bands. (the latter is normally used for most applications). The minimum requirements of the standards require for the frequency range from 100 Hz to 3.15 kHz to be measured (16 ?1?3 octave bands). In some situations measurements may be carried out in the bands down to 50 Hz and/or up to 10 kHz.
The measured levels in each 1/3 octave band (or octave band) from the source room (or area) (S) are then compared to the measured levels in the receiving room (R), and the difference is taken (S-R). this produces a measured difference level 'D' for each frequency band in the measured spectrum.
To produce a single integer number the measured spectrum is plotted on a graph, and compared against a reference curve (defined in ISO 717-1 for airborne sound insulation, and 717-2 for impact sound insulation). The reference curve is moved in 1 dB steps until the total of the unfavorable deviations (measured points on the graph below the reference graph) is as close to 32 as possible but not greater than 32.
The value of the reference curve at 500 Hz is taken as the Weighted Difference Level, Dw This is considered to be approximately equal to the A-weighted level difference which would be observed if normal speech was used as the test signal.

Sound Reduction Index (R)
The Sound Reduction Index is expressed in decibels (dB). It is the weighted sound reduction index for a partition or single component only. This is a laboratory-only measurement, which uses knowledge of the relative sizes of the rooms in the test suite, and the reverberation time in the receiving room, and the known level of noise which can pass between the rooms in the suite by other routes (flanking) plus the size of the test sample to produce a very accurate and repeatable measurement of the performance of the sampled material or construction.

Apparent Sound Reduction Index (R')
This is a field measurement which attempts to measure the sound reduction index of a material on a real completed construction (e.g. a wall between two offices, houses or cinema auditoriums). It is unable to isolate or allow for the result of alternate sound transmission routes and therefore will generally produce a lower result than the laboratory measured value.
The calculation method used to produce the Sound Reduction Index takes into account the relative size of the tested rooms, and the size of the tested panel, and is therefore (theoretically) independent of these features, therefore a 1×1 panel of plasterboard (drywall) should have the same Rw as a 10×10 panel.

Normalized Level Difference (Dn)
This is an index which is measured in field conditions, between ""real"" rooms. It is a measurement which deliberately includes effects due to flanking routes and differences in the relative size of the rooms. It attempts, however, to normalize the measured difference level to the level which would be present when the rooms are furnished by measuring the quantity of acoustic absorption in the receiving room and correcting the difference level to the level which would be expected if there was 10m2 Sabine absorption in the receiving room. Detailed, accurate knowledge of the dimensions of the receiving room are required.

Standardized Level Difference (DnT)
Similar to the normalized level difference, this index corrects the measured difference to a standardized reverberation time. For dwellings, the standard reverberation time used is 0.5 seconds, for other larger spaces longer reverberation times will be used. 0.5 seconds is often cited as approximately average for a medium-sized, carpeted and furnished living room. Due to not requiring detailed and accurate knowledge of the dimensions of the test rooms, this index is easier to obtain, and arguably of slightly more relevance.
Once the difference level or sound reduction index is obtained, the weighted value may be obtained from the corrected spectrum as described above from the reference curve.

References

ISO 16283 (Part 1: Airborne sound insulation, Part 2: Impact sound insulation, Part 3: Façade sound insulation)
United Kingdom HM Government The Building regulations 2010 Part E: Resistance to the passage of sound
ISO 140 parts 1 - 14
ISO 717 parts 1, 2
360 video of a reverberation chamber used for measuring sound insulation",Category:Noise reduction,1
138,139,Acoustic shadow,"An acoustic shadow or sound shadow is an area through which sound waves fail to propagate, due to topographical obstructions or disruption of the waves via phenomena such as wind currents, buildings, or sound barriers.

Short distance acoustic shadow
A short distance acoustic shadow occurs behind a building or a sound barrier. The sound from a source is shielded by the obstruction. Due to diffraction around the object, it will not be completely silent in the sound shadow. The amplitude of the sound can be reduced considerably however, depending on the additional distance the sound has to travel between source and receiver.

Long distance acoustic shadow
As one website refers to it, ""an acoustic shadow is to sound what a mirage is to light"". For example, at the Battle of Iuka, a northerly wind prevented General Ulysses S. Grant from hearing the sounds of battle and sending more troops. Many other instances of acoustic shadowing were prevalent during the American Civil War, including the Battles of Seven Pines, Gaines' Mill, Perryville and Five Forks. Indeed, this is addressed in the Ken Burns's documentary The Civil War, produced by Florentine Films and aired on PBS in September 1990. Observers of nearby battles would sometimes see the smoke and flashes of light from cannon but not hear the corresponding roar of battle, while those in more distant locations would hear the sounds distinctly.

Further reading
Garrison Jr., Webb, Strange Battles of the Civil War, Cumberland House, 2001, ISBN 1-58182-226-X

See also
Gobo (recording)

External links
Acoustic Shadows - What is an acoustic shadow and how does it work? - Lisa
Acoustic Shadow
Civil War Acoustic Shadows


== References ==",Category:Acoustics,1
139,140,Room modes,"Room modes are the collection of resonances that exist in a room when the room is excited by an acoustic source such as a loudspeaker. Most rooms have their fundamental resonances in the 20 Hz to 200 Hz region, each frequency being related to one or more of the room's dimension's or a divisor thereof. These resonances affect the low-frequency low-mid-frequency response of a sound system in the room and are one of the biggest obstacles to accurate sound reproduction.

The mechanism of the room's resonances
The input of acoustic energy to the room at the modal frequencies and multiples thereof causes standing waves. The nodes and antinodes of these standing waves result in the loudness of the particular resonant frequency being different at different locations of the room. These standing waves can be considered a temporary storage of acoustic energy as they take a finite time to build up and a finite time to dissipate once the sound energy source has been removed.

Minimizing the effect of room resonances
A room with generally hard surfaces will exhibit, high Q, sharply tuned resonances. Absorbent material can be added to the room to damp such resonances which work by more quickly dissipating the stored acoustic energy.
In order to be effective, a layer of porous, absorbent material has to be of the order of a quarter-wavelength thick if placed on a wall, which at low frequencies with their long wavelengths requires very thick absorbers. Absorption occurs through friction of the air motion against individual fibres, with kinetic energy converted to heat, and so the material must be of just the right 'density' in terms of fibre packing. Too loose, and sound will pass through, but too firm and reflection will occur. Technically it is a matter of impedance matching between air motion and the individual fibres. Glass fibre, as used for thermal insulation, is very effective, but needs to be very thick (perhaps four to six inches) if the result is not to be a room that sounds unnaturally 'dead' at high frequencies but remains 'boomy' at lower frequencies, so that it provides absorption across a broad range of frequencies. Curtains and carpets are only effective at high frequencies (say 5 kHz and above).
As a rule of thumb, sound travels at one foot per millisecond (344 m/s), so the wavelength of notes at 1 kHz is about a foot (344mm), and at 10 kHz about an inch (34mm). Even six inches of glass fibre has little effect at 100 Hz, where a quarter wavelength is over 2 feet (860mm), and so adding absorbent material has virtually no effect in the lower bass region in the 20–50 Hz region, though it can bring about great improvement in the upper bass region above 100 Hz.
Open apertures, dispersion cylinders (large diameter and usually wall height), carefully sized and placed panels, and irregular room shapes are another way of either absorbing energy or breaking up resonant modes. For absorption, as with large foam wedges seen in anechoic chambers, the loss occurs ultimately through turbulence, as colliding air molecules convert some of their kinetic energy into heat. Damped panels, typically consisting of sheets of hardboard between glass fibre battens, have been used to absorb bass, by allowing movement of the surface panel and energy absorption by friction with the fibre battens.
If a room is being constructed, it is possible to choose room dimensions for which its resonances are less audible. This is done by ensuring that multiple room resonances are not at similar frequencies. For example a cubic room would exhibit three resonances at the same frequency.
Equalisation of the sound system to compensate for the uneven frequency response caused by room resonances is of very limited use as the equalisation only works for one specific listening position and will actually cause the response to be worse in other listening positions. Also large bass boosts by sound system EQ can severely reduce the headroom in the sound system itself. Some vendors are currently providing elaborate room tuning equipment which requires precision microphones, extensive data collection, and uses computerised electronic filtering to implement the necessary compensation for the rooms modes. There is some controversy about the relative worth of the improvement in ordinary rooms, given the very high cost of these systems.

Concert halls
Very large rooms like concert halls or large television studios have fundamental resonances which are much lower in frequency than small rooms. This means the closely spaced harmonic resonances are likely to lie in the low frequency region and thus the response tends to be more uniform.

See also
Loudspeaker measurement

References
External links
A simulation of the buildup of axial room modes (needs WebGL)
HTML5 Mode Calculator (3D view of each mode, audio playback, Bonello diagram, Bolt area, Schröder frequency,...)
Graphical mode calculator
Standing Waves - Room Modes
Room mode calculations and tables
Test tones playable online: helps localizing resonant frequencies in your room.
Standing waves (room modes) between sonically hard parallel walls",Category:Articles that may contain original research from January 2011,1
140,141,Dynamic aperture,,Category:Acoustics,1
141,142,Akoustolith,"Akoustolith is a porous ceramic material resembling stone. Akoustolith was a patented product of a collaboration between Rafael Guastavino Jr. and Harvard professor Wallace Sabine over a period of years starting in 1911.It was used to limit acoustic reflection and noise in large vaulted ceilings. Akoustolith was bonded as an additional layer to the structural tile of the Tile Arch System ceilings built by the Rafael Guastavino Company of New Jersey. The most prevalent use was to aid speech intelligibility in cathedrals and churches prior to the widespread use of public address systems.

History
Akoustolith was first introduced by the Guastavino Fireproof Construction Company, in collaboration with Wallace Sabine of Harvard University, in 1915. The founder of the Guastavino Company, Rafael Guastavino Sr., had immigrated to the United States from Spain in 1881, bringing with him the method of timbrel-vault construction, also known as cohesive construction. The Raphael Guastavino Company's vaulting technique created monolithic assemblies by layering thin bricks and structural tiles with fast-drying mortar. The Guastavino Technique, as it came to be known, consisted of multiple layers of plaster and tile in the construction of masonry vaulting; the first course of tile was set in its position with quick setting mortar creating form-work for the subsequent layers. Tiles were placed in concentric circles in the construction of domes, while in ribbed vaults, ribs served as the general form-work. Upon Guastavino Sr.'s death in 1908, his son, Rafael Guastavino Jr. took over the Guastavino Fireproof Construction Company; he was largely responsible for the company's development of acoustical finishes, including the incorporation and development of Rumford and Akoustolith tiles.
Raphael Guastavino Jr. and Wallace Sabine patented Akoustolith in 1916, to be used as a facing for Guastavino's timbrel vaults. The two had previously collaborated in the development of the Rumford tile, a ceramic acoustical finish used in the construction of the St. Thomas Church in New York City. While initially a success, the cost to manufacture Rumford tile led the company to focus on the development of the cheaper and more durable Akoustolith. As a non-ceramic tile, the sound absorption properties produced by Akoustolith's rough and porous surface, was an improvement on the Rumford tile . With the exception of the replacement of the first layer of tiles with the sound-absorbing Akoustolith, the Guastavino method of construction was unaltered. The effectiveness of Akoustolith in the reduction of reverberation led to its use in the construction of ecclesiastical spaces.
Following Sabine's death in 1919, Guastavino continued to patent acoustical building products. The Guastavino Fireproof Construction Company remained in business until 1962, its decline is attributed to the increased cost of hand labor in conjunction with the rise of concrete-shell construction. As timbrel-vault construction waned, the installation and production of acoustical materials helped sustain the company. By the late 1920s and early 1930s a considerable portion of the firm's business was related to these products. However, as other corporation began to mass-produce less-expensive acoustical building materials, Guastavino products ceased to be competitive.

Composition and Properties
Akoustolith developed as an improvement on the earlier Rumford tile. Rumford tiles had previously been made with rich organic soil that burned off during the firing process and created pores, this procedure was ultimately irregular and difficult to control. Consequently, Akoustolith was produced by binding well-sorted pumice particles with Portland cement to create an artificial stone, a process which offered consistency and allowed for a variety of shapes and color. Although sand and Portland Cement were typically used in the production of Akoustolith, the tile patent states that crushed rock or brick could be used as the aggregate, while lime or Plaster of Paris could be used as the binding material.
Akoustolith's efficiency in absorbing different pitches was largely dependent upon the dimensions of its particles; its most imperative feature was its use of aggregate graded to a uniform size. Finer grades of aggregate were sieved out, leaving spaces between the particles, creating an intercommunicating pore structure that absorbed sound. According to Guastavino's and Sabine's 1916 patent Akoustolith absorbed ""much in excess of 15% of sounds in the pitch between the middle C and the third octave above the middle C, which are the characteristic sounds which distinguish articulate speech.""
Designed with a graded porosity to increase their range of absorption, the stone-like finish of Akoustolith tiles was comprised of a mix of coarser aggregate to facilitate the absorption of low pitches. Similarly, the bedding face was comprised of a mix of finer aggregate to absorb higher pitches. Eventually, different grades of the material were sold; these varied in size and sound-absorption coefficients.

Building with Akoustolith
Although the production of Akoustolith tile was short-lived, its effectiveness in reducing reverberation in ecclesiastical spaces led to its installation in a variety of building types, including commercial, industrial, and institutional structures. The acoustical and fireproof nature of Akoustolith was advertised, and to a lesser degree, its ability to resist the condensation of moisture. In addition, Akoustolith's aesthetic qualities were touted: the tiles were available in several shades of gray and buff intended to blend with the warm colors of adjacent stone.
Resembling a stone-like masonry material, Akoustolith tile was incorporated into several of the Guastavino Company's major building projects, including the 1929 construction of the Buffalo Central Terminal. Completed in the late 1920s, Fellheimer & Wagner's Buffalo Central Terminal was the largest installation of Akoustolith completed by the Guastavino Company.

List of Example Projects
Fellheimer & Wagner's design of the Buffalo Central Terminal, in New York, was the largest installation of Akoustolith completed by the Guastavino Company.
New York architect Bertram Goodhue specified the use of the Guastavino tile in his 1920 design for the Nebraska Capitol. Consequently, the Nebraska Capitol features tiled vaults and domes, and meeting rooms constructed with Akoustolith tiles.
Ralph Adams Cram's 1921 design for the Princeton University Chapel employs the Guastavino Company's Akoustolith tile vaulting.

References
External links
A Tale of Two Physicists: mentions the collaboration between Sabine and Guastavino.

Further reading
G.F.S. ""A Simple Method of Finding the Sound Absorbing Power of a Building Material."" Journal of the Franklin Institute 206, no. 1 (1928): 130-31.
Liu, Yishi. ""Building Guastavino Dome in China: A Historical Survey of the Dome of the Auditorium at Tsinghua University."" Frontiers of Architectural Research 3, no. 2 (2014): 121-40.
Smilor, Raymond. ""Confronting the Industrial Environment: The Noise Problem in America, 1893-1932., 1978, ProQuest Dissertations and Theses.
Thompson, Emily. ""Dead Rooms and Live Wires: Harvard, Hollywood, and the Deconstruction of Architectural Acoustics, 1900-1930."" Isis 88, no. 4 (1997): 597-626.",Category:Acoustics,1
142,143,First break picking,"First-break picking detecting or picking the onset arrivals of refracted signals from all the signals received by receiver arrays and produced by a particular source signal generation. It is also called first arrival picking or first break detection. First-break picking can be done automatically, manually or as a combination of both. With the development of computer science and the size of seismic surveys, automatic picking is often preferred.

Significance
First-break picks associated with the refracted arrival times are used in an inversion scheme to study the near-surface low-velocity zone and subsequent determination of static corrections. Static correction is a correction applied to geophysical data, especially seismic data, to compensate for the effect of near-surface irregularities, differences in the elevation of shots and geophones, or any application to correct the positions of source and receivers.

History of First Break Picking
Gelchinsky and Shtivelman(1983) used correlation properties of signals and applied a statistical criterion for the estimation of first arrivals time.
Coppens(1985) calculated the ratio of energy of seismogram of the two windows and used that to differentiate in signal and noise.
Michael D. McCormark et al.(1993) introduced a backpropagation neural network (BNN) method. The Neural network which edits seismic data or pick first breaks was trained by users, who were just selecting and presenting to the network examples of trace edits or refraction picks. The network then changes internal weights iteratively until it can reproduce the examples accurately provided by the users.
Fabio Boschetti et al.(1996) introduce a fractal-based algorithm which detects the presence of a signal by analyzing the variation in fractal dimension along the trace. This method works when signal-to-noise ratio is small, but it is considerably slow.
A direct correlation method was introduced by Joseph et al.(1999) which was developed for use in highly time-resolved, low-noise signals acquired in the laboratory. In this method, the greatest value of Pearson's correlation coefficient between segments of observed waveforms near the pulse onset and at an appropriate reference serves as the time determination criterion.
Zuolin Chen, et al.(2005) introduced a multi-window algorithm to detect the first break. In this method, three moving windows were used and the averages of absolute amplitudes in each window need to be calculated, then ratios based on the averages of the windows provide standards to differentiate signals from unwanted noise.
Wong et al.(2009) introduced STA/LTA ratio method. This method is similar as Coppens’ algorithm. The difference is to do the ratio of two averages of energy between a short-term window and a long-term window, which is denoted as STA/LTA (short-term average/long-term average), instead of calculating the ratio of energy of seismogram of the two windows in Coppens’ algorithm.

Methods of Automatic First Break Picking
STA/LTA ratio Method
This method is similar as Coppens’ (1985) algorithm. The difference is to do the ratio of two averages of energy between a short-term window and a long-term window, which is denoted as STA/LTA (short-term average/long-term average), instead of calculating the ratio of energy of seismogram of the two windows in Coppens’ algorithm. The numerical derivative of the ratio can be defined as,

  
    
      
        
          
            
              
              
                
                  d
                  
                    i
                  
                
                =
                
                  r
                  
                    i
                    +
                    1
                  
                
                ?
                
                  r
                  
                    i
                  
                
                ,
                i
                =
                1
                ,
                2
                ,
                .
                .
                .
                (
                n
                ?
                1
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&d_{i}=r_{i+1}-r_{i},i=1,2,...(n-1)\\\end{aligned}}}
  
where ri+1 is the STA/LTA ratio at time index i+1, and ri is the STA/LTA ratio at time index i. For noise-free seismograms, the maximum value of the numerical derivative of the STA/LTA ratio is close to the time of the first arrival.
Wong et al. (2009) modified the algorithm of the energy ratio method, where they named the method as modified energy ratio. In this method, they define the energy ratio as,

  
    
      
        
          
            
              
              
                e
                
                  r
                  
                    i
                  
                
                =
                
                  ?
                  
                    j
                    =
                    i
                  
                  
                    i
                    +
                    n
                    e
                  
                
                
                  x
                  
                    j
                  
                  
                    2
                  
                
                
                  /
                
                
                  ?
                  
                    j
                    =
                    i
                    ?
                    n
                    e
                  
                  
                    i
                  
                
                
                  x
                  
                    j
                  
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&er_{i}=\sum _{j=i}^{i+ne}x_{j}^{2}/\sum _{j=i-ne}^{i}x_{j}^{2}\\\end{aligned}}}
  
where xi is the times series representing a seismogram with the time index i=1, 2 … N. and the number of points in an energy window is ne. Then, the modified energy ratio is defined as

  
    
      
        
          
            
              
              
                e
                r
                
                  3
                  
                    i
                  
                
                =
                (
                a
                b
                s
                (
                
                  x
                  
                    i
                  
                
                
                  )
                  
                    ?
                  
                
                e
                
                  r
                  
                    i
                  
                
                
                  )
                  
                    3
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&er3_{i}=(abs(x_{i})^{*}er_{i})^{3}\\\end{aligned}}}
  
The peak of the modified energy ratio er3i is very closed to the time of first arrivals on noise-free seismograms.

Multi-Window Method
This method needs to calculate the averages of absolute amplitudes from a seismic trace by using three moving time windows before and after each time point (sample).
When the instantaneous absolute amplitude exceeds an automatically adjusted threshold, ratios based on the averages of the windows over previous time samples provide standards to differentiate signals from unwanted noise.
The multi-window automatic P phase picker operates in the time-domain. It includes procedures to define: time windows, standards, corresponding thresholds and waveform correction.
1. The averages of absolute amplitudes within BTA (Before Term Average), ATA (After Term Average) and DTA (Delayed Term Average) windows are respectively defined as follows:

  
    
      
        
          
            
              
              
                
                  
                    
                      B
                      T
                      A
                      (
                      t
                      )
                    
                    ¯
                  
                
                =
                
                  ?
                  
                    i
                    =
                    i
                  
                  
                    m
                  
                
                
                  
                    
                      
                        |
                      
                      u
                      (
                      t
                      ?
                      i
                      )
                      
                        |
                      
                    
                    m
                  
                
              
            
            
              
              
                
                  
                    
                      A
                      T
                      A
                      (
                      t
                      )
                    
                    ¯
                  
                
                =
                
                  ?
                  
                    i
                    =
                    i
                  
                  
                    n
                  
                
                
                  
                    
                      
                        |
                      
                      u
                      (
                      t
                      +
                      j
                      )
                      
                        |
                      
                    
                    n
                  
                
              
            
            
              
              
                
                  
                    
                      D
                      T
                      A
                      (
                      t
                      )
                    
                    ¯
                  
                
                =
                
                  ?
                  
                    k
                    =
                    i
                  
                  
                    q
                  
                
                
                  
                    
                      
                        |
                      
                      u
                      (
                      t
                      +
                      j
                      +
                      d
                      )
                      
                        |
                      
                    
                    q
                  
                
              
            
            
              
              
                
                  R
                  
                    2
                  
                
                (
                t
                )
                =
                
                  
                    
                      
                        A
                        T
                        A
                        (
                        t
                        )
                      
                      ¯
                    
                    
                      
                        B
                        T
                        A
                        (
                        t
                        )
                      
                      ¯
                    
                  
                
              
            
            
              
              
                
                  R
                  
                    3
                  
                
                (
                t
                )
                =
                
                  
                    
                      
                        D
                        T
                        A
                        (
                        t
                        )
                      
                      ¯
                    
                    
                      
                        B
                        T
                        A
                        (
                        t
                        )
                      
                      ¯
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\overline {BTA(t)}}=\sum _{i=i}^{m}{\frac {|u(t-i)|}{m}}\\&{\overline {ATA(t)}}=\sum _{i=i}^{n}{\frac {|u(t+j)|}{n}}\\&{\overline {DTA(t)}}=\sum _{k=i}^{q}{\frac {|u(t+j+d)|}{q}}\\&R_{2}(t)={\frac {\overline {ATA(t)}}{\overline {BTA(t)}}}\\&R_{3}(t)={\frac {\overline {DTA(t)}}{\overline {BTA(t)}}}\\\end{aligned}}}
  
Standards R2(t) and R3(t) are used for the discrimination of high-amplitude short-duration and long-duration noise.
2.Thresholds is defined as

  
    
      
        
          
            
              
              
                
                  H
                  
                    1
                  
                
                (
                t
                )
                =
                
                  E
                  
                    m
                  
                
                (
                t
                ?
                p
                )
                +
                ?
                
                  E
                  
                    s
                    d
                  
                
                (
                t
                ?
                p
                )
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&H_{1}(t)=E_{m}(t-p)+\alpha E_{sd}(t-p)\\\end{aligned}}}
  
where Em is mean and Esd is standard deviation; p is the number of shifted samples; ? is the coefficient to adjust the height of the first threshold and is taken to be 3. From this equation it is obvious that H1(t) is automatically adjusted with the variance of the background noise.
3. H1(t) is defined larger than most pre-existing noise levels, and the instantaneous absolute amplitude at the trigger time point is higher than H1(t),according to the configuration of the first arrival of an event the real onset time must be earlier than the trigger time point. A waveform correction should be used to compensate this belated onset time. For an impulsive first arrival, the height of the absolute amplitude and the representative gradient at the trigger point can be used to accomplish the correction.

Available Code
Potash SU is a package including Seismic Unix style codes developed by Balazs Nemeth, it provides a subroutine called simple window-based first break picker, the figure shows the seismic images before and after the application of subroutine.

Future Trend of the Topic
Methods of Picking: automatic first-break picking has played an important role in seismic data processing, and directly influences the quality of seismic sections. Because of the increase of seismic survey size, more efficient and fast first break picking methods are needed, with parallel methods being preferred.
Application of First Break detection: Traditionally the geophysicist uses first breaks for static correction. First break signal can also be used as observation data for history matching.


== Notes ==",Category:Seismology,1
143,144,Transmission loss,"Transmission loss (TL) in general describes the accumulated decrease in intensity of a waveform energy as a wave propagates outwards from a source, or as it propagates through a certain area or through a certain type of structure.
It is a terminology frequently used in optics and acoustics. Measures of TL are very important in the industry of acoustic devices such as mufflers and sonars.

Measurement
Measurement of transmission loss can be in terms of decibels.
Mathematically, transmission loss is measured in dB scale and in general it can be defined using the following formula:

  
    
      
        T
        L
        =
        10
        
          log
          
            10
          
        
        ?
        
          |
          
            
              
                W
                
                  i
                
              
              
                W
                
                  t
                
              
            
          
          |
        
      
    
    {\displaystyle TL=10\log _{10}\left\vert {W_{i} \over W_{t}}\right\vert }
  
where:

  
    
      
        
          W
          
            i
          
        
      
    
    {\displaystyle W_{i}}
   is the power of incident wave coming towards a defined area (or structure);

  
    
      
        
          W
          
            t
          
        
      
    
    {\displaystyle W_{t}}
   is the power of transmitted wave going away from the defined area (or structure).
Transmission loss may refer to a more specific concept in one of the field below.

Duct Acoustics
Transmission Loss (duct acoustics) in duct acoustics describes the acoustic performances of a muffler like system.

Room Acoustics
Transmission Loss in room acoustics describes the decrease of sound intensity that is reduced by a wall or other structure at a given frequency.

Underwater Acoustics
Propagation loss (sometimes referred to as transmission loss) is a quantitative measure of the reduction in sound intensity between two points, normally the sound source and a distant receiver.


== References ==",Category:Optics,1
144,145,Pythagorean hammers,"According to legend, Pythagoras discovered the foundations of musical tuning by listening to the sounds of four blacksmith's hammers, which produced consonance and dissonance when they were struck simultaneously. According to Nicomachus in his 2nd century CE Enchiridion harmonices  Pythagoras noticed that hammer A produced consonance with hammer B when they were struck together, and hammer C produced consonance with hammer A, but hammers B and C produced dissonance with each other. Hammer D produced such perfect consonance with hammer A that they seemed to be ""singing"" the same note. Pythagoras rushed into the blacksmith shop to discover why, and found that the explanation was in the weight ratios. The hammers weighed 12, 9, 8, and 6 pounds respectively. Hammers A and D were in a ratio of 2:1, which is the ratio of the octave. Hammers B and C weighed 9 and 8 pounds. Their ratios with hammer A were (12:9 = 4:3 = perfect fourth) and (12:8 = 3:2 = perfect fifth). The space between B and C is a ratio of 9:8, which is equal to the musical whole tone, or whole step interval ( Play 9/8).
The legend is, at least with respect to the hammers, demonstrably false. It is probably a Middle Eastern folk tale. These proportions are indeed relevant to string length (e.g. that of a monochord) — using these founding intervals, it is possible to construct the chromatic scale and the basic seven-tone diatonic scale used in modern music, and Pythagoras might well have been influential in the discovery of these proportions — but the proportions do not have the same relationship to hammer weight and the tones produced by them. However, hammer-driven chisels with equal cross-section, show an exact proportion between length or weight and Eigenfrequency.
Earlier sources mention Pythagoras' interest in harmony and ratio. Xenocrates (4th century BCE), while not as far as we know mentioning the blacksmith story, described Pythagoras' interest in general terms: ""Pythagoras discovered also that the intervals in music do not come into being apart from number; for they are an interrelation of quantity with quantity. So he set out to investigate under what conditions concordant intervals come about, and discordant ones, and everything well-attuned and ill-tuned."" Whatever the details of the discovery of the relationship between music and ratio, it is regarded as historically the first empirically secure mathematical description of a physical fact. As such, it is symbolic of, and perhaps leads to, the Pythagorean conception of mathematics as nature's modus operandi. As Aristotle was later to write, ""the Pythagoreans construct the whole universe out of numbers"".

See also
Equal temperament
Just intonation
Pythagorean tuning


== References ==",Category:Musical tuning,1
145,146,Behavioral observation audiometry,,Category:Articles with multiple maintenance issues,1
146,147,Acoustic levitation,"Acoustic levitation (also: Acoustophoresis) is a method for suspending matter in a medium by using acoustic radiation pressure from intense sound waves in the medium.
Sometimes sound waves at ultrasonic frequencies can be used to levitate objects, thus creating no sound heard by the human ear, such as was demonstrated at Otsuka Lab, while others use audible frequencies. There are various ways of emitting the sound wave, from creating a wave underneath the object and reflecting it back to its source, to using a (transparent) tank to create a large acoustic field.
Acoustic levitation is usually used for containerless processing which has become more important of late due to the small size and resistance of microchips and other such things in industry. Containerless processing may also be used for applications requiring very-high-purity materials or chemical reactions too rigorous to happen in a container. This method is harder to control than other methods of containerless processing such as electromagnetic levitation but has the advantage of being able to levitate nonconducting materials.
By 2013, acoustic levitation had progressed from motionless levitation to controllably moving hovering objects, an ability useful in the pharmaceutical and electronics industries. A prototype device involved a chessboard-like array of square acoustic emitters that move an object from one square to another by slowly lowering the sound intensity emitted from one square while increasing the sound intensity from the other, allowing the object to travel virtually ""downhill"".
Current systems have lifted at most a few kilograms. Acoustic levitators are used mostly in industry. However, some products are commercially available to the public.

See also
Acoustic tweezers
Optical levitation
Radiation pressure
Electrostatic levitation
Magnetic levitation
Aerodynamic levitation
Buoyancy

References
External links
McGraw-Hill AccessScience: Acoustic radiation pressure
A Multi-Transducer Near Field Acoustic Levitation System for Noncontact Transportation of Large-Sized Planar Objects
Live Science – Scientists Levitate Small Animals",Category:Acoustics,1
147,148,Diffusion (acoustics),"Diffusion, in acoustics and architectural engineering, is the efficacy by which sound energy is spread evenly in a given environment. A perfectly diffusive sound space is one that has certain key acoustic properties which are the same anywhere in the space. A non-diffuse sound space would have considerably different reverberation time as the listener moved around the room. Virtually all spaces are non-diffuse. Spaces which are highly non-diffuse are ones where the acoustic absorption is unevenly distributed around the space, or where two different acoustic volumes are coupled. The diffusiveness of a sound field can be measured by taking reverberation time measurements at a large number of points in the room, then taking the standard deviation on these decay times. Alternately, the spatial distribution of the sound can be examined. Small sound spaces generally have very poor diffusion characteristics at low frequencies due to room modes.

Diffusor
Diffusors (or diffusers) are used to treat sound aberrations, such as echoes, in rooms. They are an excellent alternative or complement to sound absorption because they do not remove sound energy, but can be used to effectively reduce distinct echoes and reflections while still leaving a live sounding space. Compared to a reflective surface, which will cause most of the energy to be reflected off at an angle equal to the angle of incidence, a diffusor will cause the sound energy to be radiated in many directions, hence leading to a more diffusive acoustic space. It is also important that a diffusor spreads reflections in time as well as spatially. Diffusors can aid sound diffusion, but this is not why they are used in many cases; they are more often used to remove coloration and echoes.
Diffusors come in many shapes and materials. The birth of modern diffusors was marked by Manfred R. Schroeders' invention of number-theoretic diffusors in the 1970s.

Maximum length sequence diffusors
Maximum length sequence based diffusors are made of strips of material with two different depths. The placement of these strips follows an MLS. The width of the strips is smaller than or equal to half the wavelength of the frequency where the maximum scattering effect is desired. Ideally, small vertical walls are placed between lower strips, improving the scattering effect in the case of tangential sound incidence. The bandwidth of these devices is rather limited; at one octave above the design frequency, diffusor efficiency drops to that of a flat surface.

Quadratic-residue diffusors
MLS based diffusors are superior to geometrical diffusors in many respects; they have limited bandwidth. The new goal was to find a new surface geometry that would combine the excellent diffusion characteristics of MLS designs with wider bandwidth. A new design was discovered, called a quadratic-residue diffusor. Today the quadratic residue diffusor or Schroeder diffusor is still widely used. Quadratic-Residue Diffusors can be designed to diffuse sound in either one or two directions. They too suffer from ""flat plate"" frequencies, but at a higher frequencies than MLS diffusors. Fractal constructions can be used to extend bandwidth.

Primitive-root diffusors
Primitive-root diffusors are based on a number theoretic sequence based on primitive roots. Although they produce a notch in the scattering response, in reality the notch is over too narrow a bandwidth to be useful. In terms of performance, they are very similar to Quadratic-Residue Diffusors.

Optimized diffusors
By using numerical optimisation, it is possible to increase the number of theoretical designs, especially for diffusors with a small number of wells per period. But the big advantage of optimisation is that arbitrary shapes can be used which can blend better with architectural forms.

Two-dimensional (""hemispherical"") diffusors
Designed, like most diffusors, to create ""a big sound in a small room,"" unlike other diffusors, two-dimensional diffusors scatter sound in a hemispherical pattern. This is done by the creation of a grid, whose cavities have wells of varying depth, according to the matrix addition of two quadratic sequences equal or proportionate to those of a regular diffusor. These diffusors are very helpful for controlling the direction of the diffusion, particularly in studios and control rooms  .

Manufacturers
Yukon Acoustics

See also
Sound baffle

References
Further reading
T. J. Cox and P. D'Antonio, ""Acoustic Absorbers and Diffusors - Theory, Design and Application"" Spon press.",Category:Sound,1
148,149,Category:Acoustical Society of America,,Category:Commons category without a link on Wikidata,1
149,150,Chronomètre of Loulié,"The chronomètre is a precursor of the metronome. It was invented circa 1694 by Étienne Loulié to record the preferred tempo of pieces of music.

The Device
Circa 1694 Étienne Loulié, a musician who had recently collaborated with mathematician Joseph Sauveur on the education of Philippe, Duke of Chartres, was asked by Chartres to work with Sauveur on a scientific study of acoustics sponsored by the Royal Academy of Science.
To measure scientifically the number of beats per second caused by different dissonances, they used the ""seconds pendulum"" invented by Galileo earlier in the century. It doubtlessly was these experiments, on top of his lessons to Chartres, that gave Loulié the idea for his chronomètre, a precursor of the metronome.
In his Éléments (Paris: Ballard, 1696) — which resumes the lessons Loulié had given to Chartres and is dedicated to the prince — Loulié described this invention, complete with an engraving of the device. (A translation of Loulié's description is provided below.)
The device is basically a Galilean seconds pendulum disguised as a classical column. It consists of a six-foot-tall vertical ""ruler"" marked off in inches, with a little peg-hole at every inch. From the right-angle bar that protrudes at the capital of the Ionic capital, hangs a string with a plumb bob at the end. The length of the string — and therefore the speed of the pendulum swings — can be adjusted by moving the peg at the other end of the string up and down the vertical board and inserting it in one peg-hole or another. The shorter the string, the more rapid the swings; the longer the string, the slower the swings.
To specify the tempo of a piece, the composer could henceforth test the tempo at a variety of peg holes and, having determined the right tempo, could mark at the top of a piece the note value that represented the musical beat, plus the number of the hole into which the peg had been inserted.
Sauveur subsequently criticized the device because it was measured in inches, which did not conform with any known relation to the duration of a second. His échomètre tried to remedy this shortcoming, by marking the vertical ruler with the small units that the Sauveur was creating for his Nouveau système. When Loulié died in 1702, Louis Léon Pajot, comte d'Onsenbray, acquired Loulié's model and presented his own variant (the métromètre) to the Academy of Sciences in 1732. It was measured in seconds and made the swings of the pendulum audible. The size of all three devices rendered them too cumbersome for widespread use.

Its utility
On pages 85–86 of his Éléments, Loulié emphasized the usefulness of his device:
This instrument is especially suitable for marking the tempos of pieces being sent abroad, or to know the exact tempo of imported pieces, as long as they have been marked by this instrument.
A few years ago I gave a chronomètre to a person of great musical merit. I showed him how to use it. He took it with him to Italy, where he currently is. A musician who wanted to send that person a few sonatas he had written, and who was very happy to know that his airs would be executed as he intended, came to me not long ago and asked me to help him mark the tempo of his airs using the chronomètre. [...]
This instrument may not seem very necessary or even very useful.
First, there are those who, being savant and very experienced in both French and foreign music, are capable of judging their true tempo, or approximately so.
Second, those who are very familiar with the airs of Monsieur de Lully and other airs of this sort, neglect or even look down upon other types of music.
Third, those whose only musical merits are a certain routine, with no refinement of taste, think that it makes little difference whether an air is performed more quickly or more slowly.
But I flatter myself that those who have fine taste and who have observed how an air loses its beauty when executed too quickly or too slowly, will thank me for giving them a reliable way to determine the true tempo, especially people living in the provinces, who will be able to know the true tempo of all of Monsieur de Lully's works, which I have determined very accurately with the chronomètre, with the help of people who performed them for many years under the direction of Monsieur de Lully himself.
Loulié's allusions to the airs of Jean Baptiste de Lully are particularly meaningful within the broader context of Loulié's business activities. Loulié was directing a workshop that copied Lully's airs (usually in trio form), probably for sale by his friend Henri Foucault, the music-paper dealer and disseminator of Lully's works in both manuscript and print.

Loulié's Description
Loulié's own description of the invention follows, translated from pp. 83–86 of his Éléments:
The chronomètre is an instrument by means of which composers will henceforth be able to mark the true tempo of their composition; and their airs, marked according to this instrument, will be able to be performed, in their absence, as if they themselves were beating time.
This instrument has only two parts:
The first is a wooden ruler (AA) that is six feet tall (or 72 inches), about two inches wide, and approximately an inch thick. On the flat side of the ruler a line (BC) is traced from top to bottom, down the very middle. Along this line are marked, very precisely, inch-long divisions, and wherever these [horizontal] lines cross the vertical line, there is a hole about 1/6 inch in diameter and 3/4 inches deep. These holes are indicated by numbers, beginning with the lowest all the way up to 72 at the top.
I used the universal foot [as a measurement] because it is known in all sorts of countries. [...]
At the top of the ruler, at a right angle, is an iron or wooden bar (BD) six or seven inches long, inserted into the ruler an inch above section 71. About six or seven inches from the end of the angle-bar is a little hole (D) about as big as a shoelace or thick cord. At the other end, where it is inserted into the ruler, is another hole (B) at the central vertical line.
The second part of this instrument is a pendulum, that is, a plumb-bob (E) approximately one inch in diameter, pierced through with a hole the size of a thick cord, and through this hole is passed the cord, knotted above the plumb-bob.
The cord and its plumb-bob are attached to the ruler by passing the cord through holes D and B in the angle-bar, so that one of the ends of the rope coincides with the central division line, and the other end, with the plumb-bob, hangs [pend] in the air, which is why it is called a pendulum.
At the other end of the cord, along the division line, is attached a wooden or iron peg (F) with the same diameter as the holes in the ruler, so that the peg just enters.
The peg is made a bit like the pegs on a lute or a viol: there is a hole between the part one grabs with one's hand, and the part that goes into the holes on the ruler. The cord goes through this hole and is knotted, so that when the peg is in hole 72, the cord is 72 inches long from the hole (D) to the center of the plumb-bob. By means of this peg that is attached to the cord, one can modify the length of the pendulum (or pendillon) as one wishes, by putting the peg into a higher or lower place. It is set into motion at the height of a quarter of a quarter of a circle, which is done by moving the plumb-bob two feet from perpendicular, or from its resting position, when the peg is in hole 72, or one foot when the peg is in hole 36, and six inches when the peg is in hole 18, and always a bit less as the pendulum becomes shorter. If one lets go of the plumb-bob without forcing it, the pendulum can mark with utmost precision the quickness or the slowness of the musical beat, as a result of the swings [vibrations] at its various lengths.
If a composer wants to mark the desired tempo for performing an air he has composed, irrespective of the time signature he must begin by putting the peg in one of the holes and then setting the pendulum in motion. If the swings are too slow, he must shorten the pendulum by putting the peg in a lower number. If the swings are too fast, he must lengthen the pendulum by putting the peg into a higher number, until he has found the tempo he wants.
When he has found the tempo that he wants to specify, he must write down the number of the hole where the peg is, above the time signature, with a musical note showing the value or duration of each swing.
If a composer wants to mark the desired tempo for performing an air he has composed, irrespective of the time signature he must begin by putting the peg in one of the holes and then setting the pendulum in motion. If the swings are too slow, he must shorten the pendulum by putting the peg in a lower number. If the swings are too fast, he must lengthen the pendulum by putting the peg into a higher number, until he has found the tempo he wants.
When he has found the tempo that he wants to specify, he must write down the number of the hole where the peg is, above the time signature, with a musical note showing the value or duration of each swing.

References
Étienne Loulié, Les Éléments ou Principes de Musique (Paris, 1696)
Albert B. Cohen, Music in the French Royal Academy of Sciences (Princeton, 1981)
Patricia M. Ranum, ""Étienne Loulié (1654–1702), musicien de Mademoiselle de Guise, pédagogue et théoricien,"" (part 2) Recherches (25 (1988–90)
Patricia M. Ranum, "" 'Monsieur de Lully en trio': Étienne Loulié,the Foucaults, and the Transcription of the Works of Jean-Baptiste Lully (1673–1702), in J. de La Gorce and H. Schneider, eds., Jean-Baptiste Lully (Laaber, 1990), pp. 309–330
Rosamond Harding:""The metronome and it's precursors"" (Gresham Books 1938) page 8-9",Category:Musical instrument parts and accessories,1
150,151,Gobo (recording),"Gobo is a sound recording term for a movable acoustic isolation panel. In typical use, a recording engineer might put a gobo between two musicians to increase the isolation of their microphones from each other.
For the stage and photographic lighting use of gobo, see Gobo (lighting).
The origin of the term ""gobo"" is obscure, but is most likely short for ""go-between."" The gobo was invented by Charles Norris Hoyle, and was originally a product of TayTrix.

Use
Gobo panels control the acoustical properties of a room by absorbing and diffusing sound waves. Uses include treating recording and mixing areas for unwanted reverberation, or to separate two or more musicians so they can play close to each other with separate microphones. Gobo panels are typically constructed to accommodate portability and storage, an advantage over more permanent acoustical room treatments.

Construction
A gobo typically consists of a wooden panel covered with foam, carpeting or other materials with sound damping properties. A gobo can rest directly on the floor or be raised on adjustable legs.

Manufacturers
Yukon Acoustics
Forward Acoustics
Primacousics
Taytrix
Clearsonic

References
World Wide Words – Michael Quinion writes about international English from a British viewpoint.
Merriam-Webster OnLine",Category:Use dmy dates from January 2015,1
151,152,Category:Units of sound,,Category:Acoustics,1
152,153,Resonator,"A resonator is a device or system that exhibits resonance or resonant behavior, that is, it naturally oscillates at some frequencies, called its resonant frequencies, with greater amplitude than at others. The oscillations in a resonator can be either electromagnetic or mechanical (including acoustic). Resonators are used to either generate waves of specific frequencies or to select specific frequencies from a signal. Musical instruments use acoustic resonators that produce sound waves of specific tones. Another example is quartz crystals used in electronic devices such as radio transmitters and quartz watches to produce oscillations of very precise frequency.
A cavity resonator is one in which waves exist in a hollow space inside the device. In electronics and radio, microwave cavities consisting of hollow metal boxes are used in microwave transmitters, receivers and test equipment to control frequency, in place of the tuned circuits which are used at lower frequencies. Acoustic cavity resonators, in which sound is produced by air vibrating in a cavity with one opening, are known as Helmholtz resonators.

Explanation
A physical system can have as many resonant frequencies as it has degrees of freedom; each degree of freedom can vibrate as a harmonic oscillator. Systems with one degree of freedom, such as a mass on a spring, pendulums, balance wheels, and LC tuned circuits have one resonant frequency. Systems with two degrees of freedom, such as coupled pendulums and resonant transformers can have two resonant frequencies. A crystal lattice composed of N atoms bound together can have N resonant frequencies. As the number of coupled harmonic oscillators grows, the time it takes to transfer energy from one to the next becomes significant. The vibrations in them begin to travel through the coupled harmonic oscillators in waves, from one oscillator to the next.
The term resonator is most often used for a homogeneous object in which vibrations travel as waves, at an approximately constant velocity, bouncing back and forth between the sides of the resonator. The material of the resonator, through which the waves flow, can be viewed as being made of millions of coupled moving parts (such as atoms). Therefore, they can have millions of resonant frequencies, although only a few may be used in practical resonators. The oppositely moving waves interfere with each other, and at its resonant frequencies reinforce each other to create a pattern of standing waves in the resonator. If the distance between the sides is 
  
    
      
        d
        
      
    
    {\displaystyle d\,}
  , the length of a round trip is 
  
    
      
        2
        d
        
      
    
    {\displaystyle 2d\,}
  . To cause resonance, the phase of a sinusoidal wave after a round trip must be equal to the initial phase so the waves self-reinforce. The condition for resonance in a resonator is that the round trip distance, 
  
    
      
        2
        d
        
      
    
    {\displaystyle 2d\,}
  , is equal to an integer number of wavelengths 
  
    
      
        ?
        
      
    
    {\displaystyle \lambda \,}
   of the wave:

  
    
      
        2
        d
        =
        N
        ?
        ,
        
        
        N
        ?
        {
        1
        ,
        2
        ,
        3
        ,
        …
        }
      
    
    {\displaystyle 2d=N\lambda ,\qquad \qquad N\in \{1,2,3,\dots \}}
  
If the velocity of a wave is 
  
    
      
        c
        
      
    
    {\displaystyle c\,}
  , the frequency is 
  
    
      
        f
        =
        c
        
          /
        
        ?
        
      
    
    {\displaystyle f=c/\lambda \,}
   so the resonant frequencies are:

  
    
      
        f
        =
        
          
            
              N
              c
            
            
              2
              d
            
          
        
        
        
        N
        ?
        {
        1
        ,
        2
        ,
        3
        ,
        …
        }
      
    
    {\displaystyle f={\frac {Nc}{2d}}\qquad \qquad N\in \{1,2,3,\dots \}}
  
So the resonant frequencies of resonators, called normal modes, are equally spaced multiples (harmonics) of a lowest frequency called the fundamental frequency. The above analysis assumes the medium inside the resonator is homogeneous, so the waves travel at a constant speed, and that the shape of the resonator is rectilinear. If the resonator is inhomogeneous or has a nonrectilinear shape, like a circular drumhead or a cylindrical microwave cavity, the resonant frequencies may not occur at equally spaced multiples of the fundamental frequency. They are then called overtones instead of harmonics. There may be several such series of resonant frequencies in a single resonator, corresponding to different modes of vibration.

Electromagnetic
Resonant circuits
An electrical circuit composed of discrete components can act as a resonator when both an inductor and capacitor are included. Oscillations are limited by the inclusion of resistance, either via a specific resistor component, or due to resistance of the inductor windings. Such resonant circuits are also called RLC circuits after the circuit symbols for the components.
A distributed-parameter resonator has capacitance, inductance, and resistance that cannot be isolated into separate lumped capacitors, inductors, or resistors. An example of this, much used in filtering, is the helical resonator.
A single layer coil (or solenoid) that is used as a secondary or tertiary winding in a Tesla coil or magnifying transmitter is also a distributed resonator.

Cavity resonators
A cavity resonator is a hollow closed conductor such as a metal box or a cavity within a metal block, containing electromagnetic waves (radio waves) reflecting back and forth between the cavity's walls. When a source of radio waves at one of the cavity's resonant frequencies is applied, the oppositely-moving waves form standing waves, and the cavity stores electromagnetic energy.
Since the cavity's lowest resonant frequency, the fundamental frequency, is that at which the width of the cavity is equal to a half-wavelength (?/2), cavity resonators are only used at microwave frequencies and above, where wavelengths are short enough that the cavity is conveniently small in size.
Due to the low resistance of their conductive walls, cavity resonators have very high Q factors; that is their bandwidth, the range of frequencies around the resonant frequency at which they will resonate, is very narrow. Thus they can act as narrow bandpass filters. Cavity resonators are widely used as the frequency determining element in microwave oscillators. Their resonant frequency can be tuned by moving one of the walls of the cavity in or out, changing its size.

Cavity magnetron
The cavity magnetron is a vacuum tube with a filament in the center of an evacuated, lobed, circular cavity resonator. A perpendicular magnetic field is imposed by a permanent magnet. The magnetic field causes the electrons, attracted to the (relatively) positive outer part of the chamber, to spiral outward in a circular path rather than moving directly to this anode. Spaced about the rim of the chamber are cylindrical cavities. The cavities are open along their length and so they connect with the common cavity space. As electrons sweep past these openings they induce a resonant high frequency radio field in the cavity, which in turn causes the electrons to bunch into groups. A portion of this field is extracted with a short antenna that is connected to a waveguide (a metal tube usually of rectangular cross section). The waveguide directs the extracted RF energy to the load, which may be a cooking chamber in a microwave oven or a high gain antenna in the case of radar.

Klystron
The klystron, tube waveguide, is a beam tube including at least two apertured cavity resonators. The beam of charged particles passes through the apertures of the resonators, often tunable wave reflection grids, in succession. A collector electrode is provided to intercept the beam after passing through the resonators. The first resonator causes bunching of the particles passing through it. The bunched particles travel in a field-free region where further bunching occurs, then the bunched particles enter the second resonator giving up their energy to excite it into oscillations. It is a particle accelerator that works in conjunction with a specifically tuned cavity by the configuration of the structures.
The reflex klystron is a klystron utilizing only a single apertured cavity resonator through which the beam of charged particles passes, first in one direction. A repeller electrode is provided to repel (or redirect) the beam after passage through the resonator back through the resonator in the other direction and in proper phase to reinforce the oscillations set up in the resonator.

Application in particle accelerators
On the beamline of an accelerator system, there are specific sections that are cavity resonators for RF. The (charged) particles that are to be accelerated pass through these cavities in such a way that the microwave electric field transfers energy to the particles, thus increasing their kinetic energy and thus accelerating them. Several large accelerator facilities employ superconducting niobium cavities for improved performance compared to metallic (copper) cavities.

Dielectric resonators
If a piece of material with large dielectric constant is surrounded by a material with much lower dielectric constant, then this abrupt change in dielectric constant can cause confinement of an electromagnetic wave, which leads to a resonator that acts similarly to a cavity resonator.

Transmission line resonators
Transmission lines are structures that allow broadband transmission of electromagnetic waves, e.g. at radio or microwave frequencies. Abrupt change of impedance (e.g. open or short) in a transmission line causes reflection of the transmitted signal. Two such reflectors on a transmission line evoke standing waves between them and thus act as a one-dimensional resonator, with the resonance frequencies determined by their distance and the effective dielectric constant of the transmission line.
Planar transmission line resonators are commonly employed for coplanar, stripline, and microstrip transmission lines. Such planar transmission line resonators can be very compact in size and are widely used elements in microwave circuitry. In cryogenic solid-state research, superconducting transmission line resonators contribute to solid-state spectroscopy  and quantum information science.

Optical cavities
In a laser, light is amplified in a cavity resonator that is usually composed of two or more mirrors. Thus an optical cavity, also known as a resonator, is a cavity with walls that reflect electromagnetic waves (i.e. light). This allows standing wave modes to exist with little loss.

Mechanical
Mechanical resonators are used in electronic circuits to generate signals of a precise frequency. For example, piezoelectric resonators, commonly made from quartz, are used as frequency references. Common designs consist of electrodes attached to a piece of quartz, in the shape of a rectangular plate for high frequency applications, or in the shape of a tuning fork for low frequency applications. The high dimensional stability and low temperature coefficient of quartz helps keeps resonant frequency constant. In addition, the quartz's piezoelectric property converts the mechanical vibrations into an oscillating voltage, which is picked up by the attached electrodes. These crystal oscillators are used in quartz clocks and watches, to create the clock signal that runs computers, and to stabilize the output signal from radio transmitters. Mechanical resonators can also be used to induce a standing wave in other media. For example, a multiple degree of freedom system can be created by imposing a base excitation on a cantilever beam. In this case the standing wave is imposed on the beam. This type of system can be used as a sensor to track changes in frequency or phase of the resonance of the fiber. One application is as a measurement device for dimensional metrology.

Acoustic
The most familiar examples of acoustic resonators are in musical instruments. Every musical instrument has resonators. Some generate the sound directly, such as the wooden bars in a xylophone, the head of a drum, the strings in stringed instruments, and the pipes in an organ. Some modify the sound by enhancing particular frequencies, such as the sound box of a guitar or violin. Organ pipes, the bodies of woodwinds, and the sound boxes of stringed instruments are examples of acoustic cavity resonators.

Automobiles
The exhaust pipes in automobile exhaust systems are designed as acoustic resonators that work with the muffler to reduce noise, by making sound waves ""cancel each other out"". The ""exhaust note"" is an important feature for some vehicle owners, so both the original manufacturers and the after-market suppliers use the resonator to enhance the sound. In ""tuned exhaust"" systems designed for performance, the resonance of the exhaust pipes can also be used to remove combustion products from the combustion chamber at a particular engine speed or range of speeds.

Percussion instruments
In many keyboard percussion instruments, below the centre of each note is a tube, which is an acoustic cavity resonator. The length of the tube varies according to the pitch of the note, with higher notes having shorter resonators. The tube is open at the top end and closed at the bottom end, creating a column of air that resonates when the note is struck. This adds depth and volume to the note. In string instruments, the body of the instrument is a resonator. The tremolo effect of a vibraphone is achieved via a mechanism that opens and shuts the resonators.

Stringed instruments
String instruments such as the bluegrass banjo may also have resonators. Many five-string banjos have removable resonators, so players can use the instrument with a resonator in bluegrass style, or without it in folk music style. The term resonator, used by itself, may also refer to the resonator guitar.
The modern ten-string guitar, invented by Narciso Yepes, adds four sympathetic string resonators to the traditional classical guitar. By tuning these resonators in a very specific way (C, B?, A?, G?) and making use of their strongest partials (corresponding to the octaves and fifths of the strings' fundamental tones), the bass strings of the guitar now resonate equally with any of the 12 tones of the chromatic octave. The guitar resonator is a device for driving guitar string harmonics by an electromagnetic field. This resonance effect is caused by a feedback loop and is applied to drive the fundamental tones, octaves, 5th, 3rd to an infinite sustain.

See also
Coupling coefficient of resonators
Crab cavity
Nuclear magnetic resonance
Optical ring resonators
Superconducting RF


== References and notes ==",Category:Acoustics,1
153,154,Acoustic ohm,"The acoustic ohm is a unit of measurement of acoustic impedance, which is the ratio of acoustic pressure to acoustic volume flow. In SI units, pressure is measured in pascals and flow in m3/s, so the acoustic ohm has units Pa•s/m3. In the cgs system, there is a cgs ohm with units dyne•s/cm5.
The acoustic ohm can be applied to fluid flow outside the domain of acoustics. For such applications a hydraulic ohm with an identical definition may be used. That is, the unit of the ratio of hydraulic pressure to hydraulic volume flow. Acoustic impedance is to be considered an instance of hydraulic impedance.

See also
Dyne

References
Definition
Second definition
Third Definition",Category:Standards and measurement stubs,1
154,155,Measurement microphone calibration,"In order to take a scientific measurement with a microphone, its precise sensitivity must be known (in volts per pascal). Since this may change over the lifetime of the device, it is necessary to regularly calibrate measurement microphones. This service is offered by some microphone manufacturers and by independent testing laboratories. Microphone calibration by certified laboratories should ultimately be traceable to primary standards a (National) Measurement Institute that is a signatory to International Laboratory Accreditation Cooperation. These could include the National Physical Laboratory in the UK, PTB in Germany, NIST in the USA and the National Measurement Institute, Australia, where the reciprocity calibration (see below) is the internationally recognised means of realising the primary standard. Laboratory standard microphones calibrated using this method are used in-turn to calibrate other microphones using comparison calibration techniques (‘secondary calibration’), referencing the output of the ‘test’ microphone against that of the reference laboratory standard microphone.
A microphone’s sensitivity varies with frequency (as well as with other factors such as environmental conditions) and is therefore normally recorded as several sensitivity values, each for a specific frequency band (see frequency spectrum). A microphone’s sensitivity can also depend on the nature of the sound field it is exposed to. For this reason, microphones are often calibrated in more than one sound field, for example a pressure field and a free field. Depending on their application, measurement microphones must be tested periodically (every year or several months, typically), and after any potentially damaging event, such as being dropped or exposed to sound levels beyond the device’s operational range.

Reciprocity calibration
Reciprocity calibration is currently the favoured primary standard for calibration of measurement microphones. The technique exploits the reciprocal nature of certain transduction mechanisms such as the electrostatic transducer principle used in condenser measurement microphones. In order to carry out a reciprocity calibration, three uncalibrated microphones 
  
    
      
        i
      
    
    {\displaystyle i}
  , 
  
    
      
        j
      
    
    {\displaystyle j}
   and 
  
    
      
        k
      
    
    {\displaystyle k}
   are used. Microphones 
  
    
      
        i
      
    
    {\displaystyle i}
   and 
  
    
      
        j
      
    
    {\displaystyle j}
   are placed facing each other with a well known acoustical coupler between their diaphragms, allowing the acoustic transfer impedance 
  
    
      
        
          Z
          
            a
            c
          
        
      
    
    {\displaystyle Z_{ac}}
   to be easily modelled. One of the microphones is then driven by a current 
  
    
      
        
          I
          
            i
          
        
      
    
    {\displaystyle I_{i}}
   to act as the source of sound and the other responds to the pressure generated in the coupler, producing an output voltage 
  
    
      
        
          U
          
            j
          
        
      
    
    {\displaystyle U_{j}}
   resulting in the electrical transfer impedance 
  
    
      
        
          Z
          
            i
            j
          
        
      
    
    {\displaystyle Z_{ij}}
  . Provided that the microphones are reciprocal in behaviour, which means the open circuit sensitivity in V/Pa as a receiver is the same as the sensitivity in m³/s/A as a transmitter, it can be shown that the product of the transmission factors 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M_{i}}
  , 
  
    
      
        
          M
          
            j
          
        
      
    
    {\displaystyle M_{j}}
  , and the acoustical transfer impedance equals the electrical transfer impedance.

  
    
      
        
          Z
          
            i
            j
          
        
        =
        
          
            
              U
              
                j
              
            
            
              I
              
                i
              
            
          
        
        =
        
          M
          
            i
          
        
        
        
          Z
          
            a
            c
          
        
        
        
          M
          
            j
          
        
      
    
    {\displaystyle Z_{ij}={\frac {U_{j}}{I_{i}}}=M_{i}\;Z_{ac}\;M_{j}}
  
Having determined the product of the transmission factors for one pair of microphones, the process is repeated with the other two possible pair-wise combinations 
  
    
      
        i
        k
      
    
    {\displaystyle ik}
   and 
  
    
      
        j
        k
      
    
    {\displaystyle jk}
  . The set of three measurements then allows the individual microphone transmission factor to be deduced by solving three simultaneous equations.

  
    
      
        
          M
          
            i
          
        
        =
        
          
            
              
                1
                
                  Z
                  
                    a
                    c
                  
                
              
            
            
              
                
                  
                    Z
                    
                      i
                      j
                    
                  
                  
                    Z
                    
                      i
                      k
                    
                  
                
                
                  Z
                  
                    j
                    k
                  
                
              
            
          
        
      
    
    {\displaystyle M_{i}={\sqrt {{\frac {1}{Z_{ac}}}{\frac {Z_{ij}Z_{ik}}{Z_{jk}}}}}}
  
The electrical transfer impedance is determined during the calibration procedure by measuring the current and voltage and the acoustic transfer impedance depends on the acoustical coupler.

  
    
      
        
          Z
          
            a
            c
          
        
        =
        
          
            p
            Q
          
        
        =
        
          
            F
            
              v
              
                S
                
                  2
                
              
            
          
        
      
    
    {\displaystyle Z_{ac}={\frac {p}{Q}}={\frac {F}{vS^{2}}}}
  
Commonly used acoustical couplers are free field, diffuse field and compression chamber. For free field conditions between the two microphones the sound pressure in the far field can be calculated and it follows

  
    
      
        
          Z
          
            a
            c
            ,
            f
            r
            e
            e
          
        
        =
        
          
            
              
                ?
                
                  0
                
              
              ?
            
            
              4
              ?
              r
            
          
        
        
          e
          
            ?
            
              
                m
                2
              
            
            r
          
        
        
          e
          
            ?
            j
            (
            k
            r
            ?
            
              
                ?
                2
              
            
            )
          
        
      
    
    {\displaystyle Z_{ac,free}={\frac {\rho _{0}\omega }{4\pi r}}e^{-{\frac {m}{2}}r}e^{-j(kr-{\frac {\pi }{2}})}}
  
where 
  
    
      
        r
      
    
    {\displaystyle r}
   is the distance between the microphones. For diffuse field conditions follows

  
    
      
        
          Z
          
            a
            c
            ,
            d
            i
            f
            f
          
        
        =
        
          
            
              
                ?
                
                  0
                
              
              ?
            
            
              ?
              A
            
          
        
        =
        
          
            
              
                ?
                
                  0
                
              
              ?
            
            
              4
              ?
              
                d
                
                  c
                
              
            
          
        
      
    
    {\displaystyle Z_{ac,diff}={\frac {\rho _{0}\omega }{\sqrt {\pi A}}}={\frac {\rho _{0}\omega }{4\pi d_{c}}}}
  
where 
  
    
      
        A
      
    
    {\displaystyle A}
   is the equivalent absorption area and 
  
    
      
        
          d
          
            c
          
        
      
    
    {\displaystyle d_{c}}
   is the critical distance for reverberation. For compression camber conditions follows

  
    
      
        
          Z
          
            a
            c
            ,
            c
            o
            m
            p
          
        
        =
        
          
            
              
                ?
                
                  0
                
              
              
                c
                
                  2
                
              
            
            
              j
              ?
              
                V
                
                  0
                
              
            
          
        
      
    
    {\displaystyle Z_{ac,comp}={\frac {\rho _{0}c^{2}}{j\omega V_{0}}}}
  
where 
  
    
      
        
          V
          
            0
          
        
      
    
    {\displaystyle V_{0}}
   is the air volume in the chamber.
The technique provides a measurement of the sensitivity of a microphone without the need for comparison with another previously calibrated microphone, and is instead traceable to reference electrical quantities such as volts and ohms, as well as length, mass and time. Although a given calibrated microphone will often have been calibrated by other (secondary) methods, all can be traced (through a process of dissemination) back to a microphone calibrated using the reciprocity method at a National Measurement Institute. Reciprocity calibration is a specialist process, and because it forms the basis of the primary standard for sound pressure, many national measurement institutes have invested significant research efforts to refine the method and develop calibration facilities. A system is also commercially available from Brüel & Kjær.
For airborne acoustics, the reciprocity technique is currently the most precise method available for microphone calibration (i.e. has the smallest uncertainty of measurement). Free field reciprocity calibration (to give the free-field response, as opposed to the pressure response of the microphone) follows the same principles and roughly the same method as pressure reciprocity calibration, but in practice is much more difficult to implement. As such it is more usual to perform reciprocity calibration in an acoustical coupler, and then apply a correction if the microphone is to be used in free-field conditions; such corrections are standardised for laboratory standard microphones (IEC/TS 61094-7) and are generally available from the manufacturers of most of the common microphone types.

Calibration using pistonphones and sound calibrators
A pistonphone is an acoustical calibrator (sound source) that uses a closed coupling volume to generate a precise sound pressure for the calibration of measurement microphones. The principle relies on a piston mechanically driven to move at a specified cyclic rate, pushing on a fixed volume of air to which the microphone under test is coupled. The air is assumed to be compressed adiabatically and the sound pressure level in the chamber can, potentially, be calculated from internal physical dimensions of the device and the adiabatic gas law, which requires that PV? is a constant, where P is the pressure in the chamber, V is the volume of the chamber, and ? is the ratio of the specific heat of air at constant pressure to its specific heat at constant volume. Pistonphones are highly dependent on ambient pressure (always requiring a correction to ambient pressure conditions) and are generally only made to reproduce low frequencies (for practical reasons), typically 250 Hz. However, pistonphones can be very precise, with good stability over time.
However, commercially available pistonphones are not calculable devices and must themselves be calibrated using a calibrated microphone if the results are to be traceable; though generally very stable over time, there will be small differences in the sound pressure level generated between different pistonphones. Since their output is also dependent on the volume of the chamber (coupling volume), differences in shape and load volume between different models of microphone will have an influence on the resulting SPL, requiring the pistonphone to be calibrated accordingly.
Sound calibrators are used in an identical way to pistonphones, providing a known sound pressure field in a cavity to which a test microphone is coupled. Sound calibrators are different from pistonphones in that they work electronically and use a low-impedance (electrodynamic) source to yield a high degree of volume independent operation. Furthermore, modern devices often use a feedback mechanism to monitor and adjust the sound pressure level in the cavity so that it is constant regardless of the cavity / microphone size. Sound calibrators normally generate a 1 kHz sine tone; 1 kHz is chosen since the A-weighted SPL is equal to the linear level at 1 kHz. Sound calibrators should also be calibrated regularly at a nationally accredited calibration laboratory to ensure traceability. Sound calibrators tend to be less precise than pistonphones, but are (nominally) independent of internal cavity volume and ambient pressure.


== References ==",Category:Articles lacking in-text citations from June 2015,1
155,156,Sweet spot (acoustics),,Category:Wikipedia articles needing clarification from March 2011,1
156,157,Shear waves,"Shear waves and compressional waves are the two main modes of propagation of acoustic energy in solids. With shear waves, also called transverse waves, the particles of the medium oscillate at a right angle to the direction of propagation. In compressional waves the oscillations occur in the longitudinal direction or the direction of wave propagation. The wave speeds of these different kinds of waves are governed by two different types of moduli. The compressional wave speed is related to the bulk elasticity modulus of the medium while the shear wave speed is related to the shear elasticity modulus. In soft biological tissues the bulk modulus varies no more than 10% while variation of the shear elasticity modulus may be several orders of magnitude depending on the structure and state of tissue.

Shear Waves in Elastography
The bulk modulus is defined by short range molecular interaction forces and depends mainly on molecular composition of tissue which is typically 75% water with little variation. In contrast to that, the shear modulus is defined by long range interactions and is highly sensitive to structural changes. The wide range of variability makes the shear elasticity modulus and, respectively, the shear wave speed, highly sensitive to physiological and pathological structural changes of tissue. For this reason, the use of shear waves in new diagnostic methods and devices has been extensively investigated over the last two decades. Numerous new methods were developed most notable of which are Shear Wave Elasticity Imaging (SWEI), Magnetic Resonance Elastography (MRE), Supersonic Shear Imaging (SSI), Shearwave Dispersion Ultrasound Vibrometry (SDUV), Harmonic Motion Imaging (HMI), Comb-push Ultrasound Shear Elastography (CUSE), and Spatially Modulated Ultrasound Radiation Force (SMURF).

Modes of Shear Wave Generation
In elastographic applications different means to generate and measure the propagation of shear waves in tissue are used. One of the early methods for generating shear waves in tissue was to use external mechanical actuators. Starting in the mid-1990s, another method was introduced to create shear waves which involved the use of focused ultrasound to produce acoustic radiation force.

Tissue Characterization
Various parameters of tissue characterizing its structure and state such as anisotropy, viscosity, and nonlinearity can be assessed using shear waves. In contrast to compressional waves, shear waves are polarized, which makes them sensitive to tissue anisotropy, an important structural anatomical characteristic that can have diagnostic value. By directing shear waves in different directions it is possible to characterize tissue anisotropy. The large frequency range of the shear wave that can be generated in tissue provides potential for using this high bandwidth for tissue viscoelastic properties estimation.

See also
Shear Wave Elasticity Imaging
Elastography


== References ==",Category:Wave mechanics,1
157,158,Acoustic plaster,,Category:Acoustics,1
158,159,Bruitparif,"Bruitparif is a non-profit environmental organization responsible for monitoring the environmental noise in the Paris agglomeration. It was founded in 2004.

Mission
Bruitparif is a non-profit organisation accredited by the région Île-de-France, and the Ministry of Environment to monitor the environmental noise in Île-de-France. Its missions meet a regulatory requirement and come in three functions:
Measurements and assessments
Support to public policies
Awareness actions
Bruitparif monitor continuously a network of 45 long-term measurement stations named ""Rumeur"". They contribute to the assessment of health risks and environmental impacts though campaigns of awareness in schools and general media.

Harmonica Index
In 2014, Bruitparif and Acoucité created a European wide noise index named ""Harmonica"".

The Rumeur network
""Rumeur"" is a network of 45 long-term measurement stations spread across the region Île-de-France.

The Sonopode
Bruitparif designed a specific autonomous equipment to house the measurement station. The sonopode contains a noise station, acoustic detection, wind station and a methanol fuel cell.

See also
Noise pollution, Environmental noise
Noise map
Sound level meter, Acoustical Engineering
Health effects from noise

References
External links
Official website of Bruitparif (in French)
Official website of the Harmonica index",Category:Noise pollution,1
159,160,Refraction (sound),"Refraction, in acoustics, comparable to the refraction of electromagnetic radiation, is the bending of sound propagation trajectories (rays) in inhomogeneous elastic media (gases, liquids, and solids) in which the wave velocity is a function of spatial coordinates. Bending of acoustic rays in layered inhomogeneous media occurs towards a layer with a smaller sound velocity. This effect is responsible for guided propagation of sound waves over long distances in the ocean and in the atmosphere.

See also
Atmospheric refraction
Deep sound channel
Sound speed gradient
Underwater acoustics

Further reading
P.M. Morse and K.U. Ingard, Theoretical Acoustics, Princeton University Press, 1986. ISBN 0-691-08425-4",Category:Acoustics,1
160,161,Scanning acoustic microscope,"A scanning acoustic microscope (SAM) is a device which uses focused sound to investigate, measure, or image an object (a process called scanning acoustic tomography). It is commonly used in failure analysis and non-destructive evaluation. It also has applications in biological and medical research. The semiconductor industry has found the SAM useful in detecting voids, cracks, and delaminations within microelectronic packages.

History
The first scanning acoustic microscope was developed in 1974 by R. A. Lemons and C. F. Quate at the Microwave Laboratory of Stanford University. Since then, many improvements to such systems have been made to enhance resolution and accuracy.

Principles of operation
Scanning acoustic microscopy works by directing focused sound from a transducer at a small point on a target object. Sound hitting the object is either scattered, absorbed, reflected (scattered at 180°) or transmitted (scattered at 0°). It is possible to detect the scattered pulses travelling in a particular direction. A detected pulse informs of the presence of a boundary or object. The `time of flight' of the pulse is defined as the time taken for it to be emitted by an acoustic source, scattered by an object and received by the detector, which is usually coincident with the source. The time of flight can be used to determine the distance of the inhomogeneity from the source given knowledge of the speed through the medium.
Based on the measurement, a value is assigned to the location investigated. The transducer (or object) is moved slightly and then insonified again. This process is repeated in a systematic pattern until the entire region of interest has been investigated. Often the values for each point are assembled into an image of the object. The contrast seen in the image is based either on the object's geometry or material composition. The resolution of the image is limited either by the physical scanning resolution or the width of the sound beam (which in turn is determined by the frequency of the sound).

Applications
Device testing
SAM is used for counterfeit detection, product reliability testing, process validation, vendor qualification, quality control, failure analysis, research, and development. Detecting discontinuities in silicon is just one of the ways scanning acoustic microscopy is being used for testing in the semiconductor market.

Medicine and biology
SAM can provide data on the elasticity of cells and tissues, which can give useful information on the physical forces holding structures in a particular shape and the mechanics of structures such as the cytoskeleton. These studies are particularly valuable in investigating processes such as cell motility.
Some work has also been performed to assess penetration depth of particles injected into skin using needle-free injection

See also
Acoustic microscopy


== References ==",Category:Articles needing expert attention with no reason or talk parameter,1
161,162,Background noise,"Background noise or ambient noise is any sound other than the sound being monitored (primary sound). Background noise is a form of noise pollution or interference. Background noise is an important concept in setting noise levels affect your background in formations. See noise criteria for cinema/home cinema applications.
Examples of background noises are environmental noises such as waves, traffic noise, alarms, people talking, bioacoustic noise from animals or birds and mechanical noise from devices such as refrigerators or air conditioning, power supplies or motors.
The prevention or reduction of background noise is important in the field of active noise control. It is an important consideration with the use of ultrasound (e.g. for medical diagnosis or imaging), sonar, and sound reproduction.

Other uses
In astronomy, background noise or cosmic background radiation is electromagnetic radiation from the sky with no discernible source.
In information architecture, irrelevant, duplicate or incorrect information may be called background noise.
In physics and telecommunication, background signal noise can be detrimental or in some cases beneficial. The study of avoiding, reducing or using signal noise is information theory.
In telephony, artificial comfort noise is used as a substitute for natural background noise, to fill in artificial silence created by discontinuous transmission systems using voice activity detection. Background noise can also affect concentration.

See also
4'33""
Ambient noise level
Electronic noise
The Hum

External links
How low noise levels are achieved in concert halls
Background noise in acoustics (demo)",Category:Noise,1
162,163,Acoustic music,"Acoustic music is music that solely or primarily uses instruments that produce sound through acoustic means, as opposed to electric or electronic means. While all music was once acoustic, the retronym ""acoustic music"" appeared after the advent of electric instruments, such as the electric guitar, electric violin, electric organ and synthesizer.
It has its origins in the folk music of the 1960s. Following the increasing popularity of the television show MTV Unplugged during the 1990s, acoustic (though in most cases still electrically amplified) performances by musicians (most notably grunge bands) who usually rely on electronic instruments became colloquially referred to as ""unplugged"" performances. The trend has also been dubbed as ""acoustic rock"" in some cases.
Writing for Splendid, music reviewer Craig Conley suggests, ""When music is labeled acoustic, unplugged, or unwired, the assumption seems to be that other types of music are cluttered by technology and overproduction and therefore aren't as pure"".

References
Bibliography
Randel, Don Michael (2003). The Harvard Dictionary of Music. Harvard University Press. ISBN 978-0-674-01163-2. 
Jermance, Frank (2003). Navigating the Music Industry: Current Issues & Business Models. Hal Leonard Corporation. ISBN 978-0-634-02652-2. 
Ilic, Dr Ljubica (2013). Music and the Modern Condition: Investigating the Boundaries. Ashgate Publishing, Ltd. ISBN 978-1-4094-9411-9. 
Safire, William (2007). ""On Language: Retronym"". New York Times Magazine (January 7): 18.

External links
International Acoustic Music Awards",Category:Articles with failed verification from February 2017,1
163,164,Lamb waves,"Lamb waves propagate in solid plates. They are elastic waves whose particle motion lies in the plane that contains the direction of wave propagation and the plate normal (the direction perpendicular to the plate). In 1917, the English mathematician Horace Lamb published his classic analysis and description of acoustic waves of this type. Their properties turned out to be quite complex. An infinite medium supports just two wave modes traveling at unique velocities; but plates support two infinite sets of Lamb wave modes, whose velocities depend on the relationship between wavelength and plate thickness.
Since the 1990s, the understanding and utilization of Lamb waves has advanced greatly, thanks to the rapid increase in the availability of computing power. Lamb's theoretical formulations have found substantial practical application, especially in the field of nondestructive testing.
The term Rayleigh–Lamb waves embraces the Rayleigh wave, a type of wave that propagates along a single surface. Both Rayleigh and Lamb waves are constrained by the elastic properties of the surface(s) that guide them.

Lamb's characteristic equations
In general, elastic waves in solid materials are guided by the boundaries of the media in which they propagate. An approach to guided wave propagation, widely used in physical acoustics, is to seek sinusoidal solutions to the wave equation for linear elastic waves subject to boundary conditions representing the structural geometry. This is a classic eigenvalue problem.
Waves in plates were among the first guided waves to be analyzed in this way. The analysis was developed and published in 1917 by Horace Lamb, a leader in the mathematical physics of his day.
Lamb's equations were derived by setting up formalism for a solid plate having infinite extent in the x and y directions, and thickness d in the z direction. Sinusoidal solutions to the wave equation were postulated, having x- and z-displacements of the form

  
    
      
        ?
        =
        
          A
          
            x
          
        
        
          f
          
            x
          
        
        (
        z
        )
        
          e
          
            i
            (
            ?
            t
            ?
            k
            x
            )
          
        
        
        
        (
        1
        )
      
    
    {\displaystyle \xi =A_{x}f_{x}(z)e^{i(\omega t-kx)}\quad \quad (1)}
  

  
    
      
        ?
        =
        
          A
          
            z
          
        
        
          f
          
            z
          
        
        (
        z
        )
        
          e
          
            i
            (
            ?
            t
            ?
            k
            x
            )
          
        
        
        
        (
        2
        )
      
    
    {\displaystyle \zeta =A_{z}f_{z}(z)e^{i(\omega t-kx)}\quad \quad (2)}
  
This form represents sinusoidal waves propagating in the x direction with wavelength 2?/k and frequency ?/2?. Displacement is a function of x, z, t only; there is no displacement in the y direction and no variation of any physical quantities in the y direction.
The physical boundary condition for the free surfaces of the plate is that the component of stress in the z direction at z = +/- d/2 is zero. Applying these two conditions to the above-formalized solutions to the wave equation, a pair of characteristic equations can be found. These are:

  
    
      
        
          
            
              tan
              ?
              (
              ?
              d
              
                /
              
              2
              )
            
            
              tan
              ?
              (
              ?
              d
              
                /
              
              2
              )
            
          
        
        =
        ?
        
          
            
              4
              ?
              ?
              
                k
                
                  2
                
              
            
            
              (
              
                k
                
                  2
                
              
              ?
              
                ?
                
                  2
                
              
              
                )
                
                  2
                
              
            
          
        
         
        
        
        
        
        (
        3
        )
      
    
    {\displaystyle {\frac {\tan(\beta d/2)}{\tan(\alpha d/2)}}=-{\frac {4\alpha \beta k^{2}}{(k^{2}-\beta ^{2})^{2}}}\ \quad \quad \quad \quad (3)}
  
and

  
    
      
        
          
            
              tan
              ?
              (
              ?
              d
              
                /
              
              2
              )
            
            
              tan
              ?
              (
              ?
              d
              
                /
              
              2
              )
            
          
        
        =
        ?
        
          
            
              (
              
                k
                
                  2
                
              
              ?
              
                ?
                
                  2
                
              
              
                )
                
                  2
                
              
            
            
              4
              ?
              ?
              
                k
                
                  2
                
              
            
          
        
         
        
        
        
        
        (
        4
        )
      
    
    {\displaystyle {\frac {\tan(\beta d/2)}{\tan(\alpha d/2)}}=-{\frac {(k^{2}-\beta ^{2})^{2}}{4\alpha \beta k^{2}}}\ \quad \quad \quad \quad (4)}
  
where

  
    
      
        
          ?
          
            2
          
        
        =
        
          
            
              ?
              
                2
              
            
            
              c
              
                l
              
              
                2
              
            
          
        
        ?
        
          k
          
            2
          
        
        
        
        
          and
        
        
        
        
          ?
          
            2
          
        
        =
        
          
            
              ?
              
                2
              
            
            
              c
              
                t
              
              
                2
              
            
          
        
        ?
        
          k
          
            2
          
        
        .
      
    
    {\displaystyle \alpha ^{2}={\frac {\omega ^{2}}{c_{l}^{2}}}-k^{2}\quad \quad {\text{and}}\quad \quad \beta ^{2}={\frac {\omega ^{2}}{c_{t}^{2}}}-k^{2}.}
  
Inherent in these equations is a relationship between the angular frequency ? and the wave number k. Numerical methods are used to find the phase velocity cp = f? = ?/k, and the group velocity cg = d?/dk, as functions of d/? or fd. cl and ct are the longitudinal wave and shear wave velocities respectively.
The solution of these equations also reveals the precise form of the particle motion, which equations (1) and (2) represent in generic form only. It is found that equation (3) gives rise to a family of waves whose motion is symmetrical about the midplane of the plate (the plane z = 0), while equation (4) gives rise to a family of waves whose motion is antisymmetric about the midplane. Figure 1 illustrates a member of each family.
Lamb’s characteristic equations were established for waves propagating in an infinite plate - a homogeneous, isotropic solid bounded by two parallel planes beyond which no wave energy can propagate. In formulating his problem, Lamb confined the components of particle motion to the direction of the plate normal (z-direction) and the direction of wave propagation (x-direction). By definition, Lamb waves have no particle motion in the y-direction. Motion in the y-direction in plates is found in the so-called SH or shear-horizontal wave modes. These have no motion in the x- or z-directions, and are thus complementary to the Lamb wave modes. These two are the only wave types which can propagate with straight, infinite wave fronts in a plate as defined above.

Velocity dispersion inherent in the characteristic equations
Lamb waves exhibit velocity dispersion; that is, their velocity of propagation c depends on the frequency (or wavelength), as well as on the elastic constants and density of the material. This phenomenon is central to the study and understanding of wave behavior in plates. Physically, the key parameter is the ratio of plate thickness d to wavelength 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
  . This ratio determines the effective stiffness of the plate and hence the velocity of the wave. In technological applications, a more practical parameter readily derived from this is used, namely the product of thickness and frequency:
The relationship between velocity and frequency (or wavelength) is inherent in the characteristic equations. In the case of the plate, these equations are not simple and their solution requires numerical methods. This was an intractable problem until the advent of the digital computer forty years after Lamb's original work. The publication of computer-generated ""dispersion curves"" by Viktorov in the former Soviet Union, Firestone followed by Worlton in the United States, and eventually many others brought Lamb wave theory into the realm of practical applicability. Experimental waveforms observed in plates can be understood by interpretation with reference to the dispersion curves.
Dispersion curves - graphs that show relationships between wave velocity, wavelength and frequency in dispersive systems - can be presented in various forms. The form that gives the greatest insight into the underlying physics has 
  
    
      
        ?
      
    
    {\displaystyle \omega }
   (angular frequency) on the y-axis and k (wave number) on the x-axis. The form used by Viktorov, that brought Lamb waves into practical use, has wave velocity on the y-axis and 
  
    
      
        d
        
          /
        
        ?
      
    
    {\displaystyle d/\lambda }
  , the thickness/wavelength ratio, on the x-axis. The most practical form of all, for which credit is due to J. and H. Krautkrämer as well as to Floyd Firestone (who, incidentally, coined the phrase ""Lamb waves"") has wave velocity on the y-axis and fd, the frequency-thickness product, on the x-axis.
Lamb's characteristic equations indicate the existence of two entire families of sinusoidal wave modes in infinite plates of width 
  
    
      
        d
      
    
    {\displaystyle d}
  . This stands in contrast with the situation in unbounded media where there are just two wave modes, the longitudinal wave and the transverse or shear wave. As in Rayleigh waves which propagate along single free surfaces, the particle motion in Lamb waves is elliptical with its x and z components depending on the depth within the plate. In one family of modes, the motion is symmetrical about the midthickness plane. In the other family it is antisymmetric. The phenomenon of velocity dispersion leads to a rich variety of experimentally observable waveforms when acoustic waves propagate in plates. It is the group velocity cg, not the above-mentioned phase velocity c or cp, that determines the modulations seen in the observed waveform. The appearance of the waveforms depends critically on the frequency range selected for observation. The flexural and extensional modes are relatively easy to recognize and this has been advocated as a technique of nondestructive testing.

The zero-order modes
The symmetrical and antisymmetric zero-order modes deserve special attention. These modes have ""nascent frequencies"" of zero. Thus they are the only modes that exist over the entire frequency spectrum from zero to indefinitely high frequencies. In the low frequency range (i.e. when the wavelength is greater than the plate thickness) these modes are often called the “extensional mode” and the “flexural mode"" respectively, terms that describe the nature of the motion and the elastic stiffnesses that govern the velocities of propagation. The elliptical particle motion is mainly in the plane of the plate for the symmetrical, extensional mode and perpendicular to the plane of the plate for the antisymmetric, flexural mode. These characteristics change at higher frequencies.
These two modes are the most important because (a) they exist at all frequencies and (b) in most practical situations they carry more energy than the higher-order modes.
The zero-order symmetrical mode (designated s0) travels at the ""plate velocity"" in the low-frequency regime where it is properly called the ""extensional mode"". In this regime the plate stretches in the direction of propagation and contracts correspondingly in the thickness direction. As the frequency increases and the wavelength becomes comparable with the plate thickness, curving of the plate starts to have a significant influence on its effective stiffness. The phase velocity drops smoothly while the group velocity drops somewhat precipitously towards a minimum. At higher frequencies yet, both the phase velocity and the group velocity converge towards the Rayleigh wave velocity - the phase velocity from above, and the group velocity from below.
In the low-frequency limit for the extensional mode, the z- and x-components of the surface displacement are in quadrature and the ratio of their amplitudes is given by:
where 
  
    
      
        ?
      
    
    {\displaystyle \nu }
   is Poisson's ratio.
The zero-order antisymmetric mode (designated a0) is highly dispersive in the low frequency regime where it is properly called the ""flexural mode"" or the ""bending mode"". For very low frequencies (very thin plates) the phase and group velocities are both proportional to the square root of the frequency; the group velocity is twice the phase velocity. This simple relationship is a consequence of the stiffness/thickness relationship for thin plates in bending. At higher frequencies where the wavelength is no longer much greater than the plate thickness, these relationships break down. The phase velocity rises less and less quickly and converges towards the Rayleigh wave velocity in the high frequency limit. The group velocity passes through a maximum, a little faster than the shear wave velocity, when the wavelength is approximately equal to the plate thickness. It then converges, from above, to the Rayleigh wave velocity in the high frequency limit.
In experiments that allow both extensional and flexural modes to be excited and detected, the extensional mode often appears as a higher-velocity, lower-amplitude precursor to the flexural mode. The flexural mode is the more easily excited of the two, and often carries most of the energy.

The higher-order modes
As the frequency is raised, the higher-order wave modes make their appearance in addition to the zero-order modes. Each higher-order mode is “born” at a resonant frequency of the plate, and exists only above that frequency. For example, in a ¾ inch (19mm) thick steel plate at a frequency of 200 kHz, the first four Lamb wave modes are present and at 300 kHz, the first six. The first few higher-order modes can be distinctly observed under favorable experimental conditions. Under less than favorable conditions they overlap and can not be distinguished.
The higher-order Lamb modes are characterized by nodal planes within the plate, parallel to the plate surfaces. Each of these modes exists only above a certain frequency which can be called its ""nascent frequency"". There is no upper frequency limit for any of the modes. The nascent frequencies can be pictured as the resonant frequencies for longitudinal or shear waves propagating perpendicular to the plane of the plate, i.e.

  
    
      
        d
        =
        
          
            
              n
              ?
            
            2
          
        
        
        
        
          or
        
        
        
        f
        =
        
          
            
              n
              c
            
            
              2
              d
            
          
        
      
    
    {\displaystyle d={\frac {n\lambda }{2}}\quad \quad {\text{or}}\quad \quad f={\frac {nc}{2d}}}
  
where n is any positive integer. Here c can be either the longitudinal wave velocity or the shear wave velocity, and for each resulting set of resonances the corresponding Lamb wave modes are alternately symmetrical and antisymmetric. The interplay of these two sets results in a pattern of nascent frequencies that at first glance seems irregular. For example, in a 3/4 inch (19mm) thick steel plate having longitudinal and shear velocities of 5890 m/s and 3260 m/s respectively, the nascent frequencies of the antisymmetric modes a1 and a2 are 86 kHz and 310 kHz respectively, while the nascent frequencies of the symmetric modes s1, s2 and s3 are 155 kHz, 172 kHz and 343 kHz respectively.
At its nascent frequency, each of these modes has an infinite phase velocity and a group velocity of zero. In the high frequency limit, the phase and group velocities of all these modes converge to the shear wave velocity. Because of these convergences, the Rayleigh and shear velocities (which are very close to one another) are of major importance in thick plates. Simply stated in terms of the material of greatest engineering significance, most of the high-frequency wave energy that propagates long distances in steel plates is traveling at 3000–3300 m/s.
Particle motion in the Lamb wave modes is in general elliptical, having components both perpendicular to and parallel to the plane of the plate. These components are in quadrature, i.e. they have a 90° phase difference. The relative magnitude of the components is a function of frequency. For certain frequencies-thickness products, the amplitude of one component passes through zero so that the motion is entirely perpendicular or parallel to the plane of the plate. For particles on the plate surface, these conditions occur when the Lamb wave phase velocity is ?2ct or for symmetric modes onlycl, respectively. These directionality considerations are important when considering the radiation of acoustic energy from plates into adjacent fluids.
The particle motion is also entirely perpendicular or entirely parallel to the plane of the plate, at a mode's nascent frequency. Close to the nascent frequencies of modes corresponding to longitudinal-wave resonances of the plate, their particle motion will be almost entirely perpendicular to the plane of the plate; and near the shear-wave resonances, parallel.
J. and H. Krautkrämer have pointed out that Lamb waves can be conceived as a system of longitudinal and shear waves propagating at suitable angles across and along the plate. These waves reflect and mode-convert and combine to produce a sustained, coherent wave pattern. For this coherent wave pattern to be formed, the plate thickness has to be just right relative to the angles of propagation and wavelengths of the underlying longitudinal and shear waves; this requirement leads to the velocity dispersion relationships.

Lamb waves with cylindrical symmetry; plate waves from point sources
While Lamb's analysis assumed a straight wavefront, it has been shown that the same characteristic equations apply to cylindrical plate waves (i.e. waves propagating outwards from a line source, the line lying perpendicular to the plate). The difference is that whereas the ""carrier"" for the straight wavefront is a sinusoid, the ""carrier"" for the axisymmetric wave is a Bessel function. The Bessel function takes care of the singularity at the source, then converges towards sinusoidal behavior at great distances.
These cylindrical waves are the eigenfunctions from which the plate's response to point disturbances can be composed. Thus a plate's response to a point disturbance can be expressed as a combination of Lamb waves, plus evanescent terms in the near field. The overall result can be loosely visualized as a pattern of circular wavefronts, like ripples from a stone dropped into a pond but changing more profoundly in form as they progress outwards. It should be noted always that Lamb wave theory relates only to motion in the (r,z) direction; transverse motion is a different topic.

Guided Lamb waves
This phrase is quite often encountered in non-destructive testing. ""Guided Lamb Waves"" can be defined as Lamb-like waves that are guided by the finite dimensions of real test objects. To add the prefix ""guided"" to the phrase ""Lamb wave"" is thus to recognize that Lamb's infinite plate is, in reality, nowhere to be found.
In reality we deal with finite plates, or plates wrapped into cylindrical pipes or vessels, or plates cut into thin strips, etc. Lamb wave theory often gives a very good account of much of the wave behavior of such structures. It will not give a perfect account, and that is why the phrase ""Guided Lamb Waves"" is more practically relevant than ""Lamb Waves"". One question is how the velocities and mode shapes of the Lamb-like waves will be influenced by the real geometry of the part. For example, the velocity of a Lamb-like wave in a thin cylinder will depend slightly on the radius of the cylinder and on whether the wave is traveling along the axis or round the circumference. Another question is what completely different acoustical behaviors and wave modes may be present in the real geometry of the part. For example, a cylindrical pipe has flexural modes associated with bodily movement of the whole pipe, quite different from the Lamb-like flexural mode of the pipe wall.

Lamb waves in ultrasonic testing
The purpose of ultrasonic testing is usually to find and characterize individual flaws in the object being tested. Such flaws are detected when they reflect or scatter the impinging wave and the reflected or scattered wave reaches the search unit with sufficient amplitude.
Traditionally, ultrasonic testing has been conducted with waves whose wavelength is very much shorter than the dimension of the part being inspected. In this high-frequency-regime, the ultrasonic inspector uses waves that approximate to the infinite-medium longitudinal and shear wave modes, zig-zagging to and from across the thickness of the plate. Although the lamb wave pioneers worked on nondestructive testing applications and drew attention to the theory, widespread use did not come about until the 1990s when computer programs for calculating dispersion curves and relating them to experimentally observable signals became much more widely available. These computational tools, along with a more widespread understanding of the nature of Lamb waves, made it possible to devise techniques for nondestructive testing using wavelengths that are comparable with or greater than the thickness of the plate. At these longer wavelengths the attenuation of the wave is less, so that flaws can be detected at greater distances.
A major challenge and skill in the use of Lamb waves for ultrasonic testing is the generation of specific modes at specific frequencies that will propagate well and give clean return ""echoes"". This requires careful control of the excitation. Techniques for this include the use of comb transducers, wedges, waves from liquid media and electro magnetic acoustic transducers (EMAT's).

Lamb waves in acousto-ultrasonic testing
Acousto-ultrasonic testing differs from ultrasonic testing in that it was conceived as a means of assessing damage (and other material attributes) distributed over substantial areas, rather than characterizing flaws individually. Lamb waves are well suited to this concept, because they irradiate the whole plate thickness and propagate substantial distances with consistent patterns of motion.

Lamb waves in acoustic emission testing
Acoustic emission uses much lower frequencies than traditional ultrasonic testing, and the sensor is typically expected to detect active flaws at distances up to several meters. A large fraction of the structures customarily testing with acoustic emission are fabricated from steel plate - tanks, pressure vessels, pipes and so on. Lamb wave theory is therefore the prime theory for explaining the signal forms and propagation velocities that are observed when conducting acoustic emission testing. Substantial improvements in the accuracy of AE source location (a major techniques of AE testing) can be achieved through good understanding and skillful utilization of the Lamb wave body of knowledge.

Ultrasonic and acoustic emission testing contrasted
An arbitrary mechanical excitation applied to a plate will generate a multiplicity of Lamb waves carrying energy across a range of frequencies. Such is the case for the acoustic emission wave. In acoustic emission testing, the challenge is to recognize the multiple Lamb wave components in the received waveform and to interpret them in terms of source motion. This contrasts with the situation in ultrasonic testing, where the first challenge is to generate a single, well-controlled Lamb wave mode at a single frequency. But even in ultrasonic testing, mode conversion takes place when the generated Lamb wave interacts with flaws, so the interpretation of reflected signals compounded from multiple modes becomes a means of flaw characterization.

See also
Acoustics
Acoustic wave
Wave equation
Waveguide
Waveguide (acoustics)
Waveguide (electromagnetism)

References
Rose, J.L.; ""Ultrasonic Waves in Solid Media,"" Cambridge University Press, 1999.

External links
Modes of Sound Wave Propagation at NDT Resource Center
Lamb wave in Nondestructive Testing Encyclopedia
Lamb Wave Analysis of Acousto-Ultrasonic Signals in Plate by Liu Zhenqing: an article which includes the complete Lamb wave equations.",Category:Wave mechanics,1
164,165,Fundamental frequency,"The fundamental frequency, often referred to simply as the fundamental, is defined as the lowest frequency of a periodic waveform. In music, the fundamental is the musical pitch of a note that is perceived as the lowest partial present. In terms of a superposition of sinusoids (e.g. Fourier series), the fundamental frequency is the lowest frequency sinusoidal in the sum. In some contexts, the fundamental is usually abbreviated as f0 (or FF), indicating the lowest frequency counting from zero. In other contexts, it is more common to abbreviate it as f1, the first harmonic. (The second harmonic is then f2 = 2?f1, etc. In this context, the zeroth harmonic would be 0 Hz.)

Since the fundamental is the lowest frequency and is also perceived as the loudest, the ear identifies it as the specific pitch of the musical tone [harmonic spectrum]....The individual partials are not heard separately but are blended together by the ear into a single tone.

Explanation
All sinusoidal and many non-sinusoidal waveforms are periodic, which is to say they repeat exactly over time. The period of a waveform is the 
  
    
      
        T
      
    
    {\displaystyle T}
   for which the following equation is true:

  
    
      
        x
        (
        t
        )
        =
        x
        (
        t
        +
        T
        )
        
           for all 
        
        t
        ?
        
          R
        
      
    
    {\displaystyle x(t)=x(t+T){\text{ for all }}t\in \mathbb {R} }
  
Where 
  
    
      
        x
        (
        t
        )
      
    
    {\displaystyle x(t)}
   is the value of the waveform at 
  
    
      
        t
      
    
    {\displaystyle t}
  . This means that this equation and a definition of the waveforms values over any interval of length 
  
    
      
        T
      
    
    {\displaystyle T}
   is all that is required to describe the waveform completely.
Every waveform may be described using any multiple of this period. There exists a smallest period over which the function may be described completely and this period is the fundamental period. The fundamental frequency is defined as its reciprocal:

  
    
      
        
          f
          
            0
          
        
        =
        
          
            1
            T
          
        
      
    
    {\displaystyle f_{0}={\frac {1}{T}}}
  

Since the period is measured in units of time, then the units for frequency are 1/time. When the time units are seconds, the frequency is in 
  
    
      
        
          s
          
            ?
            1
          
        
      
    
    {\displaystyle s^{-1}}
  , also known as Hertz.

For a tube of length 
  
    
      
        L
      
    
    {\displaystyle L}
   with one end closed and the other end open the wavelength of the fundamental harmonic is 
  
    
      
        4
        L
      
    
    {\displaystyle 4L}
  , as indicated by the first two animations. Hence,

  
    
      
        
          ?
          
            0
          
        
        =
        4
        L
        .
      
    
    {\displaystyle \lambda _{0}=4L.}
  
Therefore, using the relation

  
    
      
        
          ?
          
            0
          
        
        =
        
          
            v
            
              f
              
                0
              
            
          
        
      
    
    {\displaystyle \lambda _{0}={\frac {v}{f_{0}}}}
   ,
where 
  
    
      
        v
      
    
    {\displaystyle v}
   is the speed of the wave, we can find the fundamental frequency in terms of the speed of the wave and the length of the tube:

  
    
      
        
          f
          
            0
          
        
        =
        
          
            v
            
              4
              L
            
          
        
        .
      
    
    {\displaystyle f_{0}={\frac {v}{4L}}.}
  

If the ends of the same tube are now both closed or both opened as in the last two animations, the wavelength of the fundamental harmonic becomes 
  
    
      
        2
        L
      
    
    {\displaystyle 2L}
  . By the same method as above, the fundamental frequency is found to be

  
    
      
        
          f
          
            0
          
        
        =
        
          
            v
            
              2
              L
            
          
        
        .
      
    
    {\displaystyle f_{0}={\frac {v}{2L}}.}
  
At 20 °C (68 °F) the speed of sound in air is 343 m/s (1129 ft/s). This speed is temperature dependent and increases at a rate of 0.6 m/s for each degree Celsius increase in temperature (1.1 ft/s for every increase of 1 °F).
The velocity of a sound wave at different temperatures:-
v = 343.2 m/s at 20 °C
v = 331.3 m/s at 0 °C

In music
In music, the fundamental is the musical pitch of a note that is perceived as the lowest partial present. The fundamental may be created by vibration over the full length of a string or air column, or a higher harmonic chosen by the player. The fundamental is one of the harmonics. A harmonic is any member of the harmonic series, an ideal set of frequencies that are positive integer multiples of a common fundamental frequency. The reason a fundamental is also considered a harmonic is because it is 1 times itself. 
The fundamental is the frequency at which the entire wave vibrates. Overtones are other sinusoidal components present at frequencies above the fundamental. All of the frequency components that make up the total waveform, including the fundamental and the overtones, are called partials. Together they form the harmonic series. Overtones which are perfect integer multiples of the fundamental are called harmonics. When an overtone is near to being harmonic, but not exact, it is sometimes called a harmonic partial, although they are often referred to simply as harmonics. Sometimes overtones are created that are not anywhere near a harmonic, and are just called partials or inharmonic overtones.
The fundamental frequency is considered the first harmonic and the first partial. The numbering of the partials and harmonics is then usually the same; the second partial is the second harmonic, etc. But if there are inharmonic partials, the numbering no longer coincides. Overtones are numbered as they appear above the fundamental. So strictly speaking, the first overtone is the second partial (and usually the second harmonic). As this can result in confusion, only harmonics are usually referred to by their numbers, and overtones and partials are described by their relationships to those harmonics.

Mechanical systems
Consider a spring, fixed at one end and having a mass attached to the other; this would be a single degree of freedom (SDoF) oscillator. Once set into motion, it will oscillate at its natural frequency. For a single degree of freedom oscillator, a system in which the motion can be described by a single coordinate, the natural frequency depends on two system properties: mass and stiffness; (providing the system is undamped). The radian frequency, ?n, can be found using the following equation:

  
    
      
        
          ?
          
            
              n
            
          
          
            2
          
        
        =
        
          
            k
            m
          
        
        
      
    
    {\displaystyle \omega _{\mathrm {n} }^{2}={\frac {k}{m}}\,}
  
Where:k = stiffness of the springm = mass?n = radian frequency (radians per second)
From the radian frequency, the natural frequency, fn, can be found by simply dividing ?n by 2?. Without first finding the radian frequency, the natural frequency can be found directly using:

  
    
      
        
          f
          
            
              n
            
          
        
        =
        
          
            1
            
              2
              ?
            
          
        
        
          
            
              k
              m
            
          
        
        
      
    
    {\displaystyle f_{\mathrm {n} }={\frac {1}{2\pi }}{\sqrt {\frac {k}{m}}}\,}
  
Where:fn = natural frequency in hertz (cycles/second)k = stiffness of the spring (Newtons/meter or N/m)m = mass(kg)
while doing the modal analysis of structures and mechanical equipment, the frequency of 1st mode is called fundamental frequency.

See also
Greatest common divisor
Hertz
Missing fundamental
Natural frequency
Oscillation
Harmonic series (music)#Terminology
Pitch detection algorithm
Scale of harmonics


== References ==",Category:Articles with dead external links from December 2017,1
165,166,Waterfall plot,"A waterfall plot is a three-dimensional plot in which multiple curves of data, typically spectra, are displayed simultaneously. Typically the curves are staggered both across the screen and vertically, with 'nearer' curves masking the ones behind. The result is a series of ""mountain"" shapes that appear to be side by side. The waterfall plot is often used to show how two-dimensional information changes over time or some other variable such as rpm. The term ""waterfall plot"" is sometimes used as a synonym of ""spectrogram"" or ""Cumulative Spectral Decay"" (CSD) plot.

Uses
Waterfall plots are often used to show the:
results of spectral density estimation, showing the spectrum of the signal at successive intervals of time.
delayed response from a loudspeaker or listening room produced by impulse response testing or MLSSA.
spectra at different engine speeds when testing engines.

See also
Loudspeaker acoustics
Loudspeaker measurement

External links
Typical engine vibration waterfall
Waterfall FFT Matlab script",Category:All articles lacking sources,1
166,167,Digital room correction,"Digital room correction (or DRC) is a process in the field of acoustics where digital filters designed to ameliorate unfavorable effects of a room's acoustics are applied to the input of a sound reproduction system. Modern room correction systems produce substantial improvements in the time domain and frequency domain response of the sound reproduction system.

History
The use of analog filters, such as equalizers, to normalize the frequency response of a playback system has a long history; however, analog filters are very limited in their ability to correct the distortion found in many rooms. Although digital implementations of the equalizers have been available for some time, digital room correction is usually used to refer to the construction of filters which attempt to invert the impulse response of the room and playback system, at least in part. Digital correction systems are able to use acausal filters, and are able to operate with optimal time resolution, optimal frequency resolution, or any desired compromise along the Gabor limit. Digital room correction is a fairly new area of study which has only recently been made possible by the computational power of modern CPUs and DSPs.

Operation
The configuration of a digital room correction system begins with measuring the impulse response of the room at the listening location for each of the loudspeakers. Then, computer software is used to compute a FIR filter, which reverses the effects of the room and linear distortion in the loudspeakers. Finally, the calculated filter is loaded into a computer or other room correction device which applies the filter in real time. Because most room correction filters are acausal, there is some delay. Most DRC systems allow the operator to control the added delay through configurable parameters.

Challenges
DRC systems are not normally used to create a perfect inversion of the room's response because a perfect correction would only be valid at the location where it was measured: a few millimeters away the arrival times from various reflections will differ and the inversion will be imperfect. The imperfectly corrected signal may end up sounding worse than the uncorrected signal because the acausal filters used in digital room correction may cause pre-echo. Room correction filter calculation systems instead favor a robust approach, and employ sophisticated processing to attempt to produce an inverse filter which will work over a usably large volume, and which avoid producing bad-sounding artifacts outside of that volume, at the expense of peak accuracy at the measurement location.

See also
Deconvolution
Digital filter
Filter (signal processing)
Filter design
LARES
Stereophonic sound
Surround sound

References
Michael Gerzon's paper on Digital Room Equalization, on audiosignal.co.uk.

External links
Open Source Implementations
Python Open Room Correction (PORC)
DRC: Digital Room Correction

Free Room Correction Software
Free Room EQ plug-in for Foobar2000 audio player

Papers
On Room Correction and Equalization of Sound Systems, by Dr. Mathias Johansson, Dirac Research AB
Digital Room Equalization, by Michael Gerzon
Audio Equalization with Fixed-Pole Parallel Filters: An Efficient Alternative to Complex Smoothing, by Balazs Bank

Articles
Room Correction: A Primer, by Nyal Mellor of Acoustic Frontiers
Sound Correction in the Frequency and Time Domain, by Bernt Ronningsbak of Audiolense
The Three Acoustical Issues a Room Correction Product Can't Actually Correct, by Nyal Mellor of Acoustic Frontiers",Category:Acoustics,1
167,168,Whispering gallery,"A whispering gallery is usually a circular, hemispherical, elliptical or ellipsoidal enclosure, often beneath a dome or a vault, in which whispers can be heard clearly in other parts of the gallery. Such galleries can also be set up using two parabolic dishes. Sometimes the phenomenon is detected in caves.

Theory
A whispering gallery is most simply constructed in the form of a circular wall, and allows whispered communication from any part of the internal side of the circumference to any other part. The sound is carried by waves, known as whispering-gallery waves, that travel around the circumference clinging to the walls, an effect that was discovered in the whispering gallery of St Paul's Cathedral in London. The extent to which the sound travels at St Paul's can also be judged by clapping in the gallery, which produces four echoes. Other historical examples are the Gol Gumbaz mausoleum in Bijapur and the Echo Wall of the Temple of Heaven in Beijing. A hemispherical enclosure will also guide whispering gallery waves. The waves carry the words so that others will be able to hear them from the opposite side of the gallery.
The gallery may also be in the form of an ellipse or ellipsoid, with an accessible point at each focus. In this case, when a visitor stands at one focus and whispers, the line of sound emanating from this focus reflects directly to the focus at the other end of the gallery, where the whispers may be heard. In a similar way, two large concave parabolic dishes, serving as acoustic mirrors, may be erected facing each other in a room or outdoors to serve as a whispering gallery, a common feature of science museums. Egg-shaped galleries, such as the Golghar Granary at Bankipore, and irregularly shaped smooth-walled galleries in the form of caves, such as the Ear of Dionysius in Syracuse, also exist.

Examples
India
The Gol Gumbaz in Bijapur, India.
The Golghar Granary in Bankipore, India.
The Victoria Memorial in Kolkata.

United Kingdom
St Paul's Cathedral in London is the place where whispering-gallery waves were first discovered by Lord Rayleigh c.?1878.
The library of Dollar Academy in Scotland.
The entrance gallery of the Aston Webb Great Hall at the University of Birmingham.
Hamilton Mausoleum in Hamilton, South Lanarkshire, Scotland.

United States
The Battle House Hotel in Mobile, Alabama has a whispering arch in the front lobby
Cincinnati Museum Center at Union Terminal
Grand Central Terminal in New York City: the gallery in front of the Oyster Bar restaurant 
The Mapparium at The Mary Baker Eddy Library in Boston, Massachusetts allows visitors to enter the interior of a reflecting surface forming a nearly complete sphere
A whispering gallery can be found on the main floor of the Museum of Science and Industry (Chicago)
Statuary Hall in the United States Capitol.
Salt Lake Tabernacle in Salt Lake City, Utah
The rotunda at San Francisco City Hall.
Curved stone benches on either side of the Smith Memorial Arch in Fairmount Park, Philadelphia, Pennsylvania.
Centennial fountain in front of Green Library at Stanford University in California.
The rotunda of the Texas State Capitol and the Missouri State Capitol.
Gates Circle, Buffalo, New York.
The Whispering Arch in St. Louis Union Station
Charles Stover Bench, Central Park, New York, New York 
Waldo Hutchins Bench, Central Park, New York, New York

Other parts of the world
Barossa Reservoir, Williamstown, South Australia.
Cathedral of Brasília in Brazil.
Martello towers.
The Echo Wall in the Temple of Heaven in Beijing.
Masjed-e Imam in Esfah?n, Iran.
Basilica of St. John Lateran, Rome.
Santa Maria del Fiore, Florence Cathedral
The Church of the Holy Sepulcher, Jerusalem.
Leaning Tower of Nevyansk, Sverdlovsk Oblast.
Selimiye Mosque in Edirne, Turkey.
St. Peter's Basilica in the Vatican City.
Monument to the Negev Brigade in Beersheba, Israel.
The Salle de Cariatides in the Louvre, Paris, France.
The Treasury of Atreus, Greece
Secret's Chamber in El Escorial in Madrid, Spain.
The Whispering Gallery in the Alhambra in Granada, Spain.
Cleopatra's Bath in the Siwa Oasis, Egypt.
Ear of Dionysius cave in Syracuse, Sicily.
Meštrovi? Pavilion in Zagreb, Croatia
Royal BC Museum in Victoria, British Columbia, Canada
Sacred Heart Cathedral, Wellington, New Zealand
The Town Ball, Christchurch, New Zealand
The Whispering Arch (Flüsterbogen) in Görlitz, Germany.
Piazza Mercanti, Milan street

Other uses
The term 'whispering gallery' has been borrowed in the physical sciences to describe other forms of whispering-gallery waves such as light or matter waves.

See also
Acoustic mirror
Parabolic loudspeaker
Room acoustics
Whispering-gallery wave

References
External links
Ear of Dionysius: visiting information, videos and sounds of this cave.
Grand Central Station: visiting information, videos and sounds of the whispering gallery.
St Paul's Cathedral: visiting information, videos and sounds of the whispering gallery.",Category:Acoustics,1
168,169,Acoustic lobing,"Acoustic lobing refers to the radiation pattern of a combination of two or more loudspeaker drivers at a certain frequency, as seen looking at the speaker from its side. In most multi-way speakers, it is at the crossover frequency(ies) that the effects of lobing are of greatest concern, since this determines how well the speaker preserves the tonality of the original recorded content.
In practice, room-effects and interactions largely mean that the ideal loudspeaker (or combination thereof) is not practically possible. However a speaker that has the best dispersion at all frequencies of interest (especially the crossover frequency(ies)), will have the least colouration of sound - i.e., it will most faithfully reproduce the recorded material. Thus, an ideal speaker would have no lobes at all frequencies - in other words it will act as a point source radiating omnidirectionally at all frequencies. In practice all speakers will exhibit some amount of lobing at the crossover frequency(ies). The primary reasons for this are the physical distance between the drivers, and the drivers' effective diameters relative to the frequency of interest.
Lobing is measured as having a comb filtering response (i.e., areas of peaks and dips) as the listening position varies vertically‡ w.r.t. the nominal on-axis position. Since a true spherical wavefront cannot be achieved in practice, designers try to make the lobe as wide as possible at the crossover frequency(ies), such that at typical listening positions, the speaker appears omnidirectional.

Lobe formation
For the sake of simplicity, the following assumes two point sources separated by a distance d vertically‡, both radiating into half-space at a certain frequency f. Thus we can express lobing as a function of d and its relation to the wavelength ?. As d becomes significant (or larger) as compared to ?, the acoustic wavefront starts becoming narrower or more directive.
The following image shows a simplified representation of how two non-coincident drivers exhibit lobing (the difference between the lobing patterns is greatly exaggerated to demonstrate the effect):

The large black dot is the vertical listening position relative to the centre, at a certain fixed horizontal distance from the speaker. For wavelengths much greater than d, the wavefront is almost spherical (circular, when seen from the side) and the sound level is constant for a variety of such listening positions - the off-axis response of the speaker is almost omnidirectional. As the distance d approaches ?/4, the wavefront starts becoming narrower. At the listening position, the sound level is not the same as it would have been, had it been exactly midway between the drivers. The area where the sound level is constant for a given range of vertical positions (and fixed listening distance) is the lobe. Outside the lobe, the sound level is much less and this is what causes the speaker to have a change in tonality as one's listening height changes.
Note: For an individual driver this effect is known as directivity, and it observable in both vertical and horizontal planes, and d is now the driver's diameter relative to the wavelength, whereas, the lobing pattern due to two or more drivers is primarily an effect in the vertical plane, as a result of the distance between the two drivers.
The physical reason for a lobe to form is the fact that at any point that is at a position unequal from both drivers, at certain frequencies (i.e., wavelengths) and depending on d and relative difference between the distances to the listening position, the wavefronts from each driver will interfere constructively or destructively. This constructive or destructive interference happens due to the relative phases of the waves from each driver as they reach the listening position.
Thus, for any given frequency, there will be a minimum distance from the speaker below which there will be radical changes in sound level as the listening position is changed vertically. And this distance becomes larger as the distance between the drivers increases. Thus, the best compromise is obtained when, for practical listening distances, we can choose drivers large enough to cover as much of the audio band as possible but at the same time small enough so they can be as closely spaced as possible as to appear as a point source for any practical listening distance.
‡ - The article assumes a typical loudspeaker configuration where multiple drivers are arranged vertically. Therefore, the lobing phenomenon is observable in the vertical plane. For horizontally arranged drivers, the lobing phenomenon would be observable in the horizontal plane.


== References ==",Category:Loudspeakers,1
169,170,Resonance chamber,"A resonance chamber uses resonance to enhance the transfer of energy from a sound source (e.g. a vibrating string) to the air. The chamber has interior surfaces which reflect an acoustic wave. When a wave enters the chamber, it bounces back and forth within the chamber with low loss (See standing wave). As more wave energy enters the chamber, it combines with and reinforces the standing wave, increasing its intensity.
Since the resonance chamber is an enclosed space that has an opening where the sound wave enters and exits after bouncing off of the internal walls producing resonance, commonly acoustic resonance as in many musical instruments (see Sound board (music)), the material of the chamber, particularly that of the actual internal walls, its shape and the position of the opening, as well as the finish (porosity) of the internal walls are contributing factors for the final resulting sound produced.

See also
Cavity resonator (electrical version)
Resonance
Sounding box
Waveguide",Category:All articles lacking sources,1
170,171,Critical distance,"Critical distance is, in acoustics, the distance at which the sound pressure level of the direct sound D and the reverberant sound R are equal when dealing with a directional source. In other words, it is the point in space at which the combined amplitude of all the reflected echoes are the same as the amplitude of the sound coming directly from the source (D = R). This distance, called the critical distance 
  
    
      
        
          d
          
            c
          
        
      
    
    {\displaystyle d_{c}}
  , is dependent on the geometry and absorption of the space in which the sound waves propagate, as well as the dimensions and shape of the sound source.

In a reverberant space, the sound perceived by a listener is a combination of direct and reverberant sound. The ratio of direct sound is dependent on the distance between the source and the listener, and upon the reverberation time in [the room]. At a certain distance the two will be equal. This is called the ""critical distance.""

A reverberant room generates a short critical distance and an acoustically dead (anechoic) room generates a longer critical distance.

Calculation
The calculation of the critical distance for a diffuse approximation of the reverberant field:

  
    
      
        
          d
          
            c
          
        
        =
        
          
            1
            4
          
        
        
          
            
              
                ?
                A
              
              ?
            
          
        
        ?
        0.057
        
          
            
              
                ?
                V
              
              
                R
                
                  T
                  
                    60
                  
                
              
            
          
        
        ,
      
    
    {\displaystyle d_{c}={\frac {1}{4}}{\sqrt {\frac {\gamma A}{\pi }}}\approx 0.057{\sqrt {\frac {\gamma V}{RT_{60}}}},}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \gamma }
   is the degree of directivity of the source (
  
    
      
        ?
        =
        1
      
    
    {\displaystyle \gamma =1}
   for an omnidirectional source), 
  
    
      
        A
      
    
    {\displaystyle A}
   the equivalent absorption surface, 
  
    
      
        V
      
    
    {\displaystyle V}
   the room volume in m3 and 
  
    
      
        R
        
          T
          
            60
          
        
      
    
    {\displaystyle RT_{60}}
   the reverberation time of room in seconds. The latter approximation is using Sabine's reverberation formula 
  
    
      
        R
        
          T
          
            60
          
        
        =
        V
        
          /
        
        6
        A
      
    
    {\displaystyle RT_{60}=V/6A}
  .


== Sources ==",Category:Acoustics,1
171,172,Acoustic radiation force impulse imaging,"Acoustic Radiation Force Impulse (ARFI) Imaging is a type of ultrasound elastography used in medicine, particularly for the diagnosis and monitoring of cancers. ARFI imaging uses acoustic radiation force to generate images of the mechanical properties of soft tissue.

How it works
Acoustic radiation force is a phenomenon associated with the propagation of acoustic waves in attenuating media. Attenuation includes both scattering and absorption of the acoustic wave. Attenuation is a frequency dependent phenomenon, and in soft tissues it is dominated by absorption. With increasing acoustic frequencies, the tissue does not respond fast enough to the transitions between positive and negative pressures, thus its motion becomes out of phase with the acoustic wave, and energy is deposited into the tissue. This energy results in a momentum transfer in the direction of wave propagation and tissue heating. The momentum transfer generates a force that causes displacement of the tissue, and the time scale of this response is much slower than that of the ultrasonic wave propagation. This displacement (typically a few micrometers), which is typically detected by computing the correlation of ultrasonic RF signal, can be used to derive additional information about the tissue beyond what is normally provided in an ultrasonic image. The magnitude, location, spatial extent, and duration of acoustic radiation force can be controlled to interrogate the mechanical properties of the tissue.

Clinical applications
Clinical applications of ARFI imaging include:

Liver fibrosis quantification
The speed at which shear waves propagate in tissue can be used to quantify the shear modulus of the tissue. Acoustic radiation force-based imaging modalities, including SWEI, are being studied to non-invasively characterize the liver without the need for liver biopsy.
Yoneda et al. also recently compared ARFI shear wave imaging as implemented on the Siemens Acuson S2000 with transient elastography using the FibroScan system (EchoSens, Paris, France) in the context of evaluating patients with Non-alcoholic Fatty Liver Disease (NAFLD).

Breast mass imaging
Focused acoustic waves that propagate through tissue are absorbed and generate radiation force. Acoustic radiation force results from a transfer of momentum from an acoustic wave to tissue in the direction of wave propagation, arising from absorption and scattering of the wave. The tissue displacement response to radiation force excitation occurs on a slower time-scale than ultrasonic wave propagation, thus, conventional ultrasonic methods can be used to monitor the tissue response to radiation force. For breast imaging, experiments have been conducted to measure the potential for ARFI images to provide adjunctive information to matched ultrasonic images in order to enhance clinical confidence in diagnosis of breast masses. Patients scheduled for core biopsy of breast masses are recruited and matched B-mode and ARFI images of masses were obtained. The images were then correlated with biopsy result.

Colorectal tumor imaging/staging
Colorectal cancer is the second leading cause of cancer death in the United States (after lung cancer), and the third most common cancer overall in men (after prostate and lung cancer) and women (after breast and lung cancer). Once identified, the treatment approach for rectal cancer is dictated by the stage of the tumor (T-stage) and local lymph nodes (N-stage). There are currently no imaging methods that provide reliable N-staging accuracy of colorectal cancers. While endorectal ultrasound (EUS) is the standard for staging the degree of wall invasion of rectal cancers (T-stage), its accuracy is poor in the critical determination between uT2 and uT3, with 10-35% of uT2 tumors being overstaged. The consequences for the patient are dramatic. A stage uT3 rectal carcinoma is treated by radical surgical resection, and is typically treated with neoadjuvant chemotherapy and radiation. Neoadjuvant treatment damages adjacent healthy tissues, and can impair the healing process following surgery, leading to increased complications of infection, bleeding, decreased colonic mobility, and incontinence. Whereas stage uT2, N- and lower tumors generally receive a transanal local excision and are not treated neoadjuvantly, eliminating these additional risks to the patient.
The objective of ARFI Imaging development is to develop and evaluate the ability of ARFI imaging techniques to image layered tissue structures as found in the gastrointestinal tract, and to guide treatment decisions through improved preoperative tumor and lymph node staging.

Prostate imaging
Prostate cancer is the most common cancer and the second leading cause of cancer death in American men. According to the American Cancer Society (ACS), an estimated 218,890 new cases of prostate cancer will be found and 27,050 men will die of this disease in 2007. Early diagnosis is essential for better treatment and increasing survival rate. Although the current screening techniques, antigen (PSA) blood testing and digital rectal examination (DRE) are considered sensitive enough for cancer screening, follow-up biopsies have significant shortcomings. Without a good imaging technique to target the needle biopsy in the prostate gland, only about 25% of tests are positive for cancer in more than 1 million prostate biopsies performed each year; the false negative rates range from 25-45% based on the first time biopsy.
Although imaging techniques are essential for cancer diagnosis, imaging the structures and lesions within prostates has been a challenging task. The goal of ARFI imaging is to optimize and evaluate ARFI techniques for prostate imaging, which will potentially provide a more accurate, low cost and patient friendly imaging guidance for targeting prostate biopsies.

In vivo imaging of malignant tumors
ARFI can be used to image malignant pancreatic tumors.

References
External links
Tissue Strain Analytics from Siemens Ultrasound Website
Elastography advances feature prominently among ultrasound exhibits",Category:Medical ultrasonography,1
172,173,Acoustic attenuation,"Acoustic attenuation is a measure of the energy loss of sound propagation in media. Most media have viscosity, and are therefore not ideal media. When sound propagates in such media, there is always thermal consumption of energy caused by viscosity. For inhomogeneous media, besides media viscosity, acoustic scattering is another main reason for removal of acoustic energy. Acoustic attenuation in a lossy medium plays an important role in many scientific researches and engineering fields, such as medical ultrasonography, vibration and noise reduction.

Power-law frequency-dependent acoustic attenuation
Many experimental and field measurements show that the acoustic attenuation coefficient of a wide range of viscoelastic materials, such as soft tissue, polymers, soil and porous rock, can be expressed as the following power law with respect to frequency:

  
    
      
        P
        (
        x
        +
        ?
        x
        )
        =
        P
        (
        x
        )
        
          e
          
            ?
            ?
            (
            ?
            )
            ?
            x
          
        
        ,
        ?
        (
        ?
        )
        =
        
          ?
          
            0
          
        
        
          ?
          
            ?
          
        
      
    
    {\displaystyle P(x+\Delta x)=P(x)e^{-\alpha (\omega )\Delta x},\alpha (\omega )=\alpha _{0}\omega ^{\eta }}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \omega }
   is the angular frequency, P the pressure, 
  
    
      
        ?
        x
      
    
    {\displaystyle \Delta x}
   the wave propagation distance, 
  
    
      
        ?
        (
        ?
        )
      
    
    {\displaystyle \alpha (\omega )}
   the attenuation coefficient, 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \alpha _{0}}
   and frequency dependent exponent 
  
    
      
        ?
      
    
    {\displaystyle \eta }
   are real non-negative material parameters obtained by fitting experimental data and the value of 
  
    
      
        ?
      
    
    {\displaystyle \eta }
   ranges from 0 to 2. Acoustic attenuation in water, many metals and crystalline materials are frequency-squared dependent, namely 
  
    
      
        ?
        =
        2
      
    
    {\displaystyle \eta =2}
  . In contrast, it is widely noted that the frequency dependent exponent 
  
    
      
        ?
      
    
    {\displaystyle \eta }
   of viscoelastic materials is between 0 and 2. For example, the exponent 
  
    
      
        ?
      
    
    {\displaystyle \eta }
   of sediment, soil and rock is about 1, and the exponent 
  
    
      
        ?
      
    
    {\displaystyle \eta }
   of most soft tissues is between 1 and 2.
The classical dissipative acoustic wave propagation equations are confined to the frequency-independent and frequency-squared dependent attenuation, such as damped wave equation and approximate thermoviscous wave equation. In recent decades, increasing attention and efforts are focused on developing accurate models to describe general power law frequency-dependent acoustic attenuation. Most of these recent frequency-dependent models are established via the analysis of the complex wave number and are then extended to transient wave propagation. The multiple relaxation model considers the power law viscosity underlying different molecular relaxation processes. Szabo proposed a time convolution integral dissipative acoustic wave equation. On the other hand, acoustic wave equations based on fractional derivative viscoelastic models are applied to describe the power law frequency dependent acoustic attenuation. Chen and Holm proposed the positive fractional derivative modified Szabo's wave equation and the fractional Laplacian wave equation. See  for a recent paper which compares fractional wave equations which model power-law attenuation.
The phenomenon of attenuation obeying a frequency power-law may be described using a causal wave equation, derived from a fractional constitutive equation between stress and strain. This wave equation incorporates fractional time derivatives:

  
    
      
        
          
            ?
            
              2
            
          
          u
          ?
          
            
              
                1
                
                  c
                  
                    0
                  
                  
                    2
                  
                
              
            
          
          
            
              
                
                  ?
                  
                    2
                  
                
                u
              
              
                ?
                
                  t
                  
                    2
                  
                
              
            
          
          +
          
            ?
            
              ?
            
            
              ?
            
          
          
            
              
                
                  ?
                  
                    ?
                  
                
                
                  ?
                  
                    t
                    
                      ?
                    
                  
                
              
            
          
          
            ?
            
              2
            
          
          u
          ?
          
            
              
                
                  ?
                  
                    ?
                  
                  
                    ?
                  
                
                
                  c
                  
                    0
                  
                  
                    2
                  
                
              
            
          
          
            
              
                
                  
                    ?
                    
                      ?
                      +
                      2
                    
                  
                  u
                
                
                  ?
                  
                    t
                    
                      ?
                      +
                      2
                    
                  
                
              
            
          
          =
          0.
        
      
    
    {\displaystyle {\nabla ^{2}u-{\dfrac {1}{c_{0}^{2}}}{\frac {\partial ^{2}u}{\partial t^{2}}}+\tau _{\sigma }^{\alpha }{\dfrac {\partial ^{\alpha }}{\partial t^{\alpha }}}\nabla ^{2}u-{\dfrac {\tau _{\epsilon }^{\beta }}{c_{0}^{2}}}{\dfrac {\partial ^{\beta +2}u}{\partial t^{\beta +2}}}=0.}}
  
See also and the references therein.
Such fractional derivative models are linked to the commonly recognized hypothesis that multiple relaxation phenomena (see Nachman et al.) give rise to the attenuation measured in complex media. This link is further described in and in the survey paper.
For frequency band-limited waves, Ref. describes a model-based method to attain causal power-law attenuation using a set of discrete relaxation mechanisms within the Nachman et al. framework.

See also
Absorption (acoustics)
Fractional calculus


== References ==",Category:Acoustics,1
173,174,Wildlife Acoustics,"Wildlife Acoustics, Inc. is a privately held United States company based in Concord, Massachusetts. The company provides bioacoustics monitoring technology for scientists, researchers, and government agencies internationally. The company was founded by Ian Agranat in 2003. The company originally developed a product called the Song Sleuth, a device that would attempt to automatically identify birds from their songs in real time in the field. As this concept proved too expensive for the consumer market, the underlying technology was used to develop autonomous recorders and analysis software for the professional ecologist market.

Products
Song Meter SM2 digital field recorder
The company has sold autonomous and weatherproof battery operated recorders called Song Meters for monitoring birds, frogs and other wildlife since 2007. The device can record for up to 230 hours spread out over months and can be programmed to record at specific times of day. In addition to the standard omnidirectional microphone, the company also sells a directional microphone for recording night flight calls of migrating birds as well a hydrophone that can be attached for monitoring underwater.

Song Meter SM2BAT long-term passive bat recorder
In April 2009, Wildlife Acoustics released an ultrasonic capable version of the SM2BAT. The bat detector records in full spectrum at 192 kHz sample rate and the files can be converted to zero-cross files in post processing. The SM2BAT can record two channels simultaneously allowing a second microphone to be mounted up to 100m away from the recorder. With 4 SDHC cards the device can record about 240 nights of bat calls. In August 2009 a one channel 384 kHz sample rate version was added to allow recording of higher frequency bats common in some areas of Europe.

Song Meter SM2M submersible long-term passive recorder
Adding the ability to record underwater to their SM2 range, Wildlife Acoustics released the SM2M underwater passive acoustic monitor in May 2011. The unit has a depth rating of 150m and is designed for long-term autonomous recording. Recording life of up to 1500 hours is possible using 32 standard alkaline D cell batteries. The recorder can record sounds from 2 Hz to 48 kHz and stores recordings on up to four SDHC or SDXC cards.

Echo Meter EM3 handheld active bat detector
At the UK National Bat Conference, Wildlife Acoustics announced the Echo Meter handheld bat detector. The device will be available in December 2011. The detector is capable of monitoring for bats using heterodyne, frequency division or Real Time Expansion (RTE). RTE is Wildlife Acoustics proprietary technique for shifting bat sounds to the audible range while maintaining distinctive temporal and spectral characteristics of the call. In addition, the EM3 can record in full spectrum and/or zero-cross to an SD card while monitoring. A real time spectrogram shows calls as they are happening while monitoring and/or recording. The spectrogram can be scrolled back to analyze the spectrogram of previous bat calls. Calls can be played back using time expansion.

Song Scope analysis software
Song Scope is a software program that allows viewing of calls on a spectrogram and building ""recognizers"" to automatically search recordings for specific vocalizations.

Patents
Wildlife Acoustics has been awarded the following U.S. Patents:
U.S. Patent 7,454,334 ""Method and apparatus for automatically identifying animal species from their vocalizations""
U.S. Patent 7,782,195 ""Apparatus for low power autonomous data recording""

See also
Bat detector
Bat species identification

References
External links
Official website",Category:Acoustics,1
174,175,Acoustic camera,"An acoustic camera is an imaging device used to locate sound sources and to characterize them. It consists of a group of microphones — also called microphone array — that are simultaneously acquired to form a representation of the location of the sound sources.

Terminology
The term acoustic camera has first appeared at the end of the 19th century: A physiologist, J.R. Ewald, was investigating the function of the inner ear and introduced an analogy with the Chladni plates (a domain nowadays called Cymatics), a device enabling to visually see the modes of vibration of a plate. He called this device an acoustic camera. The term has then been widely used during the 20th century to designate various types of acoustic devices, such as underwater localization systems or active systems used in medicine. It designates nowadays any transducer array used to localize sound sources (the medium is usually the air), especially when coupled with an optical camera.

Technology
General principles
An acoustic camera generally consists of a microphone array and optionally an optical camera. The microphones – analog or digital – are acquired simultaneously or with known relative time delays to be able to use the phase difference between the signals. As the sound propagate in the medium (air, water...) at a finite known speed, a sound source is perceived by the microphones at different time instants and at different sound intensities that depend on both the sound source location and the microphone location. One popular method to obtain an acoustic image from the measurement of the microphone is to use beamforming: By delaying each microphone signal relatively and adding them, the signal coming from a specific direction 
  
    
      
        
          (
          
            
              ?
              
                0
              
            
            ,
            
              ?
              
                0
              
            
          
          )
        
      
    
    {\displaystyle \left(\theta _{0},\phi _{0}\right)}
   is amplified while signals coming from other directions are canceled. The power of this resulting signal is then calculated and reported on a power map at a pixel corresponding to the direction 
  
    
      
        
          (
          
            
              ?
              
                0
              
            
            ,
            
              ?
              
                0
              
            
          
          )
        
      
    
    {\displaystyle \left(\theta _{0},\phi _{0}\right)}
  . The process is iterated at each direction where the power needs to be computed.
While this method has many advantages – robustness, easy to understand, highly parallelizable because each direction can be computed independently, versatile (there exist many types of beamformers to include various types of hypothesis), relatively fast – it also has some drawbacks: the produced acoustic map has artifacts (also called side lobes or ghost sources) and it does not model correctly correlated sound sources. Various methods have been introduced to reduce the artifacts such as DAMAS or to take in account correlated sources such as CLEAN-SC, both at the price of a higher computational cost.
When the sound sources are near the acoustic camera, the relative intensity perceived by the different microphones as well as the waves not being any more seen as planar but spherical by the acoustic camera add new information compared to the case of sources being far from the camera. It enables to use more effective methods such as acoustic holography.

Reprojection
Results of far-field beamforming can be reprojected onto planar or non-planar surfaces.

Two-dimensional
Some acoustic cameras use two-dimensional acoustic mapping, which uses a unidirectional microphone array (e.g. a rectangle of microphones, all facing the same direction). Two-dimensional acoustic mapping works best when the surface to be examined is planar and the acoustic camera can be set up facing the surface perpendicularly. However, the surfaces of real-world objects are not often flat, and it is not always possible to optimally position the acoustic camera.
Additionally, the two-dimensional method of acoustic mapping introduces error into the calculations of the sound intensity at a point. Two-dimensional mapping approximates three-dimensional surfaces into a plane, allowing the distance between each microphone and the focus point to be calculated relatively easily. However, this approximation ignores the distance differences caused by surfaces having different depths at different points. In most applications of the acoustic camera, this error is small enough to be ignored; however, in confined spaces, the error becomes significant.

Three-dimensional
Three-dimensional acoustic cameras fix the errors of two-dimensional cameras by taking into account surface depths, and therefore correctly measuring the distances between the microphone and each spatial point. These cameras produce a more accurate picture, but require a 3-D model of the object or space being analyzed. Additionally, if the acoustic camera picks up sound from a point in space that is not part of the model, the sound may be mapped to a random space in the model, or the sound may not show up at all. 3-D acoustic cameras can also be used to analyze confined spaces, such as room interiors; however, in order to do this, a microphone array that is omnidirectional (e.g. a sphere of microphones, each facing a different direction) is required. This is in addition to the first requirement of having a 3-D model.

Applications
There are many applications of the acoustic camera, with most focusing on noise reduction. The camera is frequently applied to improve the noise emission of vehicles (such as cars, airplanes) and trains, structures — such as wind turbines.
Acoustic cameras are not only used to measure the exterior emission of products but also to improve the comfort inside cabins of cars, train or airplanes. Spherical acoustic camera are preferred in this type of application because the three-dimensional placement of the microphone allows to localize sound sources in all directions.
Troubleshooting of faults that occur in machines and mechanical parts can be accomplished with an acoustic camera. To find where the problem lies, the sound mapping of a properly functional machine can be compared to one of a dysfunctional machine.
A similar setup of the acoustic camera can be used to study the noise inside passenger carts during train operation. Alternatively, the camera can be set up outside, in an area near the train tracks, to observe the train as it goes by. This can give another perspective of the noise that might be heard inside the train. Additionally, an outside setup can be used to examine the squealing of train wheels caused by a curve in the tracks.

Challenges
Dynamic range
Low frequencies in the far-field
Computational power
The signal processing required by the acoustic camera is very intensive and needs powerful hardware and plenty of memory storage. Because of this, signal processing is frequently done after the recording of data, which can hinder or prevent the use of the camera in analyzing sounds that only occur occasionally or at varying locations. Cameras that do perform signal processing in real time tend to be large and expensive. Hardware and signal processing improvements can help to overcome these difficulties. Signal processing optimizations often focus on reduction of computational complexity, storage requirements, and memory bandwidth (rate of data consumption).

References
External links
http://blog.prosig.com/2010/03/15/comparison-between-sound-intensity-probes-and-acoustic-cameras/

Vendor links
http://www.acoustic-camera.com/
http://www.cae-systems.de/
http://www.distran.ch
http://www.microflown.com/products/solutions/near-field-acoustic-camera.html
http://www.nlacoustics.com
http://www.norsonic.com/en/products/acoustic_camera/
http://www.wired.com/autopia/2013/05/kaist-sound-camera/
http://www.iadeptreliability.com/acoustic-camera.html
https://www.plm.automation.siemens.com/en/products/lms/testing/sound-camera/",Category:All articles with empty sections,1
175,176,Auralization,"Auralization is a procedure designed to model and simulate the experience of acoustic phenomena rendered as a soundfield in a virtualized space. This is useful in configuring the soundscape of architectural structures, concert venues, public-spaces and in making coherent sound environments within virtual immersion systems.

History
The English term auralization was used for the first time by Kleiner et al. in an article in the journal of the AES en 1993.
The increase of computational power allowed the development of the first acoustic simulation software towards the end of the 1960s .

Principles
Auralizations are experienced through systems rendering virtual acoustic models made by convolving or mixing acoustic events recorded 'dry' (or in an anechoic chamber) projected within a virtual model of an acoustic space, the characteristics of which are determined by means of sampling it's impulse response (IR). Once this 
  
    
      
        h
        (
        t
        )
      
    
    {\displaystyle h(t)}
   has been determined the simulation of the resulting soundfield 
  
    
      
        s
        (
        t
        )
      
    
    {\displaystyle s(t)}
   in the target environment is obtained by convolution :

  
    
      
        r
        (
        t
        )
        =
        h
        (
        t
        )
        ?
        s
        (
        t
        )
      
    
    {\displaystyle r(t)=h(t)*s(t)}
  
The resulting sound 
  
    
      
        r
        (
        t
        )
      
    
    {\displaystyle r(t)}
   is heard as it would if emitted in that acoustic space.

Binaurality
For auralizations to be perceived as realistic the physical emulation of human hearing in terms of the position of the two ears on the head and orientation with respect to the sources of sound is critical. For IR data to be convolved convincingly the acoustic events are captured using a dummy head where two microphones are positioned on each side of the head to record an emulation of sound arriving at the locations of human ears, or using an ambisonics microphone array and mixed down for binaurality. Head-related transfer functions (HRTF) datasets can be used to simplify the process insofar as a monaural IR can be measured or simulated, then audio content is convolved with its target acoustic space. In rendering the experience the transfer function corresponding to the orientation of the head is applied to simulate the corresponding spatial emanation of sound .

See also
Convolution reverb
Reverberation


== Notes and references ==",Category:Acoustics,1
176,177,Aliquot stringing,"Aliquot stringing is the use of extra, un-struck strings in the piano for the purpose of enriching the tone. Aliquot systems use an additional (hence fourth) string in each note of the top three piano octaves. This string is slightly higher than the other three strings so that it is not struck by the hammer. Whenever the hammer strikes the three conventional strings, the aliquot string vibrates sympathetically. Aliquot stringing broadens the vibrational energy throughout the instrument, and creates an unusually complex and colorful tone.

Etymology
The word “aliquot” comes ultimately from the Latin word meaning “some, several”. In mathematics “aliquot” means “an exact part or divisor”, reflecting the fact that the length of an aliquot string forms an exact division of the length of longer strings with which it vibrates sympathetically.

History
Julius Blüthner invented the aliquot stringing system in 1873. The Blüthner aliquot system uses an additional (hence fourth) string in each note of the top three piano octaves. This string is slightly higher than the other three strings so that it is not struck by the hammer. Whenever the hammer strikes the three conventional strings, the aliquot string vibrates sympathetically. This string resonance also occurs when other notes are played that are harmonically related to the pitch of an aliquot string, though only when the related notes' dampers are raised. Many piano-makers enrich the tone of the piano through sympathetic vibration, but use a different method known as duplex scaling (see piano). Confusingly, the portions of the strings used in duplex scaling are sometimes called ""aliquot strings"", and the contact points used in duplex scales are called aliquots. Aliquot stringing and the duplex scale, even if they use ""aliquots"", are not equivalent.
Because they are tuned an octave above their constituent pitch, true aliquot strings transmit strong vibrations to the soundboard. Duplex scaling, which typically is tuned a double octave or more above the speaking length, does not. And because aliquot strings are so active, they require dampers or they would sustain uncontrollably and muddy the sound. Aliquot stringing broadens the vibrational energy throughout the instrument, and creates an unusually complex and colorful tone. This results from hammers striking their respective three strings, followed by an immediate transfer of energy into their sympathetic strings. The noted piano authority Larry Fine observes that the Blüthner tone is ""refined"" and ""delicate"", particularly ""at a low level of volume"". The Blüthner company, however, claims that the effect of aliquot stringing is equally apparent in loud playing.

Tunable aliquots
Theodore Steinway of Steinway & Sons patented tunable aliquots in 1872. Short lengths of non-speaking wire were bridged by an aliquot throughout much of the upper range of the piano, always in locations that caused them to vibrate in conformity with their respective overtones — typically in doubled octaves and twelfths. This enhanced the power and sustain of the instrument's treble. Because it was time-consuming to correctly position each aliquot, Steinway abandoned individual aliquots for continuous cast-metal bars, each comprising an entire section of duplex bridge points. The company trusted that with an accurately templated bridge and carefully located duplex bar, the same result would be achieved with less fuss.
Mason & Hamlin, established in Boston in 1854, continued to use individual aliquots. They felt that the tuning of these short lengths of string was more accurate with an aliquot than what could be attained with a duplex bar. With the fixed points of a duplex bar, small variations in casting or bridge-pin positioning are liable to produce imperfections in the duplex string lengths. Furthermore, since variations in humidity can cause duplex scales to move in pitch more rapidly than the speaking scale, readjustments of aliquot positioning is more feasible than duplex bar re-positioning.
A modern piano manufacture, Fazioli (Sacile, Italy), has blended Steinway's original ideas by creating a stainless-steel track, fixed to the cast-iron plate, on which individual aliquots slide.

Other musical instruments
Makers of other string instruments sometimes use aliquot parts of the scale length to enhance the timbre. Examples of such instruments include the Japanese koto and non-Western traditional instruments with sympathetic strings.

Notes
External links
A figure from the Blüthner company showing how their Patented Aliquot System is arranged
Blüthner — Photos and Aliquot-patent",Category:Acoustics,1
177,178,Adaptive feedback cancellation,"Adaptive feedback cancellation is a common method of cancelling audio feedback in a variety of electro-acoustic systems such as digital hearing aids. The time varying acoustic feedback leakage paths can only be eliminated with adaptive feedback cancellation. When an electro-acoustic system with an adaptive feedback canceller is presented with a correlated input signal, a recurrent distortion artifact, entrainment is generated. There is a difference between the system identification and feedback cancellation.
Adaptive feedback cancellation has its application in echo cancellation. The error between the desired and the actual output is taken and given as feedback to the adaptive processor for adjusting its coefficients to minimize the error.
In hearing aids, feedback arises when a part of the receiver (loudspeaker) signal is captured by the hearing aid microphone(s), gets amplified in the device and starts to loop around through the system. When feedback occurs, it results in a disturbingly loud tonal signal. Feedback is more likely to occur when the hearing aid volume is increased, when the hearing aid fitting is not in its proper position or when the hearing aid is brought close to a reflecting surface (e.g. when using a mobile phone). Adaptive feedback cancellation algorithms are techniques that estimate the transmission path between loudspeaker and microphone(s). This estimate is then used to implement a neutralizing electronic feedback path that suppresses the tonal feedback signal.

References
External links
Diagram",Category:Acoustics,1
178,179,Audio frequency,"An audio frequency (abbreviation: AF) or audible frequency is characterized as a periodic vibration whose frequency is audible to the average human. The SI unit of audio frequency is the hertz (Hz). It is the property of sound that most determines pitch.
The generally accepted standard range of audible frequencies is 20 to 20,000 Hz, although the range of frequencies individuals hear is greatly influenced by environmental factors. Frequencies below 20 Hz are generally felt rather than heard, assuming the amplitude of the vibration is great enough. Frequencies above 20,000 Hz can sometimes be sensed by young people. High frequencies are the first to be affected by hearing loss due to age and/or prolonged exposure to very loud noises.

Frequencies and descriptions
See also
Absolute threshold of hearing
Loudspeaker
Musical acoustics
Piano key frequencies
Scientific pitch notation
Whistle register


== References ==",Category:Audio engineering,1
179,180,Category:Sound measurements,,Category:Physical quantities,1
180,181,Sound amplification by stimulated emission of radiation,"Sound amplification by stimulated emission of radiation (SASER) refers to a device that emits acoustic radiation. It focuses sound waves in a way that they can serve as accurate and high-speed carriers of information in many kinds of applications—similar to uses of laser light.
Acoustic radiation (sound waves) can be emitted by using the process of sound amplification based on stimulated emission of phonons. Sound (or lattice vibration) can be described by a phonon just as light can be considered as photons, and therefore one can state that SASER is the acoustic analogue of the laser.
In a SASER device, a source (e.g., an electric field as a pump) produces sound waves (lattice vibrations, phonons) that travel through an active medium. In this active medium, a stimulated emission of phonons leads to amplification of the sound waves, resulting in a sound beam coming out of the device. The sound wave beams emitted from such devices are highly coherent.
The first successful SASERs were developed in 2009.

Terminology
Instead of a feedback-built wave of electromagnetic radiation (i.e., a laser beam), a SASER delivers a sound wave. SASER may also be referred to as phonon laser, acoustic laser or sound laser.

Uses and applications
SASERs could have wide applications. Apart from facilitating the investigation of terahertz-frequency ultrasound, the SASER is also likely to find uses in optoelectronics (electronic devices that detect and control light—as a method of transmitting a signal from an end to the other of, for instance, fiber optics), as a method of signal modulation and/or transmission.
Such devices could be high precision measurement instruments and they could lead to high energy focused sound.
Using SASERs to manipulate electrons inside semiconductors could theoretically result in terahertz-frequency computer processors, much faster than the current chips.

History
This concept can be more conceivable by imagining it in analogy to laser theory. Theodore Maiman operated the first functioning LASER on May 16, 1960 at Hughes Research Laboratories, Malibu, California, A device that operates according to the central idea of the ""sound amplification by stimulated emission of radiation"" theory is the thermoacoustic laser. This is a half-open pipe with a heat differential across a special porous material inserted in the pipe. Much like a light laser, a thermoacoustic SASER has a high-Q cavity and uses a gain medium to amplify coherent waves. For further explanation see thermoacoustic heat engine.
The possibility of phonon laser action had been proposed in a wide range of physical systems such as nanomechanics, semiconductors, nanomagnets and paramagnetic ions in a lattice.
Finding materials that stimulate emission was needed for the development of the SASER. The generation of coherent phonons in a double-barrier semiconductor heterostructure was first proposed around 1990. The transformation of the electric potential energy in a vibrational mode of the lattice is remarkably facilitated by the electronic confinement in a double-barrier structure. On this basis, physicists were searching for materials in which stimulated emission rather than spontaneous emission, is the dominant decay process. A device was first experimentally demonstrated in the Gigahertz range in 2009.
Announced in 2010, two independent groups came up with two different devices that produce coherent phonons at any frequency in the range megahertz to terahertz. One group from the University of Nottingham consisted of A.J. Kent and his colleagues R.P. Beardsley, A.V. Akimov, W. Maryam and M. Henini. The other group from the California Institute of Technology (Caltech) consisted of Ivan S. Grudinin, Hansuek Lee, O. Painter and Kerry J. Vahala from Caltech implemented a study on Phonon Laser Action in a tunable two-level system. The University of Nottingham device operates at about 440 GHz, while the Caltech device operates in the megahertz range. According to a member of the Nottingham group, the two approaches are complementary and it should be possible to use one device or the other to create coherent phonons at any frequency in the megahertz to terahertz range. A significant result rises from the operating frequency of these devices. The differences between the two devices suggest that SASERs could be made to operate over a wide range of frequencies.
Work on the SASER continues at the University of Nottingham, the Lashkarev Institute of Semiconductor Physics at the National Academy of Sciences of Ukraine, and Caltech.

Design
SASER's central idea is based on sound waves. The set-up needed for the implement of sound amplification by stimulated emission of radiation is similar to an oscillator. An oscillator can produce oscillations without any external feed-mechanism. An example is a common sound amplification system with a microphone, amplifier and speaker. When the microphone is in front of the speaker, we hear an annoying whistle. This whistle is generated without extra contribution from the sound source, and is self-reinforced and self-sufficient while the microphone is somewhere in front of the speaker. This phenomenon, known as the Larsen effect, is the result of a positive feedback.

In general, every oscillator consists of three main parts. These are the power source or pump, the amplifier and the positive feedback leading to the output. The corresponding parts in a SASER device are the excitation or pumping mechanism, the active (amplifying) medium, and the feedback leading to acoustic radiation. Pumping can be performed, for instance, with an alternating electric field or with some mechanical vibrations of resonators. The active medium should be a material in which sound amplification can be induced. An example of a feedback mechanism into the active medium is the existence of superlattice layers that reflect the phonons back and force them to bounce repeatedly to amplify sound.
Therefore, to proceed to an understanding of a SASER design we need to imagine it in analogy with a laser device. In a laser, the active medium is placed between two mirror surfaces (reflectors)of a Fabry–Pérot interferometer. A spontaneously emitted photon inside this interferometer can force excited atoms to decay a photon of same frequency, same momentum, same polarization and same phase. Because the momentum (as a vector) of the photon is nearly parallel to the axes of the mirrors, it is possible for photons to repeat multiple reflections and force more and more photons to follow them producing an avalanche effect. The number of photons of this coherent laser beam increases and competes the number of photons perished due to losses. The basic necessary condition for the generation of a laser radiation is the population inversion, which can be achieved either by exciting atoms and inducing percussion or by external radiation absorption. A SASER device mimics this procedure using a source-pump to induce a sound beam of phonons. This sound beam propagates not in an optical cavity, but in a different active medium. An example of an active medium is the superlattice. A superlattice can consist of multiple ultra-thin lattices of two different semiconductors. These two semiconductor materials have different band gaps, and form quantum wells—which are potential wells that confine particles to move in two dimensions instead of three, forcing them to occupy a planar region. In the superlattice, a new set of selection rules is composed that affects the flow-conditions of charges through the structure. When this set-up is excited by a source, the phonons start to multiply while they reflect on the lattice levels, until they escape from the lattice structure in a form of an ultrahigh frequency-phonon beam.

Namely, a concerted emission of phonons can lead to coherent sound and an example of concerted phonon emission is the emission coming from quantum wells. This stands in similar paths with the laser where a coherent light can build up by the concerted stimulated emission of light from a lot of atoms. A SASER device transforms the electric potential energy in a single vibrational mode of the lattice (phonon).
The medium where the amplification takes place, consists of stacks of thin layers of semiconductors that together form quantum wells. In these wells, electrons can be excited by parcels of ultrasound of millielectronvolts of energy. This amount of energy is equivalent to a frequency of 0.1 to 1 THz.

Physics
Just as light is a wave motion that is considered as composed of particles called photons, we can think of the normal modes of vibration in a solid as being particle-like. The quantum of lattice vibration is called phonon. In lattice dynamics we want to find the normal modes of vibration of a crystal. In other words, we need to calculate the energies (or frequencies ) of the phonons as a function of their wave vector's k . The relationship between frequency ? and wave vector k is called phonon dispersion.
Light and sound are similar in various ways. They both can be thought of in terms of waves, and they both come in quantum mechanical units. In the case of light we have photons while in sound we have phonons. Both sound and light can be produced as random collections of quanta (e.g. light emitted by a light bulb) or orderly waves that travel in a coordinated form (e.g. laser light). This parallelism implies that lasers should be as feasible with sound as they are with light. In the 21st century, it is easy to produce low frequency sound in the range that humans can hear (~20 kHz), in either a random or orderly form. However, at the terahertz frequencies in the regime of phonon laser applications, more difficulties arise. The problem stems from the fact that sound travels much slower than light. This means that the wavelength of sound is much shorter than light at a given frequency. Instead of resulting in orderly, coherent phonon laser structures that can produce terahertz sound, tend to emit phonons randomly. Researchers have overcome the problem of terahertz frequencies by following various approaches. Scientists in Caltech have overcome this problem by assembling a pair of microscopic cavities that only permit specific frequencies of phonons to be emitted. This system can be also tuned to emit phonons of different frequencies by changing the relative separation of the microcavities. On the other hand, the group from the University of Nottingham took a different approach. They have built their device out of electrons moving through a series of structures known as quantum wells. Briefly, as an electron hops from one quantum well to another neighbouring well it produces a phonon.
External energy pumping (e.g. a light beam or voltage) can help to the excitation of an electron. Relaxation of an electron from one of the upper states may occur by emission of either a photon or a phonon. This is determined by the density of states of phonons and photons. Density of states is the number of states per volume unit in an interval of energy (E, E+dE) that are available to be occupied by electrons. Both phonons and photons are bosons and thus, they obey Bose–Einstein statistics. This means that, since bosons with the same energy can occupy the same place in space, phonons and photons are force carrier particles and they have integer spins. There are more allowed states available for occupancy in a phonon field than in a photon field. Therefore, since the density of terminal states in the phonon field exceeds that in a photon field (by up to ~105), phonon emission is by far the more likely event. We could also imagine a concept where the excitation of an electron briefly leads to vibration of the lattice and thus to phonon generation. The vibration energy of the lattice can take discrete values for every excitation. Every one of this ""excitation packages"" is called phonon. An electron does not stay in an excited state for too long. It readily releases energy to return to its stable low energy state. The electrons release energy in any random direction and at any time (after their excitation). At some particular times, some electrons get excited while others lose energy in a way that the average energy of system is the lowest possible.

By pumping energy into the system we can achieve a population inversion. This means that there are more excited electrons than electrons in the lowest energy state in the system. As electron releases energy (e.g. phonon) it interacts with another excited electron to release its energy too. Therefore, we have a stimulated emission, which means a lot of energy (e.g., acoustic radiation, phonons) is released at the same time. One can mention that the stimulated emission is a procedure where we have a spontaneous and an induced emission at the same time. The induced emission comes from the pumping procedure and then is added to the spontaneous emission.
A SASER device should consist of a pumping mechanism and an active medium. The pumping procedure can be induced for example by an alternating electric field or with some mechanical vibrations of resonators, followed by acoustic amplification in the active medium. The fact that a SASER operates on principles remarkably similar to a laser, can lead to an easier way of understanding the relevant operation circumstances. Instead of a feedback-built potent wave of electromagnetic radiation, a SASER delivers a potent sound wave. Some methods for sound amplification of GHz-THz have been proposed so far. Some have been explored only theoretically and others have been explored in non-coherent experiments.
We note that acoustic waves of 100 GHz to 1 THz have wavelengths in nanometre range. Sound amplification according to the experiment taken in the University of Nottingham could be based on an induced cascade of electrons in semiconductor superlattices. The energy levels of electrons are confined in the superlattice layers. As the electrons hop between gallium arsenide quantum wells in the superlattice they emit phonons. Then, one phonon going in, produces two phonons coming out of the superlattice. This process can be stimulated by other phonons and then give rise to an acoustic amplification. Upon the addition of electrons, short-wavelength (in the terahertz range) phonons are produced. Since the electrons are confined to the quantum wells existing within the lattice, the transmission of their energy depends upon the phonons they generate. As these phonons strike other layers in the lattice, they excite electrons, which produce further phonons, which go on to excite more electrons, and so on. Eventually, a very narrow beam of high-frequency ultrasound exits the device. Semiconductor superlattices are used as acoustic mirrors. These superlattice structures must be in the right size obeying the theory of multilayer distributed Bragg reflector, in similarity with multilayer dielectric mirrors in optics.

Proposed schemes and devices
Basic understanding of the SASER development requires the evaluation of some proposed examples of SASER devices and SASER theoretical schemes.

Liquid with gas bubbles as the active medium
In this proposed theoretical scheme, the active medium is a liquid dielectric (e.g. ordinary distilled water) in which dispersed particles are uniformly distributed. Means of electrolysis cause gas bubbles that serve as the dispersed particles. A pumped wave excited in the active medium produces a periodic variation of the volumes of the dispersed particles (gas bubbles). Since, the initial spatial distribution of the particles is uniform, the waves emitted by the particles are added with different phases and give zero on the average. Nevertheless, if the active medium is located in a resonator, then a standing mode can be excited in it. Particles then bunch under the action of the acoustic radiation forces. In this case, the oscillations of the bubbles are self-synchronized and the useful mode amplifies.
The similarity of this with the Free-electron laser is useful to understand the theoretical concepts of the scheme. In a FEL, electrons move through magnetic periodic systems producing electromagnetic radiation. The radiation of the electrons is initially incoherent but then on account of the interaction with the useful electromagnetic wave they start to bunch according to phase and they become coherent. Thus, the electromagnetic field is amplified.

We note that, in the case of the piezoelectric radiators usually used to generate ultrasound, only the working surface radiates and therefore the working system is two-dimensional. On the other hand, a sound amplification by stimulated emission of radiation device is a three-dimensional system, since the entire volume of the active medium radiates.
The active medium gas-liquid mixture fills the resonator. The bubble density in the liquid is initially distributed uniformly in space. Since the wave propagates in such a medium, the pump wave leads to the appearance of an additional quasi-periodic wave. This wave is coupled with the spatial variation of the bubble density under the action of radiation pressure forces. Hence, the wave amplitude and the bubble density vary slowly compared with the period of the oscillations.
In the theoretical scheme where the usage of resonators is essential, the SASER radiation passes through the resonator walls, which are perpendicular to the direction of propagation of the pump wave. According to an example of an electrically pumped SASER, the active medium is confined between two planes, which are defined by the solid walls of the resonator. The radiation then, propagates along an axis parallel to the plane defined by the two resonator walls. The static electric field acting on the liquid with gas bubbles results in the deformation of dielectrics and therefore leads to a change in the volumes of the particles. We note that, the electromagnetic waves in the medium propagate with a velocity much greater than the velocity of sound in the same medium. This results to the assumption that the effective pump wave acting on the bubbles does not depend on the spatial coordinates. The pressure of a wave pump in the system leads to both the appearance of a backward wave and a dynamical instability of the system.
Mathematical analyses have shown that two types of losses must be overcome for generation of oscillations to start. Losses of the first type are associated with the dispersion of energy inside the active medium and second type losses are due to radiation losses at the ends of the resonator. These types of losses are inversely proportional to the amount of energy stored in the resonator. In general, the disparity of the radiators does not play a role in any attempt of a mathematical calculation of the starting conditions. Bubbles with resonance frequencies close to the pump frequency make the main contribution to the gain of the useful mode. In contrast, the determination of the starting pressure in ordinary lasers is independent from the number of radiators. The useful mode grows with the number of particles but sound absorption increases at the same time. Both these factors neutralize each other. Bubbles play the main role in the energy dispersion in a SASER.
A relevant suggested scheme of sound amplification by stimulated emission of radiation using gas bubbles as the active medium was introduced around 1995 The pumping is created by mechanical oscillations of a cylindrical resonator and the phase bunching of bubbles is realized by acoustic radiation forces. A notable fact is that gas bubbles can only oscillate under an external action, but not spontaneously. According to other proposed schemes, the electrostriction oscillations of the dispersed particle volumes in the cylindrical resonator are realized by an alternating electromagnetic field. However, a SASER scheme with an alternating electric field as the pump has a limitation. A very large amplitude of electric field (up to tens of kV/cm) is required to realize the amplification. Such values approach the electric puncture intensity of liquid dielectrics. Hence, a study proposes a SASER scheme without this limitation. The pumping is created by radial mechanical pulsations of a cylinder. This cylinder contains an active medium—a liquid dielectric with gas bubbles. The radiation emits through the faces of the cylinder.

Narrow-gap indirect semiconductors and excitons in coupled quantum wells
A proposal for the development of a phonon laser on resonant phonon transitions has been introduced from a group in Institute of Spectroscopy in Moscow, Russia.
Two schemes for steady stimulated phonon generation were mentioned. The first scheme exploits a narrow - gap indirect semiconductor or analogous indirect gap semiconductor heterostructure where the tuning into resonance of one-phonon transition of electron - hole recombination can be carried out by external pressure, magnetic or electric fields. The second scheme uses one-phonon transition between direct and indirect exciton levels in coupled quantum wells. We note that an exciton is an electrically neutral quasiparticle that describes an elementary excitation of condensed matter. It can transports energy without transporting net electric charge. The tuning into the resonance of this transition can be accomplished by engineering of dispersion of indirect exciton by external in-plane magnetic and normal electric fields.

The magnitude of phonon wave vector in the second proposed scheme, is supposed to be determined by magnitude of in-plane magnetic field. Therefore, such kind of SASER is tunable (i.e. its wavelength of operation can be altered in a controlled manner).
Common semiconductor lasers can be realised only in direct gap semiconductors. The reasoning behind that is that a pair of electron and hole near minima of their bands in an indirect gap semiconductor can recombine only with production of a phonon and a photon, due to energy and momentum conservation laws. This kind of process is weak in comparison with electron-hole recombination in a direct semiconductor. Consequently, the pumping of these transitions has to be very intense so as to obtain a steady laser generation. Hence, the lasing transition with production of only one particle – photon – must be resonant. This means that the lasing transition must be allowed by momentum and energy conservation laws to generate in a steady form. Photons have negligible wave vectors and therefore the band extremes have to be in the same position of the Brillouin zone . On the other hand, for devices such as SASERs, acoustic phonons have a considerable dispersion. According to dynamics, this leads to the statement that the levels on which the laser should operate, must be in the k-space relatively to each other. K-space refers to a space where things are in terms of momentum and frequency instead of position and time. The conversion between real space and k-space is a mathematical transformation called the Fourier transform and thus k-space can be also called Fourier space.
We note that, the difference in energy of the photon lasing levels has to be at least smaller than the Debye energy in the semiconductor. Here we can think of the Debye energy as the maximum energy associated with the vibrational modes of the lattice. Such levels can be formed by conduction and valence bands in narrow gap indirect semiconductors.

Narrow-gap indirect semiconductor as a SASER system
The energy gap in a semiconductor under the influence of pressure or magnetic field slightly varies and thus does not deserve any consideration. On the other hand, in narrow-gap semiconductors this variation of energy is considerable and therefore external pressure or magnetic field may serve the purpose of tuning into the resonance of one-phonon interband transition. Note that interband transition is the transition between the conduction and valence band. This scheme considers of indirect semiconductors instead of direct semiconductors. The reasoning behind that comes from the fact that, due to the k-selection rule in semiconductors, interband transitions with the production of only one phonon can be only those that produce an optical phonon. However, optical phonons have a short life-time (they split into two due to anharmonicity) and therefore they add some important complications. Here we can note that even in the case of multi-stage process of acoustic phonon creation it is possible to create SASER.

Examples of narrow-gap indirect semiconductors that can be used are chalcogenides PbTe, PbSe and PbS with energy gap 0.15 – 0.3 eV. For the same scheme, the usage of a semiconductor heterostructure (layers of different semiconductors) with narrow gap indirect in momentum space between valence and conduction bands may be more effective. This could be more promising since the spatial separation of the layers provides a possibility of tuning the interband transition into resonance by an external electric field. An essential statement here is that this proposed phonon laser can operate only if the temperature is much lower than the energy gap in the semiconductor.
During the analysis of this theoretical scheme several assumptions were introduced for simplicity reasons. The method of the pumping keeps the system electro-neutral and the dispersion laws of electrons and holes are assumed to be parabolic and isotropic. Also phonon dispersion law is required to be linear and isotropic too. Since the entire system is electro-neutral, the process of pumping creates electrons and holes with the same rate. A mathematical analysis, leads to an equation for the average number of electron-hole pairs per one phonon mode per unit volume. For a low loss limit, this equation gives us a pumping rate for the SASER that is rather moderate in comparison with usual phonon lasers on a p-n transition.

Tunable exciton transition in coupled quantum wells
It has been mentioned that a quantum well is basically a potential well that confines particles to move in two dimensions instead of three, forcing them to occupy a planar region. In coupled quantum wells there are two possible ways for electrons and holes to be bound into an exciton: indirect exciton and direct exciton. In indirect exciton, electrons and holes are in different quantum wells, in contrast with direct exciton where electrons and holes are located in the same well. In a case where the quantum wells are identical, both levels have a two-fold degeneracy. Direct exciton level is lower than the level of indirect exciton because of greater Coulomb interaction. Also, indirect exciton has an electric dipole momentum normal to coupled quantum well and thus a moving indirect exciton has an in-plane magnetic momentum perpendicular to its velocity. Any interactions of its electric dipole with normal electric field, lowers one of indirect exciton sub-levels and in sufficiently strong electric fields the moving indirect exciton becomes the ground excitonic level. Having in mind these procedures, one can select velocity to have an interaction between magnetic dipole and in-plane magnetic field. This displaces the minimum of the dispersion law away from the radiation zone. The importance of this, lies on the fact that electric and in-plane magnetic fields normal to coupled quantum wells, can control the dispersion of indirect exciton. Normal electric field is needed for tuning the transition: direct exciton --> indirect exciton + phonon into resonance and its magnitude can form a linear function with the magnitude of in-plane magnetic field. We note that the mathematical analysis of this scheme considers of longitudinal acoustic (LA) phonons instead of transverse acoustic (TA) phonons. This aims to more simple numerical estimations. Generally, the preference in transverse acoustic (TA) phonons is better because TA phonons have lower energy and the greater life-time than LA phonons. Therefore, their interaction with the electronic subsystem is weak. In addition, simpler quantitative evaluations require a pumping of direct exciton level performed by a laser irradiation.
A further analysis of the scheme can help us to establish differential equations for direct exciton, indirect exciton and phonon modes. The solution of these equations gives that separately phonon and indirect exciton modes have no definite phase and only the sum of their phases is defined. The aim here is to check if the operation of this scheme with a rather moderate pumping rate can hold against the fact that excitons in coupled quantum wells have low dimensionality in comparison to phonons. Hence, phonons not confined in the coupled quantum well are considered. An example is longitudinal optical (LO) phonons that are in AlGaAs/GaAs heterostructure and thus, phonons presented in this proposed system are three-dimensional. Differences in dimensionalities of phonons and excitons cause upper level to transform into many states of phonon field. By applying this information to specific equations we can conclude to a desired result. There is no additional requirement for the laser pumping despite the difference in phonon and exciton dimensionalities.

A tunable two-level system
Phonon laser action has been stated in a wide range of physical systems (e.g. semiconductors). A 2012 publication from the Department of Applied Physics in California Institute of Technology (Caltech), introduces a demonstration of a compound micro-cavity system, coupled with a radio-frequency mechanical mode, which operates in close analogy to a two level-laser system.
This compound micro-cavity system can also be called ""photonic molecule"". Hybridized orbitals of an electrical system are replaced by optical supermodes of this photonic molecule while the transitions between their corresponding energy levels are induced by a phonon field. For typical conditions of the optical micro-resonators, the photonic molecule behaves as a two-level laser system. Nevertheless, there is a bizarre inversion between the roles of the active medium and the cavity modes (laser field). The medium becomes purely optical and the laser field is provided by the material as a phonon mode.
An inversion produces gain, causing phonon laser action above a pump power threshold of around 7 ?W. The proposed device is characterized from a continuously tunable gain spectrum that selectively amplifies mechanical modes from radio frequency to microwave rates. Viewed as Brillouin process, the system accesses a regime in which the phonon plays the role of Stokes wave. Stokes wave refers to a non-linear and periodic surface wave on an inviscid fluid (ideal fluid assumed to have no viscosity) layer of constant mean depth. For this reason it should be also possible to controllably switch between phonon and phonon laser regimes.
Compound optical microcavity systems provide beneficial spectral controls. These controls impact both phonon laser action and cooling and define some finely spaced optical levels whose transition energies are proportional to phonon energies. These level spacings are continuously tunable by a significant adjustment of optical coupling. Therefore, amplification and cooling occur around a tunable line center, in contrast with some cavity optomechanical phenomena. The creation of these finely spaced levels does not require increasing the optical microcavity dimensions. Hence, these finely spaced levels do not affect the optomechanical interaction strength in a significant degree. The approach uses intermodal coupling, induced by radiation pressure and can also provide a spectrally selective mean to detect phonons. Moreover, some evidences of intermodal cooling are observed in this kind of experiments and thus, there is an interest in optomechanical cooling. Overall, an extension to multilevel systems using multiple coupled resonators is possible.

Two-level system
In a two level system, the particles have only two available energy levels, separated by some energy difference: ?? = E2 - E1 = hv where ? is the frequency of the associated electromagnetic wave of the photon emitted and h is the Planck constant. Also note: E2 > E1. These two levels are the excited (upper) and ground (lower) states. When a particle in the upper state interacts with a photon matching the energy separation of the levels, the particle may decay, emitting another photon with the same phase and frequency as the incident photon. Therefore, by pumping energy into the system we can have a stimulated emission of radiation—which means that the pump forces the system to release a big amount of energy at a specific time. A fu",Category:Transducers,1
181,182,ISO 31-7,,Category:All articles lacking sources,1
182,183,Onset (audio),"Onset refers to the beginning of a musical note or other sound. It is related to (but different from) the concept of a transient: all musical notes have an onset, but do not necessarily include an initial transient.
In phonetics the term is used differently - see syllable onset.

Onset detection
In signal processing, onset detection is an active research area. For example, the MIREX annual competition features an Audio Onset Detection contest.
Approaches to onset detection can operate in the time domain, frequency domain, phase domain, or complex domain, and include looking for:
Increases in spectral energy
Changes in spectral energy distribution (spectral flux) or phase
Changes in detected pitch - e.g. using a polyphonic pitch detection algorithm
Spectral patterns recognisable by machine learning techniques such as neural networks.
Simpler techniques such as detecting increases in time-domain amplitude can typically lead to an unsatisfactorily high amount of false positives or false negatives.
The aim is often to judge onsets similarly to how a human would: so psychoacoustically-motivated strategies may be employed. Sometimes the onset detector can be restricted to a particular domain (depending on intended application), for example being targeted at detecting percussive onsets. With a narrower focus, it can be more straightforward to obtain reliable detection.

See also
ADSR envelope
Prefix (acoustics)

References
Bello, J.P., Daudet, L., Abdallah, S., Duxbury, C., Davies, M., Sandler, M.B. (2005) ""A Tutorial on Onset Detection in Music Signals"", IEEE Transactions on Speech and Audio Processing 13(5), pp 1035–1047
Bello, J.P, Duxbury, C., Davies, M., Sandler, M. (2004). ""On the use of phase and energy for musical onset detection in the complex domain"". IEEE Signal Processing Letters
Collins, N. (2005) ""A Comparison of Sound Onset Detection Algorithms with Emphasis on Psychoacoustically Motivated Detection Functions"". Proceedings of AES118 Convention",Category:Synthesizers,1
183,184,Presence (sound recording),"In filmmaking and television production presence, better known as room tone, is the ""silence"" recorded at a location or space when no dialogue is spoken. This term is often confused with ambience.
Every location has a distinct presence created by the position of the microphone in relation to the space boundaries. A microphone placed in two different locations of the same room will produce two different presences. This is because of the unique spatial relationship between the microphone and boundaries such as walls, ceiling, floor and other objects in the room.
Presence is recorded during the production stage of filmmaking. It is used to help create the film sound track, where presence may be intercut with dialogue to smooth out any sound edit points. The sound track ""going dead"" would be perceived by the audience not as silence, but as a failure of the sound system.
For this reason presence is normally recorded—like dialogue—in mono, with the microphone in the same position and orientation as the original dialogue recording. In the sound edit, presence occupies the same track as the dialogue to which it applies.

See also
Ambience
Filmmaking


== References ==",Category:Acoustics,1
184,185,European Conference on Underwater Acoustics,"The European Conference on Underwater Acoustics (ECUA) was a conference on underwater acoustics that took place in Europe every two years, until 2012, when it was held in Edinburgh (Scotland), and organized by the Institute of Acoustics.  Previous editions took place in Delft (Netherlands, 2004), Algarve, (Portugal, 2006) and Paris (France, 2008) and Istanbul (Turkey, 2010).

References
External links
Ocean Acoustics Library",Category:Articles with topics of unclear notability from April 2017,1
185,186,Sustain,"In music, sustain is a parameter of musical sound over time. It denotes the period of time during which the sound remains before it becomes inaudible, or silent.
Sustain is the third of the four segments in an Attack Decay Sustain Release (ADSR). The sustain portion of the ADSR envelope begins when the attack and decay portions have run their course, and continues until the key is released. The sustain control is used to determine the level at which the envelope will remain. While the attack, decay, and release controls are rate or time controls, the sustain control is a level control.

Differences
Drum percussion instruments are usually the instruments with the shortest sustain; a drum beat begins to fade almost instantly. Instruments with the highest possible sustain often involve the passage of air, such as brass or the pipe organ, which have theoretically unlimited sustain. Feedback created using the Larsen effect also provides virtually unlimited sustain, but only to electronically amplified instruments. Resonating instruments such as the piano and guitar also have fairly long sustain for string instruments.

Factors affecting sustain
Many factors affect sustain for a given instrument. For example, sustain in guitars is determined by factors including body construction (hollow versus solid), body woods, neck woods, the placing of strings (through the body or atop the body), string gauges and pickup design.
In electric guitars, bass guitar and pianos, dedicated effects pedals prolong the sustain-phase of a tone.
With a synthesizer, the sustain pedal latches the envelopes of any currently playing or subsequently played notes at their sustain levels, even after the keys are lifted.

Increasing
There are musical devices used to increase sustain, known as ""Sustainers"", such as the patented Fernandes Sustainer guitar system and the EBow, which can be used to produce virtually infinite sustain.
There are many factors that affect the level of sustain in guitars. The most obvious ones are the quality of the production or manufacturing and the quality of the wood and other material. Larger string diameters have also been shown to increase sustain.

Notes


== See also ==",Category:Articles needing additional references from December 2009,1
186,187,Singing candle,"A singing candle is an electro-acoustic installation. The movements of a candle flame are transformed into sound by means of a light dependent resistor (LDR). The sound waves, in return, have an influence on the flame, through an amplifier and loudspeaker, forming a feedback loop.

See also
Rubens' tube


== References ==",Category:All articles lacking sources,1
187,188,Acoustic contrast factor,"The acoustic contrast factor is a number used to describe the relationship between the densities and the sound velocities (or, equivalently because of the form of the expression, the densities and compressibilities) of two media. It is most often used in the context of biomedical ultrasonic imaging techniques using acoustic contrast agents and in the field of ultrasonic manipulation of particles much smaller than the wavelength using ultrasonic standing waves. In the latter context, the acoustic contrast factor is the number which, depending on its sign, tells whether a given type of particle in a given medium will be attracted to the pressure nodes or anti-nodes.

Example - particle in a medium
Given the compressibilities 
  
    
      
        
          ?
          
            m
          
        
      
    
    {\displaystyle \beta _{m}}
   and 
  
    
      
        
          ?
          
            p
          
        
      
    
    {\displaystyle \beta _{p}}
   and densities 
  
    
      
        
          ?
          
            m
          
        
      
    
    {\displaystyle \rho _{m}}
   and 
  
    
      
        
          ?
          
            p
          
        
      
    
    {\displaystyle \rho _{p}}
   of the medium and particle, respectively, the acoustic contrast factor 
  
    
      
        ?
      
    
    {\displaystyle \phi }
   can be expressed as

  
    
      
        ?
        =
        
          
            
              5
              
                ?
                
                  p
                
              
              ?
              2
              
                ?
                
                  m
                
              
            
            
              2
              
                ?
                
                  p
                
              
              +
              
                ?
                
                  m
                
              
            
          
        
        ?
        
          
            
              ?
              
                p
              
            
            
              ?
              
                m
              
            
          
        
      
    
    {\displaystyle \phi ={\frac {5\rho _{p}-2\rho _{m}}{2\rho _{p}+\rho _{m}}}-{\frac {\beta _{p}}{\beta _{m}}}}
  
For a positive value of 
  
    
      
        ?
      
    
    {\displaystyle \phi }
  , the particles will be attracted to the pressure nodes, and vice versa.

See also
Acoustic impedance
Acoustic tweezers",Category:All articles lacking sources,1
188,189,Thermoacoustics,"Thermoacoustics is the interaction between temperature, density and pressure variations of acoustic waves. Thermoacoustic heat engines can readily be driven using solar energy or waste heat and they can be controlled using proportional control. They can use heat available at low temperatures which makes it ideal for heat recovery and low power applications. The components included in thermoacoustic engines are usually very simple compared to conventional engines. The device can easily be controlled and maintained.
Thermoacoustic effects can be observed when partly molten glass tubes are connected to glass vessels. Sometimes spontaneously a loud and monotone sound is produced. A similar effect is observed if a stainless steel tube is with one side at room temperature (293 K) and with the other side in contact with liquid helium at 4.2 K. In this case, spontaneous oscillations are observed which are named ""Taconis oscillations"". The mathematical foundation of thermoacoustics is by Nikolaus Rott. Later, the field was inspired by the work of John Wheatley and Swift and his co-workers. Technologically thermoacoustic devices have the advantage that they have no moving parts which makes them attractive for applications where reliability is of key importance.

Historical review of thermoacoustics
Thermoacoustic-induced oscillations have been observed for centuries. Glass blowers produced heat generated sound when blowing a hot bulb at the end of a cold narrow tube. This phenomenon also has been observed in cryogenic storage vessels, where oscillations are induced by the insertion of a hollow tube open at the bottom end in liquid helium, called Taconis oscillations, but the lack of heat removal system causes the temperature gradient to diminish and acoustic wave to weaken and then to stop completely. Byron Higgins made the first scientific observation of heat energy conversion into acoustical oscillations. He investigated the ""singing flame"" phenomena in a portion of a hydrogen flame in a tube with both ends open. Putnam and Dennis gave a survey of the related phenomena. Rijke introduced this phenomenon into a greater scale by using a heated wire screen to induce strong oscillations in a tube (the Rijke tube). Feldman mentioned in his related review that a convective air current through the pipe is the main inducer of this phenomenon. The oscillations are strongest when the screen is at one fourth of the tube length. Research performed by Sondhauss in 1850 is known to be the first to approximate the modern concept of thermoacoustic oscillation. Sondhauss experimentally investigated the oscillations related to glass blowers. Sondhauss observed that sound frequency and intensity depends on the length and volume of the bulb. Lord Rayleigh gave a qualitative explanation of the Sondhauss thermoacoustic oscillations phenomena, where he stated that producing any type of thermoacoustic oscillations needs to meet a criterion: ""If heat be given to the air at the moment of greatest condensation or taken from it at the moment of greatest rarefaction, the vibration is encouraged"". This shows that he related thermoacoustics to the interplay of density variations and heat injection. The formal theoretical study of thermoacoustics started by Kramers in 1949 when he generalized the Kirchhoff theory of the attenuation of sound waves at constant temperature to the case of attenuation in the presence of a temperature gradient. Rott made a breakthrough in the study and modeling of thermodynamic phenomena by developing a successful linear theory. After that, the acoustical part of thermoacoustics was linked in a broad thermodynamic framework by Swift.

Sound
Usually sound is understood in terms of pressure variations accompanied by an oscillating motion of a medium (gas, liquid or solid). In order to understand thermoacoustic machines, it is of importance to focus on the temperature-position variations rather than the usual pressure-velocity variations.
The sound intensity of ordinary speech is 65 dB. The pressure variations are about 0.05 Pa, the displacements 0.2 ?m, and the temperature variations about 40 ?K. So, the thermal effects of sound cannot be observed in daily life. However, at sound levels of 180 dB, which are normal in thermoacoustic systems, the pressure variations are 30 kPa, the displacements more than 10 cm, and the temperature variations 24 K.
The one-dimensional wave equation for sound reads

  
    
      
        
          c
          
            2
          
        
        
          
            
              
                ?
                
                  2
                
              
              v
            
            
              ?
              
                x
                
                  2
                
              
            
          
        
        ?
        
          
            
              
                ?
                
                  2
                
              
              v
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        =
        0
      
    
    {\displaystyle c^{2}{\frac {\partial ^{2}v}{\partial x^{2}}}-{\frac {\partial ^{2}v}{\partial t^{2}}}=0}
  
with t time, v the gas velocity, x the position, and c the sound velocity given by c²=?p?/??. For an ideal gas, c²=?RT?/M with M the molar mass. In these expressions, p?, T?, and ?? are the average pressure, temperature, and density respectively. In monochromatic plane waves, with angular frequency ? and with ?=kc, the solution is

  
    
      
        v
        =
        
          v
          
            A
            r
          
        
        cos
        ?
        (
        ?
        t
        ?
        k
        x
        )
        +
        
          v
          
            A
            l
          
        
        cos
        ?
        (
        ?
        t
        +
        k
        x
        )
        .
      
    
    {\displaystyle v=v_{Ar}\cos(\omega t-kx)+v_{Al}\cos(\omega t+kx).}
  
The pressure variations are given by

  
    
      
        ?
        p
        =
        c
        
          ?
          
            0
          
        
        [
        
          v
          
            A
            r
          
        
        cos
        ?
        (
        ?
        t
        ?
        k
        x
        )
        ?
        
          v
          
            A
            l
          
        
        cos
        ?
        (
        ?
        t
        +
        k
        x
        )
        ]
        .
      
    
    {\displaystyle \delta p=c\rho _{0}[v_{Ar}\cos(\omega t-kx)-v_{Al}\cos(\omega t+kx)].}
  
The deviation ?x of a gas-particle with equilibrium position x is given by

and the temperature variations are

The last two equations form a parametric representation of a tilted ellipse in the ?T – ?x plane with t as the parameter.

If 
  
    
      
        
          v
          
            A
            r
          
        
        =
        
          v
          
            A
            l
          
        
      
    
    {\displaystyle v_{Ar}=v_{Al}}
  , we are dealing with a pure standing wave. Figure 1a gives the dependence of the velocity and position amplitudes (red curve) and the pressure and temperature amplitudes (blue curve) for this case. The ellipse of the ?T – ?x plane is reduced to a straight line as shown in Fig. 1b. At the tube ends ?x =0, so the ?T – ?x plot is a vertical line here. In the middle of the tube the pressure and temperature variations are zero, so we have a horizontal line. It can be shown that the power, transported by sound, is given by

  
    
      
        P
        =
        
          
            
              ?
              
                p
                
                  0
                
              
            
            
              2
              c
            
          
        
        A
        (
        
          v
          
            A
            r
          
          
            2
          
        
        ?
        
          v
          
            A
            l
          
          
            2
          
        
        )
      
    
    {\displaystyle P={\frac {\gamma p_{0}}{2c}}A(v_{Ar}^{2}-v_{Al}^{2})}
  
where ? is the ratio of the gas specific heat at fixed pressure to the specific heat at fixed volume and A is the area of the cross section of the sound duct. Since in a standing wave, 
  
    
      
        
          v
          
            A
            r
          
        
        =
        
          v
          
            A
            l
          
        
      
    
    {\displaystyle v_{Ar}=v_{Al}}
  , the average energy transport is zero.
If 
  
    
      
        
          v
          
            A
            r
          
        
        =
        0
      
    
    {\displaystyle v_{Ar}=0}
   or 
  
    
      
        
          v
          
            A
            l
          
        
        =
        0
      
    
    {\displaystyle v_{Al}=0}
  , we have a pure traveling wave. In this case, Eqs.(1) and (2) represent circles in the ?T – ?x diagram as shown in Fig. 1c, which applies to a pure traveling wave to the right. The gas moves to the right with a high temperature and back with a low temperature, so there is a net transport of energy.

Penetration depths
The thermoacoustic effect inside the stack takes place mainly in the region that is close to the solid walls of the stack. The layers of gas too far away from the stack walls experience adiabatic oscillations in temperature that result in no heat transfer to or from the walls, which is undesirable.Therefore, an important characteristic for any thermoacoustic element is the value of the thermal and viscous penetration depths. The thermal penetration depth ?? is the thickness of the layer of the gas where heat can diffuse through during half a cycle of oscillations. Viscous penetration depth ?v is the thickness of the layer where viscosity effect is effective near the boundaries. In case of sound, the characteristic length for thermal interaction is given by the thermal penetration depth ??

  
    
      
        
          ?
          
            ?
          
          
            2
          
        
        =
        
          
            
              2
              ?
              
                V
                
                  m
                
              
            
            
              ?
              
                C
                
                  p
                
              
            
          
        
        .
      
    
    {\displaystyle \delta _{\kappa }^{2}={\frac {2\kappa V_{m}}{\omega C_{p}}}.}
  
Here ? is the thermal conductivity, Vm the molar volume, and Cp the molar heat capacity at constant pressure. Viscous effects are determined by the viscous penetration depth ??

  
    
      
        
          ?
          
            ?
          
          
            2
          
        
        =
        
          
            
              2
              ?
            
            
              ?
              ?
            
          
        
      
    
    {\displaystyle \delta _{\nu }^{2}={\frac {2\eta }{\omega \rho }}}
  
with ? the gas viscosity and ? its density. The Prandtl number of the gas is defined as

  
    
      
        
          P
          
            r
          
        
        =
        
          
            
              ?
              
                C
                
                  p
                
              
            
            
              M
              ?
            
          
        
        .
      
    
    {\displaystyle P_{r}={\frac {\eta C_{p}}{M\kappa }}.}
  
The two penetration depths are related as follows

  
    
      
        
          ?
          
            ?
          
          
            2
          
        
        =
        
          P
          
            r
          
        
        
          ?
          
            ?
          
          
            2
          
        
        .
      
    
    {\displaystyle \delta _{\nu }^{2}=P_{r}\delta _{\kappa }^{2}.}
  
For many working fluids, like air and helium, Pr is of order 1, so the two penetration depths are about equal. For helium at normal temperature and pressure, Pr?0.66. For typical sound frequencies the thermal penetration depth is ca. 0.1 mm. That means that the thermal interaction between the gas and a solid surface is limited to a very thin layer near the surface. The effect of thermoacoustic devices is increased by putting a large number of plates (with a plate distance of a few times the thermal penetration depth) in the sound field forming a stack. Stacks play a central role in so-called standing-wave thermoacoustic devices.

Thermoacoustic systems
Acoustic oscillations in a media are a set of time depending properties, which may transfer energy along its path. Along the path of an acoustic wave, pressure and density are not the only time dependent property, but also entropy and temperature. Temperature changes along the wave can be invested to play the intended role in the thermoacoustic effect. The interplay of heat and sound is applicable in both conversion ways. The effect can be used to produce acoustic oscillations by supplying heat to the hot side of a stack, and sound oscillations can be used to induce a refrigeration effect by supplying a pressure wave inside a resonator where a stack is located. In a thermoacoustic prime mover, a high temperature gradient along a tube where a gas media is contained induces density variations. Such variations in a constant volume of matter force changes in pressure. The cycle of thermoacoustic oscillation is a combination of heat transfer and pressure changes in a sinusoidal pattern. Self-induced oscillations can be encouraged, according to Lord Raleigh, by the appropriate phasing of heat transfer and pressure changes.

Standing-wave systems
The thermoacoustic engine (TAE) is a device that converts heat energy into work in the form of acoustic energy. A thermoacoustic engine operates using the effects that arise from the resonance of a standing-wave in a gas. A standing-wave thermoacoustic engine typically has a thermoacoustic element called the ""stack"". A stack is a solid component with pores that allow the operating gas fluid to oscillate while in contact with the solid walls. The oscillation of the gas is accompanied with the change of its temperature. Due to the introduction of solid walls into the oscillating gas, the plate modifies the original, unperturbed temperature oscillations in both magnitude and phase for the gas about a thermal penetration depth ?=?(2k/?) away from the plate, where k is the thermal diffusivity of the gas and ?=2?f is the angular frequency of the wave. Thermal penetration depth is defined as the distance that heat can diffuse though the gas during a time 1/?. In air oscillating at 1000 Hz, the thermal penetration depth is about 0.1 mm. Standing-wave TAE must be supplied with the necessary heat to maintain the temperature gradient on the stack. This is done by two heat exchangers on both sides of the stack.

If we put a thin horizontal plate in the sound field, the thermal interaction between the oscillating gas and the plate leads to thermoacoustic effects. If the thermal conductivity of the plate material would be zero, the temperature in the plate would exactly match the temperature profiles as in Fig. 1b. Consider the blue line in Fig. 1b as the temperature profile of a plate at that position. The temperature gradient in the plate would be equal to the so-called critical temperature gradient. If we would fix the temperature at the left side of the plate at ambient temperature Ta (e.g. using a heat exchanger), then the temperature at the right would be below Ta. In other words: we have produced a cooler. This is the basis of thermoacoustic cooling as shown in Fig. 2b which represents a thermoacoustic refrigerator. It has a loudspeaker at the left. The system corresponds with the left half of Fig. 1b with the stack in the position of the blue line. Cooling is produced at temperature TL.
It is also possible to fix the temperature of the right side of the plate at Ta and heat up the left side so that the temperature gradient in the plate would be larger than the critical temperature gradient. In that case, we have made an engine (prime mover) which can e.g. produce sound as in Fig. 2a. This is a so-called thermoacoustic prime mover. Stacks can be made of stainless steel plates but the device works also very well with loosely packed stainless steel wool or screens. It is heated at the left, e.g., by a propane flame and heat is released to ambient temperature by a heat exchanger. If the temperature at the left side is high enough, the system starts to produces a loud sound.
Thermoacoustic engines still suffer from some limitations, including that:
The device usually has low power to volume ratio.
Very high densities of operating fluids are required to obtain high power densities
The commercially available linear alternators used to convert acoustic energy into electricity currently have low efficiencies compared to rotary electric generators
Only expensive specially-made alternators can give satisfactory performance.
TAE uses gases at high pressures to provide reasonable power densities which imposes sealing challenges particularly if the mixture has light gases like helium.
The heat exchanging process in TAE is critical to maintain the power conversion process. The hot heat exchanger has to transfer heat to the stack and the cold heat exchanger has to sustain the temperature gradient across the stack. Yet, the available space for it is constrained with the small size and the blockage it adds to the path of the wave. The heat exchange process in oscillating media is still under extensive research.
The acoustic waves inside thermoacoustic engines operated at large pressure ratios suffer many kinds of non-linearities, such as turbulence which dissipates energy due to viscous effects, harmonic generation of different frequencies that carries acoustic power in frequencies other than the fundamental frequency.
The performance of thermoacoustic engines usually is characterized through several indicators as follows:
The first and second law efficiencies.
The onset temperature difference, defined as the minimum temperature difference across the sides of the stack at which the dynamic pressure is generated.
The frequency of the resultant pressure wave, since this frequency should match the resonance frequency required by the load device, either a thermoacoustic refrigerator/heat pump or a linear alternator.
The degree of harmonic distortion, indicating the ratio of higher harmonics to the fundamental mode in the resulting dynamic pressure wave.
The variation of the resultant wave frequency with the TAE operating temperature

Travelling-wave systems
Figure 3 is a schematic drawing of a travelling-wave thermoacoustic engine. It consists of a resonator tube and a loop which contains a regenerator, three heat exchangers, and a bypass loop. A regenerator is a porous medium with a high heat capacity. As the gas flows back and forth through the regenerator, it periodically stores and takes up heat from the regenerator material. In contrast to the stack, the pores in the regenerator are much smaller than the thermal penetration depth, so the thermal contact between gas and material is very good. Ideally, the energy flow in the regenerator is zero, so the main energy flow in the loop is from the hot heat exchanger via the pulse tube and the bypass loop to the heat exchanger at the other side of the regenerator (main heat exchanger). The energy in the loop is transported via a travelling wave as in Fig. 1c, hence the name travelling-wave systems. The ratio of the volume flows at the ends of the regenerator is TH/Ta, so the regenerator acts as a volume-flow amplifier. Just like in the case of the standing-wave system, the machine ""spontaneously"" produces sound if the temperature TH is high enough. The resulting pressure oscillations can be used in a variety of ways, such as in producing electricity, cooling, and heat pumping.

See also
Cryocooler
Photoacoustic effect
Thermoelectric cooling

References
External links
Thermoacoustic research at Los Alamos National Laboratory
M. Emam, Experimental Investigations on a Standing-Wave Thermoacoustic Engine, M.Sc. Thesis, Cairo University, Egypt (2013)
M.E.H. Tijani, Loudspeaker-driven thermo-acoustic refrigeration, Ph.D. Thesis, Technische Universiteit Eindhoven, (2001)
Design Environment for Low-amplitude ThermoAcoustic Energy Conversion",Category:CS1 errors: chapter ignored,1
189,190,Acoustical measurements and instrumentation,"Analysis of sound and acoustics plays a role in such engineering tasks as product design, production test, machine performance, and process control. For instance, product design can require modification of sound level or noise for compliance with standards from ANSI, IEC, and ISO. The work might also involve design fine-tuning to meet market expectations. Here, examples include tweaking an automobile door latching mechanism to impress a consumer with a satisfying click or modifying an exhaust manifold to change the tone of an engine's rumble. Aircraft designers are also using acoustic instrumentation to reduce the noise generated on takeoff and landing.
Acoustical measurements and instrumentation range from a handheld sound level meter to a 1000-microphone phased array. Most of the acoustical measurement and instrumentation systems can be broken down into three components:
1) Sensors
2) Data Acquisition
3) Analysis

Sensors
The most common sensor used for acoustic measurement is the microphone. Measurement-grade microphones are different from typical recording-studio microphones because they can provide a detailed calibration for their response and sensitivity. Other sensors include hydrophones for measuring sound in water or accelerometers for measuring vibrations causing sound. The three main groups of microphones are pressure, free-field, and random-incidence, each with their own correction factors for different applications. Well-known microphone suppliers include PCB Piezotronics, Brüel & Kjær and GRAS.

Data acquisition
Data acquisition hardware for acoustic measurements typically utilizes 24-bit analog-to-digital converters (ADCs), anti-aliasing filters, and other signal conditioning. This signal conditioning may include amplification, filtering, sensor excitation, and input configuration. Another consideration is the frequency range of the instrumentation. It should be large enough to cover the frequency range of signal interest, taking into account the range of the sensor. To prevent aliasing, many devices come with antialiasing filters, which cut the maximum frequency range of the device to a little less than one-half the maximum sampling rate, as prescribed by the Nyquist sampling theorem. Dynamic range is a common way to compare performance from one instrument to another. Dynamic range is a measure of how small you can measure a signal relative to the maximum input signal the device can measure. Expressed in decibels, the dynamic range is 20 log (Vmax/Vmin). For example, a device with an input range of ±10 V and a dynamic range of 110 dB will be able to measure a signal as small as 10 µV. Thus, the input range and the specified dynamic range are important for determining the needs of your instrumentation system. Some well known vendors include OROS, HEAD acoustics, Norsonic, Brüel & Kjær, Prosig, National Instruments, LMS International and GRAS has a selection guide detailing the difference between microphones..

Analysis
Audio and acoustic analysis includes: fractional-octave analysis, sound-level measurements, power spectra, frequency response measurements, and transient analysis. Results are viewed on waterfall displays, colormap displays, and octave graphs.


== References ==",Category:Acoustics,1
190,191,Backward masking,,Category:Acoustics,1
191,192,Sound speed gradient,"In acoustics, the sound speed gradient is the rate of change of the speed of sound with distance, for example with depth in the ocean, or height in the Earth's atmosphere. A sound speed gradient leads to refraction of sound wavefronts in the direction of lower sound speed, causing the sound rays to follow a curved path. The radius of curvature of the sound path is inversely proportional to the gradient.
When the sun warms the Earth's surface, there is a negative temperature gradient in atmosphere. The speed of sound decreases with decreasing temperature, so this also creates a negative sound speed gradient. The sound wave front travels faster near the ground, so the sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. The opposite effect happens when the ground is covered with snow, or in the morning over water, when the sound speed gradient is positive. In this case, sound waves can be refracted from the upper levels down to the surface.
In underwater acoustics, speed of sound depends on pressure (hence depth), temperature, and salinity of seawater, thus leading to vertical speed gradients similar to those that exist in atmospheric acoustics. However, when there is a zero sound speed gradient, values of sound speed have the same ""isospeed"" in all parts of a given water column (there is no change in sound speed with depth). The same effect happens in an isothermal atmosphere with the ideal gas assumption.

References
See also
SOFAR channel
Wind gradient",Category:Spatial gradient,1
192,193,Acoustic harassment device,"Acoustic harassment and acoustic deterrents are technologies used to keep animals and in some cases humans away from an area. Applications of the technology are used to keep marine mammals away from aquaculture facilities and to keep birds away from certain areas (for instance in the vicinity of airports and blueberry fields). The devices have also been employed to keep marine mammals away from fishing nets. The devices are known as acoustic harassment devices (AHDs) and acoustic deterrent devices, which are smaller AHDs or intended as an awareness tool to warn away species to the presence of danger rather than as a tool of harassment at a much louder level.
While they have proven effective over the short-term, animals tend to become conditioned over time and can even be drawn to the sounds once they habituate to the lack of real danger and the presence of sustenance.  Only acoustic harassment devices that cause actual pain have been found to be effective over the longer term. The devices can cause hearing damage in non-targeted species and design changes in the fishing gear, fishing methods, and fish farm design to provide a permanent solution are preferable.

History
Primitive harassment methods included firecrackers, rubber bullets, chasing animals by boat, banging pipes and seal bombs (incendiary devices). Devices emitting loud noises have also been used, including broadcasts of killer whale sounds, pingers, and acoustic buzzers. These often employ shrill sounding screams broadcast between 12 and 17 kHz. Acoustic deterrent devices normally broadcast near 10 kHz and use high volume. The intensity level of acoustic harassment devices has been measured up to 194 dB re 1uPa @ 1 m and the noise can be audible up to 50 kilometers away.

Assessments
Studies of long-term effects on the marine environment have not been carried out, including damage to non-targeted species. Results of the devices are mixed, and they have proved ineffective in some circumstances, especially over the long term, while design improvements such as electric fences to keep seals from climbing into enclosures, gear modification to exclude certain species, and keeping aquaculture plants clean of dead fish have often been effective at solving the problem of keeping predatory species away. Reports indicate that in contrast to the harassment devices, the deterrent devices have been very effective in dealing with cetacean bycatch.
Recent research shows that acoustic deterrent devices intended to scare off seals do not work, but they do scare off porpoises.
A new technique called ""startle technology"" is currently in development. Preliminary trials conducted by the University of St. Andrews shows great promise as a substitute for ADDs.

Acoustic devices and acoustic weapon use on humans
Acoustic devices have been used for military purposes including to stress enemies, as an aid in interrogation, and to create ""an infrasonic sound barrier"". The British Army used ""Squawk Boxes"" to emit ultrasonic frequencies causing various discomforts. Audio harassment was also used by the U.S. military in the Vietnam War and was famously depicted in the fictional movie Apocalypse Now as helicopters descend on the enemy with loud speakers. Operation Wandering Soul broadcast voices purported to be dead Vietcong. Other examples include the 350 watt HPS-1 Sound System that could be heard 2.5 miles away used on the Vatican embassy in Panama where ousted president Manuel Noriega was in refuge. At the Branch Davidian siege in Waco, Texas, loud music was broadcast. Devices utilising the deterioration of hearing with age have been deployed to discourage younger people loitering, e.g. The Mosquito.

See also
Acoustic Hailing Device
The Mosquito
Long Range Acoustic Device (LRAD)
Sonic weapon
Directional sound
Bird scarer#auditory scarers
Microwave auditory effect

References
Further reading
A Study Into the Effectiveness of Acoustic Harassment Devices-AHDs in Deterring Seals from Salmon Farms Around Shetland by Rachel Beacham Aberdeen University: Dissertation. M. Sc Marine and Fisheries Science",Category:Non-lethal weapons,1
193,194,Sounding board,"A sounding board, also known as a tester and abat-voix is a structure placed above and sometimes also behind a pulpit or other speaking platform which helps to project the sound of the speaker. It is usually made of wood. The structure may be specially shaped to assist the projection, for example, being formed as a parabolic reflector. In the typical setting of a church building, the sounding board may be ornately carved or constructed. The term abat-voix, from the French word for the same thing (abattre (“to beat down”) + voix (“voice”)) is also used in English.
Sounding board may also be used figuratively to describe a person who listens to a speech or proposal in order that the speaker may rehearse or explore the proposition more fully. The term is also used inter-personally to describe one person listening to another, and especially to their ideas. When a person listens and responds with comments, they provide perspective that otherwise would not be available through introspection or thought alone.

See also
Baldachin - canopy over altar or throne
Chhatri


== References ==",Category:Acoustics,1
194,195,Basic physics of the violin,"The sound of a violin is the result of interactions between its many parts. Drawing a bow across the strings causes the strings to vibrate. This vibration is transmitted through the bridge and sound post to the body of the violin, which allows the sound to effectively radiate into the surrounding air. The tension and type of strings, placement and tension of the sound post, quality of the bow, and the construction of the body, all contribute to the loudness and tonal quality of the sound.

Strings
The strings of a violin are stretched across the bridge and nut of the violin so that the ends are essentially stationary, allowing for the creation of standing waves. The fundamental frequency and harmonic overtones of the resulting sound depend on the material properties of the string, such as the tension, length, mass, elasticity and damping factor.

Tension
String tension affects the sound a violin produces in an obvious way. Increasing the tension on a string results in a higher frequency note. The strings of a violin are wrapped around adjustable tuning pegs. By turning its associated peg, each string may be loosened or tightened until it produces the desired pitch, which may be described in terms of frequency. The playing tension of a violin string ranges from about 9 lbf (40 N) to 20 lbf (89 N).

Length
The length of the string also influences its pitch, and is the basis for how a violin is played. Violinists ""stop"" a string with a left-hand fingertip, shortening its playing length. Most often the string is stopped against the violin's fingerboard, but in some cases fingertip contact alone is enough to stop the string at the desired sounding length. Stopping the string at a shorter length has the effect of raising its pitch.

Materials
String material affects the quality of the sound. A vibrating string does not produce a single frequency. The sound may be described as a combination of a fundamental frequency and overtones, which form the sound's timbre. String material influences the overtone mix. Response and ease of articulation are also affected by choice of string materials.
Violin strings were originally made from catgut which is still available, although its market niche is limited, due to its cost and tuning sensitivity to humidity and temperature. Modern strings are made of steel, stranded steel, or a variety of synthetic materials. Violin strings (with the exception of most E strings and some ""historically informed"" gut strings) are helically wound or ""overspun"" with metal to keep their thickness within comfortable limits as well as to manage their surface friction properties. Some stranded steel strings have an additional inner winding under the surface winding. The added material increases the mass per unit length of the string, lowering the pitch of the sound produced by a string of a given gauge and tension.

Bridge
The bridge supports one end of the strings' playing length. It must stand up under a combined down force of about 20 lbf (89 N). Down force preloads the violin's top, or table, affecting the sound of a sensitive instrument. String break angle across the bridge affects the down force, and is typically 158°.
More importantly, the bridge transfers vibration from the strings to the top of the violin. The most significant bridge motion is side-to-side rocking, coming from the transverse component of the strings' vibration. The bridge may be usefully viewed as a mechanical filter, or an arrangement of masses and ""springs"" that filters and shapes the timbre of the sound. Often the bridge is shaped to emphasize the singers' formant at about 3000 Hz.

Bow
Excitation of string vibration is generally provided by a bow consisting of a flat ribbon of parallel horse hairs stretched between the ends of a stick, which may be made of wood or synthetic material such as fiberglass or carbon-fiber composite. The length, weight, and balance point of modern bows are standardized. Players may notice variations in sound and handling from bow to bow, based on these parameters as well as stiffness and moment of inertia.
The hair is coated with rosin to provide controlled stick-slip as it moves transversely to the string. Different types of rosin are available, providing varying amounts of ""grip"" or static friction.
In bowing, the three most prominent factors under the player's immediate control are bow speed, downward force, and location of the sounding point where hair crosses string. The desired sounding point will generally move closer to the bridge as the string is stopped to a shorter sounding length. The player may also vary the amount of hair in contact with the string by tilting the bow stick more or less away from the bridge. Violinists are trained to keep the bow perpendicular to the string in most cases, as other angles may adversely affect the sound.

Body
The body of a violin must be strong enough to support the tension from the strings, but also light and thin enough to vibrate properly. The body of a violin consists of two arched wooden plates as top and bottom of a box, whose sides are formed by thin curved wooden ""ribs."" The ribs are reinforced at their edges with lining strips, which provide extra gluing surface where the plates are attached. Animal hide glue is used to fasten the parts together, since it is capable of making tightly fitted joints which do not damp vibrations nor add reflective discontinuities to the vibrating structure.
An internal sound post helps transmit sound to the back of the violin and serves as structural support.
The body of the violin acts as a ""sound box"" to couple the vibration of strings to the surrounding air, making it audible. The construction of this sound box, and especially the arching of the top and back, have a profound effect on the overall sound quality of the instrument. The sound-producing system of the violin body includes the top and back (and to some degree the sides, or ribs), the bass bar that is glued to the underside of the top, and the bridge and sound post. In addition to the resonant modes of the body structure, the enclosed air volume exhibits Helmholtz resonance modes.

Bibliography
Hutchins, M. 1962. The Physics of Music. Scientific American, W. H. Freeman and Company, 1974.
Hutchins, M. The Acoustics of Violin Plates. Scientific American, vol 245, No. 4. Oct 1981

Notes
External links
Violin acoustics: an introduction
William F. Fry: A Physicist's Quest for the ""Secrets"" of Stradivari (Wisconsin academy review)
Path Through the Woods - The Use of Medical Imaging in Examining Historical Instruments The use of computer aided tomography (CT Scanning) to examine great Italian instruments in order to replicate their acoustics in modern instruments.
Animations of violins showing how the plates vibrate at various frequencies
Wire-frame animation of a 1712 Stradivari violin at various eigenmode frequencies",Category:Articles needing expert attention with no reason or talk parameter,1
195,196,Thermoacoustic heat engine,"Thermoacoustic engines (sometimes called ""TA engines"") are thermoacoustic devices which use high-amplitude sound waves to pump heat from one place to another, or conversely use a heat difference to induce high-amplitude sound waves. In general, thermoacoustic engines can be divided into standing wave and travelling wave devices. These two types of thermoacoustics devices can again be divided into two thermodynamic classes, a prime mover (or simply heat engine), and a heat pump. The prime mover creates work using heat, whereas a heat pump creates or moves heat using work. Compared to vapor refrigerators, thermoacoustic refrigerators have no ozone-depleting or toxic coolant and few or no moving parts therefore require no dynamic sealing or lubrication.

Operation
Overview of device
A thermoacoustic device basically consists of heat exchangers, a resonator, and a stack (on standing wave devices) or regenerator (on travelling wave devices). Depending on the type of engine a driver or loudspeaker might be used as well to generate sound waves.
Consider a tube closed at both ends. Interference can occur between two waves traveling in opposite directions at certain frequencies. The interference causes resonance creating a standing wave. Resonance only occurs at certain frequencies called resonance frequencies, and these are mainly determined by the length of the resonator.
The stack is a part consisting of small parallel channels. When the stack is placed at a certain location in the resonator, while having a standing wave in the resonator, a temperature difference can be measured across the stack. By placing heat exchangers at each side of the stack, heat can be moved. The opposite is possible as well, by creating a temperature difference across the stack, a sound wave can be induced. The first example is a simple heat pump, while the second is a prime mover.

Heat pumping
To be able to create or move heat, work must be done, and the acoustic power provides this work. When a stack is placed inside a resonator a pressure drop occurs. Interference between the incoming and reflected wave is now imperfect since there is a difference in amplitude causing the standing wave to travel little, giving the wave acoustic power.
In the acoustic wave, parcels of gas adiabatically compress and expand. Pressure and temperature change simultaneously; when pressure reaches a maximum or minimum, so does the temperature. Heat pumping along a stack in a standing wave device can now be described using the Brayton cycle.
Below is the counter-clockwise Brayton cycle consisting of four processes for a refrigerator when a parcel of gas is followed between two plates of a stack.
Adiabatic compression of the gas. When a parcel of gas is displaced from its rightmost position to its leftmost position, the parcel is adiabatic compressed and thus the temperature increases. At the leftmost position the parcel now has a higher temperature than the warm plate.
Isobaric heat transfer. The parcel's temperature is higher than that of the plate causing it to transfer heat to the plate at constant pressure losing temperature.
Adiabatic expansion of the gas. The gas is displaced back from the leftmost position to the rightmost position and due to adiabatic expansion the gas is cooled to a temperature lower than that of the cold plate.
Isobaric heat transfer. The parcel's temperature is now lower than that of the plate causing heat to be transferred from the cold plate to the gas at a constant pressure, increasing the parcel's temperature back to its original value.
Travelling wave devices can be described using the Stirling cycle.

Temperature gradient
An engine and heat pump both typically use a stack and heat exchangers. The boundary between a prime mover and heat pump is given by the temperature gradient operator, which is the mean temperature gradient divided by the critical temperature gradient.

  
    
      
        
          I
        
        =
        
          
            
              ?
              
                T
                
                  m
                
              
            
            
              ?
              
                T
                
                  c
                  r
                  i
                  t
                
              
            
          
        
      
    
    {\displaystyle \mathrm {I} ={\frac {\nabla T_{m}}{\nabla T_{crit}}}}
  
The mean temperature gradient is the temperature difference across the stack divided by the length of the stack.

  
    
      
        ?
        
          T
          
            m
          
        
        =
        
          
            
              ?
              
                T
                
                  m
                
              
            
            
              ?
              
                x
                
                  s
                  t
                  a
                  c
                  k
                
              
            
          
        
      
    
    {\displaystyle \nabla T_{m}={\frac {\Delta T_{m}}{\Delta x_{stack}}}}
  
The critical temperature gradient is a value depending on certain characteristics of the device like frequency, cross-sectional area and gas properties.
If the temperature gradient operator exceeds one, the mean temperature gradient is larger than the critical temperature gradient and the stack operates as a prime mover. If the temperature gradient operator is less than one, the mean temperature gradient is smaller than the critical gradient and the stack operates as a heat pump.

Theoretical efficiency
In thermodynamics the highest achievable efficiency is the Carnot efficiency. The efficiency of thermoacoustic engines can be compared to Carnot efficiency using the temperature gradient operator.
The efficiency of a thermoacoustic engine is given by

  
    
      
        ?
        =
        
          
            
              ?
              
                c
              
            
            
              I
            
          
        
      
    
    {\displaystyle \eta ={\frac {\eta _{c}}{\mathrm {I} }}}
  
The coefficient of performance of a thermoacoustic heat pump is given by

  
    
      
        C
        O
        P
        =
        
          I
        
        ?
        C
        O
        
          P
          
            c
          
        
      
    
    {\displaystyle COP=\mathrm {I} \cdot COP_{c}}

Derivations
Using the Navier-Stokes equations for fluids, Rott was able to derive equations specific for thermoacoustics. Swift continued with these equations, deriving expressions for the acoustic power in thermoacoustic devices.

Efficiency in practice
The most efficient thermoacoustic devices built to date have an efficiency approaching 40% of the Carnot limit, or about 20% to 30% overall (depending on the heat engine temperatures).
Higher hot-end temperatures may be possible with thermoacoustic devices because there are no moving parts, thus allowing the Carnot efficiency to be higher. This may partially offset their lower efficiency, compared to conventional heat engines, as a percentage of Carnot.
The ideal Stirling cycle, approximated by traveling wave devices, is inherently more efficient than the ideal Brayton cycle, approximated by standing wave devices. However, the narrower pores required to give good thermal contact in a travelling wave regenerator, as compared to a standing wave stack which requires deliberately imperfect thermal contact, also gives rise to greater frictional losses, reducing the efficiency of a practical engine. The toroidal geometry often used in traveling wave devices, but not required for standing wave devices, can also give rise to losses due to Gedeon streaming around the loop.

Research in thermoacoustics
Modern research and development of thermoacoustic systems is largely based upon the work of Rott (1980) and later Greg Swift (1988), in which linear thermoacoustic models were developed to form a basic quantitative understanding, and numeric models for computation. Commercial interest has resulted in niche applications such as small to medium scale cryogenic applications.

History
The history of thermoacoustic hot air engines started about 1887, when Lord Rayleigh discussed the possibility of pumping heat with sound. Little further research occurred until Rott's work in 1969.
A very simple thermoacoustic hot air engine is the Rijke tube that converts heat into acoustic energy. This device however uses natural convection.

Current research
Orest Symko at University of Utah began a research project in 2005 called Thermal Acoustic Piezo Energy Conversion (TAPEC).
Score Ltd. was awarded £2M in March 2007 to research a cooking stove that will also deliver electricity and cooling using the Thermo-acoustic effect for use in developing countries.
A radioisotope-heated thermoacoustic system has been proposed and prototyped for deep space exploration missions by Airbus. The system has theoretical slight advantages over other generator systems like existing thermocouple based systems, or proposed Stirling engine used in ASRG prototype.

See also
SASER, Sound Amplification by Stimulated Emission of Radiation

References
Further reading
External links
Los Alamos National Laboratory, New Mexico, USA
Penn State University, USA
The Power of Sound, American Scientist Online
Thermoacoustics at the University of Adelaide, Australia, web archive backup: Discussion Forum
Adelaide University
Hear That? The Fridge Is Chilling, Wired Magazine article
""Experiments on a Standing-Wave Thermoacoustic Engine""",Category:Wikipedia articles needing clarification from July 2015,1
196,197,Underwater acoustics,"Underwater acoustics is the study of the propagation of sound in water and the interaction of the mechanical waves that constitute sound with the water and its boundaries. The water may be in the ocean, a lake or a tank. Typical frequencies associated with underwater acoustics are between 10 Hz and 1 MHz. The propagation of sound in the ocean at frequencies lower than 10 Hz is usually not possible without penetrating deep into the seabed, whereas frequencies above 1 MHz are rarely used because they are absorbed very quickly. Underwater acoustics is sometimes known as hydroacoustics.
The field of underwater acoustics is closely related to a number of other fields of acoustic study, including sonar, transduction, acoustic signal processing, acoustical oceanography, bioacoustics, and physical acoustics.

History
Underwater sound has probably been used by marine animals for millions of years. The science of underwater acoustics began in 1490, when Leonardo da Vinci wrote the following,
""If you cause your ship to stop and place the head of a long tube in the water and place the outer extremity to your ear, you will hear ships at a great distance from you.""
In 1687 Isaac Newton wrote his Mathematical Principles of Natural Philosophy which included the first mathematical treatment of sound. The next major step in the development of underwater acoustics was made by Daniel Colladon, a Swiss physicist, and Charles Sturm, a French mathematician. In 1826, on Lake Geneva, they measured the elapsed time between a flash of light and the sound of a submerged ship's bell heard using an underwater listening horn. They measured a sound speed of 1435 metres per second over a 17 kilometre(Km) distance, providing the first quantitative measurement of sound speed in water. The result they obtained was within about 2% of currently accepted values. In 1877 Lord Rayleigh wrote the Theory of Sound and established modern acoustic theory.
The sinking of Titanic in 1912 and the start of World War I provided the impetus for the next wave of progress in underwater acoustics. Systems for detecting icebergs and U-boats were developed. Between 1912 and 1914, a number of echolocation patents were granted in Europe and the U.S., culminating in Reginald A. Fessenden's echo-ranger in 1914. Pioneering work was carried out during this time in France by Paul Langevin and in Britain by A B Wood and associates. The development of both active ASDIC and passive sonar (SOund Navigation And Ranging) proceeded apace during the war, driven by the first large scale deployments of submarines. Other advances in underwater acoustics included the development of acoustic mines.
In 1919, the first scientific paper on underwater acoustics was published, theoretically describing the refraction of sound waves produced by temperature and salinity gradients in the ocean. The range predictions of the paper were experimentally validated by transmission loss measurements.
The next two decades saw the development of several applications of underwater acoustics. The fathometer, or depth sounder, was developed commercially during the 1920s. Originally natural materials were used for the transducers, but by the 1930s sonar systems incorporating piezoelectric transducers made from synthetic materials were being used for passive listening systems and for active echo-ranging systems. These systems were used to good effect during World War II by both submarines and anti-submarine vessels. Many advances in underwater acoustics were made which were summarised later in the series Physics of Sound in the Sea, published in 1946.
After World War II, the development of sonar systems was driven largely by the Cold War, resulting in advances in the theoretical and practical understanding of underwater acoustics, aided by computer-based techniques.

Theory
Sound waves in water
A sound wave propagating underwater consists of alternating compressions and rarefactions of the water. These compressions and rarefactions are detected by a receiver, such as the human ear or a hydrophone, as changes in pressure. These waves may be man-made or naturally generated.

Speed of sound, density and impedance
The speed of sound 
  
    
      
        c
        
      
    
    {\displaystyle c\,}
   (i.e., the longitudinal motion of wavefronts) is related to frequency 
  
    
      
        f
        
      
    
    {\displaystyle f\,}
   and wavelength 
  
    
      
        ?
        
      
    
    {\displaystyle \lambda \,}
   of a wave by 
  
    
      
        c
        =
        f
        ?
        ?
      
    
    {\displaystyle c=f\cdot \lambda }
  .
This is different from the particle velocity 
  
    
      
        u
        
      
    
    {\displaystyle u\,}
  , which refers to the motion of molecules in the medium due to the sound, and relates the plane wave pressure 
  
    
      
        p
        
      
    
    {\displaystyle p\,}
   to the fluid density 
  
    
      
        ?
        
      
    
    {\displaystyle \rho \,}
   and sound speed 
  
    
      
        c
        
      
    
    {\displaystyle c\,}
   by 
  
    
      
        p
        =
        c
        ?
        u
        ?
        ?
      
    
    {\displaystyle p=c\cdot u\cdot \rho }
  .
The product of 
  
    
      
        c
      
    
    {\displaystyle c}
   and 
  
    
      
        ?
        
      
    
    {\displaystyle \rho \,}
   from the above formula is known as the characteristic acoustic impedance. The acoustic power (energy per second) crossing unit area is known as the intensity of the wave and for a plane wave the average intensity is given by 
  
    
      
        I
        =
        
          q
          
            2
          
        
        
          /
        
        (
        ?
        c
        )
        
      
    
    {\displaystyle I=q^{2}/(\rho c)\,}
  , where 
  
    
      
        q
        
      
    
    {\displaystyle q\,}
   is the root mean square acoustic pressure.
At 1 kHz, the wavelength in water is about 1.5 m. Sometimes the term ""sound velocity"" is used but this is incorrect as the quantity is a scalar.
The large impedance contrast between air and water (the ratio is about 3600) and the scale of surface roughness means that the sea surface behaves as an almost perfect reflector of sound at frequencies below 1 kHz. Sound speed in water exceeds that in air by a factor of 4.4 and the density ratio is about 820.

Absorption of sound
Absorption of low frequency sound is weak. (see Technical Guides – Calculation of absorption of sound in seawater for an on-line calculator). The main cause of sound attenuation in fresh water, and at high frequency in sea water (above 100 kHz) is viscosity. Important additional contributions at lower frequency in seawater are associated with the ionic relaxation of boric acid (up to c. 10 kHz) and magnesium sulfate (c. 10 kHz-100 kHz).
Sound may be absorbed by losses at the fluid boundaries. Near the surface of the sea losses can occur in a bubble layer or in ice, while at the bottom sound can penetrate into the sediment and be absorbed.

Sound reflection and scattering
Boundary interactions
Both the water surface and bottom are reflecting and scattering boundaries.

Surface
For many purposes the sea-air surface can be thought of as a perfect reflector. The impedance contrast is so great that little energy is able to cross this boundary. Acoustic pressure waves reflected from the sea surface experience a reversal in phase, often stated as either a “pi phase change” or a “180 deg phase change”. This is represented mathematically by assigning a reflection coefficient of minus 1 instead of plus one to the sea surface.
At high frequency (above about 1 kHz) or when the sea is rough, some of the incident sound is scattered, and this is taken into account by assigning a reflection coefficient whose magnitude is less than one. For example, close to normal incidence, the reflection coefficient becomes 
  
    
      
        R
        =
        ?
        
          e
          
            ?
            2
            
              k
              
                2
              
            
            
              h
              
                2
              
            
            s
            i
            
              n
              
                2
              
            
            A
          
        
      
    
    {\displaystyle R=-e^{-2k^{2}h^{2}sin^{2}A}}
  , where h is the rms wave height.
A further complication is the presence of wind generated bubbles or fish close to the sea surface. The bubbles can also form plumes that absorb some of the incident and scattered sound, and scatter some of the sound themselves.

Seabed
The acoustic impedance mismatch between water and the bottom is generally much less than at the surface and is more complex. It depends on the bottom material types and depth of the layers. Theories have been developed for predicting the sound propagation in the bottom in this case, for example by Biot  and by Buckingham.

At target
The reflection of sound at a target whose dimensions are large compared with the acoustic wavelength depends on its size and shape as well as the impedance of the target relative to that of water. Formulae have been developed for the target strength of various simple shapes as a function of angle of sound incidence. More complex shapes may be approximated by combining these simple ones.

Propagation of sound
Underwater acoustic propagation depends on many factors. The direction of sound propagation is determined by the sound speed gradients in the water.This is an important thing that happens in water, because the speed of sound travel in water with velocity regular. In the sea the vertical gradients are generally much larger than the horizontal ones. Combining this with a tendency towards increasing sound speed at increasing depth, due to the increasing pressure in the deep sea, causes a reversal of the sound speed gradient in the thermocline, creating an efficient waveguide at the depth, corresponding to the minimum sound speed. The sound speed profile may cause regions of low sound intensity called ""Shadow Zones"", and regions of high intensity called ""Caustics"". These may be found by ray tracing methods.
At equator and temperate latitudes in the ocean, the surface temperature is high enough to reverse the pressure effect, such that a sound speed minimum occurs at depth of a few hundred metres. The presence of this minimum creates a special channel known as Deep Sound Channel, previously known as the SOFAR (sound fixing and ranging) channel, permitting guided propagation of underwater sound for thousands of kilometres without interaction with the sea surface or the seabed. Another phenomenon in the deep sea is the formation of sound focusing areas, known as Convergence Zones. In this case sound is refracted downward from a near-surface source and then back up again. The horizontal distance from the source at which this occurs depends on the positive and negative sound speed gradients. A surface duct can also occur in both deep and moderately shallow water when there is upward refraction, for example due to cold surface temperatures. Propagation is by repeated sound bounces off the surface.
In general, as sound propagates underwater there is a reduction in the sound intensity over increasing ranges, though in some circumstances a gain can be obtained due to focusing. Propagation loss (sometimes referred to as transmission loss) is a quantitative measure of the reduction in sound intensity between two points, normally the sound source and a distant receiver. If 
  
    
      
        
          I
          
            s
          
        
      
    
    {\displaystyle I_{s}}
   is the far field intensity of the source referred to a point 1 m from its acoustic centre and 
  
    
      
        
          I
          
            r
          
        
      
    
    {\displaystyle I_{r}}
   is the intensity at the receiver, then the propagation loss is given by 
  
    
      
        P
        L
        =
        10
        log
        ?
        (
        
          I
          
            s
          
        
        
          /
        
        
          I
          
            r
          
        
        )
      
    
    {\displaystyle PL=10\log(I_{s}/I_{r})}
  . In this equation 
  
    
      
        
          I
          
            r
          
        
      
    
    {\displaystyle I_{r}}
   is not the true acoustic intensity at the receiver, which is a vector quantity, but a scalar equal to the equivalent plane wave intensity (EPWI) of the sound field. The EPWI is defined as the magnitude of the intensity of a plane wave of the same RMS pressure as the true acoustic field. At short range the propagation loss is dominated by spreading while at long range it is dominated by absorption and/or scattering losses.
An alternative definition is possible in terms of pressure instead of intensity, giving 
  
    
      
        P
        L
        =
        20
        log
        ?
        (
        
          p
          
            s
          
        
        
          /
        
        
          p
          
            r
          
        
        )
      
    
    {\displaystyle PL=20\log(p_{s}/p_{r})}
  , where 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   is the RMS acoustic pressure in the far-field of the projector, scaled to a standard distance of 1 m, and 
  
    
      
        
          p
          
            r
          
        
      
    
    {\displaystyle p_{r}}
   is the RMS pressure at the receiver position.
These two definitions are not exactly equivalent because the characteristic impedance at the receiver may be different from that at the source. Because of this, the use of the intensity definition leads to a different sonar equation to the definition based on a pressure ratio. If the source and receiver are both in water, the difference is small.

Propagation modelling
The propagation of sound through water is described by the wave equation, with appropriate boundary conditions. A number of models have been developed to simplify propagation calculations. These models include ray theory, normal mode solutions, and parabolic equation simplifications of the wave equation. Each set of solutions is generally valid and computationally efficient in a limited frequency and range regime, and may involve other limits as well. Ray theory is more appropriate at short range and high frequency, while the other solutions function better at long range and low frequency. Various empirical and analytical formulae have also been derived from measurements that are useful approximations.

Reverberation
Transient sounds result in a decaying background that can be of much larger duration than the original transient signal. The cause of this background, known as reverberation, is partly due to scattering from rough boundaries and partly due to scattering from fish and other biota. For an acoustic signal to be detected easily, it must exceed the reverberation level as well as the background noise level.

Doppler shift
If an underwater object is moving relative to an underwater receiver, the frequency of the received sound is different from that of the sound radiated (or reflected) by the object. This change in frequency is known as a Doppler shift. The shift can be easily observed in active sonar systems, particularly narrow-band ones, because the transmitter frequency is known, and the relative motion between sonar and object can be calculated. Sometimes the frequency of the radiated noise (a tonal) may also be known, in which case the same calculation can be done for passive sonar. For active systems the change in frequency is 0.69 Hz per knot per kHz and half this for passive systems as propagation is only one way. The shift corresponds to an increase in frequency for an approaching target.

Intensity fluctuations
Though acoustic propagation modelling generally predicts a constant received sound level, in practice there are both temporal and spatial fluctuations. These may be due to both small and large scale environmental phenomena. These can include sound speed profile fine structure and frontal zones as well as internal waves. Because in general there are multiple propagation paths between a source and receiver, small phase changes in the interference pattern between these paths can lead to large fluctuations in sound intensity.

Non-linearity
In water, especially with air bubbles, the change in density due to a change in pressure is not exactly linearly proportional. As a consequence for a sinusoidal wave input additional harmonic and subharmonic frequencies are generated. When two sinusoidal waves are input, sum and difference frequencies are generated. The conversion process is greater at high source levels than small ones. Because of the non-linearity there is a dependence of sound speed on the pressure amplitude so that large changes travel faster than small ones. Thus a sinusoidal waveform gradually becomes a sawtooth one with a steep rise and a gradual tail. Use is made of this phenomenon in parametric sonar and theories have been developed to account for this, e.g. by Westerfield.

Measurements
Sound in water is measured using a hydrophone, which is the underwater equivalent of a microphone. A hydrophone measures pressure fluctuations, and these are usually converted to sound pressure level (SPL), which is a logarithmic measure of the mean square acoustic pressure.
Measurements are usually reported in one of three forms :-
RMS acoustic pressure in micropascals (or dB re 1 ?Pa)
RMS acoustic pressure in a specified bandwidth, usually octaves or thirds of octave (dB re 1 ?Pa)
spectral density (mean square pressure per unit bandwidth) in micropascals-squared per hertz (dB re 1 ?Pa²/Hz)
The scale for acoustic pressure in water differs from that used for sound in air. In air the reference pressure is 20 ?Pa rather than 1 ?Pa. For the same numerical value of SPL, the intensity of a plane wave (power per unit area, proportional to mean square sound pressure divided by acoustic impedance) in air is about 202×3600 = 1 440 000 times higher than in water. Similarly, the intensity is about the same if the SPL is 61.6 dB higher in the water.

Sound speed
Approximate values for fresh water and seawater, respectively, at atmospheric pressure are 1450 and 1500 m/s for the sound speed, and 1000 and 1030 kg/m³ for the density. The speed of sound in water increases with increasing pressure, temperature and salinity. The maximum speed in pure water under atmospheric pressure is attained at about 74 °C; sound travels slower in hotter water after that point; the maximum increases with pressure. On-line calculators can be found at Technical Guides – Speed of Sound in Sea-Water and Technical Guides – Speed of Sound in Pure Water.

Absorption
Many measurements have been made of sound absorption in lakes and the ocean  (see Technical Guides – Calculation of absorption of sound in seawater for an on-line calculator).

Ambient noise
Measurement of acoustic signals are possible if their amplitude exceeds a minimum threshold, determined partly by the signal processing used and partly by the level of background noise. Ambient noise is that part of the received noise that is independent of the source, receiver and platform characteristics. This it excludes reverberation and towing noise for example.
The background noise present in the ocean, or ambient noise, has many different sources and varies with location and frequency. At the lowest frequencies, from about 0.1 Hz to 10 Hz, ocean turbulence and microseisms are the primary contributors to the noise background. Typical noise spectrum levels decrease with increasing frequency from about 140 dB re 1 ?Pa²/Hz at 1 Hz to about 30 dB re 1 ?Pa²/Hz at 100 kHz. Distant ship traffic is one of the dominant noise sources in most areas for frequencies of around 100 Hz, while wind-induced surface noise is the main source between 1 kHz and 30 kHz. At very high frequencies, above 100 kHz, thermal noise of water molecules begins to dominate. The thermal noise spectral level at 100 kHz is 25 dB re 1 ?Pa²/Hz. The spectral density of thermal noise increases by 20 dB per decade (approximately 6 dB per octave).
Transient sound sources also contribute to ambient noise. These can include intermittent geological activity, such as earthquakes and underwater volcanoes, rainfall on the surface, and biological activity. Biological sources include cetaceans (especially blue, fin and sperm whales), certain types of fish, and snapping shrimp.
Rain can produce high levels of ambient noise. However the numerical relationship between rain rate and ambient noise level is difficult to determine because measurement of rain rate is problematic at sea.

Reverberation
Many measurements have been made of sea surface, bottom and volume reverberation. Empirical models have sometimes been derived from these. A commonly used expression for the band 0.4 to 6.4 kHz is that by Chapman and Harris. It is found that a sinusoidal waveform is spread in frequency due to the surface motion. For bottom reverberation a Lambert's Law is found often to apply approximately, for example see Mackenzie. Volume reverberation is usually found to occur mainly in layers, which change depth with the time of day, e.g., see Marshall and Chapman. The under-surface of ice can produce strong reverberation when it is rough, see for example Milne.

Bottom loss
Bottom loss has been measured as a function of grazing angle for many frequencies in various locations, for example those by the US Marine Geophysical Survey. The loss depends on the sound speed in the bottom (which is affected by gradients and layering) and by roughness. Graphs have been produced for the loss to be expected in particular circumstances. In shallow water bottom loss often has the dominant impact on long range propagation. At low frequencies sound can propagate through the sediment then back into the water.

Underwater hearing
Comparison with airborne sound levels
As with airborne sound, sound pressure level underwater is usually reported in units of decibels, but there are some important differences that make it difficult (and often inappropriate) to compare SPL in water with SPL in air. These differences include:
difference in reference pressure: 1 ?Pa (one micropascal, or one millionth of a pascal) instead of 20 ?Pa.
difference in interpretation: there are two schools of thought, one maintaining that pressures should be compared directly, and the other that one should first convert to the intensity of an equivalent plane wave.
difference in hearing sensitivity: any comparison with (A-weighted) sound in air needs to take into account the differences in hearing sensitivity, either of a human diver or other animal.

Hearing sensitivity
The lowest audible SPL for a human diver with normal hearing is about 67 dB re 1 ?Pa, with greatest sensitivity occurring at frequencies around 1 kHz. This corresponds to a sound intensity 5.4 dB, or 3.5 times, higher than the threshold in air (see #Measurements above). Dolphins and other toothed whales are renowned for their acute hearing sensitivity, especially in the frequency range 5 to 50 kHz. Several species have hearing thresholds between 30 and 50 dB re 1 ?Pa in this frequency range. For example, the hearing threshold of the killer whale occurs at an RMS acoustic pressure of 0.02 mPa (and frequency 15 kHz), corresponding to an SPL threshold of 26 dB re 1 ?Pa. By comparison the most sensitive fish is the soldier fish, whose threshold is 0.32 mPa (50 dB re 1 ?Pa) at 1.3 kHz, whereas the lobster has a hearing threshold of 1.3 Pa at 70 Hz (122 dB re 1 ?Pa).

Safety thresholds
High levels of underwater sound create a potential hazard to marine and amphibious animals as well as to human divers. Guidelines for exposure of human divers and marine mammals to underwater sound are reported by the SOLMAR project of the NATO Undersea Research Centre. Human divers exposed to SPL above 154 dB re 1 ?Pa in the frequency range 0.6 to 2.5 kHz are reported to experience changes in their heart rate or breathing frequency. Diver aversion to low frequency sound is dependent upon sound pressure level and center frequency.

Applications of underwater acoustics
Sonar
Sonar is the name given to the acoustic equivalent of radar. Pulses of sound are used to probe the sea, and the echoes are then processed to extract information about the sea, its boundaries and submerged objects. An alternative use, known as passive sonar, attempts to do the same by listening to the sounds radiated by underwater objects.

Underwater communication
The need for underwater acoustic telemetry exists in applications such as data harvesting for environmental monitoring, communication with and between manned and unmanned underwater vehicles, transmission of diver speech, etc. A related application is underwater remote control, in which acoustic telemetry is used to remotely actuate a switch or trigger an event. A prominent example of underwater remote control are acoustic releases, devices that are used to return sea floor deployed instrument packages or other payloads to the surface per remote command at the end of a deployment. Acoustic communications form an active field of research  with significant challenges to overcome, especially in horizontal, shallow-water channels. Compared with radio telecommunications, the available bandwidth is reduced by several orders of magnitude. Moreover, the low speed of sound causes multipath propagation to stretch over time delay intervals of tens or hundreds of milliseconds, as well as significant Doppler shifts and spreading. Often acoustic communication systems are not limited by noise, but by reverberation and time variability beyond the capability of receiver algorithms. The fidelity of underwater communication links can be greatly improved by the use of hydrophone arrays, which allow processing techniques such as adaptive beamforming and diversity combining.

Underwater navigation and tracking
Underwater navigation and tracking is a common requirement for exploration and work by divers, ROV, autonomous underwater vehicles (AUV), manned submersibles and submarines alike. Unlike most radio signals which are quickly absorbed, sound propagates far underwater and at a rate that can be precisely measured or estimated. It can thus be used to measure distances between a tracked target and one or multiple reference of baseline stations precisely, and triangulate the position of the target, sometimes with centimeter accuracy. Starting in the 1960s, this has given rise to underwater acoustic positioning systems which are now widely used.

Seismic exploration
Seismic exploration involves the use of low frequency sound (< 100 Hz) to probe deep into the seabed. Despite the relatively poor resolution due to their long wavelength, low frequency sounds are preferred because high frequencies are heavily attenuated when they travel through the seabed. Sound sources used include airguns, vibroseis and explosives.

Weather and climate observation
Acoustic sensors can be used to monitor the sound made by wind and precipitation. For example, an acoustic rain gauge is described by Nystuen. Lightning strikes can also be detected. Acoustic thermometry of ocean climate (ATOC) uses low frequency sound to measure the global ocean temperature.

Oceanography
Large scale ocean features can be detected by acoustic tomography. Bottom characteristics can be measured by side-scan sonar and sub-bottom profiling.

Marine biology
Due to its excellent propagation properties, underwater sound is used as a tool to aid the study of marine life, from microplankton to the blue whale. Echo sounders are often used to provide data on marine life abundance, distribution, and behavior information. Echo sounders, also referred to as hydroacoustics is also used for fish location, quantity, size, and biomass.
Acoustic telemetry is also used for monitoring fishes and marine wildlife. An acoustic transmitter is attached to the fish (sometimes internally) while an array of receivers listen to the information conveyed by the sound wave. This enables the researchers to track the movements of individuals in a small-medium scale.
Pistol shrimp create sonoluminescent cavitation bubbles that reach up to 5,000 K (4,700 °C)

Particle physics
A neutrino is a fundamental particle that interacts very weakly with other matter. For this reason, it requires detection apparatus on a very large scale, and the ocean is sometimes used for this purpose. In particular, it is thought that ultra-high energy neutrinos in seawater can be detected acoustically.

See also
Acoustic Tags (Acoustic Telemetry)
Bioacoustics
Hydroacoustics
Ocean Tracking Network
Refraction (sound)
Sonar
Underwater Acoustic Positioning System
SOFAR channel
Underwater acoustic wireless communication system
European Conference on Underwater Acoustics

References
External links
Ocean Acoustics Library
Ultrasonics and Underwater Acoustics
Monitoring the global ocean through underwater acoustics
ASA Underwater Acoustics Technical Committee
An Ocean of Sound
Underwater Acoustic Communications
Acoustic Communications Group at the Woods Hole Oceanographic Institution
Sound in the Sea
SFSU Underwater Acoustics Research Group
Discovery of Sound in the Sea
PAMBuoy Passively Acoustic Monitoring",Category:Articles using small message boxes,1
197,198,Acoustic approximation,"A fundamental principle in the field of acoustics, the acoustic approximation states that an acoustic wave is created by a small, adiabatic, pressure ripple riding on a comparatively large equilibrium (bias) pressure. Typically, the acoustic pressure is on the order of a few ppm of the equilibrium pressure.
By extension, the acoustic approximation also guarantees that an acoustic wave travels at a speed exactly equal to the local speed of sound, such that the Mach number:

  
    
      
        M
        =
        1
      
    
    {\displaystyle M=1}

See also
Sound

References


== External links ==",Category:Acoustics,1
198,199,Helmholtz resonance,"Helmholtz resonance or wind throb is the phenomenon of air resonance in a cavity, such as when one blows across the top of an empty bottle. The name comes from a device created in the 1850s by Hermann von Helmholtz, the Helmholtz resonator, which he used to identify the various frequencies or musical pitches present in music and other complex sounds.

History
Helmholtz described in his 1862 book, ""On the Sensations of Tone"", an apparatus able to pick out specific frequencies from a complex sound. The Helmholtz resonator, as it is now called, consists of a rigid container of a known volume, nearly spherical in shape, with a small neck and hole in one end and a larger hole in the other end to admit the sound.
When the resonator's 'nipple' is placed inside one's ear, a specific frequency of the complex sound can be picked out and heard clearly. In Helmholtz’ book we read: When we ""apply a resonator to the ear, most of the tones produced in the surrounding air will be considerably damped; but if the proper tone of the resonator is sounded, it brays into the ear most powerfully…. The proper tone of the resonator may even be sometimes heard cropping up in the whistling of the wind, the rattling of carriage wheels, the splashing of water.""
A set of varied size resonators was sold to be used as discrete acoustic filters for the spectral analysis of complex sounds.
There is also an adjustable type, called a universal resonator, which consists of two cylinders, one inside the other, which can slide in or out to change the volume of the cavity over a continuous range. This type of resonator is in use in the Fourier analyzer, and is equivalent to the tone variator in its function. When air is forced into a cavity, the pressure inside increases. When the external force pushing the air into the cavity is removed, the higher-pressure air inside will flow out. Due to the inertia of the moving air the cavity will be left at a pressure slightly lower than the outside, causing air to be drawn back in. This process repeats, with the magnitude of the pressure oscillations increasing and decreasing asymptotically after the sound starts and stops.
The port (the neck of the chamber) is placed in the external meatus of the ear, allowing the experimenter to hear the sound and to determine its loudness. The resonant mass of air in the chamber is set in motion through the second hole, which is larger and doesn't have a neck.
A gastropod seashell can form a low Q Helmholtz resonator, amplifying many frequencies, resulting in the ""sounds of the sea"".
The term Helmholtz resonator is now more generally applied to include bottles from which sound is generated by blowing air across the mouth of the bottle. In this case the length and diameter of the bottle neck also contribute to the resonance frequency and its Q factor.
By one definition a Helmholtz resonator augments the amplitude of the vibratory motion of the enclosed air in a chamber by taking energy from sound waves passing in the surrounding air. In the other definition the sound waves are generated by a uniform stream of air flowing across the open top of an enclosed volume of air.

Quantitative explanation
It can be shown that the resonant angular frequency is given by:

  
    
      
        
          ?
          
            H
          
        
        =
        
          
            ?
            
              
                
                  A
                  
                    2
                  
                
                m
              
            
            
              
                
                  P
                  
                    0
                  
                
                
                  V
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{H}={\sqrt {\gamma {\frac {A^{2}}{m}}{\frac {P_{0}}{V_{0}}}}}}
   (rad/s),
where:

  
    
      
        ?
      
    
    {\displaystyle \gamma }
   (gamma) is the adiabatic index or ratio of specific heats. This value is usually 1.4 for air and diatomic gases.

  
    
      
        A
      
    
    {\displaystyle A}
   is the cross-sectional area of the neck;

  
    
      
        m
      
    
    {\displaystyle m}
   is the mass in the neck;

  
    
      
        
          P
          
            0
          
        
      
    
    {\displaystyle P_{0}}
   is the static pressure in the cavity;

  
    
      
        
          V
          
            0
          
        
      
    
    {\displaystyle V_{0}}
   is the static volume of the cavity.
For cylindrical or rectangular necks, we have

  
    
      
        A
        =
        
          
            
              V
              
                n
              
            
            
              L
              
                e
                q
              
            
          
        
      
    
    {\displaystyle A={\frac {V_{n}}{L_{eq}}}}
  ,
where:

  
    
      
        
          L
          
            e
            q
          
        
      
    
    {\displaystyle L_{eq}}
   is the equivalent length of the neck with end correction, which can be calculated as :
  
    
      
        
          L
          
            e
            q
          
        
        =
        
          L
          
            n
          
        
        +
        0.3
        D
      
    
    {\displaystyle L_{eq}=L_{n}+0.3D}
  , where 
  
    
      
        
          L
          
            n
          
        
      
    
    {\displaystyle L_{n}}
   is the actual length of the neck and 
  
    
      
        D
      
    
    {\displaystyle D}
   is the hydraulic diameter of the neck;

  
    
      
        
          V
          
            n
          
        
      
    
    {\displaystyle V_{n}}
   is the volume of air in the neck,
thus:

  
    
      
        
          ?
          
            H
          
        
        =
        
          
            ?
            
              
                A
                m
              
            
            
              
                
                  V
                  
                    n
                  
                
                
                  L
                  
                    e
                    q
                  
                
              
            
            
              
                
                  P
                  
                    0
                  
                
                
                  V
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{H}={\sqrt {\gamma {\frac {A}{m}}{\frac {V_{n}}{L_{eq}}}{\frac {P_{0}}{V_{0}}}}}}
  .
From the definition of mass density (
  
    
      
        
          ?
        
      
    
    {\displaystyle {\rho }}
  ): 
  
    
      
        
          
            
              V
              
                n
              
            
            m
          
        
        =
        
          
            1
            ?
          
        
      
    
    {\displaystyle {\frac {V_{n}}{m}}={\frac {1}{\rho }}}
  , thus:

  
    
      
        
          ?
          
            H
          
        
        =
        
          
            ?
            
              
                
                  P
                  
                    0
                  
                
                ?
              
            
            
              
                A
                
                  
                    V
                    
                      0
                    
                  
                  
                    L
                    
                      e
                      q
                    
                  
                
              
            
          
        
      
    
    {\displaystyle \omega _{H}={\sqrt {\gamma {\frac {P_{0}}{\rho }}{\frac {A}{V_{0}L_{eq}}}}}}
   ,
and

  
    
      
        
          f
          
            H
          
        
        =
        
          
            
              ?
              
                H
              
            
            
              2
              ?
            
          
        
      
    
    {\displaystyle f_{H}={\frac {\omega _{H}}{2\pi }}}
   ,
where:
fH is the resonant frequency (Hz).
The speed of sound in a gas is given by:

  
    
      
        v
        =
        
          
            ?
            
              
                
                  P
                  
                    0
                  
                
                ?
              
            
          
        
      
    
    {\displaystyle v={\sqrt {\gamma {\frac {P_{0}}{\rho }}}}}
   ,
thus, the frequency of the resonance is:

  
    
      
        
          f
          
            H
          
        
        =
        
          
            v
            
              2
              ?
            
          
        
        
          
            
              A
              
                
                  V
                  
                    0
                  
                
                
                  L
                  
                    e
                    q
                  
                
              
            
          
        
      
    
    {\displaystyle f_{H}={\frac {v}{2\pi }}{\sqrt {\frac {A}{V_{0}L_{eq}}}}}
  .
The length of the neck appears in the denominator because the inertia of the air in the neck is proportional to the length. The volume of the cavity appears in the denominator because the spring constant of the air in the cavity is inversely proportional to its volume. The area of the neck matters for two reasons. Increasing the area of the neck increases the inertia of the air proportionately, but also decreases the velocity at which the air rushes in and out.
Depending on the exact shape of the hole, the relative thickness of the sheet with respect to the size of the hole and the size of the cavity, this formula can have limitations. More sophisticated formulae can still be derived analytically, with similar physical explanations (although some differences matter). See for example the book by F. Mechels. Furthermore, if the mean flow over the resonator is high (typically with a Mach number above 0.3), some corrections must be applied.

Applications
Helmholtz resonance finds application in internal combustion engines (see airbox), subwoofers and acoustics. Intake systems described as 'Helmholtz Systems' have been used in the Chrysler V10 engine built for both the Dodge Viper and the Ram pickup truck, and several of the Buell tube-frame series of motorcycles. In stringed instruments as old as the veena or sitar, or as recent as the guitar and violin, the resonance curve of the instrument has the Helmholtz resonance as one of its peaks, along with other peaks coming from resonances of the vibration of the wood. An ocarina is essentially a Helmholtz resonator where the combined area of the opened finger holes determines the note played by the instrument. The West African djembe is the original Helmholtz resonator with a small neck area, giving it a deep bass tone. It has been in use for thousands of years.
The theory of Helmholtz resonators is used in motorcycle and car exhausts to alter the sound of the exhaust note and for differences in power delivery by adding chambers to the exhaust. Exhaust resonators are also used to reduce potentially loud and obnoxious engine noise where the dimensions are calculated so that the waves reflected by the resonator help cancel out certain frequencies of sound in the exhaust.
In some two-stroke engines, a Helmholtz resonator is used to remove the need for a reed valve. A similar effect is also used in the exhaust system of most two-stroke engines, using a reflected pressure pulse to supercharge the cylinder (see Kadenacy effect.)
Helmholtz resonators are used in architectural acoustics to reduce undesirable low frequency sounds (standing waves, etc.) by building a resonator tuned to the problem frequency, thereby eliminating it.
Helmholtz resonators are also used to build acoustic liners for reducing the noise of aircraft engines, for example. These acoustic liners are made of two components:
a simple sheet of metal (or other material) perforated with little holes spaced out in a regular or irregular pattern; this is called a resistive sheet;
a series of so-called honeycomb cavities (holes with a honeycomb shape, but in fact only their volume matters).
Such acoustic liners are used in most of today's aircraft engines. The perforated sheet is usually visible from inside or outside the airplane; the honeycomb is just under it. The thickness of the perforated sheet is of importance, as shown above. Sometimes there are two layers of liners; they are then called ""2-DOF liners"" (DOF meaning Degrees Of Freedom), as opposed to ""single DOF liners"".
This effect might also be used to reduce skin friction drag on aircraft wings by 40%.
Helmholtz resonance sometimes occurs when a slightly open single car window makes a very loud sound, also called side window buffeting or wind throb.
it is also partly the principle behind how piezoelectric Buzzer work, a piezoelectric disc acts as the excitation source, but it depends on the acoustic cavity resonance to produce the audible beep

Notes


== Further reading ==",Category:Acoustics,1
199,200,Scanning Acoustic Tomography,"A scanning acoustic microscope (SAM) is a device which uses focused sound to investigate, measure, or image an object (a process called scanning acoustic tomography). It is commonly used in failure analysis and non-destructive evaluation. It also has applications in biological and medical research. The semiconductor industry has found the SAM useful in detecting voids, cracks, and delaminations within microelectronic packages.

History
The first scanning acoustic microscope was developed in 1974 by R. A. Lemons and C. F. Quate at the Microwave Laboratory of Stanford University. Since then, many improvements to such systems have been made to enhance resolution and accuracy.

Principles of operation
Scanning acoustic microscopy works by directing focused sound from a transducer at a small point on a target object. Sound hitting the object is either scattered, absorbed, reflected (scattered at 180°) or transmitted (scattered at 0°). It is possible to detect the scattered pulses travelling in a particular direction. A detected pulse informs of the presence of a boundary or object. The `time of flight' of the pulse is defined as the time taken for it to be emitted by an acoustic source, scattered by an object and received by the detector, which is usually coincident with the source. The time of flight can be used to determine the distance of the inhomogeneity from the source given knowledge of the speed through the medium.
Based on the measurement, a value is assigned to the location investigated. The transducer (or object) is moved slightly and then insonified again. This process is repeated in a systematic pattern until the entire region of interest has been investigated. Often the values for each point are assembled into an image of the object. The contrast seen in the image is based either on the object's geometry or material composition. The resolution of the image is limited either by the physical scanning resolution or the width of the sound beam (which in turn is determined by the frequency of the sound).

Applications
Device testing
SAM is used for counterfeit detection, product reliability testing, process validation, vendor qualification, quality control, failure analysis, research, and development. Detecting discontinuities in silicon is just one of the ways scanning acoustic microscopy is being used for testing in the semiconductor market.

Medicine and biology
SAM can provide data on the elasticity of cells and tissues, which can give useful information on the physical forces holding structures in a particular shape and the mechanics of structures such as the cytoskeleton. These studies are particularly valuable in investigating processes such as cell motility.
Some work has also been performed to assess penetration depth of particles injected into skin using needle-free injection

See also
Acoustic microscopy


== References ==",Category:Articles needing expert attention with no reason or talk parameter,1
200,201,Stretched tuning,"Stretched tuning is a detail of musical tuning, applied to wire-stringed musical instruments, older, non-digital electric pianos (such as the Fender Rhodes piano and Wurlitzer electric piano), and some sample-based synthesizers based on these instruments, to accommodate the natural inharmonicity of their vibrating elements. In stretched tuning, two notes an octave apart, whose fundamental frequencies theoretically have an exact 2:1 ratio, are tuned slightly farther apart (a stretched octave). ""For a stretched tuning the octave is greater than a factor of 2; for a compressed tuning the octave is smaller than a factor of 2.""
Melodic stretch refers to tunings with fundamentals stretched relative to each other, while harmonic stretch refers to tunings with harmonics stretched relative to fundamentals which are not stretched. For example, the piano features both stretched harmonics and, to accommodate those, stretched fundamentals.

Fundamentals and harmonics
In most musical instruments, the tone-generating component (a string or resonant column of air) vibrates at many frequencies simultaneously: a fundamental frequency that is usually perceived as the pitch of the note, and harmonics or overtones that are multiples of the fundamental frequency and whose wavelengths therefore divide the tone-generating region into simple fractional segments (1/2, 1/3, 1/4, etc.). (See harmonic series.) The fundamental note and its harmonics sound together, and the amplitude relationships among them strongly affect the perceived tone or timbre of the instrument.
In the acoustic piano, harpsichord, and clavichord, the vibrating element is a metal wire or string; in many non-digital electric pianos, it is a tapered metal tine (Rhodes piano) or reed (Wurlitzer electric piano) with one end clamped and the other free to vibrate. Each note on the keyboard has its own separate vibrating element whose tension and/or length and weight determines its fundamental frequency or pitch. In electric pianos, the motion of the vibrating element is sensed by an electromagnetic pickup and amplified electronically.

Intervals and inharmonicity
In tuning, the relationship between two notes (known musically as an interval) is determined by evaluating their common harmonics. For example, we say two notes are an octave apart when the fundamental frequency of the upper note exactly matches the second harmonic of the lower note. Theoretically, this means the fundamental frequency of the upper note is exactly twice that of the lower note, and we would assume that the second harmonic of the upper note will exactly match the fourth harmonic of the lower note.
On instruments strung with metal wire, however, neither of these assumptions is valid, and inharmonicity is the reason.
Inharmonicity refers to the difference between the theoretical and actual frequencies of the harmonics or overtones of a vibrating tine or string. The theoretical frequency of the second harmonic is twice the fundamental frequency, and of the third harmonic is three times the fundamental frequency, and so on. But on metal strings, tines, and reeds, the measured frequencies of those harmonics are slightly higher, and proportionately more so in the higher than in the lower harmonics. A digital emulation of these instruments must recreate this inharmonicity if it is to sound convincing.
The theory of temperaments in musical tuning do not normally take into account inharmonicity, which varies from instrument to instrument (and from string to string), but in practice the amount of inharmonicity present in a particular instrument will effect a modification to the theoretical temperament which is being applied to it.

Vibration of wire strings
When a stretched wire string is excited into motion by plucking or striking, a complex wave travels outward to the ends of the string. As it travels outward, this initial impulse forces the wire out of its resting position all along its length. After the impulse has passed, each part of the wire immediately begins to return toward (and overshoot) its resting position, which means vibration has been induced. Meanwhile, the initial impulse is reflected at both ends of the string and travels back toward the center. On the way, it interacts with the various vibrations it induced on the initial pass, and these interactions reduce or cancel some components of the impulse wave and reinforce others. When the reflected impulses encounter each other, their interaction again cancels some components and reinforces others.[1]
Within a few transits of the string, all these cancellations and reinforcements sort the vibration into an orderly set of waves that vibrate over 1/1, 1/2, 1/3, 1/4, 1/5, 1/6, etc. of the length of the string. These are the harmonics. As a rule, the amplitude of its vibration is less for higher harmonics than for lower, meaning that higher harmonics are softer—though the details of this differ from instrument to instrument. The exact combination of different harmonics and their amplitudes is a primary factor affecting the timbre or tone quality of a particular musical tone.
Theoretically, vibration over half the string's length will be twice as fast, and vibration over one-third of the string will be three times as fast, as the fundamental vibration over the whole string's length. In the theoretical string, however, the only force acting to return a part of the string to its rest position is the tension between its ends.
If you try bending a short piece of piano wire or guitar string slightly with your fingers, you can feel the wire's resistance to bending. In a vibrating string, that resistance adds to the effect of string tension in returning a given part of the string toward its rest position. The result is a frequency of vibration higher than the theoretical frequency. And because the wire's resistance to bending increases as its length decreases, its effect is greater in higher harmonics than in lower.

Tines and reeds
Tines and reeds differ from strings in that they are held at one end and free to vibrate at the other. The frequencies of their fundamental and harmonic vibrations are subject to the same inharmonicity as strings. However, because of the comparative thickness of the bars that terminate the tines in an electric piano, the larger (and stronger) vibrations tend to ""see"" termination points slightly deeper in the bar than do smaller, weaker vibrations. This enhances inharmonicity in tines.

Effects on tuning
Inharmonicity ""stretches"" harmonics beyond their theoretical frequencies, and higher harmonics are stretched proportionally more than lower. Thus, in our example of an octave, exactly matching the lowest common harmonic causes a slight amount of stretch, matching the next higher common harmonic causes a greater amount of stretch, and so on. If the interval is a double octave, exactly matching the upper note to the fourth harmonic of the lower complicates the tuning of that upper note with the one an octave below it.
Solving such dilemmas is at the heart of precise tuning by ear, and all solutions involve some stretching of the higher notes upward and the lower notes downward from their theoretical frequencies. In shorter pianos the wire stiffness in the bass register is proportionately high and therefore causes greater stretch; on larger concert grand pianos this effect is reduced. Online sources[2] suggest that the total amount of ""stretch"" over the full range of a small piano may be on the order of ±35 cents: this also appears in the empirical Railsback curve.

See also
Electronic tuner
Piano acoustics
Piano tuning

References
Further information
Five lectures on the acoustics of the piano
Inharmonicity in piano tuning

External links
""Octave Types"", BillBremmer.com.",Category:Acoustics,1
201,202,Whispering-gallery wave,"Whispering-gallery waves, or whispering-gallery modes, are a type of wave that can travel around a concave surface. Originally discovered for sound waves in the whispering gallery of St Paul’s Cathedral, they can exist for light and for other waves, with important applications in nondestructive testing, lasing, cooling and sensing, as well as in astronomy.

Introduction
Whispering-gallery waves were first explained for the case of St Paul's Cathedral circa 1878 by Lord Rayleigh, who revised a previous misconception that whispers could be heard across the dome but not at any intermediate position. He explained the phenomenon of travelling whispers with a series of specularly reflected sound rays making up chords of the circular gallery. Clinging to the walls the sound should decay in intensity only as the inverse of the distance — rather than the inverse square as in the case of a point source of sound radiating in all directions. This accounts for the whispers being audible all round the gallery.
Rayleigh developed wave theories for St Paul’s in 1910 and 1914. Fitting sound waves inside a cavity involves the physics of resonance based on wave interference; the sound can exist only at certain pitches as in the case of organ pipes. The sound forms patterns called modes, as shown in the diagram.
Many other monuments have been shown to exhibit whispering-gallery waves, such as the Gol Gumbaz in Bijapur and the Temple of Heaven in Beijing.
In the strict definition of whispering-gallery waves, they cannot exist when the guiding surface becomes straight. Mathematically this corresponds to the limit of an infinite radius of curvature. Whispering-gallery waves are guided by the effect of the wall curvature.

Other acoustic whispering-gallery waves
Whispering-gallery waves for sound exist in a wide variety of systems. Examples include the vibrations of the whole Earth or stars.
Such acoustic whispering-gallery waves can be used in nondestructive testing in the form of waves that creep around holes filled with liquid, for example. They have also been detected in solid cylinders and spheres, with applications in sensing, and visualized in motion on microscopic discs [2].
Whispering gallery waves are more efficiently guided in spheres than in cylinders because the effects of acoustic diffraction (lateral wave spreading) are then completely compensated.

Whispering-gallery waves for light
Whispering-gallery waves exist for light waves. They have been produced in microscopic glass spheres or toruses, for example, with applications in lasing, optomechanical cooling, frequency comb generation and sensing. The light waves are almost perfectly guided round by optical total internal reflection, leading to Q factors in excess of 1010 being achieved. This is far greater than the best values, about 104, that can be similarly obtained in acoustics. Optical modes in a whispering gallery resonator are inherently lossy due to a mechanism similar to quantum tunneling. As a result, light inside a whispering gallery mode experiences a degree of radiation loss even in theoretically ideal conditions. Such a loss channel has been known from research on optical waveguide theory and is dubbed tunneling ray attenuation in the field of fiber optics. The Q factor is proportional to the decay time of the waves, which in turn is inversely proportional to both the surface scattering rate and the wave absorption in the medium making up the gallery.  Whispering-gallery waves for light have been investigated in chaotic galleries, whose cross-sections deviate from a circle. And such waves have been used in quantum information applications.
Whispering-gallery waves have also been demonstrated for other electromagnetic waves such as radio waves, microwaves, terahertz radiation, infrared radiation, ultraviolet waves and x-rays.

Whispering-gallery waves for other systems
Whispering-gallery waves have been seen in the form of matter waves for neutrons, and electrons, and they have been proposed as an explanation for vibrations of a single nucleus. Analogies of whispering-gallery waves also exist for gravitational waves at the event horizon of black holes. A hybrid of waves of light and electrons known as surface plasmons has been demonstrated in the form of whispering-gallery waves, and likewise for exciton-polaritons in semiconductors. Galleries simultaneously containing both acoustic and optical whispering-gallery waves have also been made, exhibiting very strong mode coupling and coherent effects. Hybrid solid-fluid-optical whispering-gallery structures have been observed as well.

See also
Whispering gallery
Optical ring resonator
Resonator
Architectural acoustics

References
External links
Investigations of Whisper Gallery Mirrors for EUV and Soft X-Rays, T.Y. Hung and P.L. Hagelstein
Applied Solid State Physics Laboratory at Hokkaido University, Watching Whispering-Gallery Waves
Armani Lab, University of Southern California
Baba Lab, Yokohama National University
Capasso Group, Harvard University
Coherent Microoptics and Radiophotonics Group, RQC
Gallery of Whispers, Physics World 25, No. 2, Feb. 2012, p. 31
Gong Qihuang Lab, Beijing University
Harald Schwefel, Max Planck Institute for the Science of Light, Erlangen
Hui Cao Research Laboratory, Yale University
JPL Quantum Science and Technology Group
Kyungwon An Laboratory, Seoul National University
Laboratory of Photonics and Quantum Measurements K-Lab, École Polytechnique Fédérale de Lausanne (EPFL)
Lan Yang Laboratory, Washington University in St. Louis
Micro-optics and Quantum Chaos Group, University of Oregon
Steve Arnold's Microparticle Photophysics Laboratory for BioPhotonics
St Paul's Cathedral
The Aerosol Dynamics Research Group, University of Bristol.
Vahala Research Group, California Institute of Technology
Vollmer Lab of Biophotonics and Biosensing
Ultrafast Lasers and Optical Amplifiers Lab, IIT Madras, India
Yamanaka Lab, Tohoku University
Yong-Hee Lee Lab, KAIST",Category:Acoustics,1
202,203,Geometrical acoustics,"Geometrical acoustics or ray acoustics is a branch of acoustics that studies propagation of sound on the basis of the concept of rays considered as lines along which the acoustic energy is transported. This concept is similar to the concept of geometrical optics, or ray optics, that studies light propagation in terms of rays. Geometrical acoustics is the approximate theory, which is valid in the limiting case of very small acoustic wavelengths, or very high frequencies. The principal task of geometrical acoustics is to determine the trajectories of sound rays. The rays have the simplest form in a homogeneous medium, where they are straight lines. If the acoustic parameters of the medium are functions of spatial coordinates, the ray trajectories become curvilinear, describing sound reflection, refraction, possible focusing, etc. The equations of geometric acoustics have essentially the same form as those of geometric optics. The same laws of reflection and refraction hold for sound rays as for light rays. Geometrical acoustics does not take into account such important wave effects as diffraction. However, it provides a very good approximation when the wavelength is very small compared to the characteristic dimensions of inhomogeneous inclusions through which the sound propagates.

Mathematical description
The below discussion is due to Landau. If the amplitude and the direction of propagation varies slowly over the distances of wavelength, then an arbitrary sound wave can be approximated locally as a plane wave. In this case, the velocity potential can be written as

  
    
      
        ?
        =
        
          
            e
          
          
            
              i
            
            ?
          
        
      
    
    {\displaystyle \phi =\mathrm {e} ^{\mathrm {i} \psi }}
  
For plane wave 
  
    
      
        ?
        =
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        ?
      
    
    {\displaystyle \psi ={\boldsymbol {k}}\cdot {\boldsymbol {r}}-\omega t+\alpha }
  , where 
  
    
      
        
          k
        
      
    
    {\displaystyle {\boldsymbol {k}}}
   is a constant wavenumber vector, 
  
    
      
        ?
      
    
    {\displaystyle \omega }
   is a constant frequency, 
  
    
      
        
          r
        
      
    
    {\displaystyle {\boldsymbol {r}}}
   is the radius vector, 
  
    
      
        t
      
    
    {\displaystyle t}
   is the time and 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
   is some arbitrary complex constant. The function 
  
    
      
        ?
      
    
    {\displaystyle \psi }
   is called the eikonal. We expect the eikonal to vary slowly with coordinates and time consistent with the approximation, then in that case, a Taylor series expansion provides

  
    
      
        ?
        =
        
          ?
          
            o
          
        
        +
        
          r
        
        ?
        
          
            
              ?
              ?
            
            
              ?
              
                r
              
            
          
        
        +
        t
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        .
      
    
    {\displaystyle \psi =\psi _{o}+{\boldsymbol {r}}\cdot {\frac {\partial \psi }{\partial {\boldsymbol {r}}}}+t{\frac {\partial \psi }{\partial t}}.}
  
Equating the two terms for 
  
    
      
        ?
      
    
    {\displaystyle \psi }
  , one finds

  
    
      
        
          k
        
        =
        
          
            
              ?
              ?
            
            
              ?
              
                r
              
            
          
        
        ,
        
        ?
        =
        ?
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
      
    
    {\displaystyle {\boldsymbol {k}}={\frac {\partial \psi }{\partial {\boldsymbol {r}}}},\quad \omega =-{\frac {\partial \psi }{\partial t}}}
  
For sound waves, the relation 
  
    
      
        
          ?
          
            2
          
        
        =
        
          c
          
            2
          
        
        
          k
          
            2
          
        
      
    
    {\displaystyle \omega ^{2}=c^{2}k^{2}}
   holds, where 
  
    
      
        c
      
    
    {\displaystyle c}
   is the speed of sound and 
  
    
      
        k
      
    
    {\displaystyle k}
   is the magnitude of the wavenumber vector. Therefore, the eikonal satisfies a first order nonlinear partial differential equation,

  
    
      
        
          
            (
            
              
                
                  ?
                  ?
                
                
                  ?
                  x
                
              
            
            )
          
          
            2
          
        
        +
        
          
            (
            
              
                
                  ?
                  ?
                
                
                  ?
                  y
                
              
            
            )
          
          
            2
          
        
        +
        
          
            (
            
              
                
                  ?
                  ?
                
                
                  ?
                  z
                
              
            
            )
          
          
            2
          
        
        ?
        
          
            1
            
              c
              
                2
              
            
          
        
        
          
            (
            
              
                
                  ?
                  ?
                
                
                  ?
                  t
                
              
            
            )
          
          
            2
          
        
        =
        0.
      
    
    {\displaystyle \left({\frac {\partial \psi }{\partial x}}\right)^{2}+\left({\frac {\partial \psi }{\partial y}}\right)^{2}+\left({\frac {\partial \psi }{\partial z}}\right)^{2}-{\frac {1}{c^{2}}}\left({\frac {\partial \psi }{\partial t}}\right)^{2}=0.}
  
where 
  
    
      
        c
      
    
    {\displaystyle c}
   can be a function of coordinates if the fluid is not homogeneous. The above equation is same as Hamilton–Jacobi equation where the eikonal can be considered as the action. Since Hamilton–Jacobi equation is equivalent to Hamilton's equations, by analogy, one finds that

  
    
      
        
          
            
              
                d
              
              
                k
              
            
            
              
                d
              
              t
            
          
        
        =
        ?
        
          
            
              ?
              ?
            
            
              ?
              
                r
              
            
          
        
        ,
        
        
          
            
              
                d
              
              
                r
              
            
            
              
                d
              
              t
            
          
        
        =
        
          
            
              ?
              ?
            
            
              ?
              
                k
              
            
          
        
      
    
    {\displaystyle {\frac {\mathrm {d} {\boldsymbol {k}}}{\mathrm {d} t}}=-{\frac {\partial \omega }{\partial {\boldsymbol {r}}}},\quad {\frac {\mathrm {d} {\boldsymbol {r}}}{\mathrm {d} t}}={\frac {\partial \omega }{\partial {\boldsymbol {k}}}}}

Practical applications
Practical applications of the methods of geometrical acoustics can be found in very different areas of acoustics. For example, in architectural acoustics the rectilinear trajectories of sound rays make it possible to determine reverberation time in a very simple way. The operation of fathometers and hydrolocators is based on measurements of the time required for sound rays to travel to a reflecting object and back. The ray concept is used in designing sound focusing systems. Also, the approximate theory of sound propagation in inhomogeneous media (such as the ocean and the atmosphere) has been developed largely on the basis of the laws of geometrical acoustics.
The methods of geometrical acoustics have a limited range of applicability because the ray concept itself is only valid for those cases where the amplitude and direction of a wave undergo little changes over distances of the order of wavelength of a sound wave. More specifically, it is necessary that the dimensions of the rooms or obstacles in the sound path should be much greater than the wavelength. If the characteristic dimensions for a given problem become comparable to the wavelength, then wave diffraction begins to play an important part, and this is not covered by geometric acoustics.

Software applications
The concept of geometrical acoustics is widely used in software applications. Some software applications that use geometrical acoustics for their calculations are ODEON, Enhanced Acoustic Simulator for Engineers, and Olive Tree Lab Terrain.

References
External links
ODEON Room Acoustics Software
EASE – Industry Standard for Acoustical Simulation of Rooms
Olive Tree Lab Terrain",Category:Acoustics,1
203,204,Proximity effect (audio),"The proximity effect in audio is an increase in bass or low frequency response when a sound source is close to a directional or cardioid microphone.

Technical explanation
Proximity effect is a change in the frequency response of a directional pattern microphone that results in an emphasis on lower frequencies. It is caused by the use of ports to create directional polar pickup patterns, so omni-directional microphones do not exhibit the effect (it should be noted that this is not necessarily true of the ""omni"" pattern on multipattern condenser mics, which create the ""omni"" pattern by summing two back-to-back cardioid capsules, which may or may not share a common backplate.) Depending on the microphone design, proximity effect may result in a boost of up to 16 dB or more at lower frequencies, depending on the size of the microphone's diaphragm and the distance of the source. A ready (and common) example of proximity effect can be observed with cardioid dynamic vocal microphones (though it is not limited to this class of microphone) when the vocalist is very close to or even touching the mic with their lips. The effect is heard as a 'fattening up' of the voice. Many radio broadcast microphones are large diameter cardioid pickup pattern microphones, and radio announcers are often observed to employ proximity effect, adding a sense of gravitas and depth to the voice. Proximity effect is sometimes referred to as ""bass tip-up.""

Angular dependence
To explain how the proximity effect arises in directional microphones, it is first necessary to briefly describe how a directional microphone works. A microphone is constructed with a diaphragm whose mechanical movement is converted to electrical signals (via a magnetic coil, for example). The movement of the diaphragm is a function of the air pressure difference across the diaphragm arising from incident sound waves. In a directional microphone, sound reflected from surfaces behind the diaphragm is permitted to impinge on the rear side of the diaphragm. Since the sound reaching the rear of the diaphragm travels slightly farther than the sound at the front, it is slightly out of phase. The greater this phase difference, the greater the pressure difference and the greater the diaphragm movement. As the sound source moves off the diaphragm axis, this phase difference decreases due to decreasing path length difference. This is what gives a directional microphone its directivity.
In addition to the angular dependence described above, the response of a directional microphone depends on the amplitude, frequency and distance of the source. These latter two dependencies are used to explain the proximity effect.

Phase difference
As described above, the phase difference across the diaphragm gives rise to the pressure difference that moves the diaphragm. This phase difference increases with frequency as the difference in path length becomes a larger portion of the wavelength of the sound. This frequency dependence is offset by damping the diaphragm 6 dB per octave to achieve a flat frequency response (but this is not germane to the proximity effect so nothing more will be said about it here). The point to be made regarding the frequency dependency is that the phase difference across the diaphragm is the smallest at low frequencies.

Amplitude difference
In addition to phase differences, amplitude differences also result in pressure differences across the diaphragm. This amplitude component arises from the fact that the far side of the diaphragm is farther from the sound source than the front side. Since sound pressure level decreases as the inverse of the distance from the source (it is sound intensity level that drops as the inverse of the distance squared, for those familiar with the inverse square law), the amplitude of the sound will be slightly less at the rear of the diaphragm as compared to the front of the diaphragm. Since the pressure difference due to the amplitude component is dependent only on the amplitude differential with respect to the two sides of the diaphragm, it is independent of frequency, per se.
The properties of the amplitude component that are applicable to the proximity effect are that the contribution to the pressure difference is small and independent of frequency. At large distances between the source and the microphone, the amplitude component of the pressure difference is negligible compared to the phase component at all audio frequencies. As the source is brought closer to the directional microphone, the amplitude component of the pressure difference increases and becomes the dominant component at lower frequencies (recall that the phase component is relatively small at the low frequencies). At higher frequencies, the phase component of the pressure difference continues to dominate for all practical distances between source and microphone.
The result is that the frequency response of the microphone changes; specifically, it increases at the low frequency (bass) end, as the audio source is brought closer to the microphone. This is the proximity effect as it pertains to audio.


== References ==",Category:Articles with unsourced statements from December 2009,1
204,205,Equivalent input,"Equivalent Input (also Input-Referred or Input-Related), is a method of referring to the signal or noise level at the output of a system as if it were an input to the same system. This is accomplished by removing all signal changes (e.g. amplifier gain, transducer sensitivity, etc.) to get the units to match the input.

Examples
Equivalent Input Noise (EIN)
A microphone converts acoustical energy to electrical energy. Microphones have some level of electrical noise at their output. This noise may have contributions from random diaphragm movement, thermal noise, or a dozen other sources, but those can all be thought of as an imaginary acoustic noise source injecting sound into the (now noiseless) microphone. The units on this noise are no longer volts, but units of sound pressure (pascals or dBSPL), which can be directly compared to the desired sound pressure inputs.

Input-Related Interference Level (IRIL)
A device which uses a microphone may be susceptible to electromagnetic interference which causes sonic artifacts. The problem is not in the microphone, but the interference level can be related back to the input to compare to the level of typical inputs to see how audible the artifact is.


== References ==",Category:Acoustics,1
205,206,Microbarom,"In acoustics, microbaroms, also known as the ""voice of the sea"", are a class of atmospheric infrasonic waves generated in marine storms by a non-linear interaction of ocean surface waves with the atmosphere. They typically have narrow-band, nearly sinusoidal waveforms with amplitudes up to a few microbars, and wave periods near 5 seconds (0.2 hertz). Due to low atmospheric absorption at these low frequencies, microbaroms can propagate thousands of kilometers in the atmosphere, and can be readily detected by widely separated instruments on the Earth's surface.
Microbaroms are a significant noise source that can potentially interfere with the detection of infrasound from nuclear explosions that is a goal of the International Monitoring System organized under the Comprehensive Nuclear-Test-Ban Treaty (which has not entered into force). It is a particular problem for detecting low-yield tests in the one-kiloton range because the frequency spectra overlap.

History
Microbaroms were first described in 1939 by American seismologists Hugo Benioff and Beno Gutenberg at the California Institute of Technology at Pasadena, based on observations from an electromagnetic microbarograph, consisting of a wooden box with a low-frequency loudspeaker mounted on top. They noted their similarity to microseisms observed on seismographs, and correctly hypothesized that these signals were the result of low pressure systems in the Northeast Pacific Ocean. In 1945, Swiss geoscientist L. Saxer showed the first relationship of microbaroms with wave height in ocean storms and microbarom amplitudes. Eric S. Posmentier published his ""theory of microbaroms"" in 1967 based on the oscillations of the center of gravity of the air above the Ocean surface on which the standing waves appear, which fits well with observed data, including the doubling of the ocean wave frequency in the observed microbarom frequency.

Theory
Isolated traveling, ocean surface gravity waves radiate only evanescent acoustic waves, and don't generate microbaroms. Microbaroms are generated by nonlinear interactions of ocean surface waves traveling in nearly opposite directions with similar frequencies in the lee of a storm, which produce the required standing wave conditions, also known as the clapotis. When the ocean storm is a tropical cyclone, the microbaroms are not produced near the eye wall where wind speeds are greatest, but originate from the trailing edge of the storm where the storm generated waves interact with the ambient ocean swells.
Microbaroms may also be produced by standing waves created between two storms, or when an ocean swell is reflected at the shore. Waves with approximately 10-second periods are abundant in the open oceans, and correspond to the observed 0.2 Hz infrasonic spectral peak of microbaroms, because microbaroms exhibit frequencies twice that of the individual ocean waves. Studies have shown that the coupling produces propagating atmospheric waves only when non-linear terms are considered.
Microbaroms are a form of persistent low-level atmospheric infrasound, generally between 0.1 and 0.5 Hz, that may be detected as coherent energy bursts or as a continuous oscillation. When the plane wave arrivals from a microbarom source are analyzed from a phased array of closely spaced microbarographs, the source azimuth is found to point toward the low-pressure center of the originating storm. When the waves are received at multiple distant sites from the same source, triangulation can confirm the source is near the center of an ocean storm.
Microbaroms that propagate up to the lower thermosphere may be carried in an atmospheric waveguide, refracted back toward the surface from below 120 km and above 150 km altitudes, or dissipated at altitudes between 110 and 140 km. They may also be trapped near the surface in the lower troposphere by planetary boundary layer effects and surface winds, or they may by ducted in the stratosphere by upper level winds and returned to the surface through refraction, diffraction or scattering. These tropospheric and stratospheric ducts are only generated along the dominant wind directions, may vary by time of day and season, and will not return the sound rays to the ground when the upper winds are light.
The angle of incidence of the microbarom ray determines which of these propagation modes it experiences. Rays directed vertically toward the zenith are dissipated in the thermosphere, and are a significant source of heating in that layer of the upper atmosphere. At mid latitudes in typical summer conditions, rays between approximately 30 and 60 degrees from the vertical are reflected from altitudes above 125 km where the return signals are strongly attenuated first. Rays launched at shallower angles may be reflected from the upper stratosphere at approximately 45 km above the surface in mid latitudes, or from 60–70 km in low latitudes.
Atmospheric scientists have used these effects for inverse remote sensing of the upper atmosphere using microbaroms. Measuring the trace velocity of the reflected microbarom signal at the surface gives the propagation velocity at the reflection height, as long as the assumption that the speed of sound only varies along the vertical, and not over the horizontal, is valid. If the temperature at the reflection height can be estimated with sufficient precision, the speed of sound can be determined and subtracted from the trace velocity, giving the upper level wind speed. One advantage of this method is the ability to measure continuously – other methods that can only take instantaneous measurements may have their results distorted by short-term effects.
Additional atmospheric information can be deduced from microbarom amplitude if the source intensity is known. Microbaroms are produced by upward directed energy transmitted from the ocean surface through the atmosphere. The downward directed energy is transmitted through the ocean to the sea floor, where it is coupled to the Earth's crust and transmitted as microseisms with the same frequency spectrum. However, unlike microbaroms, where the near vertical rays are not returned to the surface, only the near vertical rays in the ocean are coupled to the sea floor. By monitoring the amplitude of received microseisms from the same source using seismographs, information on the source amplitude can be derived. Because the solid earth provides a fixed reference frame, the transit time of the microseisms from the source is constant, and this provides a control for the variable transit time of the microbaroms through the moving atmosphere.

Further reading
Benioff H.; Gutenberg B. (1939). ""Waves and currents recorded by electromagnetic barographs"". Bull. Am. Meteor. Soc. 20: 421. 
Saxer, L. (1945). ""Electrical measurement of small atmospheric pressure oscillations"". Helv. Phys. Acta. 18: 527–550. 
Posmentier, E.S. (1967). ""A Theory of Microbaroms, Geophys"". JR Ast. Soc. 13. 
Donn, W.L.; Naini, B. (1973). ""Sea wave origin of microbaroms and microseisms"". J. Geophys. Res. 78 (21): 4482–4488. Bibcode:1973JGR....78.4482D. doi:10.1029/JC078i021p04482. 


== References ==",Category:Acoustics,1
206,207,QuietRock,"QuietRock is a brand of internally damped drywall panels by PABCO Gypsum which acquired the line from Serious Materials, an American manufacturer of building materials started by Kevin Surace. It is designed to provide high levels of sound transmission loss between rooms. the first product was introduced in 2003.

Design
QuietRock uses a damping technique called constrained-layer damping (CLD). This technology had been used for at least 20 years to reduce vibrations in mechanical objects from disk drive heads to bridges, but had not been applied to architectural acoustics prior to 2003. QuietRock panels use several tuned constrained-layer systems to create a higher ability to damp vibrational (and therefore acoustic) energy. In essence, the panel does not ""want"" to vibrate due to stress and strain caused by the damping method. Acoustic energy ends up dissipating as small amounts of heat.
Sound attenuation is measured using tests known as ASTM E90 and ASTM E413 to achieve a single sound-transmission-class (STC) rating. Note that the STC method has changed significantly over the years, and STCs from publications prior to 1995 may not be accurate to today's standards. QuietRock is one of a class of soundproof drywall products that, according to independent lab tests and field reports, may add 15 to 20 STC points in comparison to standard drywall. STC does not consider the most problematic frequencies below 125 Hz.
Unlike resilient channels (RC) and other methods, internally damped drywall cannot be shorted out by the builder or homeowner. Therefore, the use of ""soundproof drywall"" may be a more reliable method for sustained high STCs.
QuietRock panels are manufactured primarily from gypsum drywall and viscoelastic polymers. One QuietRock model (530)  uses a thin sheet of metal for added shear and impact resistance. Many QuietRock models are UL certified for fire rated assemblies. The company said in 2008 it had shipped more than 1 million panels. and 2 million panels had shipped by 2010.
QuietRock ES (EZ-SNAP) was developed and invented primarily by Kevin Surace and introduced in 2009. All QuietRock EZ-SNAP panels are manufactured as a single panel without paper or metal in the center. This patented technology makes the panel easier to score and snap using a common drywall knife – no special tools required. EZSnap and Video of using. The company has several patents covering the invention and manufacture including U.S. Patent 7,908,818.

Uses and availability
QuietRock panels can be used in any new construction or remodeling project where ""soundproofing"" is the main objective. Typical use for residential includes acoustic treatment for home theaters. QuietRock models baring a type X designation for fire resistance are ideal for commercial projects such as hotels, hospitals, schools, condominium party walls where additional fire resistance is mandated by local building code. There are several types of QuietRock, including a THX-certified version introduced in 2005 (now called QuietRock 545) used to build walls rated to STC 80.

See also
Soundproofing
Acoustic transmission
Noise barrier
Noise pollution
Noise regulation
Noise, vibration, and harshness


== References ==",Category:Noise reduction,1
207,208,Variable-geometry acoustical dome,"Variable-geometry acoustical dome is a dome-like or shaped ceiling, installed in a new or an existing building that modifies the acoustical performance of that space. Acoustical domes are used in auditoriums and theaters as surface to either reverberate the sound to a whole audience (acoustical shell) or to absorb acoustical wave and avoid echoes.

Concept
“Variable-geometry acoustical domes” is a design concept developed by David Serero in 2004, at the Villa Medici in Rome. Suspended to the vaulted ceiling of the grand salon of Villa Medici, the device is an articulated faceted surface which form and position can be modulated by 12 control points tight to ropes. This transformation of shape allows for modification of the acoustical behavior of the hall in real time depending on the type of event, such as concerts, lectures and plays. As Serero states: “This project defines architectural form not in a final state, but rather as an ephemeral and variable condition, where the possibility of its transformation is inscribed inside of its geometry.”

External links
http://www.serero.com
http://www.villamedici.it",Category:Acoustics,1
208,209,Home ultrasound,"Home ultrasound is the provision of therapeutic ultrasound via the use of a portable or home ultrasound machine. This method of medical ultrasound therapy can be used for various types of pain relief and physical therapy.
In physics, the term ""ultrasound"" applies to all acoustic energy with a frequency above the audible range of human hearing. The audible range of sound is 20 hertz – 20 kilohertz. Ultrasound frequency is greater than 20 kilohertz.

Home ultrasound machines
Ultrasound energy is transferred based on the frequency and power output of the ultrasonic waves that an ultrasound machine or device creates. Home ultrasound machines and doctor's office machines both operate between 1 and 5 megahertz, however, home machines utilize pulsed ultrasonic waves while professional ultrasound machines in a doctor's office use continuous waves.
Typically, when using a home ultrasound machine, you will use it more frequently than if you were to have ultrasound treatments at a therapist's office, but the end results are the same as if using a continuous wave machine less frequently.

Home ultrasound benefits
Home ultrasound machines may have several benefits: long-term cost savings, portable physical therapy treatment, long-term pain relief for multiple symptoms, possible decrease in healing time, and can reduce chronic inflammation.

Different types of ultrasound therapy
Home ultrasound machines operate within the range of frequencies of therapeutic ultrasound, as opposed to the more commonly known diagnostic ultrasound, or Diagnostic sonography. Typical diagnostic ultrasound machines operate in the frequency range of 2-18 megahertz, whereas home ultrasound machines and therapeutic ultrasound machines operate in the frequency range of .7-3.3 megahertz. Diagnostic sonography is typically used to create an audio ""image"", such as during pregnancy to visualize the developing baby.

Home ultrasound and phonophoresis
Phonophoresis, also known as sonophoresis, is the use of ultrasound to enhance the delivery of topically applied drugs. Home ultrasound allows the application of topically applied analgesics and anti-inflammatory agents through the therapeutic application of ultrasound. It is widely used in hospitals to deliver drugs through the skin. Pharmacists compound the drugs by mixing them with a coupling agent (gel, cream, ointment) that transfers ultrasonic energy from the ultrasound transducer to the skin. The ultrasound potentially enhances drug transport by cavitation, microstreaming, and heating.

References
External links
American Institute of Ultrasound in Medicine Professional Association for Ultrasound in Medicine",Category:Orphaned articles from July 2011,1
209,210,Conditioned play audiometry,,Category:Acoustics,1
210,211,Ultrasound attenuation spectroscopy,"Ultrasound attenuation spectroscopy is a method for characterizing properties of fluids and dispersed particles. It is also known as acoustic spectroscopy
There is an international standard for this method.
Measurement of attenuation coefficient versus ultrasound frequency yields raw data for further calculation of various system properties. Such raw data are often used in the calculation of the particle size distribution in heterogeneous systems such as emulsions and colloids. In the case of acoustic rheometers, the raw data are converted into extensional viscosity or volume viscosity.
Instruments that employ ultrasound attenuation spectroscopy are referred to as Acoustic spectrometers.

References
External links
Ultrasonic Spectrometer",Category:Chemistry stubs,1
211,212,Outdoor–indoor transmission class,"Outdoor–indoor transmission class (OITC) is a standard used for indicating the rate of transmission of sound between outdoor and indoor spaces in a structure. It is based on the ASTM E-1332 Standard Classification for the Determination of Outdoor–Indoor Transmission Class. An alternative similar standard for determining the rate of acoustic isolation of a separation between spaces is Sound transmission class (STC). While STC is based on a noise spectrum targeting speech sounds, OITC utilizes a source noise spectrum that considers frequencies down to 80 Hz (Aircraft/Rail/Truck traffic) and is weighted more to lower frequencies.

External links
ASTM International: [1]
Viraconsulting Glossary: [2]
Consulting: [3]",Category:Acoustics,1
212,213,Bioacoustics,"Bioacoustics is a cross-disciplinary science that combines biology and acoustics. Usually it refers to the investigation of sound production, dispersion and reception in animals (including humans). This involves neurophysiological and anatomical basis of sound production and detection, and relation of acoustic signals to the medium they disperse through. The findings provide clues about the evolution of acoustic mechanisms, and from that, the evolution of animals that employ them.
In underwater acoustics and fisheries acoustics the term is also used to mean the effect of plants and animals on sound propagated underwater, usually in reference to the use of sonar technology for biomass estimation. The study of substrate-borne vibrations used by animals is considered by some a distinct field called biotremology.

History
For a long time humans have employed animal sounds to recognise and find them. Bioacoustics as a scientific discipline was established by the Slovene biologist Ivan Regen who began systematically to study insect sounds. In 1925 he used a special stridulatory device to play in a duet with an insect. Later, he put a male cricket behind a microphone and female crickets in front of a loudspeaker. The females were not moving towards the male but towards the loudspeaker. Regen's most important contribution to the field apart from realization that insects also detect airborne sounds was the discovery of tympanal organ's function.
Relatively crude electro-mechanical devices available at the time (such as phonographs) allowed only for crude appraisal of signal properties. More accurate measurements were made possible in the second half of the 20th century by advances in electronics and utilization of devices such as oscilloscopes and digital recorders.
The most recent advances in bioacoustics concern the relationships among the animals and their acoustic environment and the impact of anthropogenic noise. Bioacoustic techniques have recently been proposed as a non-destructive method for estimating biodiversity of an area.

Methods
Listening is still one of the main methods used in bioacoustical research. Little is known about neurophysiological processes that play a role in production, detection and interpretation of sounds in animals, so animal behaviour and the signals themselves are used for gaining insight into these processes.

Acoustic signals
An experienced observer can use animal sounds to recognize a ""singing"" animal species, its location and condition in nature. Investigation of animal sounds also includes signal recording with electronic recording equipment. Due to the wide range of signal properties and media they propagate through, specialized equipment may be required instead of the usual microphone, such as a hydrophone (for underwater sounds), detectors of ultrasound (very high-frequency sounds) or infrasound (very low-frequency sounds), or a laser vibrometer (substrate-borne vibrational signals). Computers are used for storing and analysis of recorded sounds. Specialized sound-editing software is used for describing and sorting signals according to their intensity, frequency, duration and other parameters.
Animal sound collections, managed by museums of natural history and other institutions, are an important tool for systematic investigation of signals. Many effective automated methods involving signal processing, data mining and machine learning techniques have been developed to detect and classify the bioacoustic signals.

Sound production, detection, and use in animals
Scientists in the field of bioacoustics are interested in anatomy and neurophysiology of organs involved in sound production and detection, including their shape, muscle action, and activity of neuronal networks involved. Of special interest is coding of signals with action potentials in the latter.
But since the methods used for neurophysiological research are still fairly complex and understanding of relevant processes is incomplete, more trivial methods are also used. Especially useful is observation of behavioural responses to acoustic signals. One such response is phonotaxis – directional movement towards the signal source. By observing response to well defined signals in a controlled environment, we can gain insight into signal function, sensitivity of the hearing apparatus, noise filtering capability, etc.

Biomass estimation
Biomass estimation is a method of detecting and quantifying fish and other marine organisms using sonar technology. As the sound pulse travels through water it encounters objects that are of different density than the surrounding medium, such as fish, that reflect sound back toward the sound source. These echoes provide information on fish size, location, and abundance. The basic components of the scientific echo sounder hardware function is to transmit the sound, receive, filter and amplify, record, and analyze the echoes. While there are many manufacturers of commercially available ""fish-finders,"" quantitative analysis requires that measurements be made with calibrated echo sounder equipment, having high signal-to-noise ratios.

Animal sounds
Sounds used by animals that fall within the scope of bioacoustics include a wide range of frequencies and media, and are often not ""sound"" in the narrow sense of the word (i.e. compression waves that propagate through air and are detectable by the human ear). Katydid crickets, for example, communicate by sounds with frequencies higher than 100 kHz, far into the ultrasound range. Lower, but still in ultrasound, are sounds used by bats for echolocation. On the other side of the frequency spectrum are low frequency-vibrations, often not detected by hearing organs, but with other, less specialized sense organs. The examples include ground vibrations produced by elephants whose principal frequency component is around 15 Hz, and low- to medium-frequency substrate-borne vibrations used by most insect orders. Many animal sounds, however, do fall within the frequency range detectable by a human ear, between 20 and 20,000 Hz. Mechanisms for sound production and detection are just as diverse as the signals themselves.

Plant sounds
In a series of scientific journal articles published between 2013 and 2016, Dr Monica Gagliano of the University of Western Australia extended the science to include plant bioacoustics.

See also
References
Further reading
Ewing A.W. (1989): Arthropod bioacoustics: Neurobiology and behaviour. Edinburgh: Edinburgh University Press. ISBN 0-7486-0148-1
Fletcher N. (2007): Animal Bioacoustics. IN: Rossing T.D. (ed.): Springer Handbook of Acoustics, Springer. ISBN 978-0-387-33633-6

External links
BioAcoustica: Wildlife Sounds Database
The British Library Sound Archive has 150,000 recordings of over 10,000 species.
International Bioacoustics Council links to many bioacoustics resources.
Borror Laboratory of Bioacoustics at The Ohio State University has a large archive of animal sound recordings.
Listen to Nature 400 examples of animal songs and calls
Wildlife Sound Recording Society
Bioacoustic Research Program at the Cornell Lab of Ornithology distributes a number of different free bioacoustics synthesis & analysis programs.
Macaulay Library at the Cornell Lab of Ornithology is the world's largest collection of animal sounds and associated video.
Xeno-canto A collection of bird vocalizations from around the world.",Category:Articles with Slovene-language external links,1
213,214,Underwater acoustic communication,"Underwater acoustic communication is a technique of sending and receiving messages below water. There are several ways of employing such communication but the most common is by using hydrophones. Underwater communication is difficult due to factors such as multi-path propagation, time variations of the channel, small available bandwidth and strong signal attenuation, especially over long ranges. Compared to terrestrial communication, underwater communication has low data rates because it uses acoustic waves instead of electromagnetic waves.
At the beginning of the 20th century, some ships communicated by underwater bells, the system being competitive with the primitive Maritime radionavigation service of the time. The later Fessenden oscillator allowed communication with submarines.

Types of modulation used for Underwater Acoustic Communications
In general the modulation methods developed for radio communications can be adapted for underwater acoustic communications (UAC). However some of the modulation schemes are more suited to the unique underwater acoustic communication channel than others. Some of the modulation methods used for UAC are as follows:
Frequency Shift Keying (FSK)
Phase Shift Keying (PSK)
Frequency Hopped Spread Spectrum (FHSS)
Direct Sequence Spread Spectrum (DSSS)
Frequency and Pulse-position modulation (FPPM and PPM)
Multiple Frequency Shift Keying (MFSK)
Orthogonal Frequency-Division Multiplexing (OFDM)
The following is a discussion on the different types of modulation and their utility to UAC.

Frequency Shift Keying as applied to UAC
FSK is the earliest form of modulation used for more advanced forms of UAC by acoustic modems. The earliest forms of UAC prior to FSK has been by percussion of different objects underwater and this method has been used to measure the speed of sound in water.
FSK usually employs two distinct frequencies to modulate data. For example, Frequency F1 to indicate bit 0 and frequency F2 to indicate bit 1. Hence a binary string can be transmitted by alternating these two frequencies depending on whether it is a 0 or 1. The receiver can be as simple as having analogue matched filters to the two frequencies and a level detector to decide if a 1 or 0 was received. This is a relatively easy form of modulation and therefore used in the earliest acoustic modems. However more sophisticated Demodulator using Digital Signal Processors (DSP) can be used in the present day.
The biggest challenge FSK faces in the UAC is multi-path reflections. With multi-path (particularly in UAC) several strong reflections can be present at the receiving hydrophone and the threshold detectors become confused, thus severely limiting the use of this type of UAC to vertical channels. Adaptive equalization methods have been tried with limited success. Adaptive equalization tries to model the highly reflective UAC channel and subtract the effects from the received signal. The success has been limited due to the rapidly varying conditions and the difficulty to adapt in time.

Phase Shift Keying
Phase-shift keying (PSK) is a digital modulation scheme that conveys data by changing (modulating) the phase of a reference signal (the carrier wave).The signal is impressed into the magnetic field x,y area by varying the sine and cosine inputs at a precise time. It is widely used for wireless LANs, RFID and Bluetooth communication.
Any digital modulation scheme uses a finite number of distinct signals to represent digital data. PSK uses a finite number of phases, each assigned a unique pattern of binary digits. Usually, each phase encodes an equal number of bits. Each pattern of bits forms the symbol that is represented by the particular phase. The demodulator, which is designed specifically for the symbol-set used by the modulator, determines the phase of the received signal and maps it back to the symbol it represents, thus recovering the original data. This requires the receiver to be able to compare the phase of the received signal to a reference signal — such a system is termed coherent (and referred to as CPSK).
Alternatively, instead of operating with respect to a constant reference wave, the broadcast can operate with respect to itself. Changes in phase of a single broadcast waveform can be considered the significant items. In this system, the demodulator determines the changes in the phase of the received signal rather than the phase (relative to a reference wave) itself. Since this scheme depends on the difference between successive phases, it is termed differential phase-shift keying (DPSK). DPSK can be significantly simpler to implement than ordinary PSK since there is no need for the demodulator to have a copy of the reference signal to determine the exact phase of the received signal (it is a non-coherent scheme). In exchange, it produces more erroneous demodulation.

Orthogonal Frequency-Division Multiplexing
Orthogonal Frequency-Division Multiplexing (OFDM) is a digital multi-carrier modulation scheme. OFDM conveys data on several parallel data channel by incorporating closely spaced orthogonal sub-carrier signals
OFDM is a favorable communication scheme in underwater acoustic communications thanks to its resilience against frequency selective channels with long delay spreads.

Use of vector sensor receivers
A vector sensor is capable of measuring important non-scalar components of the acoustic field such as the wave velocity, which cannot be obtained by a single scalar pressure sensor.
In recent decades, extensive research has been conducted on the theory and design of vector sensors. Many vector sensor signal processing algorithms have been designed. They have been mainly used for underwater target localization and sonar applications.
Earlier underwater acoustic communication systems have been relying on scalar sensors only, which measure the pressure of the acoustic field. Vector sensors measure the scalar and vector components of the acoustic field in a single point in space, therefore can serve as a compact multichannel receiver. This is different from the existing multichannel underwater receivers, which are composed of spatially separated pressure-only sensors, which may result in large-size arrays.
In general, there are two types of vector sensors: inertial and gradient. Inertial sensors truly measure the velocity or acceleration by responding to the acoustic medium motion, whereas gradient sensors employ a finite-difference approximation to estimate the gradients of the acoustic field such as velocity and acceleration.
In the example of vector sensor communications shown, there is one transmitter pressure transducer, shown by a black dot, whereas for reception we use a vector sensor, shown by a black square, which measures the pressure and the y and z components of the velocity. This is a 1×3 single-input multiple-output (SIMO) system. With more pressure transmitters, one can have a multiple-input multiple-output (MIMO) system also.

See also
Underwater acoustics
Acoustic release

References
External links
A paper on denoising of underwater signals
DSPComm - Underwater acoustic modem manufacturer",Category:Telecommunications techniques,1
214,215,Fracture sonography,"Fracture sonography is the use of medical ultrasound to detect bone fractures. While medical ultrasound is used to visualize soft tissues like skin, organs, and blood vessels, fracture sonography is used to visualize fractures on only bone surfaces. It is useful for children aged 12 or younger because all fractures cause alterations of the bone surface, and joint fractures are uncommon at such ages. For joint fractures that are common in adult bones and cannot be visualized properly, patients older than 12 years are not eligible for ultrasound fracture diagnosis. The method is feasible for detecting fractures of the wrist, elbow, shoulder and clavicle. The advantages of fracture sonography are the avoidance of radiation exposure, faster examinations, and the ability to use standard ultrasound devices, which are more widespread. In the mentioned fields of application, ultrasound is as safe as X-ray diagnosis.

Application
Physical basis
In fracture sonography, regular 4 to 12 MHz linear transducers are used in B-Mode (Medical ultrasonography) with standard ultrasound devices. The high-impedance difference between bone and soft tissue causes an almost complete reflection of the acoustic waves at the bone's surface. As a result, the bone surface is seen, and the underlying structures are not seen.

Visualization and limitations
With fracture sonography, the surface of nearly all extremity bones not covered by other bones can be seen. Thus, the joint facets cannot be accessed. Vertebral structures are not suitable for ultrasound fracture diagnosis.
The additional imaging of soft tissue like haematomas, joint effusion, and blood vessels is an advantage over to X-ray-imaging.
Due to the size of the linear transducer, only a limited section of the bone can be visualized, so longer fractures may require step-by-step assessments.

Diagnosis
Fracture sonography is suitable for the diagnosis of fractures of the shaft and metaphysis of bones. Because only the cortical surface can be visualized, sonography is suitable only for specific fractures. Joint fractures cannot be assessed properly. Sonography is feasible only in the growing bone.
For adult patients, sonography can be used to rule out an increasing deformity of fractures.

Applications
Wrist fractures
In patients younger than 12, wrist fractures cause specific alterations at the surface of the bone (bulge, angulation, offset or fracture gap), and through diagnosis and treatment can be identified without X-ray imaging. Intra-articular fractures are rare and require X-ray imaging. The standard procedure is the wrist SAFE algorithm. The sensitivity of the method in comparison with X-ray imaging is 96 percent, the specificity is 100 percent, the positive predictive value is 1, and the negative predictive value is 0.88.

Elbow fractures
In patients younger than 12, intra-articular fractures of the elbow can be ruled out through sonography. Because intra-articular fractures cause a joint effusion, the dorsal fat pad sign is a reliable parameter for diagnosis of elbow fractures. If a joint effusion is depicted in the ultrasound, two-plane X-ray imagery is necessary to diagnose the fracture. The standard procedure is the elbow-SAFE algorithm. The sensitivity of the method in comparison with X-ray imaging is 97.9 percent, the specificity is 95 percent, the positive predictive value is 0.95, and the negative predictive value is 0.98.

Proximal humerus fracture
In patients younger than 12, proximal humerus fractures can be visualized due to the changes at the bone surface. Because bone tumors can appear at this location, X-ray imaging is necessary following a fracture diagnosis. The standard procedure is the shoulder-SAFE algorithm. The sensitivity of the method in comparison with X-ray imaging is 94.4 percent, and the specificity is 100 percent.

Clavicle fracture
In patients younger than 12, clavicle fractures are common. They can be visualized using sonography and are mostly treated conservatively. The clavicle's proximity to the throat and its curved shape can complicate an examination.

Risks
The side effects are identical to those of regular sonography. The examination causes no radiation exposure.
In the case of an unstable fracture, the examination can disrupt the bones because the splint or cast must be removed beforehand.

Documentation
Thorough labeling is important because a specific bone cannot be identified on a printout.

Alternatives
X-ray imagery is the primary alternative to sonography. Sonography is 25 minutes faster, and the pain is reduced from 1.7 to 1.2 (visual analog scale VAS 0-5).

History
Fracture sonography's first papers were published in 1986 by Leitgeb. Since then, numerous trials have been published. Sonography has not become the standard diagnostic procedure because X-ray facilities exist worldwide. However, fracture sonography's demonstrated sensitivity and specificity may boost its popularity in the future.


== References ==",Category:Acoustics,1
215,216,Loudspeaker acoustics,"Loudspeaker acoustics is a subfield of acoustical engineering concerned with the reproduction of sound and the parameters involved in doing so in actual equipment.
Engineers measure the performance of drivers and complete speaker systems to characterize their behavior, often in an anechoic chamber, outdoors, or using time windowed measurement systems -- all to avoid including room effects (e.g., reverberation) in the measurements.
Designers use models (from electrical filter theory) to predict the performance of drive units in different enclosures, now almost always based on the work of A N Thiele and Richard Small.
Important driver characteristics are:
Frequency response
Off-axis response (dispersion pattern, lobing)
Sensitivity (dB SPL for 1 watt input)
Maximum power handling
Non-linear distortion
Colouration (i.e., more or less, delayed resonance).
It is the performance of a loudspeaker/listening room combination that really matters, as the two interact in multiple ways. There are two approaches to high-quality reproduction. One ensures the listening room be reasonably 'alive' with reverberant sound at all frequencies, in which case the speakers should ideally have equal dispersion at all frequencies in order to equally excite the reverberant fields created by reflections off room surfaces. The other attempts to arrange the listening room to be 'dead' acoustically, leaving indirect sound to the dispersion of the speakers need only be sufficient to cover the listening positions.
A dead or inert acoustic may be best, especially if properly filled with 'surround' reproduction, so that the reverberant field of the original space is reproduced realistically. This is currently quite hard to achieve, and so ideal loudspeaker systems for stereo reproduction would have a uniform dispersion at all frequencies. Listening to sound in an anechoic ""dead"" room is quite different from listening in a conventional room, and, while revealing about loudspeaker behaviour it has an unnatural sonic character that some listeners find uncomfortable. Conventional stereo reproduction is more natural if the listening environment has some acoustically reflective surfaces.
It is in large part the directional properties of speaker systems, which vary with frequency that make them sound different, even when they measure similarly well on-axis. Acoustical engineering in this instance is concerned with adapting these variations to each other.

See also
Audio quality measurement
Acoustic lobing
Loudspeaker time alignment
Digital room correction
Directional Sound
Impulse response
Loudspeaker
Loudspeaker measurement
MLSSA
Sound quality
Spectrogram

External links
Conversion of sensitivity in dB per watt and meter to energy efficiency in percent of passive loudspeakers",Category:All articles lacking sources,1
216,217,Acoustic mirror,"An acoustic mirror is a passive device used to reflect and focus (concentrate) sound waves. Parabolic acoustic mirrors are widely used in parabolic microphones to pick up sound from great distances, employed in surveillance and reporting of outdoor sporting events. Pairs of large parabolic acoustic mirrors which function as ""whisper galleries"" are displayed in science museums to demonstrate sound focusing.
Between the World Wars, before the invention of radar, parabolic sound mirrors were used experimentally as early-warning devices by military air defence forces to detect incoming enemy aircraft by listening for the sound of their engines. During World War 2 on the coast of southern England, a network of large concrete acoustic mirrors was in the process of being built when the project was cancelled owing to the development of the Chain Home radar system. Many of these mirrors are still standing today.

Acoustic aircraft detection
Before World War II and the invention of radar, acoustic mirrors were built as early warning devices around the coasts of Great Britain, with the aim of detecting incoming enemy aircraft by the sound of their engines. The most famous of these devices still stand at Denge on the Dungeness peninsula and at Hythe in Kent. Other examples exist in other parts of Britain (including Sunderland, Redcar, Boulby, Kilnsea) and Selsey Bill, and Ba?ar i?-?ag?aq in Malta. The Maltese sound mirror is known locally as ""the ear"" (il-Widna) and appears to be the only sound mirror built outside Great Britain.

The Dungeness mirrors, known colloquially as the ""listening ears"", consist of three large concrete reflectors built in the 1920s–1930s. Their experimental nature can be discerned by the different shapes of each of the three reflectors: one is a long curved wall about 5 m (16 ft) high by 70 m (230 ft) long, while the other two are dish-shaped constructions approximately 4–5 m (13–16 ft) in diameter. Microphones placed at the foci of the reflectors enabled a listener to detect the sound of aircraft far out over the English Channel. The reflectors are not parabolic, but are actually spherical mirrors. Spherical mirrors can be used for direction finding by moving the sensor rather than the mirror; another unusual example is the Arecibo Observatory.
Acoustic mirrors had a limited effectiveness, and the increasing speed of aircraft in the 1930s meant that they would already be too close to deal with by the time they had been detected. The development of radar put an end to further experimentation with the technique. Nevertheless, there were long-lasting benefits. The acoustic mirror programme, led by Dr William Sansome Tucker, had given Britain the methodology to use interconnected stations to pinpoint the position of an enemy in the sky. The system they developed for linking the stations and plotting aircraft movements was given to the early radar team and contributed to their success in World War II; although the British radar was less sophisticated than the German system, the British system was used more successfully.

Modern uses
Parabolic acoustic mirrors called ""whisper dishes"" are used as participatory exhibits in science museums to demonstrate focusing of sound. Examples are located at Bristol's @Bristol Science Museum At-Bristol UK, Ontario Science Centre, Baltimore's Maryland Science Center, Oklahoma City's Science Museum Oklahoma, San Francisco's Exploratorium, the Science Museum of Minnesota, the Museum of Science and Industry in Chicago, and Parkes Observatory in Australia. A pair of dishes, typically 2 to 3 m (7 to 10 ft) in diameter, is installed facing each other, separated by around a hundred metres. A person standing at the focus of one can hear another person speaking in a whisper at the focus of the other, despite the wide separation between them.
Parabolic microphones depend on a parabolic dish to reflect sound coming from a specific direction into the microphone placed at the focus. They are extremely directional: sensitive to sounds coming from a specific direction. However, they generally have poor bass response because a dish small enough to be portable cannot focus long wavelengths. Small portable parabolic microphones are used to record wildlife sounds such as birdsongs, in televised sports events to pick up the conversations of players, such as in the huddle during American Football games, or to record the sounds of the sport, and in audio surveillance to record speech without the knowledge of the speaker.

Locations
Acoustic aircraft detection mirrors are known to have been built at:
Denge, Kent
Abbot’s Cliff, Kent (at OS grid reference TR27083867)
Boulby, Yorkshire
Dover, Kent, at Fan Bay (OS grid reference TR352428)
Hartlepool, Co. Durham, in the Clavering area
Hythe, Kent
Malta – five sound mirrors were planned for Malta, serialled alphabetically, but only the Mag?tab wall is known to have been built:
A. Mag?tab (colloquially Il Widna - The Ear)
B. Zonkor
C. Ta Karach
D. Ta Zura
E. Tal Merhla

Joss Gap, Kent
Kilnsea, Yorkshire
Redcar, Yorkshire
Romney Marsh, Kent – a series of horizontal discs
Seaham, Co. Durham
Selsey, Sussex – converted into a residence
Sunderland, at Namey Hill (OS grid reference NZ38945960)
Warden Point, Isle of Sheppey, Kent – the Warden Point mirror, sited on a cliff-top, fell onto the beach below ca 1978-9
Modern acoustic mirrors built for entertainment
Pennypot, Royal Military Canal
Wat Tyler country park, nr. Pitsea, Essex – modern sculpture in the form of functional sonic mirrors
The Brickyard (NC State) – North Carolina State University campus
Discovery Green in downtown Houston has a sculpture made of limestone called the Listening Vessels.

See also
Acoustic location
Sound ranging, for the artillery use
Parabolic microphone

References
Richard Newton Scarth, Echoes from the Sky (Revised 2017, Independent Books, London) (ISBN 978-1-872836-17-1)

External links
Acoustic mirrors in Britain
Military acoustic locators
White Cliffs Underground further details of variety of East Kent defences
Visiting information for UK (and Maltese) sound mirrors",Category:Warning systems,1
217,218,Dropped ceiling,"A dropped ceiling is a secondary ceiling, hung below the main (structural) ceiling. It may also be referred to as a drop ceiling, T-bar ceiling, false ceiling, suspended ceiling, grid ceiling, drop in ceiling, drop out ceiling, or ceiling tiles and is a staple of modern construction and architecture in both residential and commercial applications.

History
Dropped ceilings and ceiling tiles were being used in Japan for aesthetic reasons as early as the Muromachi Period (1337 to 1573). Blackfriars Theater in London, England, built in 1596, had dropped ceilings to aid acoustics.
U. S. Patent No. 1,470,728 for modern dropped ceilings was applied for by E. E. Hall on May 28, 1919 and granted on October 16, 1923. Initially modern dropped ceilings were built using interlocking tiles and the only way to provide access for repair or inspection of the area above the tiles was by starting at the edge of the ceiling, or at a designated ""key tile"", and then removing contiguous tiles one at a time until the desired place of access was reached. Once the repair or inspection was completed, the tiles had to be reinstalled. This process could be very time-consuming and expensive. On September 8, 1958 Donald A. Brown of Westlake, Ohio filed for a patent for Accessible Suspended Ceiling Construction. This invention provided suspended ceiling construction in which access may readily be obtained at any desired location. Patent Number US 2,984,946 A was granted on May 23, 1961. Brown has sometimes been credited as being the inventor of the dropped ceiling  even though other patents preceded his as shown in the table below.

Design objectives
Effective building design requires balancing multiple objectives: aesthetics, acoustics, environmental factors, and integration with the building's infrastructure—not to mention cost of construction as well as long-term operation costs.

Aesthetics
Modern dropped ceilings were initially created to hide the building infrastructure, including piping, wiring, and/or ductwork, by creating a plenum space above the dropped ceiling, while allowing access for repairs and inspections. Drop ceilings may also be used to hide problems, such as structural damage. Further, drop out ceilings can also conceal the sprinkler systems while still providing full fire suppression functionality.
For many years, dropped ceilings were made of basic white tiles, but modern innovations now offer a plethora of options in sizes, colors, materials (including retro designs and faux leather, wood, or metal), visual effects and shapes, patterns, and textures as well as support systems and ways to access the plenum. Custom runs of specialty ceiling tiles can be done at relatively low cost compared with the past.

Acoustics
Acoustic balance and control was another early objective of dropped ceilings. A noisy room can overstimulate occupants, while a too quiet interior may seem dull and uninviting.
The acoustic performance of suspended ceilings has improved dramatically over the years, with enhanced sound absorption and attenuation. This is sometimes achieved by adding insulation known as Sound Attenuation Batts (SABs), more commonly referred to as ""sound batts"", above the panels to help deaden sounds and keep adjacent rooms quieter.

Environmental factors
Indoor environmental quality
Indoor environmental quality includes ventilation, VOC emissions, lighting and thermal system control, thermal comfort, use of daylight for natural illumination, acoustics, and optimization of outdoor view availability.

Sustainability
Many manufacturers of modern dropped ceilings include sustainability as an objective. Sustainable features may include:
Energy efficiency, including daylight efficacy and thermal insulating qualities. This uses the ceiling plane to reflect daylight as well as electrical illumination to maximize lumen efficacy, which also improves the comfort and usability of interior spaces. A common measure of the light reflectance of a ceiling material is ASTM E 1477 for Light Reflectance (LR-1). A level of about 75% is considered good, although higher levels are possible.
Reduced resources needed for construction of the tiles
Recyclable/reused/renewable materials

Integration with infrastructure
Integration with mechanical, electrical, and plumbing (MEP) is important with dropped ceilings, since most of these systems are by definition above the ceiling. Fortunately, most ceiling system products are now designed with this integration in mind. Decisions here can also affect aesthetics as well as access and maintenance.

Cost
Dropped ceilings may have an improved return on investment (ROI) over open ceilings

Suspension grids
A typical dropped ceiling consists of a grid-work of metal channels in the shape of an upside-down ""T"", suspended on wires from the overhead structure. These channels snap together in a regularly spaced pattern of cells. Each cell is then filled with lightweight ceiling tiles or ""panels"" which simply drop into the grid. The primary grid types are ""Standard 1"" (15/16 face), Slimline (9/16"" grid), and concealed grid.
In the United States the cell size in the suspension grids is typically either 2 ft × 2 ft or 2 ft × 4 ft and the ceiling tiles are the same size. In Europe the cell size in the suspension grids is 600×600 mm, while the ceiling tiles are slightly smaller at 595mm x 595mm or 595mm x 1195mm.

Concealed grid
An older, less common type of dropped ceiling is the concealed grid system. This type of dropped ceiling employs a method of interlocking panels into each other and the grid with the use of small strips of metal called 'splines', thus making it difficult to remove panels to gain access above the ceiling without damaging the installation or the panels. Normally, these type of ceilings will have a ""key panel"" (usually in the corner) which can be removed, allowing for the other panels to be slid out of the grid (a series of metal channels called 'z bars') one by one, until eventually removing the desired panel. This type of ceiling is more commonly found in older installations or installations where access to above the ceiling is generally considered unnecessary.
This system has some major disadvantages compared to the more common ""drop panel"" system, most notably the difficulty in removing and reattaching panels from the grid, which in some cases can cause irreparable damage to the panels removed. Finding replacement panels for this type of dropped ceiling is becoming increasingly more difficult as demand for them is slowing, as is production of the parts. Small clips are still available which allow tiles to be inserted into gaps in the ceiling where a tile is missing, they work by being placed on the edge of a concealed tile, then being slid along as the tile is placed to 'lock' it in place.

Stretch Ceiling
With similar advantages to a dropped ceiling, a stretch ceiling is often used to conceal pipework, wires or the existing ceiling. On top of this there is usually a broad choice of colour or texture and the membrane can be manipulated into a variety of shapes.

A stretch ceiling is a suspended ceiling system and it is made of three main components

Perimeter Track - Aluminium or Plastic PVC.
Membrane - Typically a PVC or Nylon material, lightweight sheets are made to size/shape from roll material. Can be printed or painted to achieve the desired effect.
Harpoon or Catch - This is ultrasonically welded to the edge of the membrane or sheet in the factory, the edging slots into the perimeter track to keep the ceiling in place.

When installing a stretch ceiling, semi-concealed plastic/aluminum track, the preferred choice for most architects and designers, is cut to size and fixed to the perimeter of the ceiling area. The membrane is stretched and the harpoon or catch edge is clipped into the track. Stretching is aided by heating up the membrane or sheet prior to fitting.

Drop out ceilings
Approved Drop out (or drop-out) ceilings allow the installation of a dropped ceiling beneath existing fire sprinklers because the tiles, sometimes called melt-out ceiling tiles, are heat sensitive and designed to fall from the dropped ceiling suspension grid in the event of a fire, allowing the sprinklers to do their job.
Drop out ceiling tiles can add to the aesthetic appeal of a ceiling since the fire sprinklers are hidden by the tiles. Commonly made from vinyl or expanded polystyrene, drop out ceiling tiles are available in multiple sizes and finishes from a variety of manufacturers.
Installation is subject to the local Authority Having Jurisdiction (AHJ) and, in the U. S., must meet the standards listed in the section below at a minimum.

Drop out ceiling standards (U.S.)
The standards listed below are in addition to those for ceiling tiles in general. No clips, fasteners, or impediments of any kind can be used to limit the ceiling tile's ability to drop from the suspension system without restraint in the event of a fire unless these have been used in the testing process. Painting can void an approval. Note that additional local requirements may exist.
FM Global - Approval Standards for Plastic Suspended Ceilings
UL - Ceiling Panels for Use Beneath Sprinklers
ICC-ES - AC-12 Section 4.4 - Foam Plastic Drop-Out Ceiling Panels and Tiles
NFPA 13 - Standard for the Installation of Sprinkler Systems

Safety issues
In older buildings the space above the dropped ceiling was often used as a plenum space for ventilation systems, requiring only enclosed ducts that deliver fresh air into the room below, with return air entering the ceiling space through open grilles across the ceiling. This practice is now used less frequently in new construction.
In the event that the dropped ceiling is used as a plenum, low-voltage cables and wiring not installed inside conduit need to use a special low-smoke and low-toxicity wire insulation which will tend to char and stop burning on its own. This helps to protect building occupants so that they are not poisoned with toxic chemicals sucked through the ventilation system in the event of a fire, and helps to prevent fires from spreading inside the hidden plenum space. This special low-smoke cable is typically referred to as plenum cable or Low Smoke Zero Halogen (LSZH or LS0H) cable. While twisted pair cable for networking and telephone service is the most common form of plenum cable, coaxial cable also needs to be plenum-rated for safety.
High-voltage electrical equipment (generally regarded as being over 50 volts) is not permitted to be exposed in the plenum space above a drop ceiling. High voltage wiring must be enclosed in conduit or raceways, and must be physically isolated from low-voltage wiring. High voltage electrical devices similarly must be enclosed in a plenum space, inside a metallic container. Similarly, electrical outlets for domestic powered devices are not permitted inside the plenum space, though outlets can be installed on ceiling tiles inside electrical boxes, with the sockets exposed on the exterior bottom face of the drop ceiling. The purpose of these restrictions is to limit flame spread inside the unseen plenum space, in the event of high voltage equipment or wiring failure. Low voltage cabling is permitted because current flow is typically negligible so the risk of overheating and fire is limited.
In earthquake prone areas (e.g., California) diagonal wire stays are often required by building codes in order to ensure the ceiling grid won't sway laterally during an earthquake, which can lead to partial or total collapse of the ceiling grid on the occupants below during a severe tremor. Compression posts may also be added to keep the ceiling from bouncing vertically during an earthquake.
Lighting fixtures and other devices installed in a dropped ceiling are required to be firmly secured to the dropped ceiling framework. In the event of a fire above a dropped ceiling it is often necessary for firemen to have to pull down the ceiling in a hurry to quickly gain access to the conflagration. Loose fixtures merely resting in the framework by force of gravity can become unseated and swing down on their armorflex power cables to hit the firemen below. Binding the fixtures to the framework assures that if the framework must be pulled down the fixture will come down with it and not become a pendulous swinging hazard to the firemen.

Advantages
Fire safety
To address fire safety, ceiling tiles made from mineral fibres, plastic, tin, composite, or fire-rated wood panels can be used within the construction to meet acceptable standards/ratings. Some tiles, in specific situations, can provide the needed additional resistance to meet the ""time rating"" required for various fire code, city ordinance, commercial, or other similar building construction regulations. Fire ratings for ceiling panels vary based on the materials used, the preparation of each panel, and the safety testing and third party evaluation done to determine where and how they can be safely installed. In the UK it can be required for the tiles from certain manufacturers to be clipped into the grid with special ceiling clips in order to provide a fire rating; there are special tiles designed for the underside of mezzanine floors however that can give a fire rating without being clipped.
Drop out ceilings have a further advantage in that they can be mounted underneath fire sprinklers, thus hiding the sprinklers for a more attractive appearance. When installed underneath fire sprinklers, certain requirements for materials, applications, installation, and maintenance of drop out ceilings must be met in order to comply with fire safety regulations. (The white paper Drop-out Ceiling Panels–A Discussion on Their Use With Fire Sprinklers, referenced by the article, is available here)

Ease of modification
Another advantage of a dropped ceiling is that the easily removed ceiling panels offer instant access to the plenum, greatly simplifying repairs or alterations.
Wiring and piping installed behind traditional plaster or wallboard ceilings is extremely difficult to modify once the finished ceiling is in place. Wires must either be fished through hollow spaces in the walls behind the finished ceiling, or the ceiling must be demolished in order for wiring or piping changes to be made.
In contrast, the tiles and other parts of a dropped or stretch ceiling are easily removed to allow access to the area above the grid to do any necessary wiring or plumbing modifications. In the event of remodelling, nearly all components of the grid can be dismantled and reassembled somewhere else.
In office buildings, the drop ceiling is often used in conjunction with hollow steel studs to construct small office spaces out of a much larger cavernous space. Wiring and other services are run through the open ceiling, down through the hollow stud walls, and to outlets in the work areas. If business needs change, the office spaces are easily dismantled and the overall cavernous space reconfigured with a different floor plan.
In older buildings that have seen multiple renovations over time, it is not uncommon for a dropped ceiling to have been installed in one renovation and then subsequently removed in another, its installation having been an inexpensive fix to prolong the time between major renovations.

Disadvantages
One disadvantage with this ceiling system is reduced headroom i.e. it can reduce height of room by some inches. Clearance is required between the grid and any pipes or ductwork above to install the ceiling tiles and light fixtures. In general, a minimum clearance of 100 to 200 millimetres (4 to 8 in) is often needed between the lowest obstruction and the level of the ceiling grid. A direct-mount grid may work for those who want the convenience of a dropped ceiling, but have limited headroom. Stretch ceiling supports require less than one inch of vertical space, and no space is required for tiles to be lifted out with a stretch ceiling, but a greater clearance space may be chosen to allow room for MEC or for aesthetic reasons.
Dropped ceilings generally conceal many of the functional and structural elements of a building, creating an aesthetic paradigm that discourages the use of functional building systems as aesthetic design elements. Concealing these elements makes the complexity of today's advanced building technologies more difficult to appreciate. It is also more difficult to perform maintenance on or diagnose problems with the concealed systems.
As a renovation tool, dropped ceilings are a quick and inexpensive way to repair a ceiling or reduce HVAC costs. Some materials may show their age quickly— for example, mineral fiber sags, is damaged easily when handled, and stains easily, but stretch ceiling, tin and vinyl do not have these characteristics.

See also
Ceiling tiles
Gypsum
Drywall

Further reading
Drop?Out Ceiling Panels – A Discussion on Their Use with Fire Sprinklers by Gary G. Piermattei of Rolf Jensen & Associates, Inc., 2014
Thermoformed Ceiling Panels and Tiles: Drop-out Ceiling Panels Installed Beneath Fire Sprinklers, The Construction Specifier Magazine, 2014 (especially the Maintenance section)
Specifying Drop-Out Ceilings beneath Fire Sprinklers by Ed Davis and Michael Chusid, Consulting-Specifying Engineer Magazine, 2016
Safety, Code Issues of Drop-Out Ceilings by Ed Davis and Michael Chusid, Consulting-Specifying Engineer Magazine, 2016


== References ==",Category:Acoustics,1
218,219,Poromechanics,"Poromechanics is a branch of physics and specifically continuum mechanics and acoustics that studies the behaviour of fluid-saturated porous media. A porous medium or a porous material is a solid (often called matrix) permeated by an interconnected network of pores (voids) filled with a fluid (liquid or gas). Usually both solid matrix and the pore network (also known as the pore space) are assumed to be continuous, so as to form two interpenetrating continua such as in a sponge. Many natural substances such as rocks, soils, biological tissues, and man made materials such as foams and ceramics can be considered as porous media. Porous media whose solid matrix is elastic and the fluid is viscous are called poroelastic. A poroelastic medium is characterised by its porosity, permeability as well as the properties of its constituents (solid matrix and fluid).
The concept of a porous medium originally emerged in soil mechanics, and in particular in the works of Karl von Terzaghi, the father of soil mechanics. However a more general concept of a poroelastic medium, independent of its nature or application, is usually attributed to Maurice Anthony Biot (1905–1985), a Belgian-American engineer. In a series of papers published between 1935 and 1957 Biot developed the theory of dynamic poroelasticity (now known as Biot theory) which gives a complete and general description of the mechanical behaviour of a poroelastic medium. Biot's equations of the linear theory of poroelasticity are derived from
Equations of linear elasticity for the solid matrix,
Navier–Stokes equations for the viscous fluid, and
Darcy's law for the flow of fluid through the porous matrix.
One of the key findings of the theory of poroelasticity is that in poroelastic media there exist three types of elastic waves: a shear or transverse wave, and two types of longitudinal or compressional waves, which Biot called type I and type II waves. The transverse and type I (or fast) longitudinal wave are similar to the transverse and longitudinal waves in an elastic solid, respectively. The slow compressional wave, (also known as Biot’s slow wave) is unique to poroelastic materials. The prediction of the Biot’s slow wave generated some controversy, until it was experimentally observed by Thomas Plona in 1980. Other important early contributors to the theory of poroelasticity were Yakov Frenkel and Fritz Gassmann.
Recent applications of poroelasticity to biology such as modeling of blood flows through the beating myocardium have also required an extension of the equations to nonlinear (large deformation) elasticity and the inclusion of inertia forces.

See also
Petrophysics
Rock physics

References
Terzaghi, K., 1943, Theoretical Soil Mechanics, John Wiley and Sons, New York
Frenkel, J. (1944). ""On the theory of seismic and seismoelectric phenomena in moist soil"" (pdf). Journal of Physics. III (4): 230–241. doi:10.1061/(ASCE)0733-9399(2005)131:9(879). 
Gassmann, F., 1951. Über die elastizität poröser medien. Viertel. Naturforsch. Ges. Zürich, 96, 1 – 23. (English translation available as pdf here).
Gassmann, Fritz (1951). ""Elastic waves through a packing of spheres"". Geophysics. 16 (4): 673–685. Bibcode:1951Geop...16..673G. doi:10.1190/1.1437718. 
Biot, M.A. (1941). ""General theory of three dimensional consolidation"". Journal of Applied Physics. 12: 155–164. Bibcode:1941JAP....12..155B. doi:10.1063/1.1712886. 
Biot, M.A. (1956). ""Theory of propagation of elastic waves in a fluid saturated porous solid. I Low frequency range"". The Journal of the Acoustical Society of America. 28: 168–178. Bibcode:1956ASAJ...28..168B. doi:10.1121/1.1908239. 
Biot, M.A. (1956). ""Theory of propagation of elastic waves in a fluid saturated porous solid. II Higher frequency range"". The Journal of the Acoustical Society of America. 28: 179–191. Bibcode:1956ASAJ...28..179B. doi:10.1121/1.1908241. 
Biot, M.A. & Willis, D.G. (1957). ""The elastic coefficients of the theory of consolidation"". Journal of Applied Mechanics. Trans. ASME. 24: 594–601. 
Biot, M.A. (1962). ""Mechanics of deformation and acoustic propagation in porous media"". Journal of Applied Physics. 33: 1482–1498. Bibcode:1962JAP....33.1482B. doi:10.1063/1.1728759. 
Rice, J.R. & Cleary, M.P. (1976). ""Some basic stress diffusion solutions for fluid-saturated elastic porous media with compressible constituents"". Reviews of Geophysics and Space Physics. 14: 227–241. Bibcode:1976RvGSP..14..227R. doi:10.1029/RG014i002p00227. 
Plona, T. (1980). ""Observation of a Second Bulk Compressional Wave in a Porous Medium at Ultrasonic Frequencies"". Applied Physics Letters. 36: 259. Bibcode:1980ApPhL..36..259P. doi:10.1063/1.91445. 
Coussy, O., 2004, Poromechanics, John Wiley & Sons.
Bourbie, T., Coussy, O., Zinszner, B., 1987, Acoustics of Porous Media, Gulf Pub. Co.; Editions Technip.
Nigmatulin, R.I., 1990, Dynamics of Multiphase Media, Hemisphere.
Wang, H.F., 2000, Theory of Linear Poroelasticity with Applications to Geomechanics and Hydrogeology, Princeton University Press.
Allard, J. F., 1993, Propagation of Sound in Porous Media: Modelling Sound Absorbing Materials, Chapman & Hall.
Chapelle, D., Gerbeau, J.-F., Sainte-Marie, J. and Vignon-Clementel, I. (2010). ""A poroelastic model valid in large strains with applications to perfusion in cardiac modeling"". Computational Mechanics. 46: 91–101. Bibcode:2009CompM.tmp...90C. doi:10.1007/s00466-009-0452-x. CS1 maint: Multiple names: authors list (link)
Chapelle, D. & Moireau, P. (2014). ""General coupling of porous flows and hyperelastic formulations - From thermodynamics principles to energy balance and compatible time schemes"". European Journal of Mechanics B. 46: 82–96. Bibcode:2014EJMF...46...82C. doi:10.1016/j.euromechflu.2014.02.009.

External links
Poronet - PoroMechanics Internet Resources Network
APMR - Acoustical Porous Material Recipes",Category:CS1 maint: Multiple names: authors list,1
219,220,Absorption (acoustics),"Acoustic absorption refers to the process by which a material, structure, or object takes in sound energy when sound waves are encountered, as opposed to reflecting the energy. Part of the absorbed energy is transformed into heat and part is transmitted through the absorbing body. The energy transformed into heat is said to have been 'lost'.
When sound from a loudspeaker collides with the walls of a room part of the sound's energy is reflected, part is transmitted, and part is absorbed into the walls. Just as the acoustic energy was transmitted through the air as pressure differentials (or deformations), the acoustic energy travels through the material which makes up the wall in the same manner. Deformation causes mechanical losses via conversion of part of the sound energy into heat, resulting in acoustic attenuation, mostly due to the wall's viscosity. Similar attenuation mechanisms apply for the air and any other medium through which sound travels.
The fraction of sound absorbed is governed by the acoustic impedances of both media and is a function of frequency and the incident angle. Size and shape can influence the sound wave's behavior if they interact with its wavelength, giving rise to wave phenomena such as standing waves and diffraction.
Acoustic absorption is of particular interest in soundproofing. Soundproofing aims to absorb as much sound energy (often in particular frequencies) as possible converting it into heat or transmitting it away from a certain location.
In general, soft, pliable, or porous materials (like cloths) serve as good acoustic insulators - absorbing most sound, whereas dense, hard, impenetrable materials (such as metals) reflect most.
How well a room absorbs sound is quantified by the effective absorption area of the walls, also named total absorption area. This is calculated using its dimensions and the absorption coefficients of the walls. The total absorption is expressed in Sabins and is useful in, for instance, determining the reverberation time of auditoria. Absorption coefficients can be measured using a reverberation room, which is the opposite of an anechoic chamber (see below).

Applications
Acoustic absorption is critical in areas such as:
Soundproofing
Sound recording and reproduction
Loudspeaker design
Acoustic transmission lines
Room acoustics
Architectural acoustics
Sonar
Noise Barrier Walls

Anechoic chamber
An acoustic anechoic chamber is a room designed to absorb as much sound as possible. The walls consist of a number of baffles with highly absorptive material arranged in such a way that the fraction of sound they do reflect is directed towards another baffle instead of back into the room. This makes the chamber almost devoid of echos which is useful for measuring the sound pressure level of a source and for various other experiments and measurements.
Anechoic chambers are expensive for several reasons and are therefore not common.
They must be isolated from outside influences (e.g., planes, trains, automobiles, snowmobiles, elevators, pumps, ...; indeed any source of sound which may interfere with measurements inside the chamber) and they must be physically large. The first, environmental isolation, requires in most cases specially constructed, nearly always massive, and likewise thick, walls, floors, and ceilings. Such chambers are often built as spring supported isolated rooms within a larger building. The National Research Council in Canada has a modern anechoic chamber, and has posted a video on the Web, noting these as well as other constructional details. Doors must be specially made, sealing for them must be acoustically complete (no leaks around the edges), ventilation (if any) carefully managed, and lighting chosen to be silent.
The second requirement follows in part from the first and from the necessity of preventing reverberation inside the room from, say, a sound source being tested. Preventing echoes is almost always done with absorptive foam wedges on walls, floors and ceilings, and if they are to be effective at low frequencies, these must be physically large; the lower the frequencies to be absorbed, the larger they must be.
An anechoic chamber must therefore be large to accommodate those absorbers and isolation schemes, but still allow for space for experimental apparatus and units under test.

Electrical and mechanical analogy
The energy dissipated within a medium as sound travels through it is analogous to the energy dissipated in electrical resistors or that dissipated in mechanical dampers for mechanical motion transmission systems. All three are equivalent to the resistive part of a system of resistive and reactive elements. The resistive elements dissipate energy (irreversibly into heat) and the reactive elements store and release energy (reversibly, neglecting small losses). The reactive parts of an acoustic medium are determined by its bulk modulus and its density, analogous to respectively an electrical capacitor and an electrical inductor, and analogous to, respectively, a mechanical spring attached to a mass.
Note that since dissipation solely relies on the resistive element it is independent of frequency. In practice however the resistive element varies with frequency. For instance, vibrations of most materials change their physical structure and so their physical properties; the result is a change in the 'resistance' equivalence. Additionally, the cycle of compression and rarefaction exhibits hysteresis of pressure waves in most materials which is a function of frequency, so for every compression there is a rarefaction, and the total amount of energy dissipated due to hysteresis changes with frequency. Furthermore, some materials behave in a non-Newtonian way, which causes their viscosity to change with the rate of shear strain experienced during compression and rarefaction; again, this varies with frequency. Gasses and liquids generally exhibit less hysteresis than solid materials (e.g., sound waves cause adiabatic compression and rarefaction) and behave in a, mostly, Newtonian way.
Combined, the resistive and reactive properties of an acoustic medium form the acoustic impedance. The behaviour of sound waves encountering a different medium is dictated by the differing acoustic impedances. As with electrical impedances, there are matches and mismatches and energy will be transferred for certain frequencies (up to nearly 100%) whereas for others it could be mostly reflected (again, up to very large percentages).
In amplifier and loudspeaker design electrical impedances, mechanical impedances, and acoustic impedances of the system have to be balanced such that the frequency and phase response least alter the reproduced sound across a very broad spectrum whilst still producing adequate sound levels for the listener. Modelling acoustical systems using the same (or similar) techniques long used in electrical circuits gave acoustical designers a new and powerful design tool.

See also
Soundproofing
Acoustic attenuation
Attenuation coefficient
Anechoic chamber
Acoustic wave
Acoustic impedance


== References ==",Category:Acoustics,1
220,221,Pre-echo,"Pre-echo, sometimes called a forward echo, (not to be confused with reverse echo) is a digital audio compression artifact where a sound is heard before it occurs (hence the name). It is most noticeable in impulsive sounds from percussion instruments such as castanets or cymbals.
It occurs in transform-based audio compression algorithms – typically based on the modified discrete cosine transform (MDCT) – such as MP3, MPEG-4 AAC, and Vorbis, and is due to quantization noise being spread over the entire transform-window of the codec.

Cause
The psychoacoustic component of the effect is that one hears only the echo preceding the transient, not the one following – because this latter is drowned out by the transient. Formally, forward temporal masking is much stronger than backwards temporal masking, hence one hears a pre-echo, but no post-echo.

Mitigation
In an effort to avoid pre-echo artifacts, many sound processing systems use filters where all of the response occurs after the main impulse, rather than linear phase filters. Such filters necessarily introduce phase distortion and temporal smearing, but this additional distortion is less audible because of strong forward masking.
Avoiding pre-echo is a substantial design difficulty in transform domain lossy audio codecs such as MP3, MPEG-4 AAC, and Vorbis. It is also one of the problems encountered in digital room correction algorithms and frequency domain filters in general (denoising by spectral subtraction, equalization, and others). One way of reducing ""breathing"" for filters and compression techniques using piecewise Fourier-based transforms is picking a smaller transform window (short blocks in MP3), thus increasing the temporal resolution of the algorithm at the cost of reducing its frequency resolution.

As a sound effect
A pre-echo effect may be added intentionally as a type of sound effect. A common use for this is to make a person's voice sound ghostly.

See also
Compression artifact

References
External links
Pre-echo at Hydrogenaudio Knowledgebase
Pre-echo detection & reduction, Massachusetts Institute of Technology, 1994",Category:Articles needing additional references from January 2017,1
221,222,Xeno-canto,"xeno-canto is a citizen science project in which volunteers record, upload and annotate recordings of birdsong and bird calls. All the recordings are published under one of the Creative Commons licenses, including some with open licences. The data has been re-used in scientific papers. The website is supported by a number of academic and birdwatching institutions worldwide, with its primary support being in the Netherlands.

References
External links
xeno-canto",Category:Website stubs,1
222,223,Acoustic space,"Acoustic space is an acoustic environment in which sound can be heard by an observer. The term ""acoustic space"" was first mentioned by Marshall McLuhan, a professor and a philosopher.

Nature of acoustics
In reality, there are some properties of acoustics that affect the acoustic space. These properties can either improve the quality of the sound or interfere with the sound.
Reflection is the change in direction of a wave when it hits an object. Many acoustic engineers took advantage from this. It is used for interior designs, either use reflections for benefits or eliminates the reflections. The sound waves usually reflect off the wall and interfere with other sound waves that are generated later. To prevent sound waves reflecting directly to the receiver, a diffusor is introduced. A diffusor has different depths in it, causing the sound to scatter in random directions evenly. It changes the disturbing echo of the sound into a mild reverb which decays over time.
Diffraction is the change of a sound wave’s propagation to avoid obstacles. According to Huygens’ principle, when a sound wave is partially blocked by an obstacle, the remaining part that gets through acts as a source of secondary waves. For instance, if you are in a room and you shout with the door open, the people on either side of hallway will hear it. The sound waves that left the door become a source, then spread out in the hallway. The sounds from the surroundings might interfere with the acoustic space like the example given.

Uses of acoustic space
The application of Acoustic space is very useful in architecture. Some kinds of architecture need a proficient design to bring out the best performances. For example, concert halls, auditoriums, theaters, or even cathedrals.
Concert Hall – a place that is designed to hold a concert. A good concert hall usually holds around 1700 to 2600 audience. There are three main attributes of a good concert halls: clarity, ambience, and loudness. If the seats are well positioned, the audiences will hear clear sound from every single seat. For more ambience, reverberation times are designed as preferred. For instance, romantic music usually requires an amount of reverberation time to enhance the emotions, therefore, the ceilings of the concert hall should be high.

Theater – a place that is designed for live performances. The first priority for sound design in a theater is speech. Speech has to be heard clearly, even if it is a soft whisper. The reverb is not needed in this case, it interrupts the words spoken by the actors. The intensity has to be increased, in order to enlarge the acoustic space, to cover the theater without disrupting the dynamic. In large theaters, amplification must be used.

Cathedral ( and church) have an area called a choir, usually located near the transept, where in most cathedrals the tower is located.The choir is for the choir to sing. This kind of singing needs a soft cloudy sound for ambience and emotion. The height of the cathedral does not only show religious pride, but also improves the acoustics. There is more reverb when the source generates a sound in the space

See also
Architectural acoustics
Room acoustics


== References ==",Category:Acoustics,1
223,224,Hypersonic effect,"The hypersonic effect is a term coined to describe a phenomenon reported in a controversial scientific study by Tsutomu Oohashi et al., which claims that, although humans cannot consciously hear ultrasound (sounds at frequencies above approximately 20 kHz), the presence or absence of those frequencies has a measurable effect on their physiological and psychological reactions.
Numerous other studies have contradicted the portion of the results relating to the subjective reaction to high-frequency audio, finding that people who have ""good ears"" listening to Super Audio CDs and high resolution DVD-Audio recordings on high fidelity systems capable of reproducing sounds up to 30 kHz cannot tell the difference between high resolution audio and the normal CD sampling rate of 44.1 kHz.

Favoring evidence
In research published in 2000 in the Journal of Neurophysiology, researchers described a series of objective and subjective experiments in which subjects were played music, sometimes containing high-frequency components (HFCs) above 25 kHz and sometimes not. The subjects could not consciously tell the difference, but when played music with the HFCs they showed differences measured in two ways:
EEG monitoring of their brain activity showed statistically significant enhancement in alpha-wave activity
The subjects preferred the music with the HFCs
No effect was detected on listeners in the study when only the ultrasonic (frequencies higher than 24 kHz) portion of the test material was played for test subjects; the demonstrated effect was only present when comparing full-bandwidth to bandwidth-limited material.
It is a common understanding in psychoacoustics that the ear cannot respond to sounds at such high frequency via an air-conduction pathway, so one question that this research raised was: does the hypersonic effect occur via the ""ordinary"" route of sound travelling through the air passage in the ear, or in some other way? A peer-reviewed study in 2006 seemed to confirm the second of these options, by testing the different effect of HFCs when presented via loudspeakers or via headphones — the hypersonic effect did not occur when the HFCs were presented via headphones.
The 2006 study also investigated the comfortable listening level (CLL) of music with and without HFCs, an alternative way of measuring subject response to the sound. The CLL for the music with HFCs was higher than that for the music without HFCs - this provides a quantitative way to demonstrate general listener preference for the music with HFCs.

Contrary evidence
There are contradictions in Oohashi's results.
No effect was detected on listeners in the Oohashi study when only the ultrasonic (frequencies higher than 24 kHz) portion of the test material was played for test subjects. The demonstrated effect was only present when comparing full-bandwidth to bandwidth-limited material.
Bandwidth-limited material was more highly regarded by test subjects when full-bandwidth material was played immediately prior.
Researches from NHK laboratory have attempted carefully but unsuccessfully to reproduce Oohashi's results.
480 man-hours of listening tests conducted at the London AES convention in 1980 by Laurie Finchman of KEF concluded that subjects could not distinguish a 20 kHz band limited version of a test signal from the original played back on equipment capable of reproducing sound up to 40 kHz.
System non-linearities (present to varying degrees in all audio reproduction electronics, loudspeakers, etc.) are known to produce lower-frequency intermodulation products when the system is stimulated with high frequency signals. It is suggested that this mechanism could produce signals in the audible range that allow listeners to distinguish the signals. Artifacts like this are a common problem with PC-based hearing self-tests, for instance.
In September 2007, two members of the Boston Audio Society and the Audio Engineering Society published their study in which about half of the 554 double-blind ABX test listening trials made by 60 respondents showed the correct identification of high-resolution or CD-standard sampling rate. The results were no better than flipping a coin, producing 274 correct identifications (49.5% success), and it would have required at least 301 correct identifications given 554 trials (a modest 54.3% success rate) to exceed a 95% statistical confidence of audible difference, which will happen about once in twenty such tests by chance alone.

Counter-contrary evidence
Criticism of Oohashi's studies has been directed primarily at the conclusions regarding listener's preferences the test material; there has been little criticism aimed at the physiological aspect of the studies.
Studies cited as contrary evidence did not address the physiological brain response to high-frequency audio, only the subject's conscious response to it. Further investigation of the observed physiological response appears to show that the ear alone does not produce the extra brain waves, but when the body is exposed to high-frequency sound it gives some brain stimulus.

See also
Ultrasonic hearing
Sound from ultrasound (known commercially as HyperSonic Sound)


== References ==",Category:Wikipedia articles needing factual verification from July 2012,1
224,225,Acoustic droplet ejection,"Acoustic droplet ejection (ADE) uses a pulse of ultrasound to move low volumes of fluids (typically nanoliters or picoliters) without any physical contact. This technology focuses acoustic energy into a fluid sample in order to eject droplets as small as a picoliter. ADE technology is a very gentle process, and it can be used to transfer proteins, high molecular weight DNA and live cells without damage or loss of viability. This feature makes the technology suitable for a wide variety of applications including proteomics and cell-based assays.

History
Acoustic droplet ejection was first reported in 1927 by Robert W. Wood and Alfred Loomis, who noted that when a high-power acoustic generator was immersed in an oil bath, a mound formed on the surface of the oil and, like a “miniature volcano,” ejected a continuous stream of droplets. Ripples that appear in a glass of water placed on a loud speaker show that acoustic energy can be converted to kinetic energy in a fluid. If the sound is turned up enough, droplets will jump from the liquid. This technique was refined in the 1970s and 1980s by Xerox and IBM and other organizations to provide a single droplet on-demand for printing ink onto a page. Two California-based companies, EDC Biosystems Inc. and Labcyte Inc., exploit acoustic energy for two separate functions: 1) as a liquid transfer device and 2) as a device for liquid auditing.

Ejection mechanism
To eject a droplet, a transducer generates and transfers acoustic energy to a source well. When the acoustic energy is focused near the surface of the liquid, a mound of liquid is formed and a droplet is ejected. [Figure 1] The diameter of the droplet scales inversely with the frequency of the acoustic energy—higher frequencies produce smaller droplets. Unlike other liquid transfer devices, no pipette tips, pin tools, or nozzles touch the source liquid or destination surfaces. Liquid transfer methods that rely on droplet formation through an orifice, e.g., disposable tips or capillary nozzles, invariably lose precision as the transfer volume decreases. Touchless acoustic transfer provides a coefficient of variation (CV) that is significantly lower than other techniques and is independent of volume at the levels tested.

ADE shoots a droplet from a source well upward to an inverted receiving plate positioned above the source plate. Liquids ejected from the source are captured by dry plates due to surface tension. For larger volumes, multiple droplets can be rapidly ejected from the source (typically 100 to 500 droplets/sec) to the destination with the coefficient of variation typically <4% over a volume range of two orders of magnitude.

Applications of acoustic transfer
The following applications are among those that can benefit from the features of acoustic droplet ejection:
High throughput screening
Microelectromechanical systems
Assay miniaturization
Eliminating cross-contamination
Reducing plastic waste in biological research
Direct loading of mass spectrometers

See also
Acoustic droplet vaporization


== References ==",Category:Nanotechnology,1
225,226,Room acoustics,"Room acoustics describes how sound behaves in an enclosed space.
The way that sound behaves in a room can be broken up into roughly four different frequency zones:
The first zone is below the frequency that has a wavelength of twice the longest length of the room. In this zone, sound behaves very much like changes in static air pressure.
Above that zone, until the frequency is approximately 
  
    
      
        11250
        ×
        
          
            
              
                RT60
              
              
                V
              
            
          
        
      
    
    {\displaystyle 11250\times {\sqrt {\frac {\textit {RT60}}{\textit {V}}}}}
   (when Volume is measured in cubic feet and 
  
    
      
        2000
        ×
        
          
            
              
                RT60
              
              
                V
              
            
          
        
      
    
    {\displaystyle 2000\times {\sqrt {\frac {\textit {RT60}}{\textit {V}}}}}
   when Volume is measured in cubic metres), wavelengths are comparable to the dimensions of the room, and so room resonances dominate. This transition frequency is popularly known as the Schroeder frequency, or the cross-over frequency and it differentiates the low frequencies which creates standing waves within small rooms from the mid and high frequencies.
The third region which extends approximately 2 octaves is a transition to the fourth zone.
In the fourth zone, sounds behave like rays of light bouncing around the room.

Natural modes
The sound wave has reflections at the walls, floor and ceiling of the room. The incident wave then has interference with the reflected one. This action creates standing waves that generate nodes and high pressure zones.
In 1981, in order to solve this problem, Oscar Bonello, professor at the University of Buenos Aires, formulated a modal density concept solution which used concepts from psychoacoustics. Called ""Bonello Criteria"", the method analyzes the first 48 room modes and plots the number of modes in each one-third of an octave. The curve increases monotonically (each one-third of an octave must have more modes than the preceding one). Other systems to determine correct room ratios have more recently been developed

Reverberation of the room
After determining the best dimensions of the room, using the modal density criteria, the next step is to find the correct reverberation time. The most appropriate reverberation time depends on the use of the room. Times about 1.5 to 2 seconds are needed for opera theaters and concert halls. For broadcasting and recording studios and conference rooms, values under one second are frequently used. The recommended reverberation time is always a function of the volume of the room. Several authors give their recommendations  A good approximation for Broadcasting Studios and Conference Rooms is: TR[1 kHz] = [0,4 log (V+62)] – 0,38 TR in seconds and V=volume of the room in m3  The ideal RT60 must have the same value at all frequencies from 30 to 12,000 Hz. Or, at least, it is acceptable to have a linear rising from 100% at 500 Hz to 150% down to 62 Hz
To get the desired RT60, several acoustics materials can be used as described in several books. A valuable simplification of the task was proposed by Oscar Bonello in 1979  It consists of using standard acoustic panels of 1 m2 hung from the walls of the room (only if the panels are parallel). These panels use a combination of three Helmholtz resonators and a wooden resonant panel. This system gives a large acoustic absorption at low frequencies (under 500 Hz) and reduces at high frequencies to compensate for the typical absorption by people, lateral surfaces, ceilings, etc.

See also
Acoustic board
Acoustic space
Acoustics
Anechoic room
Architectural acoustics
Digital room correction
Noise control
Reverberation
Sound proofing
Whispering gallery


== References ==",Category:Building engineering,1
226,227,Sonomètre of Loulié,"The sonomètre is a tuning device invented circa 1694 by Étienne Loulié to facilitate the tuning of stringed instruments. Sébastien de Brossard considered this device to be ""one of the finest inventions"" of the seventeenth century.
On July 4, 1699, Étienne Loulié had the honor of presenting two inventions before the Royal Academy of Sciences in Paris. The records of the Academy (Procès verbaux) for that day read:
""Monsieur Loulié showed the Company an new machine he has invented, which he calls the sonomètre, by means of which anyone who has never tuned a harpsichord, as long as he has a sufficiently good ear to tune one string in unison with another, and one octave in unison with another, will on the first attempt tune the harpsichord as quickly, more easily, and in less time than the best music master tuning it by the ordinary method.""

The devices
Loulié invented both a table-top and a portable model of the sonomètre. Both versions are based on the same components. The first component is an instrument string (shown in red on the modified engravings) that is strung from one end to the other of a shallow rectangular wooden box. This string represents the ""monochord"" with which the sources equate the invention. The second component consists of one or more movable pieces of wood (shown in yellow on the engravings) that are marked to represent the 11 notes of the musical scale. The third component consists of something with which to pluck the string (shown in green on the engravings), in one case the user's finger, in the other a lever.

The box of the portable sonomètre is only 5?1?4 inches long. It could fit into a large pocket. To obtain a specific note of the scale – for example the F# shown in the engraving – a graduated strip of wood (highlighted in yellow) is slid out from at opening at the of the box and is held in position (at ""Fa#"") with a pin inserted in a hole in the strip. Pulling out this graduated strip of wood causes the inner end of the movable piece to rest on the string, shortening or lengthening its length so that the desired pitch will be sounded. When everything is in place, the user plucks the string with his finger, at the point marked N (circled in green), and F# is sounded. He plucks the string as often as needed, until the F-sharp key of the instrument being tuned matches the sound being produced by the sonomètre. At that point he tunes all the F-sharp strings on the instrument, to put them in unison with the original F#. And so on, for the entire octave.

At first glance the larger sonomètre resembles the keyboard of a harpsichord, with its dark and light keys. However, the 11 raised dark objects are not keys, they are low triangular wedges that function like the frets of a viol. That is to say, when one of the 11 levers is pulled forward – as UT, C, is in the engraving (yellow) – the little wedge comes into contact with the string (red). A small L-shaped tool is then positioned over the string, pressing it down firmly, as a finger presses the string of a viol onto a fret. When everything is ready, the user presses a lever (green) in the front of the box and activates a harpsichord-like jack that rises and plucks the string, sounding a C. And so on, until the user has set the 11 pitches that will permit him to tune the entire keyboard.
The original illustration of this table-top device included the ""proportions"" of an ""exact division of the notes"" into the larger or smaller intervals characteristic of the unequal temperament preferred by Loulié.
In 1701 Loulié's former colleague, Joseph Sauveur, pointed out to his colleagues at the Academy of Sciences the shortcomings of Loulié's sonomètre and presented his own version of the device.

The Academy's description
The sonomètre invented by Monsieur Loulié:
[In Figure 1] AB is a box that contains a sliding piece, DEF, that runs along the other piece, LM, that is attached to the bottom of the box. The end at ED comes out through an opening similar to the one cut in B. The other end [of the sliding piece], F, has a sort of right-angle bar [équerre] that is attached to it with a screw and that is pushed by a spring so that this right angle will pluck the string HNG at the place marked I.
The second figure [the sliding piece of wood] is the actual size and is divided according to the necessary proportions that will permit the string to produce the sound one desires for tuning any instrument whatsoever, which is done as follows:
At each dividing line on piece DE, which one passes through the opening B in the box. there is a little pin. When one wishes to hear a note, one pulls on the piece, to move the pin for that note. Then, applying that pin with precision against the opening in the box, one plucks the string with one's finger at N, and the string produces the desired sound. This effect is produced by the different positions in which one puts the sliding piece (DE) and the right-angle piece over the string (HG) The different distances from H create the different sounds.
This instrument is portable. It fits easily into a pocket. It is even used by harpsichord makers, who employ it to tune those sorts of instruments.
Another sonomètre invented by Monsieur Loulié
The top of the box (ABCD) has, in its mid-length, several bridges fixed to the ends of that same number of little movable boards, which fit between [the horizontal sticks] FG and EH. There are twelve of these little boards, which mark the divisions of all the notes of the entire octave, including flats and sharps. A string (OQP) produces the sound of these different notes when it is pinched by the little jack [sauterau] that is activated inside the box by the key R, where it rests upon a little fulcrum. Note that do [ut or C] is exactly in the middle of the two fixed points O and P.
When one wishes to tune an instrument, one pulls toward oneself the [little board] of the note one wishes to produce, so that the raised bridge is under the string. And to ensure that the string will touch the bridge firmly, one places over it a piece in the form of a right angle square [équerre]. For example, if one wants a do, one pulls board LI and the bridge affixed to it (MN); and one puts the square behind the bridge, then plucks the string with the jack.
The third figure shows the exact division of the notes, the different distances being marked out.

References
Archives of the Académie des Sciences, Procès verbaux, vol. 18 bis
Machines et Inventions approuvées par l'Académie royale des sciences ... (Ghent: Martin, 1735), pp. 187–89
Étienne Loulié, Nouveau Sistème de musique... avec la description et l'usage du sonomètre (Paris: Ballard, 1698)",Category:Acoustics,1
227,228,Brainwave entrainment,"Brainwave entrainment, also referred to as brainwave synchronization and neural entrainment, refers to the capacity of the brain to naturally synchronize its brainwave frequencies with the rhythm of periodic external stimuli, most commonly auditory, visual, or tactile.
It is hypothesized that listening to these beats of certain frequencies one can induce a desired state of consciousness that corresponds with specific neural activity. It is widely accepted that patterns of neural firing, measured in Hz, correspond with states of alertness such as focused attention, deep sleep, etc.

Neural oscillation and electroencephalography (EEG)
Neural oscillations are rhythmic or repetitive electrochemical activity in the brain and central nervous system. Such oscillations can be characterized by their frequency, amplitude and phase. Neural tissue can generate oscillatory activity driven by mechanisms within individual neurons, as well as by interactions between them. They may also adjust frequency to synchronize with the periodic vibration of external acoustic or visual stimuli.
The activity of neurons generate electric currents; and the synchronous action of neural ensembles in the cerebral cortex, comprising large numbers of neurons, produce macroscopic oscillations. These phenomena can be monitored and graphically documented by an electroencephalogram (EEG). The electroencephalographic representations of those oscillations are typically denoted by the term 'brainwaves' in common parlance.
The technique of recording neural electrical activity within the brain from electrochemical readings taken from the scalp originated with the experiments of Richard Caton in 1875, whose findings were developed into electroencephalography (EEG) by Hans Berger in the late 1920s.

Neural oscillation and cognitive functions
The functional role of neural oscillations is still not fully understood; however they have been shown to correlate with emotional responses, motor control, and a number of cognitive functions including information transfer, perception, and memory. Specifically, neural oscillations, in particular theta activity, are extensively linked to memory function, and coupling between theta and gamma activity is considered to be vital for memory functions, including episodic memory.

Awareness and consciousness
Electroencephalography (EEG) has been most widely used in the study of neural activity generated by large groups of neurons, known as neural ensembles, including investigations of the changes that occur in electroencephalographic profiles during cycles of sleep and wakefulness. EEG signals change dramatically during sleep and show a transition from faster frequencies to increasingly slower frequencies, indicating a relationship between the frequency of neural oscillations and cognitive states including awareness and consciousness.

Entrainment
Meaning and origin of the term 'entrainment'
Entrainment is a term originally derived from complex systems theory, and denotes the way that two or more independent, autonomous oscillators with differing rhythms or frequencies, when situated in a context and at a proximity where they can interact for long enough, influence each other mutually, to a degree dependent on coupling force, such that they adjust until both oscillate with the same frequency. Examples include the mechanical entrainment or cyclic synchronization of two electric clothes dryers placed in close proximity, and the biological entrainment evident in the synchronized illumination of fireflies.
Entrainment is a concept first identified by the Dutch physicist Christiaan Huygens in 1665 who discovered the phenomenon during an experiment with pendulum clocks: He set them each in motion and found that when he returned the next day, the sway of their pendulums had all synchronized.
Such entrainment occurs because small amounts of energy are transferred between the two systems when they are out of phase in such a way as to produce negative feedback. As they assume a more stable phase relationship, the amount of energy gradually reduces to zero, with systems of greater frequency slowing down, and the other speeding up.
Subsequently, the term 'entrainment' has been used to describe a shared tendency of many physical and biological systems to synchronize their periodicity and rhythm through interaction. This tendency has been identified as specifically pertinent to the study of sound and music generally, and acoustic rhythms specifically. The most ubiquitous and familiar examples of neuromotor entrainment to acoustic stimuli is observable in spontaneous foot or finger tapping to the rhythmic beat of a song.

Exogenous entrainment
Exogenous rhythmic entrainment, which occurs outside the body, has been identified and documented for a variety of human activities, which include the way people adjust the rhythm of their speech patterns to those of the subject with whom they communicate, and the rhythmic unison of an audience clapping.
Even among groups of strangers, the rate of breathing, locomotive and subtle expressive motor movements, and rhythmic speech patterns have been observed to synchronize and entrain, in response to an auditory stimuli, such as a piece of music with a consistent rhythm. Furthermore, motor synchronization to repetitive tactile stimuli occurs in animals, including cats and monkeys as well as humans, with accompanying shifts in electroencephalogram (EEG) readings.

Endogenous entrainment
Examples of endogenous entrainment, which occurs within the body, include the synchronizing of human circadian sleep-wake cycles to the 24-hour cycle of light and dark. and the synchronization of a heartbeat to a cardiac pacemaker.

Brainwave entrainment
Brainwaves, or neural oscillations, share the fundamental constituents with acoustic and optical waves, including frequency, amplitude and periodicity. Consequently, Huygens' discovery precipitated inquiry into whether or not the synchronous electrical activity of cortical neural ensembles might not only alter in response to external acoustic or optical stimuli but also entrain or synchronize their frequency to that of a specific stimulus.
Brainwave entrainment is a colloquialism for such 'neural entrainment', which is a term used to denote the way in which the aggregate frequency of oscillations produced by the synchronous electrical activity in ensembles of cortical neurons can adjust to synchronize with the periodic vibration of an external stimuli, such as a sustained acoustic frequency perceived as pitch, a regularly repeating pattern of intermittent sounds, perceived as rhythm, or of a regularly rhythmically intermittent flashing light.

Music and the frequency following response
Changes in neural oscillations, demonstrable through electroencephalogram (EEG) measurements, are precipitated by listening to music, which can modulate autonomic arousal ergotropically and trophotropically, increasing and decreasing arousal respectively. Musical auditory stimulation has also been demonstrated to improve immune function, facilitate relaxation, improve mood, and contribute to the alleviation of stress. These findings have contributed to the development of neurologic music therapy, which uses music and song as an active and receptive intervention, to contribute to the treatment and management of disorders characterized by impairment to parts of the brain and central nervous system, including stroke, traumatic brain injury, Parkinson's disease, Huntington's disease, cerebral palsy, Alzheimer's disease, and autism.
Meanwhile, the therapeutic benefits of listening to sound and music is a well-established principle upon which the practice of receptive music therapy is founded. The term 'receptive music therapy' denotes a process by which patients or participants listen to music with specific intent to therapeutically benefit; and is a term used by therapists to distinguish it from 'active music therapy' by which patients or participants engage in producing vocal or instrumental music. Receptive music therapy is an effective adjunctive intervention suitable for treating a range of physical and mental conditions.
The Frequency following response (FFR), also referred to as Frequency Following Potential (FFP), is a specific response to hearing sound and music, by which neural oscillations adjust their frequency to match the rhythm of auditory stimuli. The use of sound with intent to influence cortical brainwave frequency is called auditory driving, by which frequency of neural oscillation is 'driven' to entrain with that of the rhythm of a sound source.

See also
Beat (acoustics)
Electroencephalography
Neural oscillation

References
Further reading
""Brain wave synchronization and entrainment to periodic acoustic stimuli"". Neuroscience Letters. 31 August 2007. Retrieved April 5, 2017. 
Kitajo, K.; Hanakawa, T.; Ilmoniemi, R.J.; Miniussi, C. (2015). Manipulative approaches to human brain dynamics:. Frontiers Research Topics. Frontiers Media SA. p. 165. ISBN 978-2-88919-479-7. 
Thaut, M. H., Rhythm, Music, and the Brain: Scientific Foundations and Clinical Applications (Studies on New Music Research). New York, NY: Routledge, 2005.
Berger, J. and Turow, G. (Eds.), Music, Science, and the Rhythmic Brain : Cultural and Clinical Implications. New York, NY: Routledge, 2011.

External links
This is your brain on communication | Uri Hasson (TEDtalk)",Category:All articles that may contain original research,1
228,229,Seashell resonance,"There is a popular folk myth that if one holds a seashell—specifically, most often, a conch shell—to one's ear, one can hear the sound of the ocean.
The rushing sound that one hears is in fact the noise of the surrounding environment, resonating within the cavity of the shell. The same effect can be produced with any resonant cavity, such as an empty cup or even by simply cupping one's hand over one's ear. The similarity of the noise produced by the resonator to that of the oceans is due to the resemblance between ocean movements and airflow.
The resonator is simply attenuating some frequencies of the ambient noise in the environment, including air flowing within the resonator and sound originating within the human body itself, more than others.
The human ear picks up sounds made by the human body as well, including the sounds of blood flowing and muscles acting. These sounds are normally discarded by the brain; however, they become more obvious when louder external sounds are filtered out. This occlusion effect occurs with seashells, cups, or hands held over one's ears, and also with circumaural headphones, whose cups form a seal around the ear, raising the acoustic impedance to external sounds.

References
External links
How Stuff Works",Category:Acoustics,1
229,230,Acousto-electronics,"Acousto-electronics (also spelled 'Acoustoelectronics') is a branch of physics, acoustics and electronics that studies interactions of ultrasonic and hypersonic waves in solids with electrons and with electro-magnetic fields. Typical phenomena studied in acousto-electronics are acousto-electric effect and also amplification of acoustic waves by flows of electrons in piezoelectric semiconductors, when the drift velocity of the electrons exceeds the velocity of sound. The term 'acousto-electronics' is often understood in a wider sense to include numerous practical applications of the interactions of electro-magnetic fields with acoustic waves in solids. In particular, these are signal processing devices using surface acoustic waves (SAW), different sensors of temperature, pressure, humidity, acceleration, etc.

See also
Acousto-optics
Rayleigh wave
Love wave
Interdigital transducer
Picosecond ultrasonics

Further reading
White, D.L., Amplification of ultrasonic waves in piezoelectric semiconductors, Journal of Applied Physics, 33(8), 2547 - 2554 (1962).
Hickernell, F.S., The piezoelectric semiconductor and acoustoelectronic device development in the sixties, IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control, 52(5), 737-45 (2005).
Gulyaev, Yu.V., Hickernell, F.S., Acoustoelectronics: History, present state, and new ideas for a new era, Acoustical Physics, 51(1), 81-88 (2005).

External links
Consortium for Applied Acoustoelectronic Technology - University of Central Florida",Category:Physics stubs,1
230,231,Category:Acousticians,,Category:Commons category without a link on Wikidata,1
231,232,Free field (acoustics),,Category:Acoustics,1
232,233,Thin-film bulk acoustic resonator,"A thin-film bulk acoustic resonator (FBAR or TFBAR) is a device consisting of a piezoelectric material sandwiched between two electrodes and acoustically isolated from the surrounding medium. FBAR devices using piezoelectric films with thicknesses ranging from several micrometres down to tenth of micrometres resonate in the frequency range of roughly 100 MHz to 10 GHz. Aluminium nitride and zinc oxide are two common piezoelectric materials used in FBARs.

Applications
A common application of FBARs is radio frequency (RF) filters for use in cell phones and other wireless applications. Such filters made from a network of resonators (either in half-ladder, full-ladder, lattice or stacked topologies) and are designed to remove unwanted frequencies from being transmitted in such devices, while allowing other specific frequencies to be received and transmitted. FBAR filters can also be found in duplexers. They have partially replaced an earlier technology based on surface acoustic wave (SAW) devices, due to smaller size and increased fabrication and operating efficiencies.
FBARs can be used by microwave oscillators.
FBARs can be used in sensor applications. For instance, when a FBAR device is put under mechanical pressure its resonance frequency will shift.
FBARs can also be integrated with power amplifiers (PA) or low noise amplifiers (LNA) to form an integrated module solution such as PA-duplexer module (PAD), or LNA-filter module.

See also
Resonance
Acoustic resonance

External links
University of Southern California explanation on the operation of FBAR's
Avago Technology product listing for FBAR duplexers


== Notes ==",Category:Sound,1
233,234,Acoustic cleaning,"Acoustic cleaning is a maintenance method used in material-handling and storage systems that handle bulk granular or particulate materials, such as grain elevators, to remove the buildup of material on surfaces. Acoustic cleaning apparatus, usually built into the material-handling equipment, works by generating powerful sound waves which shake particulates loose from surfaces, reducing the need for manual cleaning.

History and design
An acoustic cleaner consists of a sound source similar to an air horn found on trucks and trains, attached to the material-handling equipment, which directs a loud sound into the interior. It is powered by compressed air rather than electricity so there is no danger of sparking, which could set off an explosion. It consists of two parts:
The acoustic driver. In the driver, compressed air escaping past a diaphragm causes it to vibrate, generating the sound. It is usually made from solid machined stainless steel. The diaphragm, the only moving part, is usually manufactured from special aerospace grade titanium to ensure performance and longevity.
The bell, a flaring horn, usually made from spun 316 grade stainless steel. The bell serves as a sound resonator, and its flaring shape couples the sound efficiently to the air, increasing the volume of sound radiated.
The overall length of acoustic cleaner horns range from 430 mm to over 3 metres long. The device can operate from a pressure range of 4.8 to 6.2 bars or 70 to 90 psi. The resultant sound pressure level will be around 200 dB.
There are generally 4 ways to control the operation of an acoustic cleaner.
The most common is by a simple timer.
SCADA (Supervisory Control and Data Acquisition).
PLC (programmable logic controller).
Manually by Ball valve.
An acoustic cleaner will typically sound for 10 seconds and then wait for a further 500 seconds before sounding again. This ratio for on/off is approximately proportional to the working life of the diaphragm. Provided the operating environment is between ?40 C and 100 °C, a diaphragm should last between 3 and 5 years. The wave generator and the bell have a much longer life span and will often outlast the environment in which they operate.
The older bells which were made from cast iron were susceptible to rusting in certain environments. The new bells made from 316 spun steel have no problem with rust and are ideal for sterile environments such as found in the food industry or in pharmaceutical plants.
Acoustic cleaning began in the early 1970s with experiments using ship horns or air raid sirens. The first acoustic cleaners were made from cast iron. From 1990 onwards the technology became commercially viable and began to be used in dry processing, storage, transport, power generation and manufacturing industries. The latest technology uses 316 spun stainless steel to ensure optimum performance.

Operation and performance
The majority of acoustic cleaners operate in the audio frequency range from 60 hertz up to 420 Hz. However a few operate in the infrasonic range, below 40 Hz, which is mostly below the human hearing range, to satisfy strict noise control requirements. There are three scientific fields which converge in the understanding of acoustic cleaning technology.
Sound propagation. This relates to an understanding of the nature of the sound waves, how they vary and how they will interact with the environment.
Mathematics of the environment. Materials science, surface friction, distance and areas familiar to a mechanical engineer.
Chemical engineering. The chemical properties of the powder or substance to be debonded. Especially the auto adhesive properties of the powder.
An acoustic cleaner will create a series of very rapid and powerful sound induced pressure fluctuations which are then transmitted into the solid particles of ash, dust, granules or powder. This causes them to move at differing speeds and debond from adjoining particles and the surface that they are adhering to. Once they have been separated then the material will fall off due to gravity or it will be carried away by the process gas or air stream.
The key features which determine whether or not an acoustic cleaner will be effective for any given problem are the particle size range, the moisture content and the density of the particles as well as how these characteristics will change with temperature and time. Typically particles between 20 micrometres and 5 mm with moisture content below 8.5% are ideal. Upper temperature limits are dependent upon the melting point of the particles and acoustic cleaners have been employed at temperatures above 1000 C to remove ash build-up in boiler plants.
It is important to match the operating frequencies to the requirements. Higher frequencies can be directed more accurately whilst lower frequencies will carry further, and are generally used for more demanding requirements. A typical selection of frequencies available would be as follows:
420 Hz for a small acoustic cleaner which might be used to clear bridging at the base of a silo.
350 Hz will be more powerful and this frequency can be used to unblock material build-up in ID (induced draft) fans, filters, cyclones, mixers, dryers and coolers.
230 Hz. At this frequency, the power involved is sufficient to use in most electricity generation applications.
75 Hz and 60 Hz. These are generally the most powerful acoustic cleaners and are often used in large vessels and silos.

Health and safety
The introduction of acoustic cleaners has been a significant improvement in many areas of health and safety. For instance in silo cleaning - the previous solutions tended to be intrusive or destructive. Air cannons, soot blowers, external vibrators, hammering or costly man entry are all superseded by noninvasive sonic horns. An acoustic cleaner requires no down time and will operate during normal usage of the site. Taking the example of silo cleaning a little further, there are two typical problems.

Bridging
This is when the silo blocks at the outlet. Previously the problem was addressed by manual cleaning from underneath the silo which in its turn introduced significant risk from falling material when the blockage was cleared. An acoustic cleaner is able to operate from the top of a silo through in situ material to clear the blockage at the base.

Rat holing
Compaction on the side of a silo. This not only reduces the operating volume in a silo but it also compromises quality control by disrupting the first in first out cycle. Older material compacted on the side of a silo can also start to degrade and produce dangerous gases. An acoustic cleaner will produce sound waves which will make the compacted material resonate at a different rate to the surrounding environment resulting in debonding and clearance.

Advantages of acoustic cleaners
Repetitive use during operations means that there are fewer unscheduled shut downs.
Improved material flow by the elimination of hang-ups, blocking and bridging.
Minimisation of cross contamination by ensuring complete emptying of the environment.
Improved cleaning and reduction of health and safety risks.
Increased energy efficiency. Reducing the buildup on heat exchange surfaces results in lower energy usage.
Extended plant life. Aggressive cleaning regimes are avoided.
Ease of operation. It is easy to automate the horns either at regular intervals or to tie the sounding in to changes in their environment such as pressure or flow rates.
Importantly they prevent the material buildup problem from occurring in the first place.
These advantages mean that the financial payback is often very quick.
It is also possible to compare acoustic cleaners directly to alternative solutions.
Air cannons. These are well established but are expensive with limited coverage thus requiring multi unit purchase. They are also noise intrusive and have a high compressed air consumption.
Vibrators. These are easy to fit to an empty silo but can cause structural damage as well as contributing to powder compaction.
Low friction linings. These are very quiet but are expensive to install. Also they are prone to erosion and can then contaminate the environment or product.
Inflatable pads and liners. Again these are easy to install in an empty silo. They help side wall buildup but have no impact on bridging. They are also hard to maintain and can cause compaction.
Fluidisation through a 1 way membrane. This can help already compacted material. However they are expensive and difficult to install and maintain. They can also contribute to mechanical interlocking and bridging.

Specific applications for acoustic cleaners
Boilers. Cleaning of the heat transfer surfaces.
Electrostatic precipitators. Acoustic cleaners are being used for cleaning hoppers, turning vanes, distribution plates, collecting plates and electrode wires.
Super heaters, economisers and air heaters.
Duct work.
Filters. Acoustic cleaners are used on reverse air, pulse jet and shaker units. They are effective in reducing pressure drop across the collection surface which will increase bag life and prevent hopper pluggage. Generally they can totally replace the both reverse air fans and shaker units and significantly reduce the compressed air requirement on pulse jet filters.
ID fans. Acoustic cleaning helps to provide a uniform cleaning pattern even for inaccessible parts of the fan. This maintains the balance of the fan.
Kiln inlet. Acoustic cleaners help to prevent particulate buildup at the kiln inlet and this will minimise nose ring formation.
Mechanical pre Collectors. Acoustic cleaners help prevent buildup around the impellers and between the tubes.
Mills. Acoustic cleaners help maintain material flow and also prevent blockages in the pre grind silos. They also help prevent material buildup in the downstream separators and fans.
Planetary Coolers. Acoustic cleaners help prevent bridging and ensure complete evacuation.
Precipitator. Acoustic cleaners help clean the turning vanes, distribution plates, collecting plates and electrode wires. They can either assist or replace the mechanical rapping systems. They also prevent particulate buildup in the under hoppers which would otherwise result in opacity spiking.
Pre heaters. Used in towers, gas risers, cyclones and fans.
Ship cargo holds. Used both to clean and de aerate current loads.
Silos and hoppers. To prevent bridging and rat holing.
Static cyclones. Acoustic cleaners will work both within the cyclone and with the associated duct work.

See also
Ultrasonic cleaner - Cleaning using higher frequencies than those found in acoustic cleaners.
Sonic soot blowers - Cleaners targeted at ash and particulate build ups.
Ultrasonic homogenizer

External links
Infrafone - Sonic soot cleaning
Acoustic soot blower and method
Acoustic cleaners The technology and specifications provided by Primasonics.
Acoustic Cleaning Systems Acoustic cleaning specifications, troubleshooting, and technical information provided by GE Energy.",Category:Audio engineering,1
234,235,Acoustic quieting,"This article primarily discusses mechanical and acoustic noise. See Noise reduction for electronic noise.
For noise masking by saturation, see Sound masking or pink noise.
For signature reduction in general see Stealth technology.

Acoustic quieting is the process of making machinery quieter by damping vibrations to prevent them from reaching the observer. Machinery vibrates, causing sound waves in air, hydroacoustic waves in water, and mechanical stresses in solid matter. Quieting is achieved by absorbing the vibrational energy or minimizing the source of the vibration. It may also be redirected away from the observer.
One of the major reasons for the development of acoustic quieting techniques was for making submarines difficult to detect by sonar. This military goal of the mid- and late-twentieth century allowed the technology to be adapted to many industries and products, such as computers (e.g. hard drive technology), automobiles (e.g. motor mounts), and even sporting goods (e.g. golf clubs).

Aspects of acoustic quieting
When the goal is acoustic quieting, a number of different aspects might be considered. Each aspect of acoustics can be taken alone or in concert so that the end result is that the reception of noise by the observer is minimized.
Acoustic quieting might consider...
Noise generation: by limiting the noise at its source,
Sympathetic vibrations: by acoustic decoupling,
Resonations: by acoustic damping or changing the size of the resonator,
Sound transmissions: by reducing transmission using many methods (depending whether the transmission is through air, liquid, or solid), or
Sound reflections: by limiting the reflection using many methods, e.g. by using acoustic absorption (deadening) materials, trapping the sound, opening a ""window"" to let sound out, etc.
By analyzing the entire sequence of events, from the source to the observer, an acoustic engineer can provide many ways to quieten the machine. The challenge is to do this in a practical and inexpensive way. The engineer might focus on changing materials, using a damping material, isolating the machine, running the machine in a vacuum, or running the machine slower.

Methods of quieting
Mechanical acoustic quieting
Sound isolation: Noise isolation is isolating noise to prevent it from transferring out of one area, using barriers like deadening materials to trap sound and vibrational energy. Example: In home and office construction, many builders place sound-control barriers (such as fiberglass batting) in walls to deaden the transmission of noise through them.

Noise absorption: In architectural acoustics, unwanted sounds can be absorbed rather than reflected inside the room of an observer. This is useful for noises with no point source and when a listener needs to hear sounds only from a point source and not echo reflections. Example: In a recording studio, sound proofing is accomplished with bass traps and anechoic chambers. Wallace Sabine, an American physicist, is credited with studying sound reverberations in 1900, and Carl Eyring revised his equations in 1930 for Bell Labs. Another example is the ubiquitous use of dropped ceilings and acoustical tiles in modern office buildings with high ceilings. Submarine hulls have special coatings that absorb sound.
Acoustic damping: Vibration isolation prevents vibration from transferring beyond the device into another material. Damping mounts have progressed in the industry to offer vibrational resistance in many degrees of freedom. Recent advances include shock isolators damping in at least six degrees of freedom. Acoustic damping also has uses in seismic shock protection of buildings. Motors and rotating shafts are commonly fitted with these mounts at the points where they contact the building or the chassis of a large machine.
Acoustic decoupling: certain parts of a machine can be built to keep the frame, chassis, or external shafts from receiving unwanted vibrations from a moving part. Example: Volkswagen has registered a patent for an ""acoustically decoupled underbody for a motor vehicle."". Another example: Western Digital has registered a patent for an ""acoustic vibration decoupler for a disk drive pivot bearing assembly."".
Preventing stalls: Whenever a machine undergoes an aerodynamic stall, it will abruptly vibrate.
Preventing cavitation: When a machine is in contact with a fluid, it may be susceptible to cavitation. The sounds of gas bubbles imploding is the source of the noise. Ships and submarines which have screws that cavitate are more vulnerable to detection by sonar.
Preventing water hammer: In hydraulics and plumbing, water hammer is a known cause for the failure of piping systems. It also generates considerable noise. A valve that abruptly opens or shuts is the most common cause for water hammer.
Shock absorption: Just as automotive shock absorbers are used to prevent mechanical shocks from reaching the passengers in a car, they are also important for quieting shocks.
Reduction of resonance: Essentially any piece of metal or glass has certain frequencies to which it is susceptible to resonate. A machine that resonates would make a tremendous noise. Resonance also occurs in enclosures, such as when echoes reverberate in an ocarina or the pipe of a pipe organ.
Material selection: By choosing nonmetallic components, the transmission of sound and vibrations can be minimized. For example: instead of using rigid brass fittings, a machine using flexible plastic pipe fittings may be much quieter. In some cases air can be evacuated from a machine and sealed hermetically, the vacuum inside becoming a barrier to sound transmission. In cases where porous plastic materials are used in acoustic applications, the porosity of the plastic is adjusted to either dampen specific wavelengths or for minimal sound loss in a speaker grill cover.

Quieting for specific observers
Underwater acoustics: All of the above types of acoustic quieting apply to submarines. Additionally, a submarine may employ a tactic that prevent sounds from reaching a listener at a particular ocean depth. Operating below the depth of the sound channel axis, where the speed of sound in water is the lowest, a submarine can prevent detection by surface ships.
Sound refraction: Just as a submarine can use refraction to hide its acoustic signature from surface vessels, the same principle of sound refraction can be used to prevent certain observers from hearing the noise. For example, an outdoor observer close to the ground will have sound waves refracted toward him when the ground is cooler than the ambient air and away from him when the ground is hotter than the air.
Sound Redirection: One of the obvious ways to reduce the received sound level of an observer is to place the observer out of the path of the highest amplitude sounds. For example, if we mark off a circle around a jet engine and make sound power level observations along that circle, we would expect that the sound is loudest directly in line with the jet's exhaust. Observations perpendicular to the exhaust would be significantly quieter.
Hearing protection: An observer may be forced to wear ear plugs in areas of high ambient noise levels. This may be the only quieting method available in areas of noise pollution, such as an open-air firing range or an airport.

Electronic quieting
Electronic vibration control: Electronics, sensors, and computers are now employed to reduce vibration. Using high speed logic, vibrations can be damped quickly and effectively by counteracting the motion before it exceeds a certain threshold.
Electronic noise control: Electronics, sensors, and computers are also employed to cancel noise by using phase cancellation which matches the sound amplitude with a wave of the opposite polarity. This method employs the use of an active sound generating device, such as a loudspeaker to counteract ambient noise in an area. See noise-canceling headphone. Workers in noisy environments may favor this method over ear plugs.
Noise reduction: In sound and video equipment, noise reduction is the process of removing noise from a signal. This is strictly for electronic noise or noise which has been detected and put into electronic form.
Noise canceling: If both the noise and the signal are received by an electronic or digital medium, noise can be filtered from the signal electronically and retransmitted without the noise. See noise-canceling microphone. Helicopter pilots rely on this technology to speak on the radio.

See also
Longitudinal wave
Soundproofing
Mechanical resonance
Sound masking
Seismic retrofit
Helicopter noise reduction
Muffler
Deperming
Degaussing


== References ==",Category:Stealth technology,1
235,236,Stokes's law of sound attenuation,"Stokes's law of sound attenuation is a formula for the attenuation of sound in a Newtonian fluid, such as water or air, due to the fluid's viscosity. It states that the amplitude of a plane wave decreases exponentially with distance traveled, at a rate 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
   given by

  
    
      
        ?
        =
        
          
            
              2
              ?
              
                ?
                
                  2
                
              
            
            
              3
              ?
              
                V
                
                  3
                
              
            
          
        
      
    
    {\displaystyle \alpha ={\frac {2\eta \omega ^{2}}{3\rho V^{3}}}}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \eta }
   is the dynamic viscosity coefficient of the fluid, 
  
    
      
        ?
      
    
    {\displaystyle \omega }
   is the sound's frequency, 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   is the fluid density, and 
  
    
      
        V
      
    
    {\displaystyle V}
   is the speed of sound in the medium:
The law and its derivation were published in 1845 by physicist G. G. Stokes, who also developed the well-known Stokes's law for the friction force in fluid motion.

Interpretation
Stokes's law applies to sound propagation in an isotropic and homogeneous Newtonian medium. Consider a plane sinusoidal pressure wave that has amplitude 
  
    
      
        
          A
          
            0
          
        
      
    
    {\displaystyle A_{0}}
   at some point. After traveling a distance 
  
    
      
        d
      
    
    {\displaystyle d}
   from that point, its amplitude 
  
    
      
        A
        (
        d
        )
      
    
    {\displaystyle A(d)}
   will be

  
    
      
        A
        (
        d
        )
        =
        
          A
          
            0
          
        
        
          e
          
            ?
            ?
            d
          
        
      
    
    {\displaystyle A(d)=A_{0}e^{-\alpha d}}
  
The parameter 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
   is dimensionally the reciprocal of length. In the International System of Units (SI), it is expressed in neper per meter or simply reciprocal of meter (
  
    
      
        
          
            m
          
          
            ?
            1
          
        
      
    
    {\displaystyle \mathrm {m} ^{-1}}
  ). That is, if 
  
    
      
        ?
        =
        1
        
          
            m
          
          
            ?
            1
          
        
      
    
    {\displaystyle \alpha =1\mathrm {m} ^{-1}}
  , the wave's amplitude decreases by a factor of 
  
    
      
        1
        
          /
        
        e
      
    
    {\displaystyle 1/e}
   for each meter traveled.

Importance of volume viscosity
The law is amended to include a contribution by the volume viscosity 
  
    
      
        
          ?
          
            
              v
            
          
        
      
    
    {\displaystyle \eta ^{\mathrm {v} }}
  :

  
    
      
        ?
        =
        
          
            
              2
              (
              ?
              +
              3
              
                ?
                
                  
                    v
                  
                
              
              
                /
              
              2
              )
              
                ?
                
                  2
                
              
            
            
              3
              ?
              
                V
                
                  3
                
              
            
          
        
      
    
    {\displaystyle \alpha ={\frac {2(\eta +3\eta ^{\mathrm {v} }/2)\omega ^{2}}{3\rho V^{3}}}}
  
The volume viscosity coefficient is relevant when the fluid's compressibility cannot be ignored, such as in the case of ultrasound in water. The volume viscosity of water at 15 C is 3.09 centipoise.

Modification for very high frequencies
Stokes's law is actually an asymptotic approximation for low frequencies of a more general formula:

  
    
      
        2
        
          
            (
            
              
                
                  ?
                  V
                
                ?
              
            
            )
          
          
            2
          
        
        =
        
          
            1
            
              1
              +
              
                ?
                
                  2
                
              
              
                ?
                
                  2
                
              
            
          
        
        ?
        
          
            1
            
              1
              +
              
                ?
                
                  2
                
              
              
                ?
                
                  2
                
              
            
          
        
      
    
    {\displaystyle 2\left({\frac {\alpha V}{\omega }}\right)^{2}={\frac {1}{\sqrt {1+\omega ^{2}\tau ^{2}}}}-{\frac {1}{1+\omega ^{2}\tau ^{2}}}}
  
where the relaxation time 
  
    
      
        ?
      
    
    {\displaystyle \tau }
   is given by:

  
    
      
        ?
        =
        
          
            
              4
              ?
              
                /
              
              3
              +
              
                ?
                
                  
                    v
                  
                
              
            
            
              ?
              
                V
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \tau ={\frac {4\eta /3+\eta ^{\mathrm {v} }}{\rho V^{2}}}}
  
The relaxation time is about 
  
    
      
        
          10
          
            ?
            12
          
        
        
          s
        
      
    
    {\displaystyle 10^{-12}\mathrm {s} }
   (one picosecond), corresponding to a frequency of about 1000 GHz. Thus Stokes's law is adequate for most practical situations.


== References ==",Category:Acoustics,1
236,237,Geophysical MASINT,"Geophysical MASINT is a branch of Measurement and Signature Intelligence (MASINT) that involves phenomena transmitted through the earth (ground, water, atmosphere) and manmade structures including emitted or reflected sounds, pressure waves, vibrations, and magnetic field or ionosphere disturbances.
According to the United States Department of Defense, MASINT is technically derived intelligence (excluding traditional imagery IMINT and signals intelligence SIGINT) that – when collected, processed, and analyzed by dedicated MASINT systems – results in intelligence that detects, tracks, identifies, or describes the signatures (distinctive characteristics) of fixed or dynamic target sources. MASINT was recognized as a formal intelligence discipline in 1986. Another way to describe MASINT is a ""non-literal"" discipline. It feeds on a target's unintended emissive by-products, the ""trails"" - the spectral, chemical or RF that an object leaves behind. These trails form distinct signatures, which can be exploited as reliable discriminators to characterize specific events or disclose hidden targets.""
As with many branches of MASINT, specific techniques may overlap with the six major conceptual disciplines of MASINT defined by the Center for MASINT Studies and Research, which divides MASINT into Electro-optical, Nuclear, Geophysical, Radar, Materials, and Radiofrequency disciplines.

Military Requirements
Geophysical sensors have a long history in conventional military and commercial applications, from weather prediction for sailing, to fish finding for commercial fisheries, to nuclear test ban verification. New challenges, however, keep emerging.
For first-world military forces opposing other conventional militaries, there is an assumption that if a target can be located, it can be destroyed. As a result, concealment and deception have taken on new criticality. ""Stealth"" low-observability aircraft have gotten much attention, and new surface ship designs feature observability reduction. Operating in the confusing littoral environment produces a great deal of concealing interference.
Of course, submariners feel they invented low observability, and others are simply learning from them. They know that going deep or at least ultraquiet, and hiding among natural features, makes them very hard to detect.
Two families of military applications, among many, represent new challenges against which geophysical MASINT can be tried. Also, see Unattended Ground Sensors.

Deeply Buried Structures
One of the easiest ways for nations to protect weapons of mass destruction, command posts, and other critical structures is to bury them deeply, perhaps enlarging natural caves or disused mines. Deep burial is not only a means of protection against physical attack, as even without the use of nuclear weapons, there are deeply penetrating precision guided bombs that can attack them. Deep burial, with appropriate concealment during construction, is a way to avoid the opponent's knowing the buried facility's position well enough to direct precision guided weapons against it.
Finding deeply buried structures, therefore, is a critical military requirement. The usual first step in finding a deep structure is IMINT, especially using hyperspectral IMINT sensors to help eliminate concealment. ""Hyperspectral images can help reveal information not obtainable through other forms of imagery intelligence such as the moisture content of soil. This data can also help distinguish camouflage netting from natural foliage."" Still, a facility dug under a busy city would be extremely hard to find during construction. When the opponent knows that it is suspected that a deeply buried facility exists, there can be a variety of decoys and lures, such as buried heat sources to confuse infrared sensors, or simply digging holes and covering them, with nothing inside.
MASINT using acoustic, seismic, and magnetic sensors would appear to have promise, but these sensors must be fairly close to the target. Magnetic Anomaly Detection (MAD) is used in antisubmarine water, for final localization before attack. The existence of the submarine is usually established through passive listening and refined with directional passive sensors and active sonar.
Once these sensors (as well as HUMINT and other sources) have failed, there is promise for surveying large areas and deeply concealed facilities using gravitimetric sensors. Gravity sensors are a new field, but military requirements are making it important while the technology to do it is becoming possible.

Naval Operations in Shallow Water
Especially in today's ""green water"" and ""brown water"" naval applications, navies are looking at MASINT solutions to meet new challenges of operating in littoral areas of operations. This symposium found it useful to look at five technology areas, which are interesting to contrast to the generally accepted categories of MASINT: acoustics and geology and geodesy/sediments/transport, nonacoustical detection (biology/optics/chemistry), physical oceanography, coastal meteorology, and electromagnetic detection.
Although it is unlikely there will ever be another World War II-style opposed landing on a fortified beach, another aspect of the littoral is being able to react to opportunities for amphibious warfare. Detecting shallow-water and beach mines remains a challenge, since mine warfare is a deadly ""poor man's weapon.""
While initial landings from an offshore force would be from helicopters or tiltrotor aircraft, with air cushion vehicles bringing ashore larger equipment, traditional landing craft, portable causeways, or other equipment will eventually be needed to bring heavy equipment across a beach. Shallow depth and natural underwater obstacles can block beach access to these craft and equipment, as can shallow-water mines. Synthetic Aperture Radar (SAR), airborne laser detection and ranging (LIDAR) and use of bioluminescence to detect wake trails around underwater obstacles all may help solve this challenge.
Moving onto and across the beach has its own challenges. Remotely operated vehicles may be able to map landing routes, and they, as well as LIDAR and multispectral imaging, may be able to detect shallow water. Once on the beach, the soil has to support heavy equipment. Techniques here include estimating soil type from multispectral imaging, or from an airdropped penetrometer that actually measures the loadbearing capacity of the surface.

Weather and Sea Intelligence MASINT
The science and art of weather prediction used the ideas of measurement and signatures to predict phenomena, long before there were any electronic sensors. Masters of sailing ships might have no more sophisticated instrument than a wetted finger raised to the wind, and the flapping of sails.
Weather information, in the normal course of military operations, has a major effect on tactics. High winds and low pressures can change artillery trajectories. High and low temperatures cause both people and equipment to require special protection. Aspects of weather, however, also can be measured and compared with signatures, to confirm or reject the findings of other sensors.
The state of the art is to fuse meteorological, oceanographic, and acoustic data in a variety of display modes. Temperature, salinity and sound speed can be displayed horizontally, vertically, or in three-dimensional perspective.

Predicting Weather based on Measurements and Signatures
While early sailors had no sensors beyond their five senses, the modern meteorologist has a wide range of geophysical and electro-optical measuring devices, operating on platforms from the bottom of the sea to deep space. Prediction based on these measurements are based on signatures of past weather events, a deep understanding of theory, and computational models.
Weather predictions can give significant negative intelligence, when the signature of some combat system is such that it can operate only under certain weather conditions. Weather has long been an extremely critical part of modern military operations, as when the decision to land at Normandy on June 6, rather than June 5, 1944 depended on Dwight D. Eisenhower's trust in his staff weather advisor, Group Captain James Martin Stagg. It is rarely understood that something as fast as a ballistic missile reentry vehicle, or as ""smart"" as a precision guided munition, can still be affected by winds in the target area.
As part of Unattended Ground Sensors,. The Remote Miniature Weather Station (RMWS), from System Innovations, is an air-droppable version with a lightweight, expendable and modular system with two components: a meteorological (MET) sensor and a ceilometer (cloud ceiling height) with limited MET. The basic MET system is surface-based and measures wind speed and direction, horizontal visibility, surface atmospheric pressure, air temperature and relative humidity. The ceilometer sensor determines cloud height and discrete cloud layers. The system provides near-real-time data capable of 24-hour operation for 60 days. The RMWS can also go in with US Air Force Special Operations combat weathermen
The man-portable version, brought in by combat weathermen, has an additional function, as remote miniature ceilometer. Designed to measure multiple layer cloud ceiling heights and then send that data via satellite communications link to an operator display, the system uses a Neodinum YAG (NdYAG), 4 megawatt non-eye safe laser. According to one weatherman, ""We have to watch that one,” he said. “Leaving it out there basically we’re worried about civilian populace going out there and playing with it—firing the laser and there goes somebody’s eye. There are two different units [to RMWS]. One has the laser and one doesn’t. The basic difference is the one with the laser is going to give you cloud height.""

Hydrographic Sensors
Hydrographic MASINT is subtly different from weather, in that it considers factors such as water temperature and salinity, biologic activities, and other factors that have a major effect on sensors and weapons used in shallow water. ASW equipment, especially acoustic performance depends on the season the specific coastal site. Water column conditions, such as temperature, salinity, and turbidity are more variable in shallow than deep water. Water depth will influence bottom bounce conditions, as will the material of the bottom. Seasonal water column conditions (particularly summer versus winter) are inherently more variable in shallow water than in deep water.
While much attention is given to shallow waters of the littoral, other areas have unique hydrographic characteristics.

regional areas with fresh water eddies
open ocean salinity fronts
near ice floes
under ice

A submarine tactical development activity observed, ""Fresh water eddies exist in many areas of the world. As we have experienced recently in the Gulf of Mexico using the Tactical Oceanographic Monitoring System (TOMS), there exist very distinct surface ducts that causes the Submarine Fleet Mission Program Library (SFMPL) sonar prediction to be unreliable. Accurate bathythermic information is paramount and a precursor for accurate sonar predictions.”

Temperature and Salinity
Critical to the prediction of sound, needed by active and passive MASINT systems operating in water is knowing the temperature and salinity at specific depths. Antisubmarine aircraft, ships, and submarines can release independent sensors that measure the water temperature at various depths. The water temperature is critically important in acoustic detections, as changes in water temperature at thermoclines can act as a ""barrier"" or ""layer"" to acoustic propagation. To hunt a submarine, which is aware of water temperature, the hunter must drop acoustic sensors below the thermocline.
Water conductivity is used as a surrogate marker for salinity. The current and most recently developed software, however, does not give information on suspended material in the water or bottom characteristics, both considered critical in shallow-water operations.
The US Navy does this by dropping expendable probes, which transmit to a recorder, of 1978-1980 vintage, the AN/BQH-7 for submarines and the AN/BQH-71 for surface ships. While the redesign of the late seventies did introduce digital logic, the devices kept hard-to-maintain analog recorders, and maintainability became critical by 1995. A project was begun to extend with COTS components, to result in the AN/BQH-7/7A EC-3. In 1994-5, the maintainability of the in-service units became critical.
Variables in selecting the appropriate probe include:

Maximum depth sounded
Speed of launching vessel
Resolution Vertical Distance Between Data Points (ft)
Depth Accuracy

Biomass
Large schools of fish contain enough entrapped air to conceal the sea floor, or manmade underwater vehicles and structures. Fishfinders, developed for commercial and recreational fishing, are specialized sonars that can identify acoustic reflections between the surface and the bottom. Variations on commercial equipment are apt to be needed, especially in littoral areas rich in marine life.

Sea Bottom Measurement
A variety of sensors can be used to characterise the sea bottom into, for example, mud, sand, and gravel. Active acoustic sensors are the most obvious, but there is potential information from gravitimetric sensors, electro-optical and radar sensors for making inferences from the water surface, etc.
Relatively simple sonars such as echo sounders can be promoted to seafloor classification systems via add-on modules, converting echo parameters into sediment type. Different algorithms exist, but they are all based on changes in the energy or shape of the reflected sounder pings.
Side-scan sonars can be used to derive maps of the topography of an area by moving the sonar across it just above the bottom. Multibeam hull-mounted sonars are not as precise as a sensor near the bottom, but both can give reasonable three-dimensional visualization.
Another approach comes from greater signal processing of existing military sensors. The US Naval Research Laboratory demonstrated both seafloor characterization, as well as subsurface characteristics of the seafloor. Sensors used, in different demonstrations, included normal incidence beams from the AM/UQN-4 surface ship depthfinder, and AN/BQN-17 submarine fathometer; backscatter from the Kongsberg EM-121 commercial multibeam sonar; AN/UQN-4 fathometers on mine countermeasures (MCM) ships, and the AN/AQS-20 mine-hunting system. These produced the ""Bottom and Subsurface Characterization"" graphic.

Weather effects on chemical, biological, and radiological weapon propagation
One of the improvements in the Fuchs 2 reconnaissance vehicle is adding onboard weather instrumentations, including data such as wind direction and speed;, air and ground temperature; barometric pressure and humidity.

Acoustic MASINT
This includes the collection of passive or active emitted or reflected sounds, pressure waves or vibrations in the atmosphere (ACOUSTINT) or in the water (ACINT) or conducted through the ground Going well back into the Middle Ages, military engineers would listen to the ground for sounds of telltale digging under fortifications.
In modern times, acoustic sensors were first used in the air, as with artillery ranging in World War I. Passive hydrophones were used by the World War I Allies against German submarines; the UC-3, was sunk with the aid of hydrophone on 23 April 1916. Since submerged submarines cannot use radar, passive and active acoustic systems are their primary sensors. Especially for the passive sensors, the submarine acoustic sensor operators must have extensive libraries of acoustic signatures, to identify sources of sound.
In shallow water, there are sufficient challenges to conventional acoustic sensors that additional MASINT sensors may be required. Two major confounding factors are:

Boundary interactions. The effects of the seafloor and the sea surface on acoustic systems in shallow water are highly complex, making range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.
Practical limitations. Another key issue is the range dependence of shallow water propagation and reverberation. For example, shallow water limits the depth of towed sound detection arrays, thus increasing the possibility of the system's detecting its own noise. In addition, closer ship spacing increases the potential for mutual interference effects. It is believed that nonacoustic sensors, of magnetic, optical, bioluminescent, chemical, and hydrodynamic disturbances will be necessary in shallow-water naval operations.

Counterbattery and Countersniper Location and Ranging
While now primarily of historical interest, one of the first applications of acoustic and optical MASINT was locating enemy artillery by the sound of their firing and flashes respectively during World War I. Effective sound ranging was pioneered by the British Army under the leadership of the Nobel Lauriate William Bragg. Flash spotting developed in parallel in the British, French and German armies. The combination of sound ranging (i.e., acoustic MASINT) and flash ranging (i.e., before modern optoelectronics) gave information unprecedented for the time, in both accuracy and timeliness. Enemy gun positions were located within 25 to 100 yards, with the information coming in three minutes or less.

Initial WWI Counterbattery Acoustic Systems
In the ""Sound Ranging"" graphic, the manned Listening (or Advanced) Post, is sited a few 'sound seconds (or about 2000 yards) forward of the line of the unattended microphones, it sends an electrical signal to the recording station to switch on the recording apparatus. The positions of the microphones are precisely known. The differences in sound time of arrival, taken from the recordings, were then used to plot the source of the sound by one of several techniques. See http://nigelef.tripod.com/p_artyint-cb.htm#SoundRanging
Where sound ranging is a time-of-arrival technique not dissimilar to that of modern multistatic sensors, flash spotting used optical instruments to take bearings on the flash from accurately surveyed observation posts. The location of the gun was determined by plotting the bearings reported to the same gun flashes. See http://nigelef.tripod.com/p_artyint-cb.htm#FieldSurveyCoy Flash ranging, today, would be called electro-optical MASINT.
Artillery sound and flash ranging remained in use through World War II and in its latest forms until the present day, although flash spotting generally ceased in the 1950s due to the widespread adoption of flashless propellants and the increasing range of artillery. Mobile counterbattery radars able to detect guns, itself a MASINT radar sensor, became available in the late 1970s, although countermortar radars appeared in World War II. These techniques paralleled radio direction finding in SIGINT that started in World War I, using graphical bearing plotting and now, with the precision time synchronization from GPS, is often time-of-arrival.

Modern Acoustic Artillery Locators
Artillery positions now are located primarily with Unmanned Air Systems and IMINT or counterartillery radar, such as the widely used Swedish ArtHuR. SIGINT also may give clues to positions, both with COMINT for firing orders, and ELINT for such things as weather radar. Still, there is renewed interest in both acoustic and electro-optical systems to complement counterartillery radar.
Acoustic sensors have come a long way since World War I. Typically, the acoustic sensor is part of a combined system, in which it cues radar or electro-optical sensors of greater precision, but narrower field of view.

HALO
The UK's hostile artillery locating system (HALO) has been in service with the British Army since the 1990s. HALO is not as precise as radar, but especially complements the directional radars. It passively detects artillery cannon, mortars and tank guns, with 360 degree coverage and can monitor over 2,000 square kilometers. HALO has worked in urban areas, the mountains of the Balkans, and the deserts of Iraq.
The system consists of three or more unmanned sensor positions, each with four microphones and local processing, these deduce the bearing to a gun, mortar, etc. These bearings are automatically communicated to a central processor that combines them to triangulate the source of the sound. It can compute location data on up to 8 rounds per second, and display the data to the system operator. HALO may be used in conjunction with COBRA and ArtHur counter battery radars, which are not omnidirectional, to focus on the correct sector.

UTAMS
Another acoustic system is the US Army Unattended Transient Acoustic MASINT Sensor (UTAMS), developed by the U.S. Army Research Laboratory, which detects detect mortar and rocket launches and impacts. UTAMS has three to five acoustic arrays, each with four microphones, a processor, radio link, power source, and a laptop control computer. UTAMS, which was first operational in Iraq, first tested in November 2004 at a Special Forces Operating Base (SFOB) in Iraq. UTAMS was used in conjunction with AN/TPQ-36 and AN/TPQ-37 counter-artillery radar. While UTAMS was intended principally for detecting indirect artillery fire, Special Forces and their fire support officer learned it could pinpoint improvised explosive device (IED) explosions and small arms/rocket-propelled grenade (RPG) fires. It detected Points of Origin (POO) up to 10 kilometers from the sensor.
Analyzing the UTAMS and radar logs revealed several patterns. The opposing force was firing 60 mm mortars during observed dining hours, presumably since that gave the largest groupings of personnel and the best chance of producing heavy casualties. That would have been obvious from the impact history alone, but these MASINT sensors established a pattern of the enemy firing locations.
This allowed the US forces to move mortars into range of the firing positions, give coordinates to cannon when the mortars were otherwise committed, and to use attack helicopters as a backup to both. The opponents changed to night fires, which, again, were countered with mortar, artillery, and helicopter fires. They then moved into an urban area where US artillery was not allowed to fire, but a combination of PSYOPS leaflet drops and deliberate near misses convinced the locals not to give sanctuary to the mortar crews.

Originally for a Marine requirement in Afghanistan, UTAMS was combined with electro-optical MASINT to produce the Rocket Launch Spotter (RLS) system useful against both rockets and mortars.
In the Rocket Launch Spotter (RLS) application, each array consists of four microphones and processing equipment. Analyzing the time delays between an acoustic wavefront’s interaction with each microphone in the array UTAMS provides an azimuth of origin. The azimuth from each tower is reported to the UTAMS processor at the control station, and a POO is triangulated and displayed. The UTAMS subsystem can also detect and locate the point of impact (POI), but, due to the difference between the speeds of sound and light, it may take UTAMS as long as 30 seconds to determine the POO for a rocket launch 13 km away. In this application, the electro-optical component of RLS will detect the rocket POO earlier, while UTAMS may do better with the mortar prediction.

Passive sea-based acoustic sensors (hydrophones)
Modern hydrophones convert sound to electrical energy, which then can undergo additional signal processing, or that can be transmitted immediately to a receiving station. They may be directional or omnidirectional.
Navies use a variety of acoustic systems, especially passive, in antisubmarine warfare, both tactical and strategic. For tactical use, passive hydrophones, both on ships and airdropped sonobuoys, are used extensively in antisubmarine warfare. They can detect targets far further away than with active sonar, but generally will not have the precision location of active sonar, approximating it with a technique called Target Motion Analysis (TMA). Passive sonar has the advantage of not revealing the position of the sensor.
The Integrated Undersea Surveillance System (IUSS) consists of multiple subsystems in SOSUS, Fixed Distributed System (FDS), and the Advanced Deployable System (ADS or SURTASS). Reducing the emphasis on Cold War blue-water operations put SOSUS, with more flexible ""tuna boat"" sensing vessels called SURTASS being the primary blue-water long-range sensors
SURTASS used longer, more sensitive towed passive acoustic arrays than could be deployed from maneuvering vessels, such as submarines and destroyers.

SURTASS is now being complemented by Low Frequency Active (LFA) sonar; see the sonar section.

Air-dropped passive acoustic sensors
Passive sonobuoys, such as the AN/SSQ-53F, can be directional or omnidirectional and can be set to sink to a specific depth. These would be dropped from helicopters and maritime patrol aircraft such as the P-3.

Fixed underwater passive acoustic sensors
The US installed massive Fixed Surveillance System (FSS, also known as SOSUS) hydrophone arrays on the ocean floor, to track Soviet and other submarines.

Surface ship passive acoustic sensors
Purely from the standpoint of detection, towed hydrophone arrays offer a long baseline and exceptional measurement capability. Towed arrays, however, are not always feasible, because when deployed, their performance can suffer, or they can suffer outright damage, from fast speeds or radical turns. A state-of-the-art British towed array, with both passive and active capabilities, is Sonar 2087 made by Thales Underwater Systems.
Steerable sonar arrays on the hull or bow usually have a passive as well as active mode, as do variable-depth sonars
Surface ships may have warning receivers to detect hostile sonar.

Submarine passive acoustic sensors
Modern submarines have multiple passive hydrophone systems, such as a steerable array in a bow dome, fixed sensors along the sides of the submarines, and towed arrays. They also have specialized acoustic receivers, analogous to radar warning receivers, to alert the crew to the use of active sonar against their submarine.
US submarines made extensive clandestine patrols to measure the signatures of Soviet submarines and surface vessels. This acoustic MASINT mission included both routine patrols of attack submarines, and submarines sent to capture the signature of a specific vessel. US antisubmarine technicians on air, surface, and subsurface platforms had extensive libraries of vessel acoustic signatures.
Passive acoustic sensors can detect aircraft flying low over the sea.

Land-based Passive Acoustic Sensors (geophones)
Vietnam-era acoustic MASINT sensors included ""Acoubuoy (36 inches long, 26 pounds) floated down by camouflaged parachute and caught in the trees, where it hung to listen. The Spikebuoy (66 inches long, 40 pounds) planted itself in the ground like a lawn dart. Only the antenna, which looked like the stalks of weeds, was left showing above ground."" This was part of Operation Igloo White.
Part of the AN/GSQ-187 Improved Remote Battlefield Sensor System (I-REMBASS) is a passive acoustic sensor, which, with other MASINT sensors, detects vehicles and personnel on a battlefield. Passive acoustic sensors provide additional measurements that can be compared with signatures, and used to complement other sensors. I-REMBASS control will integrate, in approximately 2008, with the Prophet SIGINT/EW ground system.
For example, a ground search radar may not be able to differentiate between a tank and a truck moving at the same speed. Adding acoustic information, however, may quickly distinguish between them.

Active Acoustic Sensors and Supporting Measurements
Combatant vessels, of course, made extensive use of active sonar, which is yet another acoustic MASINT sensor. Besides the obvious application in antisubmarine warfare, specialized active acoustic systems have roles in:

Mapping the seafloor for navigation and collision avoidance. These include basic depth gauges, but quickly get into devices that do 3-dimensional underwater mapping
Determining seafloor characteristics, for applications varying from understanding its sound-reflecting properties, to predicting the type of marine life that may be found there, to knowing when a surface is appropriate for anchoring or for using various equipment that will contact the seafloor

Various synthetic aperture sonars have been built in the laboratory and some have entered use in mine-hunting and search systems. An explanation of their operation is given in synthetic aperture sonar.

Water Surface, Fish Interference and Bottom Characterization
The water surface and bottom are reflecting and scattering boundaries. Large schools of fish, with air in their swim bladder balance apparatus, can also have a significant effect on acoustic propagation.
For many purposes, but not all naval tactical applications, the sea-air surface can be thought of as a perfect reflector. ""The effects of the seafloor and the sea surface on acoustic systems in shallow water are highly complex, making range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.""
The acoustic impedance mismatch between water and the bottom is generally much less than at the surface and is more complex. It depends on the bottom material types and depth of the layers. Theories have been developed for predicting the sound propagation in the bottom in this case, for example by Biot and by Buckingham.

Water Surface
For high frequency sonars (above about 1 kHz) or when the sea is rough, some of the incident sound is scattered, and this is taken into account by assigning a reflection coefficient whose magnitude is less than one.
Rather than measuring surface effects directly from a ship, radar MASINT, in aircraft or satellites, may give better measurements. These measurements would then be transmitted to the vessel's acoustic signal processor.

Under Ice
A surface covered with ice, of course, is tremendously difference than even storm-driven water. Purely from a collision avoidance and acoustic propagation, a submarine needs to know how close it is to the bottom of ice. Less obvious is the need to know the three-dimensional structure of the ice, because submarines may need to break through it to launch missiles, to raise4 electronic masts, or to surface the boat. Three-dimensional ice information also can tell the submarine captain whether antisubmarine warfare aircraft can detect or attack the boat.
The state of the art is providing the submarine with a three-dimensional visualization of the ice above: the lowest part (ice keel) and the ice canopy. While sound will propagate differently in ice than liquid water, the ice still needs to be considered as a volume, to understand the nature of reverberations within it.

Bottom
A typical basic depth measuring device is the US AN/UQN-4A. Both the water surface and bottom are reflecting and scattering boundaries. For many purposes, but not all naval tactical applications, the sea-air surface can be thought of as a perfect reflector. In reality, there are complex interactions of water surface activity, seafloor characteristics, water temperature and salinity, and other factors that make ""...range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.""
This device, however, does not give information on the characteristics of the bottom. In many respects, commercial fishing and marine scientists have equipment that is perceived as needed for shallow water operation.

Biologic Effects on Sonar Reflection
A further complication is the presence of wind generated bubbles or fish close to the sea surface. . The bubbles can also form plumes that absorb some of the incident and scattered sound, and scatter some of the sound themselves. .
This problem is distinct from biologic interference caused by acoustic energy generated by marine life, such as the squeaks of porpoises and other cetaceans, and measured by acoustic receivers. The signatures of biologic sound generators need to be differentiated from more deadly denizens of the depths. Classifying biologics is a very good example of an acoustic MASINT process.

Surface Combatants
Modern surface combatants with an ASW mission will have a variety of active systems, with a",Category:CS1 errors: chapter ignored,1
237,238,Acoustical engineering,"Acoustical engineering (also known as acoustic engineering) is the branch of engineering dealing with sound and vibration. It is the application of acoustics, the science of sound and vibration, in technology. Acoustical engineers are typically concerned with the design, analysis and control of sound.
One goal of acoustical engineering can be the reduction of unwanted noise, which is referred to as noise control. Unwanted noise can have significant impacts on animal and human health and well-being, reduce attainment by pupils in schools, and cause hearing loss. Noise control principles are implemented into technology and design in a variety of ways, including control by redesigning sound sources, the design of noise barriers, sound absorbers, suppressors, and buffer zones, and the use of hearing protection (earmuffs or earplugs).

But acoustical engineering is not just about noise control; it also covers positive uses of sound, from the use of ultrasound in medicine to the programming of digital sound synthesizers, and from designing a concert hall to enhance the sound of an orchestra to specifying a railway station's sound system so announcements are intelligible.

Acoustic engineer (professional)
Acoustic engineers usually possess a bachelor's degree or higher qualification in acoustics, physics or another engineering discipline. Practicing as an acoustic engineer usually requires a bachelor's degree with significant scientific and mathematical content. Acoustic engineers might work in acoustic consultancy, specializing in particular fields, such as architectural acoustics, environmental noise or vibration control. In other industries, acoustic engineers might: design automobile sound systems; investigate human response to sounds, such as urban soundscapes and domestic appliances; develop audio signal processing software for mixing desks, and design loudspeakers and microphones for mobile phones. Acousticians are also involved in researching and understanding sound scientifically. Some positions, such as faculty require a Doctor of Philosophy.
In most countries, a degree in acoustics can represent the first step towards professional certification and the degree program may be certified by a professional body. After completing a certified degree program the engineer must satisfy a range of requirements before being certified. Once certified, the engineer is designated the title of Chartered Engineer (in most Commonwealth countries).

Subdisciplines
The listed subdisciplines are loosely based on the PACS (Physics and Astronomy Classification Scheme) coding used by the Acoustical Society of America.

Aeroacoustics
Aeroacoustics is concerned with how noise is generated by the movement of air, for instance via turbulence, and how sound propagates through the fluid air. Aeroacoustics plays an important role in understanding how noise is generated by aircraft and wind turbines, as well as exploring how wind musical instruments work.

Audio signal processing
Audio signal processing is the electronic manipulation of audio signals using analog and digital signal processing.
Audio signal processing is done for a variety of reasons such as:
to enhance a sound, for instance by applying an audio effect such as reverberation;
to remove unwanted noises from a signal, for instance echo cancellation on Skype;
to compress an audio signal to allow efficient transmission, e.g. perceptual coding in MP3 and Opus, and
to understand the content of the signal, e.g. music information retrieval to allow the identification of music tracks via Shazam (service).
Audio engineers develop and use audio signal processing algorithms.

Architectural acoustics
Architectural acoustics (also known as building acoustics) is the science and engineering of achieving a good sound within a building. Architectural acoustics can be about achieving good speech intelligibility in a theatre, restaurant or railway station, enhancing the quality of music in a concert hall or recording studio, or suppressing noise to make offices and homes more productive and pleasant places to work and live in. Architectural acoustic design is usually done by acoustic consultants.

Bioacoustics
Bioacoustics usually concerns the scientific study of sound production and hearing in animals. It can include: acoustic communication and associated animal behaviour and evolution of species; how sound is produced by animals; the auditory mechanisms and neurophysiology of animals; the use of sound to monitor animal populations, and the effect of man-made noise on animals.

Electroacoustics
This branch of acoustic engineering deals with the design of headphones, microphones, loudspeakers, sound systems, sound reproduction and recording. There has been a rapid increase in the use of portable electronic devices which can reproduce sound and rely on electroacoustic engineering, e.g. mobile phones, portable media players, and tablet computers.

Environmental noise
Environmental acoustics is concerned with the control of noise and vibrations caused by traffic, aircraft, industrial equipment, recreational activities and anything else that might be considered a nuisance. Acoustical engineers concerned with environmental acoustics face the challenge of measuring or predicting likely noise levels, determining an acceptable level for that noise, and determining how the noise can be controlled. Environmental acoustics work is usually done by acoustic consultants or those working in environmental health. Recent research work has put a strong emphasis on soundscapes, the positive use of sound (e.g. fountains, bird song), and the preservation of tranquility.

Musical acoustics
Musical acoustics is concerned with researching and describing the physics of music and its perception – how sounds employed as music work. This includes: the function and design of musical instruments including electronic synthesizers; the human voice (the physics and neurophysiology of singing); computer analysis of music and composition; the clinical use of music in music therapy, and the perception and cognition of music.

Noise control
Noise control is a set of strategies to reduce noise pollution by reducing noise at its source, by inhibiting sound propagation using noise barriers or similar, or by the use of ear protection (earmuffs or earplugs). Control at the source is the most cost-effective way of providing noise control. Noise control engineering applied to cars and trucks is known as noise, vibration, and harshness (NVH). Other techniques to reduce product noise include vibration isolation, application of acoustic absorbent and acoustic enclosures. Acoustical engineering can go beyond noise control to look at what is the best sound for a product, for instance, manipulating the sound of door closures on automobiles.

Psychoacoustics
Psychoacoustics tries to explain how humans respond to what they hear, whether that is an annoying noise or beautiful music. In many branches of acoustic engineering, a human listener is a final arbitrator as to whether a design is successful, for instance, whether sound localisation works in a surround sound system. ""Psychoacoustics seeks to reconcile acoustical stimuli and all the scientific, objective, and physical properties that surround them, with the physiological and psychological responses evoked by them.""

Speech
Speech is a major area of study for acoustical engineering, including the production, processing and perception of speech. This can include physics, physiology, psychology, audio signal processing and linguistics. Speech recognition and speech synthesis are two important aspects of the machine processing of speech. Ensuring speech is transmitted intelligibly, efficiently and with high quality; in rooms, through public address systems and through telephone systems are other important areas of study.

Ultrasonics
Ultrasonics deals with sound waves in solids, liquids and gases at frequencies too high to be heard by the average person. Specialists areas include medical ultrasonics (including medical ultrasonography), sonochemistry, nondestructive testing, material characterisation and underwater acoustics (sonar).

Underwater acoustics
Underwater acoustics is the scientific study of sound in water. It is concerned with both natural and man-made sound and its generation underwater; how it propagates, and the perception of the sound by animals. Applications include sonar to locate submerged objects such as submarines, underwater communication by animals, observation of sea temperatures for climate change monitoring, and marine biology.

Vibration and dynamics
Acoustic engineers working on vibration study the motions and interactions of mechanical systems with their environments, including measurement, analysis and control. This might include: ground vibrations from railways and construction; vibration isolation to reduce noise getting into recording studios; studying the effects of vibration on humans (vibration white finger); vibration control to protect a bridge from earthquakes, or modelling the propagation of structure-borne sound through buildings.

Fundamental science
Although the way in which sound interacts with its surroundings is often extremely complex, there are a few ideal sound wave behaviours that are fundamental to understanding acoustical design. Complex sound wave behaviors include absorption, reverberation, diffraction, and refraction. Absorption is the loss of energy that occurs when a sound wave reflects off of a surface. Just as light waves reflect off of surfaces, sound waves also reflect off of surfaces, and every reflection results in a loss of energy. Absorption refers both to the sound that transmits through and the energy that is dissipated by a material. Reverberation is the persistence of sound that is caused by repeated boundary reflections after the source of the sound stops. This principle is particularly important in enclosed spaces. In addition to reflecting off of surfaces, sound waves also bend around surfaces in the path of the waves. This bending is known as diffraction. Refraction is another kind of sound wave bending. This type of bending, however, is caused by changes in the medium through which the wave is passing and not the presence of obstacles in the path of a sound wave. Temperature gradients, for example, cause bending in sound waves. Acoustical engineers apply these fundamental concepts, along with complex mathematical analysis, to control sound for a variety of applications.

Associations
Acoustical Society of America
Audio Engineering Society
Australian Acoustical Society
Canadian Acoustical Association
Institute of Acoustics

See also
Audio Engineering
Category:Acoustical engineers
Category:Audio engineers

References

Barron, R. (2003). Industrial noise control and acoustics. New York: Marcel Dekker Inc. Retrieved from CRCnetBase
Hemond, C. (1983). In Ingerman S. ( Ed.), Engineering acoustics and noise control. New Jersey: Prentice-Hall.
Highway traffic noise barriers at a glance. Retrieved February 1, 2010, from http://www.fhwa.dot.gov/environment/keepdown.htm
Kinsler, L., Frey, A., Coppens, A., & Sanders, J. (Eds.). (2000). Fundamentals of acoustics (4th ed.). New York: John Wiley and Sons.
Kleppe, J. (1989). Engineering applications of acoustics. Sparks, Nevada: Artech House.
Moser, M. (2009). Engineering acoustics (S. Zimmerman, R. Ellis Trans.). (2nd ed.). Berlin: Springer-Verlag.",Category:Noise reduction,1
238,239,Acoustic board,"An acoustic board is a special kind of board made of sound absorbing materials. Its job is to provide sound insulation. Between two outer walls sound absorbing material is inserted and the wall is porous. Thus, when sound passes through an acoustic board, the intensity of sound is decreased. The loss of sound energy is balanced by producing heat energy.

Uses
They are used in auditoriums, halls, seminar rooms, libraries, courts and wherever sound insulation is needed. Acoustic boards are also used in speaker boxes.

See also
Acoustics
Architectural acoustics
Room acoustics
Absorption (acoustics)


== References ==",Category:Building engineering,1
239,240,Zakharov–Schulman system,"In mathematics, the Zakharov–Schulman system is a system of nonlinear partial differential equations introduced in (Zakharov & Schulman 1980) to describe the interactions of small amplitude, high frequency waves with acoustic waves. The equations are

  
    
      
        i
        
          ?
          
            t
          
          

          
        
        u
        +
        
          L
          
            1
          
        
        u
        =
        ?
        u
      
    
    {\displaystyle i\partial _{t}^{}u+L_{1}u=\phi u}
  

  
    
      
        
          L
          
            2
          
        
        ?
        =
        
          L
          
            3
          
        
        (
        
          |
        
        u
        
          
            |
          
          
            2
          
        
        )
      
    
    {\displaystyle L_{2}\phi =L_{3}(|u|^{2})}
  
where L1, L2, and L3, are constant coefficient differential operators.

References
V.E. Zakharov, E.I. Schulman, Degenerated dispersion laws, motion invariant and kinetic equations, Physica 1D (1980), 185-250.

External links
""Zakharov-Schulman_system"". Dispersive PDE Wiki.",Category:Acoustics,1
240,241,Reverberation room,"A reverberation chamber or room is a room designed to create a diffuse or random incidence sound field (i.e. one with a uniform distribution of acoustic energy and random direction of sound incidence over a short time period). Reverberation chambers tend to be large rooms (the resulting sound field becomes more diffused with increased path length) and have very hard exposed surfaces. The change of impedance (compared to the air) these surfaces present to incident sound is so large that virtually all of the acoustic energy that hits a surface is reflected back into the room. Arranging the room surfaces (including the ceiling) to be non-parallel helps inhibit the formation of standing waves - additional acoustic diffusers are often used to create more reflecting surfaces and further encourage even distribution of any particular sound field.
Reverberation chambers are used in acoustics as well as in electrodynamics, such as for measurement microphone calibration, measurement of the sound power of a source, and measurement of the absorption coefficient of a material. All these techniques assume the sound field in the chamber to be diffuse, and will normally use a broadband sound source (e.g. white noise or pink noise) so that the resulting sound field contains acoustic energy across the whole audible range.

See also
Anechoic Chamber: another acoustic test facility
Echo chamber: a reverberation room used for music recording
Electromagnetic Reverberation Chamber: an electromagnetic environment mainly for Electromagnetic Compatibility testing

References
ISO Standard for Measurement of Acoustic Absorption in a Reverberation Room

External links
360 video of a reverberation chamber
Pictures and further description of a reverberation chamber",Category:Acoustics,1
241,242,Sound particle,"In the context of particle displacement and velocity, a sound particle is an imaginary infinitesimal volume of a medium that shares the movement of the medium in response to the presence of sound at a specified point or in a specified region. Sound particles are not molecules in the physical or chemical sense; they do not have defined physical or chemical properties, or the temperature-dependent kinetic behaviour of ordinary molecules. Sound particles are, then, indefinitely small (small compared to the wavelength of sound) so that their movement truly represents the movement of the medium in their locality. They exist in the mind’s eye to enable this movement to be visualized and described quantitatively. Assuming the medium as a whole to be at rest, sound particles are imagined to vibrate about fixed points.

See also
Sound
Particle displacement
Particle velocity
Particle acceleration

References
Haughton, P. M. (2002). Acoustics for Audiologists. Academic Press. 
Unnikrishnan, C. S. (10 April 2005). ""On the gravitational deflection of light and particles"" (PDF). Current Science. 88 (7).",Category:Articles lacking in-text citations from May 2015,1
242,243,Dynamical energy analysis,"Dynamical energy analysis (DEA) is a method for numerically modelling structure borne sound and vibration in complex structures. It is applicable in the mid-to-high frequency range and is in this regime computational more efficient than traditional deterministic approaches (such as finite element and boundary element methods). In comparison to conventional statistical approaches such as statistical energy analysis (SEA), DEA provides more structural details and is less problematic with respect to subsystem division. The DEA method predicts the flow of vibrational wave energy across complex structures in terms of (linear) transport equations. These equations are then discretized and solved on meshes.

Key point summary of DEA
High frequency method in numerical acoustics.
The flow of energy is tracked across a mesh. Can be thought of as ray tracing using density of rays instead of individual rays.
Can use existing FEM meshes. No remodelling necessary.
Computational time is independent of frequency.
The necessary mesh resolution does not depend on frequency and can be chosen coarser than in FEM. It only should resolve the geometry.
Fine structural details can be resolved, in contrast to SEA which gives only one number per subsystem.
Greater flexibility for the models usable by DEA. No implicit assumptions (equilibrium in weakly coupled subsystems) as in SEA.

Introduction
Simulations of the vibro-acoustic properties of complex structures (such as cars, ships, airplanes,...) are routinely carried out in various design stages. For low frequencies, the established method of choice is the finite element method (FEM). But high frequency analysis using FEM requires very fine meshes of the body structure to capture the shorter wavelengths and therefore is computational extremely costly. Furthermore the structural response at high frequencies is very sensitive to small variations in material properties, geometry and boundary conditions. This makes the output of a single FEM calculation less reliable and makes ensemble averages necessary furthermore enhancing computational cost. Therefore at high frequencies other numerical methods with better computational efficiency are preferable.
The statistical energy analysis (SEA) has been developed to deal with high frequency problems and leads to relatively small and simple models. However, SEA is based on a set of often hard to verify assumptions, which effectively require diffuse wave fields and quasi-equilibrium of wave energy within weakly coupled (and weakly damped) sub-systems.
One alternative to SEA is to instead consider the original vibrational wave problem in the high frequency limit, leading to a ray tracing model of the structural vibrations. The tracking of individual rays across multiple reflection is not computational feasible because of the proliferation of trajectories. Instead, a better approach is tracking densities of rays propagated by a transfer operator. This forms the basis of the Dynamical Energy Analysis (DEA) method introduced in reference. DEA can be seen as an improvement over SEA where one lifts the diffusive field and the well separated subsystem assumption. One uses an energy density which depends both on position and momentum. DEA can work with relatively fine meshes where energy can flow freely between neighboring mesh cells. This allows far greater flexibility for the models used by DEA in comparison to the restriction imposed by SEA. No remodeling as for SEA is necessary as DEA can use meshes created for a FE analysis. As a result, finer structural details than SEA can be resolved by DEA.

Method
The implementation of DEA on meshes is called Discrete Flow Mapping (DFM). We will here briefly describe the idea behind DFM, for details see the references      below. Using DFM it is possible to compute vibro-acoustic energy densities in complex structures at high frequencies, including multi-modal propagation and curved surfaces. DFM is a mesh based technique where a transfer operator is used to describe the flow of energy through boundaries of subsystems of the structure; the energy flow is represented in terms of a density of rays 
  
    
      
        ?
      
    
    {\displaystyle \rho }
  , that is, the energy flux through a given surface is given through the density of rays passing through the surface at point 
  
    
      
        s
      
    
    {\displaystyle s}
   with direction 
  
    
      
        p
      
    
    {\displaystyle p}
  . Here, 
  
    
      
        s
      
    
    {\displaystyle s}
   parametrises the surface and 
  
    
      
        p
      
    
    {\displaystyle p}
   is the direction component tangential to the surface. In what follows, the surfaces is represented by the union of all boundaries of the mesh cells of the FE mesh describing the car floor. The density 
  
    
      
        ?
        (
        s
        ,
        p
        )
        =
        ?
        (
        
          X
          
            s
          
        
        )
      
    
    {\displaystyle \rho (s,p)=\rho (X_{s})}
  , with phase space coordinate 
  
    
      
        
          X
          
            s
          
        
        =
        (
        s
        ,
        p
        )
      
    
    {\displaystyle X_{s}=(s,p)}
  , is transported from one boundary to the adjacent boundary intersection via the boundary integral operator

where 
  
    
      
        ?
        (
        
          X
          
            s
          
        
        )
      
    
    {\displaystyle \phi (X_{s})}
   is the map determining where a ray starting on a boundary segment at point 
  
    
      
        s
      
    
    {\displaystyle s}
   with direction 
  
    
      
        
          p
          
            s
          
        
      
    
    {\displaystyle p_{s}}
   passes through another boundary segment, and 
  
    
      
        w
        (
        
          X
          
            s
          
        
        )
      
    
    {\displaystyle w(X_{s})}
   is a factor containing damping and reflection/transmission coefficients (akin to the coupling loss factors in SEA). It also governs the mode conversion probabilities in the case of both in-plane and flexural waves, which are derived from wave scattering theory (see). This allows DEA to take curvature and varying material parameters into account. Equation (1) is a way to write ray tracing across one single mesh cell in terms of an integral equation transferring an energy density from one surface to an adjacent surface.
In a next step, the transfer operator (1) is discretised using a set of basis functions of the phase space. Once the matrix 
  
    
      
        
          
            B
          
        
      
    
    {\displaystyle {\bf {B}}}
   has been constructed, the final energy density 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   on the boundary phase-space of each element is given in terms of the initial density 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
   by the solution of a linear system of the form

The initial density 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
   models some source distribution for vibrational excitations, for example the engine in ship. Once the final density 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   (describing the energy density on all cell boundaries) has been computed, the energy density at any location inside the structure may be computed as a post-processing step.
Concerning the terminology, there is some ambiguity concerning the terms ""Discrete Flow Mapping(DFM)"" and ""Dynamical Energy Analysis"". To some extent, one can use one term in place of the other. For example, consider a plate. In DFM, one would subdivide the plate into many small triangles and propagate the flow of energy from triangle to (neighbouring) triangle. In DEA, one would not subdivide the plate, but use some high order basis functions (both in position and momentum) on the boundary of the plate. But in principle it would be admissible to describe both procedures as either DFM or DEA.

Examples
As an example application, a simulation  of a carfloor panel is shown here. A point excitation at 2500 Hz with 0.04 hysteretic damping was applied. The results from a frequency averaged FEM simulation are compared with a DEA simulation (for DEA, no frequency averaging is necessary). The results also show a good quantitative agreement. In particular, we see the directional dependence of the energy flow, which is predominantly in the horizontal direction as plotted. This is caused by several horizontally extended out-of-plane bulges. It is only in the lower right part of the panel, with negligible energy content, that deviations between the FEM and DFM predictions are visible. The total kinetic energy given by the DFM prediction is within 12% of the FEM prediction. For more details, see the cited works.

As a more applied example, the result of a DEA simulation on a Yanmar tractor model (body in blue: chassis/cabin steel frame and windows) is shown here to the left. In the cited work, the numerical DEA results are compared with experimental measurements at frequencies between 400 Hz and 4000 Hz for an excitation on the back of the gear casing. Both results agree favorably. The DEA simulation can be extended to predict the sound pressure level at driver's ear.

Notes
References
External links
University of Nottingham Wave Modelling Research Group. One of the foci of this research group is on DEA.",Category:Acoustics,1
243,244,Acoustic emission,"Acoustic emission (AE) is the phenomenon of radiation of acoustic (elastic) waves in solids that occurs when a material undergoes irreversible changes in its internal structure, for example as a result of crack formation or plastic deformation due to aging, temperature gradients or external mechanical forces. In particular, AE is occurring during the processes of mechanical loading of materials and structures accompanied by structural changes that generate local sources of elastic waves. This results in small surface displacements of a material produced by elastic or stress waves  generated when the accumulated elastic energy in a material or on its surface is released rapidly. The waves generated by sources of AE are of practical interest in structural health monitoring (SHM), quality control, system feedback, process monitoring and other fields. In SHM applications, AE is typically used to detect, locate and characterise damage.

Acoustic emission phenomena
Acoustic emission is the transient elastic waves within a material, caused by the rapid release of localized stress energy. An event source is the phenomenon which releases elastic energy into the material, which then propagates as an elastic wave. Acoustic emissions can be detected in frequency ranges under 1 kHz, and have been reported at frequencies up to 100 MHz, but most of the released energy is within the 1 kHz to 1 MHz range. Rapid stress-releasing events generate a spectrum of stress waves starting at 0 Hz, and typically falling off at several MHz.
The three major applications of AE techniques are: 1) source location - determine the locations where an event source occurred; 2) material mechanical performance - evaluate and characterize materials/structures; and 3) health monitoring - monitor the safe operation of a structure, for example, bridges, pressure containers, and pipe lines, etc.
More recent research has focused on using AE to not only locate but also to characterise the source mechanisms such as crack growth, friction, delamination, matrix cracking, etc. This would give AE the ability to tell the end user what source mechanism is present and allow them to determine whether structural repairs are necessary.
AE can be related to an irreversible release of energy. It can also be generated from sources not involving material failure, including friction, cavitation and impact.

Uses
The application of acoustic emission to non-destructive testing of materials, typically takes place between 100 kHz and 1 MHz. Unlike conventional ultrasonic testing, AE tools are designed for monitoring acoustic emissions produced by the material during failure or stress, and not on the material's effect on externally generated waves. Part failure can be documented during unattended monitoring. The monitoring of the level of AE activity during multiple load cycles forms the basis for many AE safety inspection methods, that allow the parts undergoing inspection to remain in service.
The technique is used, for example, to study the formation of cracks during the welding process, as opposed to locating them after the weld has been formed with the more familiar ultrasonic testing technique. In a material under active stress, such as some components of an airplane during flight, transducers mounted in an area can detect the formation of a crack at the moment it begins propagating. A group of transducers can be used to record signals, then locate the precise area of their origin by measuring the time for the sound to reach different transducers. The technique is also valuable for detecting cracks forming in pressure vessels  and pipelines transporting liquids under high pressures. Also, this technique is used for estimation of corrosion in reinforced concrete structures.
In addition to non-destructive testing, acoustic emission monitoring has applications in process monitoring. Applications where acoustic emission monitoring has successfully been used include detecting anomalies in fluidized beds, and end points in batch granulation.
Standards for the use of acoustic emission for non-destructive testing of pressure vessels have been developed by the ASME, ISO and the European Community.

See also
Ultrasonic testing
Ultrasonic homogenizer

References
External links and further reading
History of the Acoustic Emission Working Group.
History of the Latin American Working Group on Acoustic Emission.
The e-Journal & Database of Nondestructive Testing.
Wolfgang Sachse, Kusuo Yamaguchi, James Roget, AEWG (Association) books.google.co.uk 100 page entries from search criteria : AE within this text (ISBN 0-8031-1389-7)
Christian U. Grosse; Masayasu Ohtsu, eds. (2008). Acoustic Emission Testing. Springer Verlag. ISBN 978-3-642-08937-4. 
Cole, P. T. (1988). Series: The Capabilities and Limitations of NDT Part 7. Acoustic Emission (INST087). British Institute of Non-Destructive Testing. ISBN 0-903132-08-7.",Category:Acoustics,1
244,245,Acoustic location,"Acoustic location is the use of sound to determine the distance and direction of its source or reflector. Location can be done actively or passively, and can take place in gases (such as the atmosphere), liquids (such as water), and in solids (such as in the earth).
Active acoustic location involves the creation of sound in order to produce an echo, which is then analyzed to determine the location of the object in question.
Passive acoustic location involves the detection of sound or vibration created by the object being detected, which is then analyzed to determine the location of the object in question.
Both of these techniques, when used in water, are known as sonar; passive sonar and active sonar are both widely used.
Acoustic mirrors and dishes, when using microphones, are a means of passive acoustic localization, but when using speakers are a means of active localization. Typically, more than one device is used, and the location is then triangulated between the several devices.
As a military air defense tool, passive acoustic location was used from mid-World War I to the early years of World War II to detect enemy aircraft by picking up the noise of their engines. It was rendered obsolete before and during World War II by the introduction of radar, which was far more effective (but interceptable). Acoustic techniques had the advantage that they could 'see' around corners and over hills, due to sound diffraction.
The civilian uses include locating wildlife and locating the shooting position of a firearm.

Overview
Acoustic source localization  is the task of locating a sound source given measurements of the sound field. The sound field can be described using physical quantities like sound pressure and particle velocity. By measuring these properties it is (indirectly) possible to obtain a source direction.
Traditionally sound pressure is measured using microphones. Microphones have a polar pattern describing their sensitivity as a function of the direction of the incident sound. Many microphones have an omnidirectional polar pattern which means their sensitivity is independent of the direction of the incident sound. Microphones with other polar patterns exist that are more sensitive in a certain direction. This however is still no solution for the sound localization problem as one tries to determine either an exact direction, or a point of origin. Besides considering microphones that measure sound pressure, it is also possible to use a particle velocity probe to measure the acoustic particle velocity directly. The particle velocity is another quantity related to acoustic waves however, unlike sound pressure, particle velocity is a vector. By measuring particle velocity one obtains a source direction directly. Other more complicated methods using multiple sensors are also possible. Many of these methods use the time difference of arrival (TDOA) technique.
Some have termed acoustic source localization an ""inverse problem"" in that the measured sound field is translated to the position of the sound source.

Methods
Different methods for obtaining either source direction or source location are possible.

Particle velocity or intensity vector
The simplest but still a relatively new method is to measure the acoustic particle velocity using a particle velocity probe. The particle velocity is a vector and thus also contains directional information.

Time difference of arrival
The traditional method to obtain the source direction is using the time difference of arrival (TDOA) method. This method can be used with pressure microphones as well as with particle velocity probes.
With a sensor array (for instance a microphone array) consisting of at least two probes it is possible to obtain the source direction using the cross-correlation function between each probes' signal. The cross-correlation function between two microphones is defined as

  
    
      
        
          R
          
            
              x
              
                1
              
            
            ,
            
              x
              
                2
              
            
          
        
        (
        ?
        )
        =
        
          ?
          
            n
            =
            ?
            ?
          
          
            ?
          
        
        
          x
          
            1
          
        
        (
        n
        )
        
          x
          
            2
          
        
        (
        n
        +
        ?
        )
      
    
    {\displaystyle R_{x_{1},x_{2}}(\tau )=\sum _{n=-\infty }^{\infty }x_{1}(n)x_{2}(n+\tau )}
  
which defines the level of correlation between the outputs of two sensors 
  
    
      
        
          x
          
            1
          
        
      
    
    {\displaystyle x_{1}}
   and 
  
    
      
        
          x
          
            2
          
        
      
    
    {\displaystyle x_{2}}
  . In general, a higher level of correlation means that the argument 
  
    
      
        ?
      
    
    {\displaystyle \tau }
   is relatively close to the actual time-difference-of-arrival. For two sensors next to each other the TDOA is given by

  
    
      
        
          ?
          
            true
          
        
        =
        
          
            
              d
              
                spacing
              
            
            c
          
        
      
    
    {\displaystyle \tau _{\text{true}}={\frac {d_{\text{spacing}}}{c}}}
  
where 
  
    
      
        c
      
    
    {\displaystyle c}
   is the speed of sound in the medium surrounding the sensors and the source.
A well-known example of TDOA is the interaural time difference. The interaural time difference is the difference in arrival time of a sound between two ears. The interaural time difference is given by

  
    
      
        ?
        t
        =
        
          
            
              x
              sin
              ?
              ?
            
            c
          
        
      
    
    {\displaystyle \Delta t={\frac {x\sin \theta }{c}}}
  
where

  
    
      
        ?
        t
      
    
    {\displaystyle \Delta t}
   is the time difference in seconds,

  
    
      
        x
      
    
    {\displaystyle x}
   is the distance between the two sensors (ears) in meters,

  
    
      
        ?
      
    
    {\displaystyle \theta }
   is the angle between the baseline of the sensors (ears) and the incident sound, in degrees.

Triangulation
In trigonometry and geometry, triangulation is the process of determining the location of a point by measuring angles to it from known points at either end of a fixed baseline, rather than measuring distances to the point directly (trilateration). The point can then be fixed as the third point of a triangle with one known side and two known angles.
For acoustic localization this means that if the source direction is measured at two or more locations in space, it is possible to triangulate its location.

Indirect methods
Steered Response Power (SRP) methods are a class of indirect acoustic source localization methods. Instead of estimating a set of time-differences of arrival (TDOAs) between pairs of microphones and combining the acquired estimates to find the source location, indirect methods search for a candidate source location over a grid of spatial points. In this context, methods such as the Steered-Response Power Phase Transform (SRP-PHAT) are usually interpreted as finding the candidate location that maximizes the output of a delay-and-sum beamformer. The method has been shown to be very robust to noise and reverberation, motivating the development of modified approaches aimed at increasing its performance in real-time acoustic processing applications.

Military use
Military uses have included locating submarines and aircraft. The first use of this type of equipment was claimed by Commander Alfred Rawlinson of the Royal Naval Volunteer Reserve, who in the autumn of 1916 was commanding a mobile anti-aircraft battery on the east coast of England. He needed a means of locating Zeppelins during cloudy conditions and improvised an apparatus from a pair of gramaphone horns mounted on a rotating pole. Several of these equipments were able to give a fairly accurate fix on the approaching airships, allowing the guns to be directed at them despite being out of sight. Although no hits were obtained by this method, Rawlinson claimed to have forced a Zeppelin to jettison its bombs on one occasion.
The air-defense instruments usually consisted of large horns or microphones connected to the operators' ears using tubing, much like a very large stethoscope.

Most of the work on anti-aircraft sound ranging was done by the British. They developed an extensive network of sound mirrors that were used from World War I through World War II. Sound mirrors normally work by using moveable microphones to find the angle that maximizes the amplitude of sound received, which is also the bearing angle to the target. Two sound mirrors at different positions will generate two different bearings, which allows the use of triangulation to determine a sound source's position.
As World War II neared, radar began to become a credible alternative to the sound location of aircraft. For typical aircraft speeds of that time, sound location only gave a few minutes of warning. The acoustic location stations were left in operation as a backup to radar, as exemplified during the Battle of Britain. Today, the abandoned sites are still in existence and are readily accessible.
After World War II, sound ranging played no further role in anti-aircraft operations.

Active / passive locators
Active locators have some sort of signal generation device, in addition to a listening device. The two devices do not have to be located together.

Sonar
SONAR or sonar (sound navigation and ranging) is a technique that uses sound propagation under water (or occasionally in air) to navigate, communicate or to detect other vessels. There are two kinds of sonar – active and passive. A single active sonar can localize in range and bearing as well as measuring radial speed. However, a single passive sonar can only localize in bearing directly, though target motion analysis can be used to localize in range, given time. Multiple passive sonars can be used for range localization by triangulation or correlation, directly.

Biological echo location
Dolphins, whales and bats use echolocation to detect prey and avoid obstacles.

Time-of-arrival localization
Having speakers/ultrasonic transmitters emitting sound at known positions and time, the position of a target equipped with a microphone/ultrasonic receiver can be estimated based on the time of arrival of the sound. The accuracy is usually poor under non-line-of-sight conditions, where there are blockages in between the transmitters and the receivers.

Seismic surveys
Seismic surveys involve the generation of sound waves to measure underground structures. Source waves are generally created by percussion mechanisms located near the ground or water surface, typically dropped weights, vibroseis trucks, or explosives. Data are collected with geophones, then stored and processed by computer. Current technology allows the generation of 3D images of underground rock structures using such equipment.

Ecotracer
Ecotracer is an acoustic locator that was used to determining the presence and position of ships in fog. Some could detect targets at distances up to 12 kilometers. Static walls could detect aircraft up to 30 miles away.

Types
There were four main kinds of system:
Personal/wearable horns
Transportable steerable horns
Static dishes
Static walls

Impact
American acoustic locators were used in 1941 to detect the Japanese attack on the fortress island of Corregidor in the Philippines.

Other
Because the cost of the associated sensors and electronics is dropping, the use of sound ranging technology is becoming accessible for other uses, such as for locating wildlife.

See also
3D sound reconstruction
Sound localization
Boomerang
Multilateration
Acoustic mirror
Acoustic wayfinding, the practice of using auditory cues and sound markers to navigate indoor and outdoor spaces
Animal echolocation, animals emitting sound and listening to the echo in order to locate objects or navigate
Echo sounding, listening to the echo of sound pulses to measure the distance to the bottom of the sea, a special case of sonar
Gunfire locator
Human echolocation, the use of echolocation by blind people
Medical ultrasonography, the use of ultrasound echoes to look inside the body
Sensory substitution
Sound localisation

References
External links
""Huge Ear Locates Planes and Tells Their Speed"" Popular Mechanics, December 1930 article on French aircraft sound detector with photo.
Many references can be found in Beamforming References
An Empirical Study of Collaborative Acoustic Source Localization",Category:Webarchive template wayback links,1
245,246,Interdigital transducer,"An interdigital transducer (IDT) is a device that consists of two interlocking comb-shaped arrays of metallic electrodes (in the fashion of a zipper). These metallic electrodes are deposited on the surface of a piezoelectric substrate, such as quartz or lithium niobate, to form a periodic structure.
IDTs primary function is to convert electric signals to surface acoustic waves (SAW) by generating periodically distributed mechanical forces via piezoelectric effect (an input transducer). The same principle is applied to the conversion of SAW back to electric signals (an output transducer). These processes of generation and reception of SAW can be used in different types of SAW signal processing devices, such as band pass filters, delay lines, resonators, sensors, etc. IDT was first proposed by White and Voltmer in 1965.

See also
Rayleigh waves


== Further reading ==",Category:All articles lacking sources,1
246,247,Jet noise,"In aeroacoustics, jet noise is the field that focuses on the noise generation caused by high-velocity jets and the turbulent eddies generated by shearing flow. Such noise is known as broadband noise and extends well beyond the range of human hearing (100 kHz and higher). Jet noise is also responsible for some of the loudest sounds ever produced by mankind.

Sources of jet noise
The primary sources of jet noise for a high-speed air jet (meaning when the exhaust velocity exceeds about 100 m/s) are ""jet mixing noise"" and, for supersonic flow, shock associated noise. Also, acoustic sources within the ""jet pipe"" also contribute to the noise, mainly at lower speeds, which include combustion noise and sounds produced by interactions of a turbulent stream with fans, compressors, and turbine systems.
The jet mixing sound is created by the turbulent mixing of a jet with the ambient fluid, in most cases, air. The mixing initially occurs in an annular shear layer, which grows with the length of the nozzle. The mixing region generally fills the entire jet at four or five diameters from the nozzle. The high-frequency components of the sound are mainly stationed close to the nozzle, where the dimensions of the turbulence eddies are small. Further down the jet, where the eddy size is similar to the jet diameter, is where lower frequency begins.
In Supersonic, or ""choked"" jets there are cells through which the flow continuously expands and contracts. Several of these ""shock cells"" can be seen extending up to ten jet diameters from the nozzle and are responsible for two additional components of jet noise, ""screech tones"" and broadband ""shock associated noises"". Screech is produced by a feedback mechanism in which a disturbance convecting in the shear layer generates sound as it traverses the standing system of shock waves in the jet. Even though screech is a side effect of the jet's flight, it can be suppressed by an appropriate design for a nozzle.
Aircraft noise is also sometimes called ""jet noise"" when emanating from jet aircraft, regardless of the mechanism of noise production.

References

Works cited
Howe, M.S. (1998). Acoustics of Fluid-Structure Interactions. Cambridge: Cambridge University Press. pp. 149–156. ISBN 978-0-521-63320-8. 
Aviation and the Environment: Noise, Hearing before the Subcommittee on Aviation of the Committee on Transportation and Infrastructure. Pennsylvania, U.S.: U.S. Government Printing Office. 2007. pp. 149–153. 
Khavaran, Abbas. (2012). Acoustic Investigation of Jet Mixing Noise in Dual Stream Nozzles. Cleveland, OH: National Aeronautics and Space Administration, Glenn Research Center.",Category:Acoustics,1
247,248,Sound ranger,"Sound rangers are military specialists who locate enemy artillery using sound. They use equipment such as super-sensitive microphones to pick up the sounds of firing. By using a combination of surveying techniques and trigonometry they then calculate the locations of enemy batteries. These results are then transmitted by radio to their own artillery, who used it for targeting.
Sound rangers were used extensively in the Second World War. Many of the officers were former surveyors, estate agents and geographers because of their surveying experience in civilian life. They would be located near the front line but not always among the firing line for their safety.

See also
Sound ranging",Category:All articles lacking sources,1
248,249,Hot chocolate effect,"The hot chocolate effect, also known as the allassonic effect, is a phenomenon of wave mechanics first documented in 1982 by Frank Crawford, where the pitch heard from tapping a cup of hot liquid rises after the addition of a soluble powder. It was first observed in the making of hot chocolate or instant coffee, but also occurs in other situations such as adding salt to supersaturated hot water or cold beer. Recent research has found many more substances which create the effect, even in initially non-supersaturated liquids.
It can be observed by pouring hot milk into a mug, stirring in chocolate powder, and tapping the bottom of the mug with a spoon while the milk is still in motion. The pitch of the taps will increase progressively with no relation to the speed or force of tapping. Subsequent stirring of the same solution (without adding more chocolate powder) will gradually decrease the pitch again, followed by another increase. This process can be repeated a number of times, until equilibrium has been reached. Upon initial stirring, entrained gas bubbles reduce the speed of sound in the liquid, lowering the frequency. As the bubbles clear, sound travels faster in the liquid and the frequency increases.

Origin of the phenomenon
The phenomenon is explained by the effect of bubble density on the speed of sound in the liquid. The note heard is the frequency of a standing wave where a quarter wavelength is the distance between the base of the mug and the liquid surface. This frequency f is equal to the speed v of the wave divided by four times the height of the water column h:

  
    
      
        f
        =
        
          
            1
            4
          
        
        
          
            v
            h
          
        
      
    
    {\displaystyle f={\frac {1}{4}}{\frac {v}{h}}}
  
The speed of sound in a homogeneous liquid or gas is dependent on the fluid's mass density (
  
    
      
        ?
      
    
    {\displaystyle \rho }
  ) and Adiabatic Bulk modulus (
  
    
      
        K
      
    
    {\displaystyle K}
  ), according to the Newton-Laplace formula:

  
    
      
        c
        =
        
          
            
              K
              ?
            
          
        
      
    
    {\displaystyle c={\sqrt {\frac {K}{\rho }}}}
  
Water is approximately 800 times denser than air, and air is approximately 15,000 times more compressible than water. When water is filled with air bubbles, however, the fluid's density is very close to the density of water, but the compressibility will be the compressibility of air. This greatly reduces the speed of sound in the liquid. Wavelength is constant for a given volume of fluid, therefore the frequency (pitch) of the sound will decrease as long as gas bubbles are present.
Different rates of bubble formation will generate different acoustic profiles, allowing differentiation of the added solutes.

See also
Broadband acoustic resonance dissolution spectroscopy, a spectroscopic technique that uses the Hot Chocolate Effect as its fundamental principle.

References
External links
Sound of a Cup With and Without Instant Coffee: A Foam-Filled Acoustics Demonstration Andrew Morrison and Thomas D. Rossing, 143rd ASA Meeting, Pittsburgh.",Category:Wave mechanics,1
249,250,Sound speed profile,"A sound speed profile shows the speed of sound in water at different vertical levels. It has two general representations:
tabular form, with pairs of columns corresponding to ocean depth and the speed of sound at that depth, respectively.
a plot of the speed of sound in the ocean as a function of depth, where the vertical axis corresponds to the depth and the horizontal axis corresponds to the sound speed. By convention, the horizontal axis is placed at the top of the plot, and the vertical axis is labeled with values which increase from top to bottom, thus reproducing visually the ocean from its surface downward.
Table 1 shows an example of the first representation; figure 1 shows the same information using the second representation.

Although given as a function of depth, the speed of sound in the ocean does not depend solely on depth. Rather, for a given depth, the speed of sound depends on the temperature at that depth, the depth itself, and the salinity at that depth, in that order.
The speed of sound in the ocean at different depths can be measured directly, e.g., by using a velocimeter, or, using measurements of temperature and salinity at different depths, it can be calculated using a number of different sound speed formulae which have been developed. Examples of such formulae include those by Wilson, Chen and Millero and Mackenzie. Each such formulation applies within specific limits of the independent variables.
From the shape of the sound speed profile in figure 1, one can see the effect of the order of importance of temperature and depth on sound speed. Near the surface, where temperatures are generally highest, the sound speed is often highest because the effect of temperature on sound speed dominates. Further down the water column, as temperature decreases in the ocean thermocline, sound speed also decreases. At a certain point, however, the effect of depth, i.e., pressure, begins to dominate, and the sound speed increases to the ocean floor. Also visible in figure 1 is a common feature in sound speed profiles: the SOFAR channel. The axis of this channel is found at the depth of minimum sound speed. Sounds emitted at or near the axis of this channel propagate for very long horizontal distances, owing to the refraction of the sound back to the channel's center.
Sound speed profile data are a necessary component of underwater acoustic propagation models, especially those based on ray tracing theory.

Notes


== References ==",Category:Acoustics,1
250,251,Underwater telephone,"The underwater telephone, also known as UQC, AN/WQC-2, or Gertrude, was developed by the U.S. Navy in 1945. The UQC underwater telephone is used on all manned submersibles and many Naval surface ships in operation. Voice or an audio tone (morse code) communicated through the UQC are heterodyned to a high pitch for acoustic transmission through water.

See also
Communication with submarines


== References ==",Category:Acoustics,1
251,252,3D sound localization,"3D sound localization refers to an acoustic technology that is used to locate the source of a sound in a three-dimensional space. The source location is usually determined by the direction of the incoming sound waves (horizontal and vertical angles) and the distance between the source and sensors. It involves the structure arrangement design of the sensors and signal processing techniques.
Most mammals (including humans) use binaural hearing to localize sound, by comparing the information received from each ear in a complex process that involves a significant amount of synthesis. It is difficult to localize using monaural hearing, especially in 3D space.

Technology
Sound localization technology is used in some audio and acoustics fields, such as hearing aids, surveillance and navigation. Existing real-time passive sound localization systems are mainly based on the time-difference-of-arrival (TDOA) approach, limiting sound localization to two-dimensional space, and are not practical in noisy conditions.

Applications
Applications of sound source localization include sound source separation, sound source tracking, and speech enhancement. Sonar uses sound source localization techniques to identify the location of a target. 3D sound localization is also used for effective human-robot interaction. With the increasing demand for robotic hearing, some applications of 3D sound localization such as human-machine interface, handicapped aid, and military applications, are being explored.

Cues for sound localization
Localization cues are features that help localize sound. Cues for sound localization include binaural and monoaural cues.
Monoaural cues can be obtained via spectral analysis and are generally used in vertical localization.
Binaural cues are generated by the difference in hearing between the left and right ears. These differences include the interaural time difference (ITD) and the interaural intensity difference (IID). Binaural cues are used mostly for horizontal localization.

Methods
There are many different methods of 3D sound localization. For instance:
Different types of sensor structure, such as microphone array and binaural hearing robot head.
Different techniques for optimal results, such as neural network, maximum likelihood and Multiple signal classification (MUSIC).
Real-time methods using an Acoustic Vector Sensor (AVS) array 
Offline methods (according to timeliness)
Microphone Array Approach

Steered Beamformer Approach
This approach utilizes eight microphones combined with a steered beamformer enhanced by the Reliability Weighted Phase Transform (RWPHAT). The final results are filtered through a particle filter that tracks sources and prevents false directions.
The motivation of using this method is that based on previous research. This method is used for multiple sound source tracking and localizing despite soundtracking and localization only apply for a single sound source.

Beamformer-based Sound Localization
To maximize the output energy of a delay-and-sum beamformer in order to find the maximum value of the output of a beamformer steered in all possible directions. Using the Reliability Weighted Phase Transform (RWPHAT) method, The output energy of M-microphone delay-and-sum beamformer is

  
    
      
        E
        =
        K
        +
        2
        
          ?
          
            
              
                m
              
              
                1
              
            
            =
            1
          
          
            M
            ?
            1
          
        
        
          ?
          
            
              
                m
              
              
                2
              
            
            =
            0
          
          
            
              
                m
              
              
                1
              
            
            ?
            1
          
        
        
          
            
              
                R
              
              
                RWPHAT
              
            
          
          
            i
            ,
            j
          
        
        
          (
          
            
              ?
            
            
              
                
                  m
                
                
                  1
                
              
            
          
          ?
          
            
              ?
            
            
              
                
                  m
                
                
                  2
                
              
            
          
          )
        
      
    
    {\displaystyle E=K+2\sum _{{m}_{1}=1}^{M-1}\sum _{{m}_{2}=0}^{{m}_{1}-1}{{R}^{\text{RWPHAT}}}_{i,j}\left({\tau }_{{m}_{1}}-{\tau }_{{m}_{2}}\right)}
  
Where E indicates the energy, and K is a constant, 
  
    
      
        
          
            
              
                R
              
              
                RWPHAT
              
            
          
          
            i
            ,
            j
          
        
        
          (
          
            
              ?
            
            
              
                
                  m
                
                
                  1
                
              
            
          
          ?
          
            
              ?
            
            
              
                
                  m
                
                
                  2
                
              
            
          
          )
        
      
    
    {\displaystyle {{R}^{\text{RWPHAT}}}_{i,j}\left({\tau }_{{m}_{1}}-{\tau }_{{m}_{2}}\right)}
   is the microphone pairs cross-correlation defined by Reliability Weighted Phase Transform:

  
    
      
        
          
            
              
                R
              
              
                RWPHAT
              
            
          
          
            i
            ,
            j
          
        
        
          (
          ?
          )
        
        =
        
          ?
          
            k
            =
            0
          
          
            L
            ?
            1
          
        
        
          
            
              
                
                  ?
                
                
                  i
                
              
              
                (
                k
                )
              
              
                
                  X
                
                
                  i
                
              
              
                (
                k
                )
              
              
                
                  ?
                
                
                  j
                
              
              
                (
                k
                )
              
              
                
                  
                    
                      X
                    
                    
                      j
                    
                  
                
                
                  ?
                
              
              
                (
                k
                )
              
            
            
              
                |
                
                  
                    X
                  
                  
                    i
                  
                
                
                  (
                  k
                  )
                
                |
              
              
                |
                
                  
                    X
                  
                  
                    j
                  
                
                
                  (
                  k
                  )
                
                |
              
            
          
        
        
          
            e
          
          
            j
            2
            ?
            k
            ?
            
              /
            
            L
          
        
      
    
    {\displaystyle {{R}^{\text{RWPHAT}}}_{i,j}\left(\tau \right)=\sum _{k=0}^{L-1}{\frac {{\zeta }_{i}\left(k\right){X}_{i}\left(k\right){\zeta }_{j}\left(k\right){{X}_{j}}^{*}\left(k\right)}{\left|{X}_{i}\left(k\right)\right|\left|{X}_{j}\left(k\right)\right|}}{e}^{j2\pi k\tau /L}}
  
the weighted factor 
  
    
      
        
          
            
              
                ?
              
              
                n
              
            
          
          
            i
          
        
        
          (
          k
          )
        
      
    
    {\displaystyle {{\zeta }^{n}}_{i}\left(k\right)}
  reflect the reliability of each frequency component, and defined as the Wiener Filter gain 
  
    
      
        
          
            
              
                ?
              
              
                n
              
            
          
          
            i
          
        
        
          (
          k
          )
        
        =
        
          
            
              
                
                  
                    
                      ?
                    
                    
                      n
                    
                  
                
                
                  i
                
              
              
                (
                k
                )
              
            
            
              
                
                  
                    
                      ?
                    
                    
                      n
                    
                  
                
                
                  i
                
              
              
                (
                k
                )
              
              +
              1
            
          
        
      
    
    {\displaystyle {{\zeta }^{n}}_{i}\left(k\right)={\frac {{{\xi }^{n}}_{i}\left(k\right)}{{{\xi }^{n}}_{i}\left(k\right)+1}}}
  , where 
  
    
      
        
          
            
              
                ?
              
              
                n
              
            
          
          
            i
          
        
        
          (
          k
          )
        
      
    
    {\displaystyle {{\xi }^{n}}_{i}\left(k\right)}
   is an estimate of a prior SNR at 
  
    
      
        
          i
          
            t
            h
          
        
      
    
    {\displaystyle i^{th}}
   microphone, at time frame 
  
    
      
        n
      
    
    {\displaystyle n}
  , for frequency 
  
    
      
        k
      
    
    {\displaystyle k}
  , computed using the decision-directed approach.
The 
  
    
      
        
          x
          
            
              m
              
                n
              
            
          
        
      
    
    {\displaystyle x_{m_{n}}}
   is the signal from 
  
    
      
        
          
            m
          
          
            t
            h
          
        
      
    
    {\displaystyle {m}^{th}}
   microphone and 
  
    
      
        
          
            ?
          
          
            
              
                m
              
              
                n
              
            
          
        
      
    
    {\displaystyle {\tau }_{{m}_{n}}}
   is the delay of arrival for that microphone. The more specific procedure of this method is proposed by Valin and Michaud
The advantage of this method is that it detects the direction of the sound and derives the distance of sound sources. The main drawback of the beamforming approach is the imperfect nature of sound localization accuracy and capability, versus the neural network approach, which uses moving speakers.

Collocated Microphone Array Approach
This approach pertains to Real-time sound localization that uses an Acoustic Vector Sensor (AVS) array.

Acoustic Vector Array
• Contains three orthogonally installed acoustic particle velocity gradient microphones (shown as X, Y and Z array) and one omnidirectional acoustic microphone (O).
• Commonly used underwater.
• Uses the Offline Calibration Process to measure and interpolate the impulse response of X, Y, Z and O arrays, to obtain their steering vector.
A sound signal is first windowed using a rectangular window, then each resulting segment signal is created as a frame. 4 parallel frames are detected from XYZO array and used for DOA estimation. The 4 frames are split into small blocks with equal size, then the Hamming window and FFT are used to convert each block from a time domain to a frequency domain. Then the output of this system is represented by a horizontal angle and a vertical angle of the sound sources which is found by the peak in the combined 3D spatial spectrum.
The advantages of this array, compared with past microphone array, are that this device has a high performance even if the aperture is small, and it can localize multiple low frequency and high frequency wide band sound sources simultaneously. Applying an O array can make more available acoustic information, such as amplitude and time difference. Most importantly, XYZO array has a better performance with a tiny size.
The AVS is one kind of collocated multiple microphone array, it makes use of a multiple microphone array approach for estimating the sound directions by multiple arrays and then finds the locations by using reflection information such as where the direction is detected where different arrays cross.

Motivation of the Advanced Microphone array
Sound reflections always occur in an actual environment and microphone arrays cannot avoid observing those reflections. This multiple array approach was tested using fixed arrays in the ceiling; the performance of the moving scenario still need to be tested.

Learning how to apply Multiple Microphone Array
Angle uncertainty (AU) will occur when estimating direction, and position uncertainty (PU) will also aggravate with increasing distance between the array and the source. We know that:

  
    
      
        P
        U
        
          (
          r
          )
        
        =
        
          
            
              ±
              A
              U
            
            360
          
        
        ×
        2
        ?
        ×
        r
      
    
    {\displaystyle PU\left(r\right)={\frac {\pm AU}{360}}\times 2\pi \times r}
  
Where r is the distance between array center to source, and AU is angle uncertainly. Measurement is used for judging whether two directions cross at some location or not. Minimum distance between two lines:

  
    
      
        d
        i
        s
        t
        
          (
          d
          i
          
            r
            
              1
            
          
          ,
          d
          i
          
            r
            
              2
            
          
          )
        
        =
        
          
            
              
                (
                
                  
                    
                      v
                      
                        1
                      
                    
                    ?
                  
                
                ×
                
                  
                    
                      v
                      
                        2
                      
                    
                    ?
                  
                
                )
              
              ×
              
                
                  
                    
                      p
                      
                        1
                      
                    
                    
                      p
                      
                        2
                      
                    
                  
                  ?
                
              
            
            
              |
              
                
                  
                    v
                    
                      1
                    
                  
                  ?
                
              
              ×
              
                
                  
                    v
                    
                      2
                    
                  
                  ?
                
              
              |
            
          
        
      
    
    {\displaystyle dist\left(dir_{1},dir_{2}\right)={\frac {\left({\overrightarrow {v_{1}}}\times {\overrightarrow {v_{2}}}\right)\times {\overrightarrow {p_{1}p_{2}}}}{\left|{\overrightarrow {v_{1}}}\times {\overrightarrow {v_{2}}}\right|}}}
  
where
  
    
      
        d
        i
        
          r
          
            1
          
        
      
    
    {\displaystyle dir_{1}}
  and 
  
    
      
        d
        i
        
          r
          
            2
          
        
      
    
    {\displaystyle dir_{2}}
   are two directions, 
  
    
      
        
          v
          
            i
          
        
      
    
    {\displaystyle v_{i}}
  are vectors parallel to detected direction, and 
  
    
      
        
          p
          
            i
          
        
      
    
    {\displaystyle p_{i}}
  are the position of arrays.
If

  
    
      
        d
        i
        s
        t
        (
        d
        i
        
          r
          
            1
          
        
        ,
        d
        i
        
          r
          
            2
          
        
        )
        <
        a
        b
        s
        (
        P
        
          U
          
            1
          
        
        (
        
          r
          
            1
          
        
        )
        )
        +
        a
        b
        s
        (
        P
        
          U
          
            2
          
        
        (
        
          r
          
            2
          
        
        )
        )
      
    
    {\displaystyle dist(dir_{1},dir_{2})<abs(PU_{1}(r_{1}))+abs(PU_{2}(r_{2}))}
  
Two lines are judged as crossing. When two lines are crossing, we can compute the sound source location using the following:

  
    
      
        
          
            P
            O
            S
          
          
            s
            o
            u
            r
            c
            e
          
        
        =
        
          
            
              (
              
                
                  P
                  O
                  S
                
                
                  1
                
              
              ×
              
                w
                
                  1
                
              
              +
              
                
                  P
                  O
                  S
                
                
                  2
                
              
              ×
              
                w
                
                  2
                
              
              )
            
            
              
                w
                
                  1
                
              
              +
              
                w
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \mathrm {POS} _{source}={\frac {\left(\mathrm {POS} _{1}\times w_{1}+\mathrm {POS} _{2}\times w_{2}\right)}{w_{1}+w_{2}}}}
  

  
    
      
        
          
            P
            O
            S
          
          
            s
            o
            u
            r
            c
            e
          
        
      
    
    {\displaystyle \mathrm {POS} _{source}}
  is the estimation of sound source position, 
  
    
      
        
          
            P
            O
            S
          
          
            n
          
        
      
    
    {\displaystyle \mathrm {POS} _{n}}
   is the position where each direction intersect the line with minimum distance, and 
  
    
      
        
          w
          
            n
          
        
      
    
    {\displaystyle w_{n}}
   is the weighted factors. As the weighting factor 
  
    
      
        
          w
          
            n
          
        
      
    
    {\displaystyle w_{n}}
  , we determined use 
  
    
      
        P
        U
      
    
    {\displaystyle PU}
   or 
  
    
      
        r
      
    
    {\displaystyle r}
   from the array to the line with minimum distance.

Learning method for binaural hearing
Binaural hearing learning is a bionic method. The sensor is a robot dummy head with 2 sensor microphones along with the artificial pinna (reflector). The robot head has 2 rotation axes and can rotate horizontally and vertically. The reflector causes the spectrum change into a certain pattern for incoming white noise sound wave and this pattern is used for the cue of the vertical localization. The cue for horizontal localization is ITD. The system makes use of a learning process using neural networks by rotating the head with a settled white noise sound source and analyzing the spectrum. Experiments show that the system can identify the direction of the source well in a certain range of angle of arrival. It cannot identify the sound coming outside the range due to the collapsed spectrum pattern of the reflector. Binaural hearing use only 2 microphones and is capable of concentrating on one source among multiple sources of noises.

Head-related Transfer Function (HRTF)
In the real sound localization, the robot head and the torso play a functional role, in addition to the two pinnae. This functions as spatial linear filtering and the filtering is always quantified in terms of Head-Related Transfer Function (HRTF). HRTF also uses the robot head sensor, which is the binaural hearing model. The HRTF can be derived based on various cues for localization. Sound localization with HRTF is filtering the input signal with a filter which is designed based on the HRTF. Instead of using the neural networks, a head-related transfer function is used and the localization is based on a simple correlation approach.
See more: Head-related transfer function.

Cross-power spectrum phase (CSP) analysis
CSP method is also used for the binaural model. The idea is that the angle of arrival can be derived through the time delay of arrival (TDOA) between two microphones, and TDOA can be estimated by finding the maximum coefficients of CSP. CSP coefficients are derived by:

  
    
      
        c
        s
        
          p
          
            i
            j
          
        
        (
        k
        )
        =
        
          IFFT
        
        
          {
          
            
              
                
                  FFT
                
                [
                
                  s
                  
                    i
                  
                
                (
                n
                )
                ]
                ?
                
                  FFT
                
                [
                
                  s
                  
                    j
                  
                
                (
                n
                )
                
                  ]
                  
                    ?
                  
                
              
              
                
                  |
                  
                    FFT
                  
                  [
                  
                    s
                    
                      i
                    
                  
                  (
                  n
                  )
                  ]
                  |
                
                ?
                
                  |
                  
                    FFT
                  
                  [
                  
                    s
                    
                      j
                    
                  
                  (
                  n
                  )
                  ]
                  |
                
                
              
            
          
          }
        
        
      
    
    {\displaystyle csp_{ij}(k)={\text{IFFT}}\left\{{\frac {{\text{FFT}}[s_{i}(n)]\cdot {\text{FFT}}[s_{j}(n)]^{*}}{\left|{\text{FFT}}[s_{i}(n)]\right\vert \cdot \left|{\text{FFT}}[s_{j}(n)]\right\vert \quad }}\right\}\quad }
  
Where 
  
    
      
        
          s
          
            i
          
        
        (
        n
        )
      
    
    {\displaystyle s_{i}(n)}
   and 
  
    
      
        
          s
          
            j
          
        
        (
        n
        )
      
    
    {\displaystyle s_{j}(n)}
   are signals entering the microphone 
  
    
      
        i
      
    
    {\displaystyle i}
   and 
  
    
      
        j
      
    
    {\displaystyle j}
   respectively
Time delay of arrival(
  
    
      
        ?
      
    
    {\displaystyle \tau }
  ) then can be estimated by:

  
    
      
        
          ?
        
        =
        arg
        ?
        max
        {
        c
        s
        
          p
          
            i
            j
          
        
        (
        k
        )
        }
      
    
    {\displaystyle {\tau }=\operatorname {arg} \max\{csp_{ij}(k)\}}
  
Sound source direction is

  
    
      
        
          ?
        
        =
        
          cos
          
            ?
            1
          
        
        ?
        
          
            
              v
              ?
              ?
            
            
              
                d
                
                  max
                
              
              ?
              
                F
                
                  s
                
              
            
          
        
      
    
    {\displaystyle {\theta }=\cos ^{-1}{\frac {v\cdot \tau }{d_{\max }\cdot F_{s}}}}
  
Where 
  
    
      
        v
      
    
    {\displaystyle v}
   is the sound propagation speed, 
  
    
      
        
          F
          
            s
          
        
      
    
    {\displaystyle F_{s}}
   is the sampling frequency and 
  
    
      
        
          d
          
            m
            a
            x
          
        
      
    
    {\displaystyle d_{max}}
   is the distance with maximum time delay between 2 microphones.
CPS method does not require the system impulse response data that HRTF needs. An expectation-maximization algorithm is also used for localizing several sound sources and reduce the localization errors. The system is capable of identifying several moving sound source using only two microphones.

2D sensor line array
In order to estimate the location of a source in 3D space, two line sensor arrays can be placed horizontally and vertically. An example is a 2D line array used for underwater source localization. By processing the data from two arrays using the maximum likelihood method, the direction, range and depth of the source can be identified simultaneously. Unlike the binaural hearing model, this method is similar to the spectral analysis method. The method can be used to localize a distant source.

Hierarchical Fuzzy Artificial Neural Networks Approach
The Hierarchical Fuzzy Artificial Neural Networks Approach sound localization system was modelled on biologically binaural sound localization. Some primitive animals with two ears and small brains can perceive 3D space and process sounds, although the process is not fully understood. Some animals experience difficulty in 3D sound location due to small head size. Additionally, the wavelength of communication sound may be much larger than their head diameter, as is the case with frogs.
Based on previous binaural sound localization methods, a hierarchical fuzzy artificial neural network system combines interaural time difference(ITD-based) and interaural intensity difference(IID-based) sound localization methods for higher accuracy that is similar to that of humans. Hierarchical Fuzzy Artificial Neural Networks were used with the goal of the same sound localization accuracy as human ears.
IID-based or ITD-based sound localization methods have a main problem called Front-back confusion. In this sound localization based on a hierarchical neural network system, to solve this issue, an IID estimation is with ITD estimation. This system was used for broadband sounds and be deployed for non-stationary scenarios.

3D sound localization for monaural sound source
Typically, sound localization is performed by using two (or more) microphones. By using the difference of arrival times of a sound at the two microphones, one can mathematically estimate the direction of the sound source. However, the accuracy with which an array of microphones can localize a sound (using Interaural time difference) is fundamentally limited by the physical size of the array. If the array is too small, then the microphones are spaced too closely together so that they all record essentially the same sound (with ITF near zero), making it extremely difficult to estimate the orientation. Thus, it is not uncommon for microphone arrays to range from tens of centimeters in length (for desktop applications) to many tens of meters in length (for underwater localization). However, microphone arrays of this size then become impractical to use on small robots. even for large robots, such microphone arrays can be cumbersome to mount and to manoeuvre. In contrast, the ability to localize sound using a single microphone (which can be made extremely small) holds the potential of significantly more compact, as well as lower cost and power, devices for localization.
• Conventional HRTF approach
A general way to implement 3d sound localization is to use the HRTF(Head-related transfer function). First, compute HRTFs for the 3D sound localization, by formulating two equations; one represents the signal of a given sound source and the other indicates the signal output from the robot head microphones for the sound transferred from the source. Monaural input data are processed by these HRTFs, and the results are output from stereo headphones. The disadvantage of this method is that many parametric operations are necessary for the whole set of filters to realize the 3D sound localization, resulting in high computational complexity.
• DSP implementation of 3D sound localization

A DSP-based implementation of a realtime 3D sound localization approach with the use of an embedded DSP can reduce the computational complexity As shown in the figure, the implementation procedure of this realtime algorithm is divided into three phases, (i) Frequency Division, (ii) Sound Localization, and (iii) Mixing. In the case of 3D sound localization for a monaural sound source, the audio input data are divided into two: left and right channels and the audio input data in time series are processed one after another.
A distinctive feature of this approach is that the audible frequency band is divided into three so that a distinct procedure of 3D sound localization can be exploited for each of the three subbands.
• Single microphone approach
Monaural localization is made possible by the structure of the pinna (outer ear), which modifies the sound in a way that is dependent on its incident angle. A machine learning approach is adapted for monaural localization using only a single microphone and an “artificial pinna” (that distorts sound in a direction-dependent way). The approach models the typical distribution of natural and artificial sounds, as well as the direction-dependent changes to sounds induced by the pinna. The experimental results also show that the algorithm is able to fairly accurately localize a wide range of sounds, such as human speech, dog barking, waterfall, thunder, and so on. In contrast to microphone arrays, this approach also offers the potential of significantly more compact, as well as lower cost and power,",Category:Acoustics,1
252,253,Beat (acoustics),"In acoustics, a beat is an interference pattern between two sounds of slightly different frequencies, perceived as a periodic variation in volume whose rate is the difference of the two frequencies.
When tuning instruments that can produce sustained tones, beats can be readily recognized. Tuning two tones to a unison will present a peculiar effect: when the two tones are close in pitch but not identical, the difference in frequency generates the beating. The volume varies like in a tremolo as the sounds alternately interfere constructively and destructively. As the two tones gradually approach unison, the beating slows down and may become so slow as to be imperceptible.

Mathematics and physics of beat tones
This phenomenon is best known in acoustics or music, though it can be found in any linear system: ""According to the law of superposition, two tones sounding simultaneously are superimposed in a very simple way: one adds their amplitudes"". If a graph is drawn to show the function corresponding to the total sound of two strings, it can be seen that maxima and minima are no longer constant as when a pure note is played, but change over time: when the two waves are nearly 180 degrees out of phase the maxima of one wave cancel the minima of the other, whereas when they are nearly in phase their maxima sum up, raising the perceived volume.
It can be proven (see List of trigonometric identities) that the envelope of the maxima and minima form a wave whose frequency is half the difference between the frequencies of the two original waves. Consider two sine waves of unit amplitude:

  
    
      
        
          cos
          ?
          (
          2
          ?
          
            f
            
              1
            
          
          t
          )
          +
          cos
          ?
          (
          2
          ?
          
            f
            
              2
            
          
          t
          )
        
        =
        
          2
          cos
          ?
          
            (
            
              2
              ?
              
                
                  
                    
                      f
                      
                        1
                      
                    
                    +
                    
                      f
                      
                        2
                      
                    
                  
                  2
                
              
              t
            
            )
          
          cos
          ?
          
            (
            
              2
              ?
              
                
                  
                    
                      f
                      
                        1
                      
                    
                    ?
                    
                      f
                      
                        2
                      
                    
                  
                  2
                
              
              t
            
            )
          
        
      
    
    {\displaystyle {\cos(2\pi f_{1}t)+\cos(2\pi f_{2}t)}={2\cos \left(2\pi {\frac {f_{1}+f_{2}}{2}}t\right)\cos \left(2\pi {\frac {f_{1}-f_{2}}{2}}t\right)}}
  
If the two original frequencies are quite close (for example, a difference of approximately twelve hertz), the frequency of the cosine of the right side of the expression above, that is f1 ? f2/2, is often too low to be perceived as an audible tone or pitch. Instead, it is perceived as a periodic variation in the amplitude of the first term in the expression above. It can be said that the lower frequency cosine term is an envelope for the higher frequency one, i.e. that its amplitude is modulated. The frequency of the modulation is f1 + f2/2, that is, the average of the two frequencies. It can be noted that every second burst in the modulation pattern is inverted. Each peak is replaced by a trough and vice versa. However, because the human ear is not sensitive to the phase of a sound, only its amplitude or intensity, only the magnitude of the envelope is heard. Therefore, subjectively, the frequency of the envelope seems to have twice the frequency of the modulating cosine, which means the audible beat frequency is:

  
    
      
        
          f
          
            beat
          
        
        =
        
          f
          
            1
          
        
        ?
        
          f
          
            2
          
        
        
      
    
    {\displaystyle f_{\text{beat}}=f_{1}-f_{2}\,}
  
This can be seen on the diagram on the right.

A physical interpretation is that when

  
    
      
        cos
        ?
        
          (
          
            2
            ?
            
              
                
                  
                    f
                    
                      1
                    
                  
                  ?
                  
                    f
                    
                      2
                    
                  
                
                2
              
            
            t
          
          )
        
        =
        1
      
    
    {\displaystyle \cos \left(2\pi {\frac {f_{1}-f_{2}}{2}}t\right)=1}
  
the two waves are in phase and they interfere constructively. When it is zero, they are out of phase and interfere destructively. Beats occur also in more complex sounds, or in sounds of different volumes, though calculating them mathematically is not so easy.
Beating can also be heard between notes that are near to, but not exactly, a harmonic interval, due to some harmonic of the first note beating with a harmonic of the second note. For example, in the case of perfect fifth, the third harmonic (i.e. second overtone) of the bass note beats with the second harmonic (first overtone) of the other note. As well as with out-of tune notes, this can also happen with some correctly tuned equal temperament intervals, because of the differences between them and the corresponding just intonation intervals: see Harmonic series (music)#Harmonics and tuning.

Binaural beats
A binaural beat is an auditory illusion perceived when two different pure-tone sine waves, both with frequencies lower than 1500 Hz, with less than a 40 Hz difference between them, are presented to a listener dichotically (one through each ear).
For example, if a 530 Hz pure tone is presented to a subject's right ear, while a 520 Hz pure tone is presented to the subject's left ear, the listener will perceive the auditory illusion of a third tone, in addition to the two pure-tones presented to each ear. The third sound is called a binaural beat, and in this example would have a perceived pitch correlating to a frequency of 10 Hz, that being the difference between the 530 Hz and 520 Hz pure tones presented to each ear.
Binaural-beat perception originates in the inferior colliculus of the midbrain and the superior olivary complex of the brainstem, where auditory signals from each ear are integrated and precipitate electrical impulses along neural pathways through the reticular formation up the midbrain to the thalamus, auditory cortex, and other cortical regions.

Uses
Musicians commonly use interference beats to objectively check tuning at the unison, perfect fifth, or other simple harmonic intervals. Piano and organ tuners even use a method involving counting beats, aiming at a particular number for a specific interval.
The composer Alvin Lucier has written many pieces that feature interference beats as their main focus. Italian composer Giacinto Scelsi, whose style is grounded on microtonal oscillations of unisons, extensively explored the textural effects of interference beats, particularly in his late works such as the violin solos Xnoybis (1964) and L'âme ailée / L'âme ouverte (1973), which feature them prominently (note that Scelsi treated and notated each string of the instrument as a separate part, so that his violin solos are effectively quartets of one-strings, where different strings of the violin may be simultaneously playing the same note with microtonal shifts, so that the interference patterns are generated). Composer Phill Niblock's music is entirely based on beating caused by microtonal differences.

Sample
See also
Autonomous sensory meridian response (ASMR)
Combination tone
Consonance and dissonance
Gamelan tuning
Heterodyne
Moiré pattern, a form of spatial interference that generates new frequencies.
Superposition principle
Voix céleste

References
External links
Java applet, MIT
Acoustics and Vibration Animations, D.A. Russell, Pennsylvania State University
A Java applet showing the formation of beats due to the interference of two waves of slightly different frequencies
Yet another interactive Java applet; also shows equation of combined waves, including phase angle.
Lissajous Curves: Interactive simulation of graphical representations of musical intervals, beats, interference, vibrating strings

Further reading
Thaut, Michael H. (2005). Rhythm, music, and the brain : scientific foundations and clinical applications (1st in paperback ed.). New York, NY [u.a.]: Routledge. ISBN 0415973708. 
Berger, Jonathan; Turow, Gabe, eds. (2011). Music, science, and the rhythmic brain : cultural and clinical implications. Routledge. ISBN 9780415890595.",Category:Auditory illusions,1
253,254,Cocktail party effect,"The cocktail party effect is the phenomenon of the brain's ability to focus one's auditory attention (an effect of selective attention in the brain) on a particular stimulus while filtering out a range of other stimuli, as when a partygoer can focus on a single conversation in a noisy room. Listeners have the ability to both segregate different stimuli into different streams, and subsequently decide which streams are most pertinent to them. Thus, it has been proposed that one’s sensory memory subconsciously siphons through all stimuli, and when an important word or phrase with high meaning appears, it stands out to the listener. This effect is what allows most people to ""tune into"" a single voice and ""tune out"" all others. It may also describe a similar phenomenon that occurs when one may immediately detect words of importance originating from unattended stimuli, for instance hearing one's name in another conversation during a cocktail party.

Neurological basis (and binaural processing)
Auditory attention in regards to the cocktail party effect primarily occurs in the left hemisphere of the superior temporal gyrus (where the primary auditory cortex); a fronto-parietal network involving the inferior frontal gyrus, superior parietal sulcus, and intraparietal sulcus also accounts for the acts of attention-shifting, speech processing, and attention control. Both the target stream (the more important information being attended to) and competing/interfering streams are processed in the same pathway within the left hemisphere, but fMRI scans shows that target streams are treated with more attention than competing streams. 
Furthermore, we see that activity in the STG toward the target stream is decreased/interfered with when competing stimuli streams (that typically hold significant value) arise. The “cocktail party effect” - the ability to detect significant stimuli in multitalker situations - has also been labeled the “cocktail party problem” because our ability to selectively attend simultaneously interferes with the effectiveness of attention at a neurological level.
The cocktail party effect works best as a binaural effect, which requires hearing with both ears. People with only one functioning ear seem much more distracted by interfering noise than people with two typical ears.
The binaural aspect of the cocktail party effect is related to the localization of sound sources. The auditory system is able to localize at least two sound sources and assign the correct characteristics to these sources simultaneously. As soon as the auditory system has localized a sound source, it can extract the signals of this sound source out of a mixture of interfering sound sources.

Early work
In the early 1950s much of the early attention research can be traced to problems faced by air traffic controllers. At that time, controllers received messages from pilots over loudspeakers in the control tower. Hearing the intermixed voices of many pilots over a single loudspeaker made the controller's task very difficult. The effect was first defined and named ""the cocktail party problem"" by Colin Cherry in 1953. Cherry conducted attention experiments in which participants listened to two different messages from a single loudspeaker at the same time and tried to separate them; this was later termed a dichotic listening task. (See Broadbent section below for more details). His work reveals that the ability to separate sounds from background noise is affected by many variables, such as the sex of the speaker, the direction from which the sound is coming, the pitch, and the rate of speech.
Cherry developed the shadowing task in order to further study how people selectively attend to one message amid other voices and noises. In a shadowing task participants wear a special headset that presents a different message to each ear. The participant is asked to repeat aloud the message (called shadowing) that is heard in a specified ear (called a channel). Cherry found that participants were able to detect their name from the unattended channel, the channel they were not shadowing. Later research using Cherry's shadowing task was done by Neville Moray in 1959. He was able to conclude that almost none of the rejected message is able to penetrate the block set up, except subjectively ""important"" messages.

More recent work
Selective attention shows up across all ages. Starting with infancy, babies begin to turn their heads toward a sound that is familiar to them, such as their parents’ voices. This shows that infants selectively attend to specific stimuli in their environment. Furthermore, reviews of selective attention indicate that infants favor “baby” talk over speech with an adult tone. This preference indicates that infants can recognize physical changes in the tone of speech. However, the accuracy in noticing these physical differences, like tone, amid background noise improves over time. Infants may simply ignore stimuli because something like their name, while familiar, holds no higher meaning to them at such a young age. However, research suggests that the more likely scenario is that infants don’t understand that the noise being presented to them amidst distracting noise is their own name, and thus do not respond. The ability to filter out unattended stimuli reaches its prime in young adulthood. In reference to the cocktail party phenomenon, older adults have a harder time than younger adults focusing in on one conversation if competing stimuli, like “subjectively” important messages, make up the background noise.
Some examples of messages that catch people’s attention include personal names and taboo words. The ability to selectively attend to one’s own name has been found in infants as young as 5 months of age and appears to be fully developed by 13 months. Along with multiple experts in the field, Anne Treisman states that people are permanently primed to detect personally significant words, like names, and theorizes that they may require less perceptual information than other words to trigger identification. Another stimulus that reaches some level of semantic processing while in the unattended channel is taboo words. These words often contain sexually explicit material that cause an alert system in people that leads to decreased performance in shadowing tasks. Taboo words do not affect children in selective attention until they develop a strong vocabulary with an understanding of language.
Selective attention begins to waiver as we get older. Older adults have longer latency periods in discriminating between conversation streams. This is typically attributed to the fact that general cognitive ability begins to decay with old age (as exemplified with memory, visual perception, higher order functioning, etc.).
Even more recently, modern neuroscience techniques are being applied to study the cocktail party problem. Some notable examples of researchers doing such work include Edward Chang, Nima Mesgarani, and Charles Schroeder using electrocorticography; Jonathan Simon, Mounya Elhilali, Adrian KC Lee, Shihab Shamma, Barbara Shinn-Cunningham and Jyrki Ahveninen using magnetoencephalography; Jyrki Ahveninen, Edmund Lalor, and Barbara Shinn-Cunningham using electroencephalography; and Jyrki Ahveninen and Lee M. Miller using functional magnetic resonance imaging.

Models of attention
Not all the information presented to us can be processed. In theory, the selection of what to pay attention to can be random or nonrandom. For example, when driving, drivers are able to focus on the traffic lights rather than on other stimuli present in the scene. In such cases it is mandatory to select which portion of presented stimuli is important. A basic question in psychology is when this selection occurs. This issue has developed into the early versus late selection controversy. The basis for this controversy can be found in the Cherry dichotic listening experiments. Participants were able to notice physical changes, like pitch or change in gender of the speaker, and stimuli, like their own name, in the unattended channel. This brought about the question of whether the meaning, semantics, of the unattended message was processed before selection. In an early selection attention model very little information is processed before selection occurs. In late selection attention models more information, like semantics, is processed before selection occurs.

Broadbent
Some of the earliest work in exploring mechanisms of early selective attention was performed by Donald Broadbent, who proposed a theory that came to be known as the filter model. This model was established using the dichotic listening task. His research showed that most participants were accurate in recalling information that they actively attended to, but were far less accurate in recalling information that they had not attended to. This led Broadbent to the conclusion that there must be a ""filter"" mechanism in the brain that could block out information that was not selectively attended to. The filter model was hypothesized to work in the following way: as information enters the brain through sensory organs (in this case, the ears) it is stored in sensory memory, a buffer memory system that hosts an incoming stream of information long enough for us to pay attention to it. Before information is processed further, the filter mechanism allows only attended information to pass through. The selected attention is then passed into working memory, the set of mechanisms that underlies short-term memory and communicates with long-term memory. In this model, auditory information can be selectively attended to on the basis of its physical characteristics, such as location and volume. Others suggest that information can be attended to on the basis of Gestalt features, including continuity and closure. For Broadbent, this explained the mechanism by which people can choose to attend to only one source of information at a time while excluding others. However, Broadbent's model failed to account for the observation that words of semantic importance, for example the individual's own name, can be instantly attended to despite having been in an unattended channel.
Shortly after Broadbent's experiments, Oxford undergraduates Gray and Wedderburn repeated his dichotic listening tasks, altered with monosyllabic words that could form meaningful phrases, except that the words were divided across ears. For example, the words, ""Dear, one, Jane,"" were sometimes presented in sequence to the right ear, while the words, ""three, Aunt, six,"" were presented in a simultaneous, competing sequence to the left ear. Participants were more likely to remember, ""Dear Aunt Jane,"" than to remember the numbers; they were also more likely to remember the words in the phrase order than to remember the numbers in the order they were presented. This finding goes against Broadbent's theory of complete filtration because the filter mechanism would not have time to switch between channels. This suggests that meaning may be processed first.

Treisman
In a later addition to this existing theory of selective attention, Anne Treisman developed the attenuation model. In this model, information, when processed through a filter mechanism, is not completely blocked out as Broadbent might suggest. Instead, the information is weakened (attenuated), allowing it to pass through all stages of processing at an unconscious level. Treisman also suggested a threshold mechanism whereby some words, on the basis of semantic importance, may grab one's attention from the unattended stream. One's own name, according to Treisman, has a low threshold value (i.e. it has a high level of meaning) and thus is recognized more easily. The same principle applies to words like fire, directing our attention to situations that may immediately require it. The only way this can happen, Treisman argued, is if information was being processed continuously in the unattended stream.

Deutsch & Deutsch
Diana Deutsch, best known for her work in music perception and auditory illusions, has also made important contributions to models of attention. In order to explain in more detail how words can be attended to on the basis of semantic importance, Deutsch & Deutsch and Norman proposed a model of attention which includes a second selection mechanism based on meaning. In what came to be known as the Deutsch-Norman model, information in the unattended stream is not processed all the way into working memory, as Treisman's model would imply. Instead, information on the unattended stream is passed through a secondary filter after pattern recognition. If the unattended information is recognized and deemed unimportant by the secondary filter, it is prevented from entering working memory. In this way, only immediately important information from the unattended channel can come to awareness.

Kahneman
Daniel Kahneman also proposed a model of attention, but it differs from previous models in that he describes attention not in terms of selection, but in terms of capacity. For Kahneman, attention is a resource to be distributed among various stimuli, a proposition which has received some support. This model describes not when attention is focused, but how it is focused. According to Kahneman, attention is generally determined by arousal; a general state of physiological activity. The Yerkes-Dodson law predicts that arousal will be optimal at moderate levels - performance will be poor when one is over- or under-aroused. Of particular relevance, Narayan et al. discovered a sharp decline in the ability to discriminate between auditory stimuli when background noises were too numerous and complex - this is evidence of the negative effect of overarousal on attention. Thus, arousal determines our available capacity for attention. Then, an allocation policy acts to distribute our available attention among a variety of possible activities. Those deemed most important by the allocation policy will have the most attention given to them. The allocation policy is affected by enduring dispositions (automatic influences on attention) and momentary intentions (a conscious decision to attend to something). Momentary intentions requiring a focused direction of attention rely on substantially more attention resources than enduring dispositions. Additionally, there is an ongoing evaluation of the particular demands of certain activities on attention capacity. That is to say, activities that are particularly taxing on attention resources will lower attention capacity and will influence the allocation policy - in this case, if an activity is too draining on capacity, the allocation policy will likely cease directing resources to it and instead focus on less taxing tasks. Kahneman's model explains the cocktail party phenomenon in that momentary intentions might allow one to expressly focus on a particular auditory stimulus, but that enduring dispositions (which can include new events, and perhaps words of particular semantic importance) can capture our attention. It is important to note that Kahneman's model doesn't necessarily contradict selection models, and thus can be used to supplement them.

Visual correlates
Some research has demonstrated that the cocktail party effect may not be simply an auditory phenomenon, and that relevant effects can be obtained when testing visual information as well. For example, Shapiro et al. were able to demonstrate an ""own name effect"" with visual tasks, where subjects were able to easily recognize their own names when presented as unattended stimuli. They adopted a position in line with late selection models of attention such as the Treisman or Deutsch-Norman models, suggesting that early selection would not account for such a phenomenon. The mechanisms by which this effect might occur were left unexplained.

See also


== References ==",Category:Acoustics,1
254,255,Echo,"In audio signal processing and acoustics, Echo is a reflection of sound that arrives at the listener with a delay after the direct sound. The delay is proportional to the distance of the reflecting surface from the source and the listener. Typical examples are the echo produced by the bottom of a well, by a building, or by the walls of an enclosed room and an empty room. A true echo is a single reflection of the sound source.
The word echo derives from the Greek ??? (?ch?), itself from ???? (?chos), ""sound"". Echo in the folk story of Greek is a mountain nymph whose ability to speak was cursed, only able to repeat the last words anyone spoke to her. Some animals use echo for location sensing and navigation, such as cetaceans (dolphins and whales) and bats.

Acoustic phenomenon
Acoustic waves akola are reflected by walls or other hard surfaces, such as mountains and privacy fences. The reason of reflection may be explained as a discontinuity in the propagation medium. This can be heard when the reflection returns with sufficient magnitude and delay to be perceived distinctly. When sound, or the echo itself, is reflected multiple times from multiple surfaces, the echo is characterized as a reverberation.

The human ear cannot distinguish echo from the original direct sound if the delay is less than 1/10 of a second. The velocity of sound in dry air is approximately 343 m/s at a temperature of 25 °C. Therefore, the reflecting object must be more than 17.2m from the sound source for echo to be perceived by a person located at the source. When a sound produces an echo in two seconds, the reflecting object is 343m away. In nature, canyon walls or rock cliffs facing water are the most common natural settings for hearing echoes. The strength of echo is frequently measured in dB sound pressure level (SPL) relative to the directly transmitted wave. Echoes may be desirable (as in sonar) or undesirable (as in telephone systems).

In music
In music performance and recording, electric echo effects have been used since the 1950s. The Echoplex is a tape delay effect, first made in 1959 that recreates the sound of an acoustic echo. Designed by Mike Battle, the Echoplex set a standard for the effect in the 1960s and was used by most of the notable guitar players of the era; original Echoplexes are highly sought after. While Echoplexes were used heavily by guitar players (and the occasional bass player, such as Chuck Rainey, or trumpeter, such as Don Ejudvntfhllis), many recording studios also used the Echoplex. Beginning in the 1970s, Market built the solid-state Echoplex for Maestro. In the 2000s, most echo effects units use electronic or digital circuitry to recreate the echo effect.

Famous echoes
Hamilton Mausoleum, Hamilton, South Lanarkshire, Scotland: Its high stone holds the record for the longest echo in the world, taking 15 seconds for the sound of a slammed door to delay.
Gol Gumbaz of Bijapur, India: Any whisper, clap or sound gets echoed repeatedly.
The Golkonda Fort of Hyderabad, India
The Echo Wall at the Temple of Heaven, Beijing, China
The Whispering Gallery of St Paul's Cathedral, London, England, UK
Echo Point, the Three Sisters, Katoomba, Australia
The Temple of Kukulcan (""El Castillo""), Chichen Itza, Mexico
The Baptistry of Pisa, Pisa, Italy
The echo near Milan visited by Mark Twain in The Innocents Abroad
The echo in Chinon, France which is used in a traditional local rhyme
The gazebo of Napier Museum in Trivandrum, Kerala, India

References
External links
More information on Chinon echo.
Listen to Duck echoes and an animated demonstration of how an echo is formed.",Category:Articles with unsourced statements from May 2016,1
255,256,Acoustic holography,"Acoustic holography is a method for estimating the sound field near a source by measuring acoustic parameters away from the source by means of an array of pressure and/or particle velocity transducers. Measuring techniques included in acoustic holography are becoming increasingly popular in various fields, most notably those of transportation, vehicle and aircraft design, and noise, vibration, and harshness (NVH). The general idea of acoustic holography has led to different versions such as near-field acoustic holography (NAH) and statistically optimal near-field acoustic holography (SONAH). For audio rendition, the wave field synthesis is the most related procedure.

References
J. D. Maynard; E. G. Williams; Y. Lee (October 1985). ""Nearfield acoustic holography: I. Theory of generalized holography and the development of NAH"". The Journal of the Acoustical Society of America. 78 (4): 1395–1413. Bibcode:1985ASAJ...78.1395M. doi:10.1121/1.392911. 
J. Hald. ""Patch near-field acoustical holography using a new statistically optimal method"". Inter-noise 2003, Jeju International Convention Center, Seogwipo, Korea, 2003-08-25–2003-08-28.

External links
Acoustic holography
Introduction to Acoustic Holography",Category:Acoustics,1
256,257,Octave band,"Analyzing a source on a frequency by frequency basis is possible but time consuming. The whole frequency range is divided into sets of frequencies called bands. Each band covers a specific range of frequencies. For this reason, a scale of octave bands and one-third octave bands has been developed. A band is said to be an octave in width when the upper band frequency is twice the lower band frequency. A one-third octave band is defined as a frequency band whose upper band-edge frequency (f2) is the lower band frequency (f1) times the cube root of two.

Octave Bands
Calculation
Naming
One-third octave bands
Calculation
Naming
See also
Octave
Octave (electronics)
One-third octave",Category:All articles lacking sources,1
257,258,Architectural acoustics,"Architectural acoustics (also known as room acoustics and building acoustics) is the science and engineering of achieving a good sound within a building and is a branch of acoustical engineering. The first application of modern scientific methods to architectural acoustics was carried out by Wallace Sabine in the Fogg Museum lecture room who then applied his new found knowledge to the design of Symphony Hall, Boston.
Architectural acoustics can be about achieving good speech intelligibility in a theatre, restaurant or railway station, enhancing the quality of music in a concert hall or recording studio, or suppressing noise to make offices and homes more productive and pleasant places to work and live in. Architectural acoustic design is usually done by acoustic consultants.

Building skin envelope
This science analyzes noise transmission from building exterior envelope to interior and vice versa. The main noise paths are roofs, eaves, walls, windows, door and penetrations. Sufficient control ensures space functionality and is often required based on building use and local municipal codes. An example would be providing a suitable design for a home which is to be constructed close to a high volume roadway, or under the flight path of a major airport, or of the airport itself.

Inter-space noise control
The science of limiting and/or controlling noise transmission from one building space to another to ensure space functionality and speech privacy. The typical sound paths are ceilings, room partitions, acoustic ceiling panels (such as wood dropped ceiling panels), doors, windows, flanking, ducting and other penetrations. Technical solutions depend on the source of the noise and the path of acoustic transmission, for example noise by steps or noise by (air, water) flow vibrations. An example would be providing suitable party wall design in an apartment complex to minimize the mutual disturbance due to noise by residents in adjacent apartments.

Interior space acoustics
This is the science of controlling a room's surfaces based on sound absorbing and reflecting properties. Excessive reverberation time, which can be calculated, can lead to poor speech intelligibility.

Sound reflections create standing waves that produce natural resonances that can be heard as a pleasant sensation or an annoying one. Reflective surfaces can be angled and coordinated to provide good coverage of sound for a listener in a concert hall or music recital space. To illustrate this concept consider the difference between a modern large office meeting room or lecture theater and a traditional classroom with all hard surfaces.

Interior building surfaces can be constructed of many different materials and finishes. Ideal acoustical panels are those without a face or finish material that interferes with the acoustical infill or substrate. Fabric covered panels are one way to heighten acoustical absorption. Perforated metal also shows sound absorbing qualities. Finish material is used to cover over the acoustical substrate. Mineral fiber board, or Micore, is a commonly used acoustical substrate. Finish materials often consist of fabric, wood or acoustical tile. Fabric can be wrapped around substrates to create what is referred to as a ""pre-fabricated panel"" and often provides good noise absorption if laid onto a wall.
Prefabricated panels are limited to the size of the substrate ranging from 2 by 4 feet (0.61 m × 1.22 m) to 4 by 10 feet (1.2 m × 3.0 m). Fabric retained in a wall-mounted perimeter track system, is referred to as ""on-site acoustical wall panels"". This is constructed by framing the perimeter track into shape, infilling the acoustical substrate and then stretching and tucking the fabric into the perimeter frame system. On-site wall panels can be constructed to accommodate door frames, baseboard, or any other intrusion. Large panels (generally, greater than 50 square feet (4.6 m2)) can be created on walls and ceilings with this method. Wood finishes can consist of punched or routed slots and provide a natural look to the interior space, although acoustical absorption may not be great.
There are three ways to improve workplace acoustics and solve workplace sound problems – the ABCs.
A = Absorb (via drapes, carpets, ceiling tiles, etc.)
B = Block (via panels, walls, floors, ceilings and layout)
C = Cover-up (via sound masking)

Mechanical equipment noise
Building services noise control is the science of controlling noise produced by:
ACMV (air conditioning and mechanical ventilation) systems in buildings, termed HVAC in North America
Elevators
Electrical generators positioned within or attached to a building
Any other building service infrastructure component that emits sound.
Inadequate control may lead to elevated sound levels within the space which can be annoying and reduce speech intelligibility. Typical improvements are vibration isolation of mechanical equipment, and sound traps in ductwork. Sound masking can also be created by adjusting HVAC noise to a predetermined level.

See also
Acoustic transmission
Noise health effects
Noise mitigation
Noise Reduction Coefficient
Noise regulation
Noise, vibration, and harshness
Room acoustics
Sound transmission class
Wallace Clement Sabine

References
External links
Acoustical Society of America
American Institute of Architects
National Council of Acoustical Consultants
Institute of Acoustics
Speech Privacy Calculator
Optimum sizes for small rooms
Concert Hall Acoustics
Everything You Always Wanted to Know About Concert Hall Acoustics
An on-line version of an exhibition on concert hall acoustics originally shown at the South Bank Centre, London",Category:Building defects,1
258,259,Physical acoustics,"Physical acoustics is the area of acoustics and physics that studies interactions of acoustic waves with a gaseous, liquid or solid medium on macro- and micro-levels. This relates to the interaction of sound with thermal waves in crystals (phonons), with light (photons), with electrons in metals and semiconductors (acousto-electric phenomena), with magnetic excitations in ferromagnetic crystals (magnons), etc. Some recently developed experimental techniques include photo-acoustics, acoustic microscopy and acoustic emission. A long-standing interest is in acoustic and ultrasonic wave propagation and scattering in inhomogeneous materials, including composite materials and biological tissues.
There are two main classes of problems studied in physical acoustics. The first one concerns understanding how the physical properties of a medium (solid, liquid, or gas) influence the propagation of acoustic waves in this medium in order to use this knowledge for practical purposes. The second important class of problems studied in physical acoustics is to obtain the relevant information about a medium under consideration by measuring the properties of acoustic waves propagating through this medium.

See also
Acoustic attenuation
Acoustic levitation
Acoustic streaming
Acousto-electric effect
Acousto-optics
Elastic waves
Interdigital transducer
Longitudinal wave
Love wave
Nonlinear acoustics
Picosecond ultrasonics
Sonoluminescence
Rayleigh wave
Shear wave
Sound absorption
Sound velocity
Thermoacoustics

References
Blackstock, D.T. (2000), Fundamentals of Physical Acoustics, New York: Wiley, ISBN 0471319791 
Pierce, A.D. (1989), Acoustics: An Introduction to its Physical Principles and Applications, Acoustical Society of America, ISBN 0883186128 
Mason, W.P.; Thurston, R.N., eds. (1999), Physical Acoustics: Principles and Methods, 24, Academic Press, ISBN 012477945X

External links
Physical Acoustics Group, The Institute of Physics (IOP) and The Institute of Acoustics (IOA)
Anglo-French Physical Acoustics Conferences
NCPA - National Center for Physical Acoustics, The University of Mississippi",Category:Acoustics,1
259,260,Particle displacement,"Particle displacement or displacement amplitude is a measurement of distance of the movement of a particle from its equilibrium position in a medium as it transmits a second wave. The SI unit of particle displacement is the metre (m). In most cases this is a longitudinal wave of pressure (such as sound), but it can also be a transverse wave, such as the vibration of a taut string. In the case of a sound wave travelling through air, the particle displacement is evident in the oscillations of air molecules with, and against, the direction in which the sound wave is travelling.
A particle of the medium undergoes displacement according to the particle velocity of the sound wave traveling through the medium, while the sound wave itself moves at the speed of sound, equal to 343 m/s in air at 20 °C.

Mathematical definition
Particle displacement, denoted ?, is given by

  
    
      
        
          ?
        
        =
        
          ?
          
            t
          
        
        
          v
        
        
        
          d
        
        t
      
    
    {\displaystyle \mathbf {\delta } =\int _{t}\mathbf {v} \,\mathrm {d} t}
  
where v is the particle velocity.

Progressive sine waves
The particle displacement of a progressive sine wave is given by

  
    
      
        ?
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            ?
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle \delta (\mathbf {r} ,\,t)=\delta \cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}),}
  
where
? is the amplitude of the particle displacement;

  
    
      
        
          ?
          
            ?
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{\delta ,0}}
   is the phase shift of the particle displacement;
k is the angular wavevector;
? is the angular frequency.
It follows that the particle velocity and the sound pressure along the direction of propagation of the sound wave x are given by

  
    
      
        v
        (
        
          r
        
        ,
        
        t
        )
        =
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        v
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            v
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle v(\mathbf {r} ,\,t)={\frac {\partial \delta }{\partial t}}(\mathbf {r} ,\,t)=\omega \delta \cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=v\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{v,0}),}
  

  
    
      
        p
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        
          c
          
            2
          
        
        
          
            
              ?
              ?
            
            
              ?
              x
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          c
          
            2
          
        
        
          k
          
            x
          
        
        ?
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        p
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            p
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle p(\mathbf {r} ,\,t)=-\rho c^{2}{\frac {\partial \delta }{\partial x}}(\mathbf {r} ,\,t)=\rho c^{2}k_{x}\delta \cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=p\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{p,0}),}
  
where
v is the amplitude of the particle velocity;

  
    
      
        
          ?
          
            v
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}}
   is the phase shift of the particle velocity;
p is the amplitude of the acoustic pressure;

  
    
      
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{p,0}}
   is the phase shift of the acoustic pressure.
Taking the Laplace transforms of v and p with respect to time yields

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        v
        
          
            
              s
              cos
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle {\hat {v}}(\mathbf {r} ,\,s)=v{\frac {s\cos \varphi _{v,0}-\omega \sin \varphi _{v,0}}{s^{2}+\omega ^{2}}},}
  

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        p
        
          
            
              s
              cos
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\hat {p}}(\mathbf {r} ,\,s)=p{\frac {s\cos \varphi _{p,0}-\omega \sin \varphi _{p,0}}{s^{2}+\omega ^{2}}}.}
  
Since 
  
    
      
        
          ?
          
            v
            ,
            0
          
        
        =
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}=\varphi _{p,0}}
  , the amplitude of the specific acoustic impedance is given by

  
    
      
        z
        (
        
          r
        
        ,
        
        s
        )
        =
        
          |
        
        z
        (
        
          r
        
        ,
        
        s
        )
        
          |
        
        =
        
          |
          
            
              
                
                  
                    
                      p
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
              
                
                  
                    
                      v
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
            
          
          |
        
        =
        
          
            p
            v
          
        
        =
        
          
            
              ?
              
                c
                
                  2
                
              
              
                k
                
                  x
                
              
            
            ?
          
        
        .
      
    
    {\displaystyle z(\mathbf {r} ,\,s)=|z(\mathbf {r} ,\,s)|=\left|{\frac {{\hat {p}}(\mathbf {r} ,\,s)}{{\hat {v}}(\mathbf {r} ,\,s)}}\right|={\frac {p}{v}}={\frac {\rho c^{2}k_{x}}{\omega }}.}
  
Consequently, the amplitude of the particle displacement is related to those of the particle velocity and the sound pressure by

  
    
      
        ?
        =
        
          
            v
            ?
          
        
        ,
      
    
    {\displaystyle \delta ={\frac {v}{\omega }},}
  

  
    
      
        ?
        =
        
          
            p
            
              ?
              z
              (
              
                r
              
              ,
              
              s
              )
            
          
        
        .
      
    
    {\displaystyle \delta ={\frac {p}{\omega z(\mathbf {r} ,\,s)}}.}

See also
Sound
Sound particle
Particle velocity
Particle acceleration

References and notes
Related Reading:
Wood, Robert Williams (1914). Physical optics. New York: The Macmillan Company. 
Strong, John Donovan & Hayward, Roger (January 2004). Concepts of Classical Optics. Dover Publications. ISBN 978-0-486-43262-5. 
Barron, Randall F. (January 2003). Industrial noise control and acoustics. NYC, New York: CRC Press. pp. 79, 82, 83, 87. ISBN 978-0-8247-0701-9.

External links
Acoustic Particle-Image Velocimetry. Development and Applications
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave",Category:Physical quantities,1
260,261,Mechanical index,,Category:Medical ultrasonography,1
261,262,Plant bioacoustics,,Category:Orphaned articles from June 2016,1
262,263,Duct modes,,Category:Acoustics,1
263,264,Octave (electronics),"In electronics, an octave (symbol oct) is a doubling or halving of a frequency. The term is derived from the Western musical scale (an octave is a doubling in frequency) and is therefore common in audio electronics. (The prefix octa-, denoting eight, refers to the eight notes of a diatonic scale; the association of the word with doubling is solely a matter of customary usage.) Along with the decade, it is a unit used to describe frequency bands or frequency ratios.
A frequency ratio expressed in octaves is the base-2 logarithm (binary logarithm) of the ratio:

  
    
      
        
          n
          u
          m
          b
          e
          r
          o
          f
          o
          c
          t
          a
          v
          e
          s
        
        =
        
          log
          
            2
          
        
        ?
        
          (
          
            
              
                f
                
                  2
                
              
              
                f
                
                  1
                
              
            
          
          )
        
      
    
    {\displaystyle \mathrm {numberofoctaves} =\log _{2}\left({\frac {f_{2}}{f_{1}}}\right)}
  
An amplifier or filter may be stated to have a frequency response of ±6dB per octave over a particular frequency range, which signifies that the power gain changes by ±6 decibels (a factor of 4 in power), when the frequency changes by a factor of 2. This slope, or more precisely 
  
    
      
        10
        
          log
          
            10
          
        
        ?
        (
        4
        )
        ?
        6.0206
      
    
    {\displaystyle 10\log _{10}(4)\approx 6.0206}
   decibels per octave, corresponds to an amplitude gain proportional to frequency, which is equivalent to ±20dB per decade (factor of 10 amplitude gain change for a factor of 10 frequency change). This would be a first-order filter.

Example
1. The distance between the frequencies 20 Hz and 40 Hz is 1 octave.
2. An amplitude of 52 dB at 4 kHz decreases as frequency increases at ?2 dB/oct. What is the amplitude at 13 kHz?

  
    
      
        
          number of octaves
        
        =
        
          log
          
            2
          
        
        ?
        
          (
          
            
              13
              4
            
          
          )
        
        =
        1.7
      
    
    {\displaystyle {\text{number of octaves}}=\log _{2}\left({\frac {13}{4}}\right)=1.7}
  

  
    
      
        
          
            Mag
          
          
            13
            
               kHz
            
          
        
        =
        52
        
           dB
        
        +
        (
        1.7
        
           oct
        
        ×
        ?
        2
        
           dB/oct
        
        )
        =
        48.6
        
           dB
        
        .
        
      
    
    {\displaystyle {\text{Mag}}_{13{\text{ kHz}}}=52{\text{ dB}}+(1.7{\text{ oct}}\times -2{\text{ dB/oct}})=48.6{\text{ dB}}.\,}

See also
Octave
Octave band
One-third octave


== References ==",Category:Acoustics,1
264,265,Musical acoustics,"Musical acoustics or music acoustics is a branch of acoustics concerned with researching and describing the physics of music – how sounds are employed to make music. Examples of areas of study are the function of musical instruments, the human voice (the physics of speech and singing), computer analysis of melody, and in the clinical use of music in music therapy.

Methods and fields of study
The physics of musical instruments
Frequency range of music
Frequency analysis
Computer analysis of musical structure
Synthesis of musical sounds
Music cognition, based on physics (also known as psychoacoustics)

Physical aspects
Whenever two different pitches are played at the same time, their sound waves interact with each other – the highs and lows in the air pressure reinforce each other to produce a different sound wave. Any repeating sound wave that is not a sine wave can be modeled by many different sine waves of the appropriate frequencies and amplitudes (a frequency spectrum). In humans the hearing apparatus (composed of the ears and brain) can usually isolate these tones and hear them distinctly. When two or more tones are played at once, a variation of air pressure at the ear ""contains"" the pitches of each, and the ear and/or brain isolate and decode them into distinct tones.
When the original sound sources are perfectly periodic, the note consists of several related sine waves (which mathematically add to each other) called the fundamental and the harmonics, partials, or overtones. The sounds have harmonic frequency spectra. The lowest frequency present is the fundamental, and is the frequency at which the entire wave vibrates. The overtones vibrate faster than the fundamental, but must vibrate at integer multiples of the fundamental frequency for the total wave to be exactly the same each cycle. Real instruments are close to periodic, but the frequencies of the overtones are slightly imperfect, so the shape of the wave changes slightly over time.

Subjective aspects
Variations in air pressure against the ear drum, and the subsequent physical and neurological processing and interpretation, give rise to the subjective experience called sound. Most sound that people recognize as musical is dominated by periodic or regular vibrations rather than non-periodic ones; that is, musical sounds typically have a definite pitch). The transmission of these variations through air is via a sound wave. In a very simple case, the sound of a sine wave, which is considered the most basic model of a sound waveform, causes the air pressure to increase and decrease in a regular fashion, and is heard as a very pure tone. Pure tones can be produced by tuning forks or whistling. The rate at which the air pressure oscillates is the frequency of the tone, which is measured in oscillations per second, called hertz. Frequency is the primary determinant of the perceived pitch. Frequency of musical instruments can change with altitude due to changes in air pressure.

Pitch ranges of musical instruments
*This chart only displays down to C0, though some pipe organs, such as the Boardwalk Hall Auditorium Organ, extend down to C?1 (one octave below C0). Also, the fundamental frequency of the subcontrabass tuba is B??1.

Harmonics, partials, and overtones
The fundamental is the frequency at which the entire wave vibrates. Overtones are other sinusoidal components present at frequencies above the fundamental. All of the frequency components that make up the total waveform, including the fundamental and the overtones, are called partials. Together they form the harmonic series.
Overtones that are perfect integer multiples of the fundamental are called harmonics. When an overtone is near to being harmonic, but not exact, it is sometimes called a harmonic partial, although they are often referred to simply as harmonics. Sometimes overtones are created that are not anywhere near a harmonic, and are just called partials or inharmonic overtones.
The fundamental frequency is considered the first harmonic and the first partial. The numbering of the partials and harmonics is then usually the same; the second partial is the second harmonic, etc. But if there are inharmonic partials, the numbering no longer coincides. Overtones are numbered as they appear above the fundamental. So strictly speaking, the first overtone is the second partial (and usually the second harmonic). As this can result in confusion, only harmonics are usually referred to by their numbers, and overtones and partials are described by their relationships to those harmonics.

Harmonics and non-linearities
When a periodic wave is composed of a fundamental and only odd harmonics (f, 3f, 5f, 7f, ...), the summed wave is half-wave symmetric; it can be inverted and phase shifted and be exactly the same. If the wave has any even harmonics (0f, 2f, 4f, 6f, ...), it is asymmetrical; the top half is not a mirror image of the bottom.
Conversely, a system that changes the shape of the wave (beyond simple scaling or shifting) creates additional harmonics (harmonic distortion). This is called a non-linear system. If it affects the wave symmetrically, the harmonics produced are all odd. If it affects the harmonics asymmetrically, at least one even harmonic is produced (and probably also odd harmonics).

Harmony
If two notes are simultaneously played, with frequency ratios that are simple fractions (e.g. 2/1, 3/2 or 5/4), the composite wave is still periodic, with a short period—and the combination sounds consonant. For instance, a note vibrating at 200 Hz and a note vibrating at 300 Hz (a perfect fifth, or 3/2 ratio, above 200 Hz) add together to make a wave that repeats at 100 Hz: every 1/100 of a second, the 300 Hz wave repeats three times and the 200 Hz wave repeats twice. Note that the total wave repeats at 100 Hz, but there is no actual 100 Hz sinusoidal component.
Additionally, the two notes have many of the same partials. For instance, a note with a fundamental frequency of 200 Hz has harmonics at: :(200,) 400, 600, 800, 1000, 1200, …
A note with fundamental frequency of 300 Hz has harmonics at: :(300,) 600, 900, 1200, 1500, … The two notes share harmonics at 600 and 1200 Hz, and more coincide further up the series.
The combination of composite waves with short fundamental frequencies and shared or closely related partials is what causes the sensation of harmony. When two frequencies are near to a simple fraction, but not exact, the composite wave cycles slowly enough to hear the cancellation of the waves as a steady pulsing instead of a tone. This is called beating, and is considered unpleasant, or dissonant.
The frequency of beating is calculated as the difference between the frequencies of the two notes. For the example above, |200 Hz - 300 Hz| = 100 Hz. As another example, a combination of 3425 Hz and 3426 Hz would beat once per second (|3425 Hz - 3426 Hz| = 1 Hz). This follows from modulation theory.
The difference between consonance and dissonance is not clearly defined, but the higher the beat frequency, the more likely the interval is dissonant. Helmholtz proposed that maximum dissonance would arise between two pure tones when the beat rate is roughly 35 Hz. [1]

Scales
The material of a musical composition is usually taken from a collection of pitches known as a scale. Because most people cannot adequately determine absolute frequencies, the identity of a scale lies in the ratios of frequencies between its tones (known as intervals).

The diatonic scale appears in writing throughout history, consisting of seven tones in each octave. In just intonation the diatonic scale may be easily constructed using the three simplest intervals within the octave, the perfect fifth (3/2), perfect fourth (4/3), and the major third (5/4). As forms of the fifth and third are naturally present in the overtone series of harmonic resonators, this is a very simple process.
The following table shows the ratios between the frequencies of all the notes of the just major scale and the fixed frequency of the first note of the scale.
There are other scales available through just intonation, for example the minor scale. Scales that do not adhere to just intonation, and instead have their intervals adjusted to meet other needs are called temperaments, of which equal temperament is the most used. Temperaments, though they obscure the acoustical purity of just intervals, often have desirable properties, such as a closed circle of fifths.

See also
Acoustic resonance
Cymatics
Mathematics of musical scales
String resonance
Vibrating string
3rd bridge (harmonic resonance based on equal string divisions)

External links
Music acoustics - sound files, animations and illustrations - University of New South Wales
Acoustics collection - descriptions, photos, and video clips of the apparatus for research in musical acoustics by Prof. Dayton Miller
The Technical Committee on Musical Acoustics (TCMU) of the Acoustical Society of America (ASA)
The Musical Acoustics Research Library (MARL)
Acoustics Group/Acoustics and Music Technology courses - University of Edinburgh
Acoustics Research Group - Open University
The music acoustics group at Speech, Music and Hearing KTH
The physics of harpsichord sound
Visual music
Savart Journal - The open access online journal of science and technology of stringed musical instruments
Audio Engineering online course under Creative Commons Licence
Interference and Consonance from Physclips
Curso de Acústica Musical (Spanish)",Category:Acoustics,1
265,266,Acoustoelastic effect,"The acoustoelastic effect describes how the sound velocities (both longitudinal and shear wave velocities) of an elastic material change if subjected to an initial static stress field. This is a non-linear effect of the constitutive relation between mechanical stress and finite strain in a material of continuous mass. In classical linear elasticity theory small deformations of most elastic materials can be described by a linear relation between the applied stress and the resulting strain. This relationship is commonly known as the generalised Hooke's law. The linear elastic theory involves second order elastic constants (e.g. 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   and 
  
    
      
        ?
      
    
    {\displaystyle \mu }
   ) and yields constant longitudinal and shear sound velocities in an elastic material, not affected by an applied stress. The acoustoelastic effect on the other hand include higher order expansion of the constitutive relation (non-linear elasticity theory) between the applied stress and resulting strain, which yields longitudinal and shear sound velocities dependent of the stress state of the material. In the limit of an unstressed material the sound velocities of the linear elastic theory are reproduced.
The acoustoelastic effect was investigated as early as 1925 by Brillouin. He found that the propagation velocity of acoustic waves would decrease proportional to an applied hydrostatic pressure. However, a consequence of his theory was that sound waves would stop propagating at a sufficiently large pressure. This paradoxial effect was later shown to be caused by the incorrect assumptions that the elastic parameters were not affected by the pressure. In 1937 Murnaghan  presented a mathematical theory extending the linear elastic theory to also include finite deformation in elastic isotropic materials. This theory included three third-order elastic constants 
  
    
      
        l
      
    
    {\displaystyle l}
  , 
  
    
      
        m
      
    
    {\displaystyle m}
  , and 
  
    
      
        n
      
    
    {\displaystyle n}
  . In 1953 Huges and Kelly  used the theory of Murnaghan in their experimental work to establish numerical values for higher order elastic constants for several elastic materials including Polystyrene, Armco iron, and Pyrex, subjected to hydrostatic pressure and uniaxial compression.

Non-linear elastic theory for hyperelastic materials
The acoustoelastic effect is an effect of finite deformation of non-linear elastic materials. A modern comprehensive account of this can be found in. This book treats the application of the non-linear elasticity theory and the analysis of the mechanical properties of solid materials capable of large elastic deformations. The special case of the acoustoelastic theory for a compressible isotropic hyperelastic material, like polycrystalline steel, is reproduced and shown in this text from the non-linear elasticity theory as presented by Ogden.
Note that the setting in this text as well as in  is isothermal, and no reference is made to thermodynamics.

Constitutive relation – hyperelastic materials (Stress-strain relation)
A hyperelastic material is a special case of a Cauchy elastic material in which the stress at any point is objective and determined only by the current state of deformation with respect to an arbitrary reference configuration (for more details on deformation see also the pages Deformation (mechanics) and Finite strain). However, the work done by the stresses may depend on the path the deformation takes. Therefore, a Cauchy elastic material has a non-conservative structure, and the stress cannot be derived from a scalar elastic potential function. The special case of Cauchy elastic materials where the work done by the stresses is independent of the path of deformation is referred to as a Green elastic or hyperelastic material. Such materials are conservative and the stresses in the material can be derived by a scalar elastic potential, more commonly known as the Strain energy density function.
The constitutive relation between the stress and strain can be expressed in different forms based on the chosen stress and strain forms. Selecting the 1st Piola-Kirchhoff stress tensor 
  
    
      
        
          P
        
      
    
    {\displaystyle {\boldsymbol {P}}}
   (which is the transpose of the nominal stress tensor 
  
    
      
        
          
            P
          
          
            T
          
        
        =
        
          N
        
      
    
    {\displaystyle {\boldsymbol {P}}^{T}={\boldsymbol {N}}}
  ), the constitutive equation for a compressible hyper elastic material can be expressed in terms of the Lagrangian Green strain (
  
    
      
        
          E
        
      
    
    {\displaystyle {\boldsymbol {E}}}
  ) as:

  
    
      
        
          P
        
        =
        
          F
        
        ?
        
          
            
              ?
              W
            
            
              ?
              
                E
              
            
          
        
        
        
          or
        
        
        
          P
          
            i
            j
          
        
        =
        
          F
          
            i
            k
          
        
         
        
          
            
              ?
              W
            
            
              ?
              
                E
                
                  k
                  j
                
              
            
          
        
        ,
        
        i
        ,
        j
        =
        1
        ,
        2
        ,
        3
         
        ,
      
    
    {\displaystyle {\boldsymbol {P}}={\boldsymbol {F}}\cdot {\frac {\partial W}{\partial {\boldsymbol {E}}}}\qquad {\text{or}}\qquad P_{ij}=F_{ik}~{\frac {\partial W}{\partial E_{kj}}},\qquad i,j=1,2,3~,}
  
where 
  
    
      
        
          F
        
      
    
    {\displaystyle {\boldsymbol {F}}}
   is the deformation gradient tensor, and where the second expression utilises the Einstein summation convention for index notation of tensors. 
  
    
      
        W
      
    
    {\displaystyle W}
   is the strain energy density function for a hyperelastic material and have been defined per unit volume rather than per unit mass since this avoids the need of multiplying the right hand side with the mass density 
  
    
      
        
          ?
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
   of the reference configuration.
Assuming that the scalar strain energy density function 
  
    
      
        W
        (
        
          E
        
        )
      
    
    {\displaystyle W({\boldsymbol {E}})}
   can be approximated by a Taylor series expansion in the current strain 
  
    
      
        
          E
        
      
    
    {\displaystyle {\boldsymbol {E}}}
  , it can be expressed (in index notation) as:

  
    
      
        W
        ?
        
          C
          
            0
          
        
        +
        
          C
          
            i
            j
          
        
        
          E
          
            i
            j
          
        
        +
        
          
            1
            
              2
              !
            
          
        
        
          C
          
            i
            j
            k
            l
          
        
        
          E
          
            i
            j
          
        
        
          E
          
            k
            l
          
        
        +
        
          
            1
            
              3
              !
            
          
        
        
          C
          
            i
            j
            k
            l
            m
            n
          
        
        
          E
          
            i
            j
          
        
        
          E
          
            k
            l
          
        
        
          E
          
            m
            n
          
        
        +
        ?
      
    
    {\displaystyle W\approx C_{0}+C_{ij}E_{ij}+{\frac {1}{2!}}C_{ijkl}E_{ij}E_{kl}+{\frac {1}{3!}}C_{ijklmn}E_{ij}E_{kl}E_{mn}+\cdots }
  
Imposing the restrictions that the strain energy function should be zero and have a minimum when the material is in the un-deformed state (i.e. 
  
    
      
        W
        (
        
          E
          
            i
            j
          
        
        =
        0
        )
        =
        0
      
    
    {\displaystyle W(E_{ij}=0)=0}
  ) it is clear that there are no constant or linear term in the strain energy function, and thus:

  
    
      
        W
        ?
        
          
            1
            
              2
              !
            
          
        
        
          C
          
            i
            j
            k
            l
          
        
        
          E
          
            i
            j
          
        
        
          E
          
            k
            l
          
        
        +
        
          
            1
            
              3
              !
            
          
        
        
          C
          
            i
            j
            k
            l
            m
            n
          
        
        
          E
          
            i
            j
          
        
        
          E
          
            k
            l
          
        
        
          E
          
            m
            n
          
        
        +
        ?
        ,
      
    
    {\displaystyle W\approx {\frac {1}{2!}}C_{ijkl}E_{ij}E_{kl}+{\frac {1}{3!}}C_{ijklmn}E_{ij}E_{kl}E_{mn}+\cdots ,}
  
where 
  
    
      
        
          C
          
            i
            j
            k
            l
          
        
      
    
    {\displaystyle C_{ijkl}}
   is a fourth-order tensor of second-order elastic moduli, while 
  
    
      
        
          C
          
            i
            j
            k
            l
            m
            n
          
        
      
    
    {\displaystyle C_{ijklmn}}
   is a sixth-order tensor of third-order elastic moduli. The symmetry of 
  
    
      
        
          E
          
            i
            j
          
        
        =
        
          E
          
            j
            i
          
        
      
    
    {\displaystyle E_{ij}=E_{ji}}
   together with the scalar strain energy density function 
  
    
      
        W
      
    
    {\displaystyle W}
   implies that the second order moduli 
  
    
      
        
          C
          
            i
            j
            k
            l
          
        
      
    
    {\displaystyle C_{ijkl}}
   have the following symmetry:

  
    
      
        
          C
          
            i
            j
            k
            l
          
        
        =
        
          C
          
            j
            i
            k
            l
          
        
        =
        
          C
          
            i
            j
            l
            k
          
        
        ,
      
    
    {\displaystyle C_{ijkl}=C_{jikl}=C_{ijlk},}
  
which reduce the number of independent elastic constants from 81 to 36. In addition the power expansion implies that the second order moduli also have the major symmetry

  
    
      
        
          C
          
            i
            j
            k
            l
          
        
        =
        
          C
          
            k
            l
            i
            j
          
        
        ,
      
    
    {\displaystyle C_{ijkl}=C_{klij},}
  
which further reduce the number of independent elastic constants to 21. The same arguments can be used for the third order elastic moduli 
  
    
      
        
          C
          
            i
            j
            k
            l
            m
            n
          
        
      
    
    {\displaystyle C_{ijklmn}}
  . These symmetries also allows the elastic moduli to be expressed by the Voigt notation (i.e. 
  
    
      
        
          C
          
            i
            j
            k
            l
          
        
        =
        
          C
          
            I
            J
          
        
      
    
    {\displaystyle C_{ijkl}=C_{IJ}}
   and 
  
    
      
        
          C
          
            i
            j
            k
            l
            m
            n
          
        
        =
        
          C
          
            I
            J
            K
          
        
      
    
    {\displaystyle C_{ijklmn}=C_{IJK}}
  ).
The deformation gradient tensor can be expressed in component form as

  
    
      
        
          F
          
            i
            j
          
        
        =
        
          
            
              ?
              
                u
                
                  i
                
              
            
            
              ?
              
                X
                
                  j
                
              
            
          
        
        +
        
          ?
          
            i
            j
          
        
        ,
      
    
    {\displaystyle F_{ij}={\frac {\partial u_{i}}{\partial X_{j}}}+\delta _{ij},}
  
where 
  
    
      
        
          u
          
            i
          
        
      
    
    {\displaystyle u_{i}}
   is the displacement of a material point 
  
    
      
        P
      
    
    {\displaystyle P}
   from coordinate 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
   in the reference configuration to coordinate 
  
    
      
        
          x
          
            i
          
        
      
    
    {\displaystyle x_{i}}
   in the deformed configuration (see Figure 2 in the finite strain theory page). Including the power expansion of strain energy function in the constitutive relation and replacing the Lagrangian strain tensor 
  
    
      
        
          E
          
            k
            l
          
        
      
    
    {\displaystyle E_{kl}}
   with the expansion given on the finite strain tensor page yields (note that lower case 
  
    
      
        u
      
    
    {\displaystyle u}
   have been used in this section compared to the upper case on the finite strain page) the constitutive equation

  
    
      
        
          P
          
            i
            j
          
        
        =
        
          C
          
            i
            j
            k
            l
          
        
        
          
            
              ?
              
                u
                
                  k
                
              
            
            
              ?
              
                X
                
                  l
                
              
            
          
        
        +
        
          
            1
            2
          
        
        
          M
          
            i
            j
            k
            l
            m
            n
          
        
        
          
            
              ?
              
                u
                
                  k
                
              
            
            
              ?
              
                X
                
                  l
                
              
            
          
        
        
          
            
              ?
              
                u
                
                  m
                
              
            
            
              ?
              
                X
                
                  n
                
              
            
          
        
        +
        
          
            1
            3
          
        
        
          M
          
            i
            j
            k
            l
            m
            n
            p
            q
          
        
        
          
            
              ?
              
                u
                
                  k
                
              
            
            
              ?
              
                X
                
                  l
                
              
            
          
        
        
          
            
              ?
              
                u
                
                  m
                
              
            
            
              ?
              
                X
                
                  n
                
              
            
          
        
        
          
            
              ?
              
                u
                
                  p
                
              
            
            
              ?
              
                X
                
                  q
                
              
            
          
        
        +
        ?
        ,
      
    
    {\displaystyle P_{ij}=C_{ijkl}{\frac {\partial u_{k}}{\partial X_{l}}}+{\frac {1}{2}}M_{ijklmn}{\frac {\partial u_{k}}{\partial X_{l}}}{\frac {\partial u_{m}}{\partial X_{n}}}+{\frac {1}{3}}M_{ijklmnpq}{\frac {\partial u_{k}}{\partial X_{l}}}{\frac {\partial u_{m}}{\partial X_{n}}}{\frac {\partial u_{p}}{\partial X_{q}}}+\cdots ,}
  
where

  
    
      
        
          M
          
            i
            j
            k
            l
            m
            n
          
        
        =
        
          C
          
            i
            j
            k
            l
            m
            n
          
        
        +
        
          C
          
            i
            j
            l
            n
          
        
        
          ?
          
            k
            m
          
        
        +
        
          C
          
            j
            n
            k
            l
          
        
        
          ?
          
            i
            m
          
        
        +
        
          C
          
            j
            l
            m
            n
          
        
        
          ?
          
            i
            k
          
        
        ,
      
    
    {\displaystyle M_{ijklmn}=C_{ijklmn}+C_{ijln}\delta _{km}+C_{jnkl}\delta _{im}+C_{jlmn}\delta _{ik},}
  
and higher order terms have been neglected   (see  for detailed derivations). For referenceM by neglecting higher order terms in 
  
    
      
        ?
        
          u
          
            k
          
        
        
          /
        
        ?
        
          X
          
            l
          
        
      
    
    {\displaystyle \partial u_{k}/\partial X_{l}}
   this expression reduce to 
  
    
      
        
          P
          
            i
            j
          
        
        =
        
          C
          
            i
            j
            k
            l
          
        
        
          
            
              ?
              
                u
                
                  k
                
              
            
            
              ?
              
                X
                
                  l
                
              
            
          
        
        ,
      
    
    {\displaystyle P_{ij}=C_{ijkl}{\frac {\partial u_{k}}{\partial X_{l}}},}
   which is a version of the generalised Hooke's law where 
  
    
      
        
          P
          
            i
            j
          
        
      
    
    {\displaystyle P_{ij}}
   is a measure of stress while 
  
    
      
        ?
        
          u
          
            k
          
        
        
          /
        
        ?
        
          X
          
            l
          
        
      
    
    {\displaystyle \partial u_{k}/\partial X_{l}}
   is a measure of strain, and 
  
    
      
        
          C
          
            i
            j
            k
            l
          
        
      
    
    {\displaystyle C_{ijkl}}
   is the linear relation between them.

Sound velocity
Assuming that a small dynamic (acoustic) deformation disturb an already statically stressed material the acoustoelastic effect can be regarded as the effect on a small deformation superposed on a larger finite deformation (also called the small-on-large theory). Let us define three states of a given material point. In the reference (un-stressed) state the point is defined by the coordinate vector 
  
    
      
        
          X
        
      
    
    {\displaystyle {\boldsymbol {X}}}
   while the same point has the coordinate vector 
  
    
      
        
          x
        
      
    
    {\displaystyle {\boldsymbol {x}}}
   in the static initially stressed state (i.e. under the influence of an applied pre-stress). Finally, assume that the material point under a small dynamic disturbance (acoustic stress field) have the coordinate vector 
  
    
      
        
          
            x
            ?
          
        
      
    
    {\displaystyle {\boldsymbol {x'}}}
  . The total displacement of the material points (under influence of both a static pre-stress and an dynamic acoustic disturbance) can then be described by the displacement vectors

  
    
      
        
          u
        
        =
        
          
            u
          
          
            (
            0
            )
          
        
        +
        
          
            u
          
          
            (
            1
            )
          
        
        =
        
          
            x
            ?
          
        
        ?
        
          X
        
        ,
      
    
    {\displaystyle {\boldsymbol {u}}={\boldsymbol {u}}^{(0)}+{\boldsymbol {u}}^{(1)}={\boldsymbol {x'}}-{\boldsymbol {X}},}
  
where

  
    
      
        
          
            u
          
          
            (
            0
            )
          
        
        =
        
          x
        
        ?
        
          X
        
        ,
        
        
          
            u
          
          
            (
            1
            )
          
        
        =
        
          
            x
            ?
          
        
        ?
        
          x
        
      
    
    {\displaystyle {\boldsymbol {u}}^{(0)}={\boldsymbol {x}}-{\boldsymbol {X}},\qquad {\boldsymbol {u}}^{(1)}={\boldsymbol {x'}}-{\boldsymbol {x}}}
  
describes the static (Lagrangian) initial displacement due to the applied pre-stress, and the (Eulerian) displacement due to the acoustic disturbance, respectively. Cauchy's first law of motion (or balance of linear momentum) for the additional Eulerian disturbance 
  
    
      
        
          
            u
          
          
            (
            1
            )
          
        
      
    
    {\displaystyle {\boldsymbol {u}}^{(1)}}
   can then be derived in terms of the intermediate Lagrangian deformation 
  
    
      
        
          
            u
          
          
            (
            0
            )
          
        
      
    
    {\displaystyle {\boldsymbol {u}}^{(0)}}
   assuming that the small-on-large assumption

  
    
      
        
          |
        
        
          
            u
          
          
            (
            1
            )
          
        
        
          |
        
        <<
        
          |
        
        
          
            u
          
          
            (
            0
            )
          
        
        
          |
        
        
          |
        
      
    
    {\displaystyle |{\boldsymbol {u}}^{(1)}|<<|{\boldsymbol {u}}^{(0)}||}
  
holds. Using the Lagrangian form of Cauchy's first law of motion, where the effect of a constant body force (i.e. gravity) has been neglected, yields

  
    
      
        Div
        ?
        
          
            P
          
          
            T
          
        
        =
        
          ?
          
            0
          
        
        
          
            
              
                
                  x
                
                ?
              
              ¨
            
          
        
        .
      
    
    {\displaystyle \operatorname {Div} {\boldsymbol {P}}^{T}=\rho _{0}{\ddot {{\boldsymbol {x}}'}}.}
  
Note that the subscript/superscript ""0"" is used in this text to denote the un-stressed reference state, and a dotted variable is as usual the time (
  
    
      
        t
      
    
    {\displaystyle t}
  ) derivative of the variable, and 
  
    
      
        Div
      
    
    {\displaystyle \operatorname {Div} }
   is the divergence operator with respect to the Lagrangian coordinate system 
  
    
      
        
          X
        
      
    
    {\displaystyle {\boldsymbol {X}}}
  .
The right hand side (the time dependent part) of the law of motion can be expressed as

  
    
      
        
          
            
              
                
                  ?
                  
                    0
                  
                
                
                  
                    
                      
                        
                          x
                        
                        ?
                      
                      ¨
                    
                  
                
              
              
                
                =
                
                  
                    
                      ?
                      
                        2
                      
                    
                    
                      ?
                      
                        t
                        
                          2
                        
                      
                    
                  
                
                (
                
                  
                    u
                  
                  
                    (
                    0
                    )
                  
                
                +
                
                  
                    u
                  
                  
                    (
                    1
                    )
                  
                
                +
                
                  X
                
                )
              
            
            
              
              
                
                =
                
                  
                    
                      
                        ?
                        
                          2
                        
                      
                      
                        
                          u
                        
                        
                          (
                          1
                          )
                        
                      
                    
                    
                      ?
                      
                        t
                        
                          2
                        
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\rho _{0}{\ddot {{\boldsymbol {x}}'}}&={\frac {\partial ^{2}}{\partial t^{2}}}({\boldsymbol {u}}^{(0)}+{\boldsymbol {u}}^{(1)}+{\boldsymbol {X}})\\&={\frac {\partial ^{2}{\boldsymbol {u}}^{(1)}}{\partial t^{2}}}\end{aligned}}}
  
under the assumption that both the unstressed state and the initial deformation state are static and thus 
  
    
      
        
          ?
          
            2
          
        
        
          /
        
        ?
        
          t
          
            2
          
        
        (
        
          
            u
          
          
            (
            0
            )
          
        
        )
        =
        
          ?
          
            2
          
        
        
          /
        
        ?
        
          t
          
            2
          
        
        (
        
          X
        
        )
        =
        0
      
    
    {\displaystyle \partial ^{2}/\partial t^{2}({\boldsymbol {u}}^{(0)})=\partial ^{2}/\partial t^{2}({\boldsymbol {X}})=0}
  .
For the left hand side (the space dependent part) the spatial Lagrangian partial derivatives with respect to 
  
    
      
        
          X
          
            j
          
        
      
    
    {\displaystyle X_{j}}
   can be expanded in the Eulerian 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
   by using the chain rule and changing the variables through the relation between the displacement vectors as 

  
    
      
        
          
            ?
            
              ?
              
                X
                
                  j
                
              
            
          
        
        =
        
          
            ?
            
              ?
              
                x
                
                  j
                
              
            
          
        
        +
        
          u
          
            k
            ,
            j
          
          
            (
            0
            )
          
        
        
          
            ?
            
              ?
              
                x
                
                  k
                
              
            
          
        
        +
        ?
      
    
    {\displaystyle {\frac {\partial }{\partial X_{j}}}={\frac {\partial }{\partial x_{j}}}+u_{k,j}^{(0)}{\frac {\partial }{\partial x_{k}}}+\cdots }
  
where the short form 
  
    
      
        
          u
          
            k
            ,
            j
          
          
            (
            0
            )
          
        
        ?
        ?
        
          u
          
            k
          
          
            (
            0
            )
          
        
        
          /
        
        ?
        
          x
          
            j
          
        
      
    
    {\displaystyle u_{k,j}^{(0)}\equiv \partial u_{k}^{(0)}/\partial x_{j}}
   has been used. Thus

  
    
      
        
          
            
              ?
              
                P
                
                  i
                  j
                
              
            
            
              ?
              
                X
                
                  j
                
              
            
          
        
        ?
        
          
            
              ?
              
                P
                
                  i
                  j
                
              
            
            
              ?
              
                x
                
                  j
                
              
            
          
        
        +
        
          u
          
            p
            .
            j
          
          
            (
            0
            )
          
        
        
          
            
              ?
              
                P
                
                  i
                  j
                
              
            
            
              ?
              
                x
                
                  p
                
              
            
          
        
      
    
    {\displaystyle {\frac {\partial P_{ij}}{\partial X_{j}}}\approx {\frac {\partial P_{ij}}{\partial x_{j}}}+u_{p.j}^{(0)}{\frac {\partial P_{ij}}{\partial x_{p}}}}
  
Assuming further that the static initial deformation 
  
    
      
        
          
            u
          
          
            (
            0
            )
          
        
      
    
    {\displaystyle {\boldsymbol {u}}^{(0)}}
   (the ",Category:Acoustics,1
266,267,Acoustic suspension,"Acoustic suspension (air suspension or sealed box) is a type of loudspeaker speaker enclosure design which uses one or more loudspeaker drivers mounted in a sealed box or cabinet. Acoustic suspension systems reduce bass distortion that can be caused by stiff motor suspensions in conventional loudspeakers. It was invented in 1954 by Edgar Villchur, and brought to commercial production by Villchur and Henry Kloss with the founding of Acoustic Research in Cambridge, Mass. Speaker cabinets with acoustic suspension can provide tight and accurate bass response, especially in comparison with an equivalently-sized speaker enclosure which has a bass reflex port or vent; the bass vent boosts low-end output, but at the tradeoff of introducing phase delay and accuracy problems. Sealed boxes are generally less efficient than a ported cabinet, so a sealed box speaker cabinet will need more power amplifier wattage to deliver the same amount of bass output.

In multi-way speakers
While boxed hi-fi speakers are often described as being acoustic suspension, or ported (bass reflex) depending on the absence or presence of a port tube/vent, it is also true that in typical box speakers with more than two drivers, the drivers between the woofer and tweeter are usually designed as acoustic suspension, with a separate, sealed air-space, even if the woofer itself is not.
One notable exception to this was the Sonus Faber Stradivari Homage which uses a ported enclosure for the midrange.

Acoustic performance
The two most common types of speaker enclosure are acoustic suspension and bass reflex, so they are worth comparing. In both cases, the tuning affects the lower end of the driver's response but above a certain frequency, the driver itself becomes the dominant factor and the size of the enclosure and ports (if any) become irrelevant.
In general, acoustic suspension systems (driver + enclosure) have a second order acoustic (12 dB/octave) roll-off below the -3 dB point. Bass reflex designs have a fourth order acoustic roll-off (24 dB/octave). Given a driver which is suitable for either type of enclosure the ideal bass reflex cabinet will be larger, have a lower -3 dB point, but equal voltage sensitivity in the pass band.

This is a simulation of a typical 5"" mid-woofer, the FaitalPRO 5FE120 mid-woofer generated by WinISD. in ideal sealed (yellow) and ported (cyan) configurations. The ported version adds an octave of bass extension dropping the -3 dB point from 100 Hz to 50 Hz but the tradeoff is the cabinet size is more than twice as large, 8 litres of interior space vs. 3.8 litres. It is also worth noting that above 200 Hz the simulations converge and there is no difference. Thus a ported cabinet does not provide improved bass output over the entire sub-bass range. The idea that a ported enclosure would be more efficient (improved voltage sensitivity) is a popular myth.
The acoustic roll-off of acoustic suspension designs makes them easier to integrate with other drivers with a crossover (passive or active). This makes it an ideal choice for midrange enclosures as well as for satellite speaker and subwoofer systems.

Theory
The acoustic-suspension woofer uses the elastic cushion of air within a sealed enclosure to provide the restoring force for the woofer diaphragm. The cushion of air acts like a spring or rubber band. Because the air in the cabinet serves to control the woofer's excursion, the physical stiffness of the driver can be reduced.
Unlike the stiff physical suspension built into the driver of conventional speakers, the trapped air inside the sealed-loudspeaker enclosure provides a more linear restoring force for the woofer's diaphragm, enabling it to oscillate a greater distance (excursion) in a linear fashion. This is a requirement for clean and loud reproduction of deep bass by drivers with relatively small cones.
Even though acoustic suspension cabinets are often called ""sealed box"" designs, they are not entirely airtight. A small amount of airflow must be allowed so the speaker can adjust to changes in atmospheric pressure. A semi-porous cone surround allows enough airflow for this purpose. Most Acoustic Research designs used a PVA sealer on the foam surrounds to enable a longer component life and enhance performance. The venting was via the cloth spider and cloth dust caps, not so much through the surround.
Acoustic suspension woofers remain popular in hi-fi systems due to their low distortion and small cabinet volume required. They also have low group delay in the bottom end compared to bass reflex designs but the audibility of this benefit is hotly contested.
In the 2010s, most subwoofers, bass amplifier cabinets and sound reinforcement system speaker cabinets use bass reflex ports, rather than a sealed box design, in order to obtain more extended low-frequency response and get more SPL. The speaker enclosure designers and their customers view the risk of increased distortion and phase delay as an acceptable price to pay for increased bass output and higher SPL. That said, if one is designing a sound system for a style of music where very accurate, precise bass rhythms are an important part of the genre, one may wish to consider the merits of sealed box woofers and subwoofers.


== References ==",Category:Acoustics,1
267,268,Speed of sound,"The speed of sound is the distance travelled per unit time by a sound wave as it propagates through an elastic medium. In dry air at 0 °C (32 °F), the speed of sound is 331.2 metres per second (1,087 ft/s; 1,192 km/h; 741 mph; 644 kn). At 20 °C (68 °F), the speed of sound is 343 metres per second (1,125 ft/s; 1,235 km/h; 767 mph; 667 kn), or a kilometre in 2.91 s or a mile in 4.69 s.
The speed of sound in an ideal gas depends only on its temperature and composition. The speed has a weak dependence on frequency and pressure in ordinary air, deviating slightly from ideal behavior.
In common everyday speech, speed of sound refers to the speed of sound waves in air. However, the speed of sound varies from substance to substance: sound travels most slowly in gases; it travels faster in liquids; and faster still in solids. For example, (as noted above), sound travels at 343 m/s in air; it travels at 1,484 m/s in water (4.3 times as fast as in air); and at 5,120 m/s in iron (about 15 times as fast as in air). In an exceptionally stiff material such as diamond, sound travels at 12,000 metres per second (26,843 mph); (about 35 times as fast as in air) which is around the maximum speed that sound will travel under normal conditions.
Sound waves in solids are composed of compression waves (just as in gases and liquids), and a different type of sound wave called a shear wave, which occurs only in solids. Shear waves in solids usually travel at different speeds, as exhibited in seismology. The speed of compression waves in solids is determined by the medium's compressibility, shear modulus and density. The speed of shear waves is determined only by the solid material's shear modulus and density.
In fluid dynamics, the speed of sound in a fluid medium (gas or liquid) is used as a relative measure for the speed of an object moving through the medium. The ratio of the speed of an object to the speed of sound in the fluid is called the object's Mach number. Objects moving at speeds greater than Mach1 are said to be traveling at supersonic speeds.

History
Sir Isaac Newton computed the speed of sound in air as 979 feet per second (298 m/s), which is too low by about 15%,. Newton's analysis was good save for neglecting the (then unknown) effect of rapidly-fluctuating temperature in a sound wave (in modern terms, sound wave compression and expansion of air is an adiabatic process, not an isothermal process). This error was later rectified by Laplace.
During the 17th century, there were several attempts to measure the speed of sound accurately, including attempts by Marin Mersenne in 1630 (1,380 Parisian feet per second), Pierre Gassendi in 1635 (1,473 Parisian feet per second) and Robert Boyle (1,125 Parisian feet per second).
In 1709, the Reverend William Derham, Rector of Upminster, published a more accurate measure of the speed of sound, at 1,072 Parisian feet per second. Derham used a telescope from the tower of the church of St Laurence, Upminster to observe the flash of a distant shotgun being fired, and then measured the time until he heard the gunshot with a half second pendulum. Measurements were made of gunshots from a number of local landmarks, including North Ockendon church. The distance was known by triangulation, and thus the speed that the sound had travelled was calculated.

Basic concepts
The transmission of sound can be illustrated by using a model consisting of an array of spherical objects interconnected by springs.
In real material terms, the spheres represent the material's molecules and the springs represent the bonds between them. Sound passes through the system by compressing and expanding the springs, transmitting the acoustic energy to neighboring spheres. This helps transmit the energy in-turn to the neighboring sphere's springs (bonds), and so on.
The speed of sound through the model depends on the stiffness/rigidity of the springs, and the mass of the spheres. As long as the spacing of the spheres remains constant, stiffer springs/bonds transmit energy quicker, while larger spheres transmit the energy slower. Effects like dispersion and reflection can also be understood using this model.
In a real material, the stiffness of the springs is known as the ""elastic modulus"", and the mass corresponds to the material density. Given that all other things being equal (ceteris paribus), sound will travel slower in spongy materials, and faster in stiffer ones. For instance, sound will travel 1.59 times faster in nickel than in bronze, due to the greater stiffness of nickel at about the same density. Similarly, sound travels about 1.41 times faster in light hydrogen (protium) gas than in heavy hydrogen (deuterium) gas, since deuterium has similar properties but twice the density. At the same time, ""compression-type"" sound will travel faster in solids than in liquids, and faster in liquids than in gases, because the solids are more difficult to compress than liquids, while liquids in turn are more difficult to compress than gases.
Some textbooks mistakenly state that the speed of sound increases with increasing density. This is usually illustrated by presenting data for three materials, such as air, water and steel, which also have vastly different levels compressibilities which more than make up for the density differences. An illustrative example of the two effects is that sound travels only 4.3 times faster in water than air, despite enormous differences in compressibility of the two media. The reason is that the larger density of water, which works to slow sound in water relative to air, nearly makes up for the compressibility differences in the two media.
A practical example can be observed in Edinburgh, when the ""One o' Clock Gun"" is fired at the eastern end of Edinburgh Castle. Standing at the base of the western end of the Castle Rock, the sound of the Gun can be heard through the rock, slightly before it arrives by the air route, partly delayed by the slightly longer route. It is particularly effective if a multi-gun salute such as for ""The Queen's Birthday"" is being fired.

Compression and shear waves
In a gas or liquid, sound consists of compression waves. In solids, waves propagate as two different types. A longitudinal wave is associated with compression and decompression in the direction of travel, and is the same process in gases and liquids, with an analogous compression-type wave in solids. Only compression waves are supported in gases and liquids. An additional type of wave, the transverse wave, also called a shear wave, occurs only in solids because only solids support elastic deformations. It is due to elastic deformation of the medium perpendicular to the direction of wave travel; the direction of shear-deformation is called the ""polarization"" of this type of wave. In general, transverse waves occur as a pair of orthogonal polarizations.
These different waves (compression waves and the different polarizations of shear waves) may have different speeds at the same frequency. Therefore, they arrive at an observer at different times, an extreme example being an earthquake, where sharp compression waves arrive first, and rocking transverse waves seconds later.
The speed of a compression wave in fluid is determined by the medium's compressibility and density. In solids, the compression waves are analogous to those in fluids, depending on compressibility and density, but with the additional factor of shear modulus which affects compression waves due to off-axis elastic energies which are able to influence effective tension and relaxation in a compression. The speed of shear waves, which can occur only in solids, is determined simply by the solid material's shear modulus and density.

Equations
The speed of sound in mathematical notation is conventionally represented by c, from the Latin celeritas meaning ""velocity"".
In general, the speed of sound c is given by the Newton–Laplace equation:

  
    
      
        c
        =
        
          
            
              
                K
                
                  s
                
              
              ?
            
          
        
        ,
      
    
    {\displaystyle c={\sqrt {\frac {K_{s}}{\rho }}},}
  
where
Ks is a coefficient of stiffness, the isentropic bulk modulus (or the modulus of bulk elasticity for gases);
? is the density.
Thus the speed of sound increases with the stiffness (the resistance of an elastic body to deformation by an applied force) of the material, and decreases with increase in density. For ideal gases the bulk modulus K is simply the gas pressure multiplied by the dimensionless adiabatic index, which is about 1.4 for air under normal conditions of pressure and temperature.
For general equations of state, if classical mechanics is used, the speed of sound c is given by

  
    
      
        c
        =
        
          
            
              
                (
                
                  
                    
                      ?
                      p
                    
                    
                      ?
                      ?
                    
                  
                
                )
              
              
                s
              
            
          
        
        ,
      
    
    {\displaystyle c={\sqrt {\left({\frac {\partial p}{\partial \rho }}\right)_{s}}},}
  
where
p is the pressure;
? is the density and the derivative is taken isentropically, that is, at constant entropy s.
If relativistic effects are important, the speed of sound is calculated from the relativistic Euler equations.
In a non-dispersive medium, the speed of sound is independent of sound frequency, so the speeds of energy transport and sound propagation are the same for all frequencies. Air, a mixture of oxygen and nitrogen, constitutes a non-dispersive medium. However, air does contain a small amount of CO2 which is a dispersive medium, and causes dispersion to air at ultrasonic frequencies (> 28 kHz).
In a dispersive medium, the speed of sound is a function of sound frequency, through the dispersion relation. Each frequency component propagates at its own speed, called the phase velocity, while the energy of the disturbance propagates at the group velocity. The same phenomenon occurs with light waves; see optical dispersion for a description.

Dependence on the properties of the medium
The speed of sound is variable and depends on the properties of the substance through which the wave is travelling. In solids, the speed of transverse (or shear) waves depends on the shear deformation under shear stress (called the shear modulus), and the density of the medium. Longitudinal (or compression) waves in solids depend on the same two factors with the addition of a dependence on compressibility.
In fluids, only the medium's compressibility and density are the important factors, since fluids do not transmit shear stresses. In heterogeneous fluids, such as a liquid filled with gas bubbles, the density of the liquid and the compressibility of the gas affect the speed of sound in an additive manner, as demonstrated in the hot chocolate effect.
In gases, adiabatic compressibility is directly related to pressure through the heat capacity ratio (adiabatic index), while pressure and density are inversely related to the temperature and molecular weight, thus making only the completely independent properties of temperature and molecular structure important (heat capacity ratio may be determined by temperature and molecular structure, but simple molecular weight is not sufficient to determine it).
In low molecular weight gases such as helium, sound propagates faster as compared to heavier gases such as xenon. For monatomic gases, the speed of sound is about 75% of the mean speed that the atoms move in that gas.
For a given ideal gas the molecular composition is fixed, and thus the speed of sound depends only on its temperature. At a constant temperature, the gas pressure has no effect on the speed of sound, since the density will increase, and since pressure and density (also proportional to pressure) have equal but opposite effects on the speed of sound, and the two contributions cancel out exactly. In a similar way, compression waves in solids depend both on compressibility and density—just as in liquids—but in gases the density contributes to the compressibility in such a way that some part of each attribute factors out, leaving only a dependence on temperature, molecular weight, and heat capacity ratio which can be independently derived from temperature and molecular composition (see derivations below). Thus, for a single given gas (assuming the molecular weight does not change) and over a small temperature range (for which the heat capacity is relatively constant), the speed of sound becomes dependent on only the temperature of the gas.
In non-ideal gas behavior regimen, for which the van der Waals gas equation would be used, the proportionality is not exact, and there is a slight dependence of sound velocity on the gas pressure.
Humidity has a small but measurable effect on the speed of sound (causing it to increase by about 0.1%–0.6%), because oxygen and nitrogen molecules of the air are replaced by lighter molecules of water. This is a simple mixing effect.

Altitude variation and implications for atmospheric acoustics
In the Earth's atmosphere, the chief factor affecting the speed of sound is the temperature. For a given ideal gas with constant heat capacity and composition, the speed of sound is dependent solely upon temperature; see Details below. In such an ideal case, the effects of decreased density and decreased pressure of altitude cancel each other out, save for the residual effect of temperature.
Since temperature (and thus the speed of sound) decreases with increasing altitude up to 11 km, sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. The decrease of the speed of sound with height is referred to as a negative sound speed gradient.
However, there are variations in this trend above 11 km. In particular, in the stratosphere above about 20 km, the speed of sound increases with height, due to an increase in temperature from heating within the ozone layer. This produces a positive speed of sound gradient in this region. Still another region of positive gradient occurs at very high altitudes, in the aptly-named thermosphere above 90 km.

Practical formula for dry air
The approximate speed of sound in dry (0% humidity) air, in meters per second, at temperatures near 0 °C, can be calculated from

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        (
        331.3
        +
        0.606
        ?
        ?
        )
         
         
         
        
          m
          
            /
          
          s
        
        ,
      
    
    {\displaystyle c_{\mathrm {air} }=(331.3+0.606\cdot \vartheta )~~~\mathrm {m/s} ,}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \vartheta }
   is the temperature in degrees Celsius (°C).
This equation is derived from the first two terms of the Taylor expansion of the following more accurate equation:

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        331.3
         
        
          
            1
            +
            
              
                ?
                273.15
              
            
          
        
         
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=331.3~{\sqrt {1+{\frac {\vartheta }{273.15}}}}~~~~\mathrm {m/s} .}
  
Dividing the first part, and multiplying the second part, on the right hand side, by ?273.15 gives the exactly equivalent form

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        20.05
         
        
          
            ?
            +
            273.15
          
        
         
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=20.05~{\sqrt {\vartheta +273.15}}~~~~\mathrm {m/s} .}
  
which can also be written as

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        20.05
         
        
          
            T
            
              /
            
            K
          
        
         
         
         
         
        
          m
          
            /
          
          s
        
      
    
    {\displaystyle c_{\mathrm {air} }=20.05~{\sqrt {T/K}}~~~~\mathrm {m/s} }
  
where T denotes the thermodynamic temperature.
The value of 331.3 m/s, which represents the speed at 0 °C (or 273.15 K), is based on theoretical (and some measured) values of the heat capacity ratio, ?, as well as on the fact that at 1 atm real air is very well described by the ideal gas approximation. Commonly found values for the speed of sound at 0 °C may vary from 331.2 to 331.6 due to the assumptions made when it is calculated. If ideal gas ? is assumed to be 7/5 = 1.4 exactly, the 0 °C speed is calculated (see section below) to be 331.3 m/s, the coefficient used above.
This equation is correct to a much wider temperature range, but still depends on the approximation of heat capacity ratio being independent of temperature, and for this reason will fail, particularly at higher temperatures. It gives good predictions in relatively dry, cold, low pressure conditions, such as the Earth's stratosphere. The equation fails at extremely low pressures and short wavelengths, due to dependence on the assumption that the wavelength of the sound in the gas is much longer than the average mean free path between gas molecule collisions. A derivation of these equations will be given in the following section.
A graph comparing results of the two equations is at right, using the slightly different value of 331.5 m/s for the speed of sound at 0 °C.

Details
Speed of sound in ideal gases and air
For an ideal gas, K (the bulk modulus in equations above, equivalent to C, the coefficient of stiffness in solids) is given by

  
    
      
        K
        =
        ?
        ?
        p
        ,
      
    
    {\displaystyle K=\gamma \cdot p,}
  
thus, from the Newton–Laplace equation above, the speed of sound in an ideal gas is given by

  
    
      
        c
        =
        
          
            ?
            ?
            
              
                p
                ?
              
            
          
        
        ,
      
    
    {\displaystyle c={\sqrt {\gamma \cdot {p \over \rho }}},}
  
where
? is the adiabatic index also known as the isentropic expansion factor. It is the ratio of specific heats of a gas at a constant-pressure to a gas at a constant-volume(
  
    
      
        
          C
          
            p
          
        
        
          /
        
        
          C
          
            v
          
        
      
    
    {\displaystyle C_{p}/C_{v}}
  ), and arises because a classical sound wave induces an adiabatic compression, in which the heat of the compression does not have enough time to escape the pressure pulse, and thus contributes to the pressure induced by the compression;
p is the pressure;
? is the density.
Using the ideal gas law to replace p with nRT/V, and replacing ? with nM/V, the equation for an ideal gas becomes

  
    
      
        
          c
          
            
              i
              d
              e
              a
              l
            
          
        
        =
        
          
            ?
            ?
            
              
                p
                ?
              
            
          
        
        =
        
          
            
              
                ?
                ?
                R
                ?
                T
              
              M
            
          
        
        =
        
          
            
              
                ?
                ?
                k
                ?
                T
              
              m
            
          
        
        ,
      
    
    {\displaystyle c_{\mathrm {ideal} }={\sqrt {\gamma \cdot {p \over \rho }}}={\sqrt {\gamma \cdot R\cdot T \over M}}={\sqrt {\gamma \cdot k\cdot T \over m}},}
  
where
cideal is the speed of sound in an ideal gas;
R (approximately 8.314,5 J · mol?1 · K?1) is the molar gas constant(universal gas constant);
k is the Boltzmann constant;
? (gamma) is the adiabatic index. At room temperature, where thermal energy is fully partitioned into rotation (rotations are fully excited) but quantum effects prevent excitation of vibrational modes, the value is 7/5 = 1.400 for diatomic molecules, according to kinetic theory. Gamma is actually experimentally measured over a range from 1.399,1 to 1.403 at 0 °C, for air. Gamma is exactly 5/3 = 1.6667 for monatomic gases such as noble gases;
T is the absolute temperature;
M is the molar mass of the gas. The mean molar mass for dry air is about 0.028,964,5 kg/mol;
n is the number of moles;
m is the mass of a single molecule.
This equation applies only when the sound wave is a small perturbation on the ambient condition, and the certain other noted conditions are fulfilled, as noted below. Calculated values for cair have been found to vary slightly from experimentally determined values.
Newton famously considered the speed of sound before most of the development of thermodynamics and so incorrectly used isothermal calculations instead of adiabatic. His result was missing the factor of ? but was otherwise correct.
Numerical substitution of the above values gives the ideal gas approximation of sound velocity for gases, which is accurate at relatively low gas pressures and densities (for air, this includes standard Earth sea-level conditions). Also, for diatomic gases the use of ? = 1.400,0 requires that the gas exists in a temperature range high enough that rotational heat capacity is fully excited (i.e., molecular rotation is fully used as a heat energy ""partition"" or reservoir); but at the same time the temperature must be low enough that molecular vibrational modes contribute no heat capacity (i.e., insignificant heat goes into vibration, as all vibrational quantum modes above the minimum-energy-mode, have energies too high to be populated by a significant number of molecules at this temperature). For air, these conditions are fulfilled at room temperature, and also temperatures considerably below room temperature (see tables below). See the section on gases in specific heat capacity for a more complete discussion of this phenomenon.
For air, we use a simplified symbol

  
    
      
        
          R
          
            ?
          
        
        =
        R
        
          /
        
        
          M
          
            
              a
              i
              r
            
          
        
        .
      
    
    {\displaystyle R_{*}=R/M_{\mathrm {air} }.}
  
Additionally, if temperatures in degrees Celsius(°C) are to be used to calculate air speed in the region near 273 kelvin, then Celsius temperature ? = T ? 273.15 may be used. Then

  
    
      
        
          c
          
            
              i
              d
              e
              a
              l
            
          
        
        =
        
          
            ?
            ?
            
              R
              
                ?
              
            
            ?
            T
          
        
        =
        
          
            ?
            ?
            
              R
              
                ?
              
            
            ?
            (
            ?
            +
            273.15
            )
          
        
        ,
      
    
    {\displaystyle c_{\mathrm {ideal} }={\sqrt {\gamma \cdot R_{*}\cdot T}}={\sqrt {\gamma \cdot R_{*}\cdot (\vartheta +273.15)}},}
  

  
    
      
        
          c
          
            
              i
              d
              e
              a
              l
            
          
        
        =
        
          
            ?
            ?
            
              R
              
                ?
              
            
            ?
            273.15
          
        
        ?
        
          
            1
            +
            
              
                ?
                273.15
              
            
          
        
        .
      
    
    {\displaystyle c_{\mathrm {ideal} }={\sqrt {\gamma \cdot R_{*}\cdot 273.15}}\cdot {\sqrt {1+{\frac {\vartheta }{273.15}}}}.}
  
For dry air, where ? (theta) is the temperature in degrees Celsius(°C).
Making the following numerical substitutions,

  
    
      
        R
        =
        8.314
        
        510
         
        
          J
          
            /
          
          (
          m
          o
          l
          ?
          K
          )
        
      
    
    {\displaystyle R=8.314\,510~\mathrm {J/(mol\cdot K)} }
  
is the molar gas constant in J/mole/Kelvin, and

  
    
      
        
          M
          
            
              a
              i
              r
            
          
        
        =
        0.028
        
        964
        
        5
         
        
          k
          g
          
            /
          
          m
          o
          l
        
      
    
    {\displaystyle M_{\mathrm {air} }=0.028\,964\,5~\mathrm {kg/mol} }
  
is the mean molar mass of air, in kg; and using the ideal diatomic gas value of ? = 1.4000.
Then

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        331.3
         
         
        
          
            1
            +
            
              
                
                  
                    ?
                    
                      ?
                    
                  
                  
                    C
                  
                
                
                  
                    273.15
                    
                      ?
                    
                  
                  
                    C
                  
                
              
            
          
        
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=331.3~~{\sqrt {1+{\frac {\vartheta ^{\circ }\mathrm {C} }{273.15^{\circ }\mathrm {C} }}}}~~~\mathrm {m/s} .}
  
Using the first two terms of the Taylor expansion:

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        331.3
         
        (
        1
        +
        
          
            
              
                ?
                
                  ?
                
              
              
                C
              
            
            
              2
              ?
              
                273.15
                
                  ?
                
              
              
                C
              
            
          
        
        )
         
         
         
        
          m
          
            /
          
          s
        
        ,
      
    
    {\displaystyle c_{\mathrm {air} }=331.3~(1+{\frac {\vartheta ^{\circ }\mathrm {C} }{2\cdot 273.15^{\circ }\mathrm {C} }})~~~\mathrm {m/s} ,}
  

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        (
        331.3
        +
        
          0.606
          
            ?
          
        
        
          
            C
          
          
            ?
            1
          
        
        ?
        ?
        )
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=(331.3+0.606^{\circ }\mathrm {C} ^{-1}\cdot \vartheta )~~~\mathrm {m/s} .}
  
The derivation includes the first two equations given in the ""Practical formula for dry air"" section above.

Effects due to wind shear
The speed of sound varies with temperature. Since temperature and sound velocity normally decrease with increasing altitude, sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. Wind shear of 4 m/(s · km) can produce refraction equal to a typical temperature lapse rate of 7.5 °C/km. Higher values of wind gradient will refract sound downward toward the surface in the downwind direction, eliminating the acoustic shadow on the downwind side. This will increase the audibility of sounds downwind. This downwind refraction effect occurs because there is a wind gradient; the sound is not being carried along by the wind.
For sound propagation, the exponential variation of wind speed with height can be defined as follows:

  
    
      
        U
        (
        h
        )
        =
        U
        (
        0
        )
        
          h
          
            ?
          
        
        ,
      
    
    {\displaystyle U(h)=U(0)h^{\zeta },}
  

  
    
      
        
          
            
              
                d
              
              U
            
            
              
                d
              
              H
            
          
        
        (
        h
        )
        =
        ?
        
          
            
              U
              (
              h
              )
            
            h
          
        
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} U}{\mathrm {d} H}}(h)=\zeta {\frac {U(h)}{h}},}
  
where
U(h) is the speed of the wind at height h;
? is the exponential coefficient based on ground surface roughness, typically between 0.08 and 0.52;
dU/dH(h) is the expected wind gradient at height h.
In the 1862 American Civil War Battle of Iuka, an acoustic shadow, believed to have been enhanced by a northeast wind, kept two divisions of Union soldiers out of the battle, because they could not hear the sounds of battle only 10 km (six miles) downwind.

Tables
In the standard atmosphere:
T0 is 273.15 K (= 0 °C = 32 °F), giving a theoretical value of 331.3 m/s (= 1086.9 ft/s = 1193 km/h = 741.1 mph = 644.0 kn). Values ranging from 331.3-331.6 may be found in reference literature, however;
T20 is 293.15 K (= 20 °C = 68 °F), giving a value of 343.2 m/s (= 1126.0 ft/s = 1236 km/h = 767.8 mph = 667.2 kn);
T25 is 298.15 K (= 25 °C = 77 °F), giving a value of 346.1 m/s (= 1135.6 ft/s = 1246 km/h = 774.3 mph = 672.8 kn).
In fact, assuming an ideal gas, the speed of sound c depends on temperature only, not on the pressure or density (since these change in lockstep for a given temperature and cancel out). Air is almost an ideal gas. The temperature of the air varies with altitude, giving the following variations in the speed of sound using the standard atmosphere—actual conditions may vary.
Given normal atmospheric conditions, the temperature, and thus speed of sound, varies with altitude:

Effect of frequency and gas composition
General physical considerations
The medium in which a sound wave is travelling does not always respond adiabatically, and as a result the speed of sound can vary with frequency.
The limitations of the concept of speed of sound due to extreme attenuation are also of concern. The attenuation which exists at sea level for high frequencies applies to successively lower frequencies as atmospheric pressure decreases, or as the mean free path increases. For this reason, the concept of speed of sound (except for frequencies approaching zero) progressively loses its range of applicability at high altitudes. The standard equations for the speed of sound apply with reasonable accuracy only to sit",Category:Physical quantities,1
268,269,Atmospheric diffraction,"Atmospheric diffraction is manifested in the following principal ways:
Optical atmospheric diffraction
Radio wave diffraction is the scattering of radio frequency or lower frequencies from the Earth's ionosphere, resulting in the ability to achieve greater distance radio broadcasting.
Sound wave diffraction is the bending of sound waves, as the sound travels around edges of geometric objects. This produces the effect of being able to hear even when the source is blocked by a solid object. The sound waves bend appreciably around the solid object.
However, if the object has a diameter greater than the acoustic wavelength, a 'sound shadow' is cast behind the object where the sound is inaudible. (Note: some sound may be propagated through the object depending on material).

Optical atmospheric diffraction
When light travels through thin clouds made up of nearly uniform sized water or aerosol droplets or ice crystals, diffraction or bending of light occurs as the light is diffracted by the edges of the particles. This degree of bending of light depends on the frequency (color) of light and the size of the particles. The result is a pattern of rings, which seem to emanate from the Sun, the Moon, a planet, or another astronomical object. The most distinct part of this pattern is a central, nearly white disk. This resembles an atmospheric Airy disc but is not actually an Airy disk. It is distinct from rainbows and halos, which are mainly caused by refraction.

The left photo shows a diffraction ring around the rising Sun caused by a veil of aerosol. This effect dramatically disappeared when the Sun rose high enough until the pattern was no longer visible on the Earth's surface. This phenomenon is sometimes called the corona effect, not to be confused with the solar corona.
On the right is a 1/10-second exposure showing an overexposed full moon. The Moon is seen through thin vaporous clouds, which glow with a bright disk surrounded by an illuminated red ring. A longer exposure would show more faint colors beyond the outside red ring.
Another form of atmospheric diffraction or bending of light occurs when light moves through fine layers of particulate dust trapped primarily in the middle layers of the troposphere. This effect differs from water based atmospheric diffraction because the dust material is opaque whereas water allows light to pass through it. This has the effect of tinting the light the color of the dust particles. This tinting can vary from red to yellow depending on geographical location. the other primary difference is that dust based diffraction acts as a magnifier instead of creating a distinct halo. This occurs because the opaque matter does not share the lensing properties of water. The effect is to make an object visibly larger while being more indistinct as the dust distorts the image. This effect varies largely based on the amount and type of dust in the atmosphere.

Radio wave propagation in the ionosphere
The ionosphere is a layer of partially ionized gases high above the majority of the Earth's atmosphere; these gases are ionized by cosmic rays originating on the sun. When radio waves travel into this zone, which commences about 80 kilometers above the earth, they experience diffraction in a manner similar to the visible light phenomenon described above. In this case some of the electromagnetic energy is bent in a large arc, such that it can return to the Earth's surface at a very distant point (on the order of hundreds of kilometers from the broadcast source. More remarkably some of this radio wave energy bounces off the Earth's surface and reaches the ionosphere for a second time, at a distance even farther away than the first time. Consequently, a high powered transmitter can effectively broadcast over 1000 kilometers by using multiple ""skips"" off of the ionosphere. And, at times of favorable atmospheric conditions good ""skip"" occurs, then even a low power transmitter can be heard halfway around th world. This often occurs for ""novice"" radio amateurs ""hams"" who are limited by law to transmitters with no more than 65 watts. The Kon-Tiki expedition communicated regularly with a 6 watt transmitter from the middle of the Pacific. For more details see the ""communications"" part of the ""Kon-Tiki expedition"" entry in Wikipedia.
An exotic variant of this radio wave propagation has been examined to show that, theoretically, the ionospheric bounce could be greatly exaggerated if a high powered spherical acoustical wave were created in the ionosphere from a source on earth.

Acoustical diffraction near the Earth's surface
In the case of sound waves travelling near the Earth's surface, the waves are diffracted or bent as they traverse by a geometric edge, such as a wall or building. This phenomenon leads to a very important practical effect: that we can hear ""around corners"". Because of the frequencies involved considerable amount of the sound energy (on the order of ten percent) actually travels into this would be sound ""shadow zone"". Visible light exhibits a similar effect, but, due to its much higher frequency, only a minute amount of light energy travels around a corner.
A useful branch of acoustics dealing with the design of noise barriers examines this acoustical diffraction phenomenon in quantitative detail to calculate the optimum height and placement of a soundwall or berm adjacent to a highway.
This phenomenon is also inherent in calculating the sound levels from aircraft noise, so that an accurate determination of topographic features may be understood. In that way one can produce sound level isopleths, or contour maps, which faithfully depict outcomes over variable terrain.

Bibliography
See also
Atmospheric refraction
Noise barrier

External links
Explanation and image gallery - Atmospheric Optics by Les Cowley",Category:Diffraction,1
269,270,Sound Suppression System,"The Mobile Launcher Platform (MLP) is one of three two-story structures used by NASA at the Kennedy Space Center to support the Space Shuttle stack throughout the build-up and launch process: during assembly at the Vehicle Assembly Building (VAB), while being transported to Launch Pads 39A and B, and as the vehicle's launch platform. NASA's three MLPs were originally constructed for the Apollo program to launch the Saturn V rockets in the 1960s and 1970s, and remained in service through the end of the shuttle program in 2011 with alterations.

Apollo
The MLP was constructed for transporting and launching the Saturn V rocket for the Apollo program lunar landing missions of the 1960s and 1970s. Each MLP originally had a single exhaust vent for the Saturn V's engines. The MLPs also featured a 120 m (380 ft) Launch Umbilical Tower (LUT) with nine arms that permitted servicing of the vehicle on the launch pad, and swung away from it at launch. For Skylab and Apollo-Soyuz, MLP No. 1 was modified with a pedestal (called ""the milkstool"") that allowed the shorter Saturn IB rocket to use the Saturn V tower and service arms, and Saturn V Ground Support Equipment (GSE) was removed or de-activated and Saturn IB GSE equipment was installed.

Space Shuttle
After the Apollo program, the launcher platforms were modified for the Space Shuttle (Space Transport System). The umbilical towers from Mobile Launchers 2 and 3 were removed. Portions of these tower structures were erected at the two launch pads, 39A and 39B. These permanent structures are now known as the Fixed Service Structures (FSS). The umbilical tower from Mobile Launcher 1 (which was the platform used for the most significant Apollo missions) was taken apart and stored in the Kennedy Space Center's industrial area. Efforts to preserve it in the 1990s failed, however, for lack of funding and it was scrapped.
In addition to removal of the umbilical towers, each Shuttle-era MLP was extensively reconfigured with the addition of two Tail Service Masts, one on either side of the Main Engine exhaust vent. These 9.4 m (31 ft) masts contained the feed lines through which liquid hydrogen (LH2) and liquid oxygen (LOX) were loaded into the shuttle's external fuel tank, as well as electrical hookups and flares that were used to burn off any ambient hydrogen vapors at the launch site immediately prior to Main Engine start.
The Space Shuttle main engines (SSMEs) vented their exhaust through the original opening used for the Saturn rocket exhaust. Two additional exhaust ports were added to vent exhaust from the solid rocket boosters that flanked the external fuel tank.
The Space Shuttle assembly was held to the MLP at eight holddown points using large studs, four on the aft skirt of each Solid Rocket Booster. Immediately before SRB ignition, frangible nuts attached to the top of these studs were detonated, releasing the Shuttle assembly from the platform.

Sound Suppression System
When NASA began launching Shuttle missions, it became clear that the MLP might inadvertently pose a danger to the crew or the vehicle, due to the possibility of massive acoustic shock waves bouncing off the platform and hitting the Shuttle as it lifted off. This was true for the Saturn V launches as well, but there was less risk because the Apollo spacecraft, atop the 111 m (363 ft) stack, was much farther away from the engines. Because the Shuttle was about half the height of the Saturn, the crew cabin and payload bay were much closer to the platform, and much more vulnerable to the tremendous forces bouncing back off the MLP - on the first mission, STS-1, the shock waves damaged many of the protective thermal tiles.
NASA's solution to this danger was to cushion the MLP at every launch with a flood of flowing water. Starting 6.6 seconds before engine ignition, a 1,100,000-litre (300,000 US gal) water tower at the launch site began dumping water down a pipeline and into the exhaust vents of the MLP. Next, six 3.7 m (12 ft)-high towers known as ""rainbirds"" began to spray water over the MLP and into the flame deflector trenches below it. The water absorbed some of the bruising forces of the acoustic waves, and discouraged fires that might be caused by the rocket exhaust. This water-dumping mechanism, known as the Sound Suppression System, emptied the launch pad tank in around 41 seconds. The giant white clouds that billowed around the shuttle at each launch were not smoke, but water vapor generated as the rocket exhaust boiled away huge quantities of water. The suppression system reduced the acoustic sound level to approximately 142 dB.

Capabilities
Each MLP weighs 3,730,000 kg (8,230,000 lb) unloaded and roughly 5,000,000 kg (11,000,000 lb) with an unfueled Shuttle aboard, measures 49 by 41 m (160 by 135 ft), and is 7.6 m (25 ft) high. It was carried by a crawler-transporter, which measures 40 by 35 m (131 by 114 ft), and is 6.1 m (20 ft) high. Each crawler weighs about 3,000,000 kg (6,000,000 lb) unloaded, has a maximum speed of about 1.6 km (1 mi) per hour loaded, and has a leveling system designed to keep the launch vehicle vertical while negotiating the 5 percent grade leading to the top of the launch pad. Two 2,050 kW (2,750 hp) diesel engines power each crawler.
Originally designated the ""Mobile Launcher"", the MLP was designed as part of NASA's strategy for vertical assembly and transport of space vehicles. Vertical assembly allows the preparation of the spacecraft in a ready-for-launch position, and avoids the additional step of lifting or craning a horizontally-assembled vehicle onto the launchpad (as the engineers of the Soviet space program chose to do).
The Mobile Launcher Platform was set atop six legs, each 6.7 m (22 ft) tall, when stationary. The Solid Rocket Boosters were mounted on top of the MLP. The External Tank was then lowered between the two boosters and attached to them. After that, the orbiter was lowered into position and attached to the External Tank. The crawler-transporter then carried the combined platform and vehicle to the launch site, and deposited them there together. Once the launch was completed, the crawler-transporter retrieved the empty MLP from the pad to be readied for its next use.

Constellation
With the retirement of the Shuttle in 2011 and its planned replacement Orion spacecraft and launcher in the design and test phase, NASA converted LC-39B from Shuttle operations to support Orion launches. The Ares I-X suborbital mission utilized MLP-1, to support the stacking and launch operations. The cancelled Ares I-Y would have used the same MLP.

Space Launch System
The MLP that was built to launch the Ares I rocket will be modified to support the Space Launch System. The mobile launcher will have to be altered in order to support the heavier weight and additional thrust of the heavy lift rocket. The biggest modifications to the MLP will be on the platform's base, where engineers will increase the size of a 2.0 m2 (22 sq ft) exhaust duct to a rectangle stretching 18.3 by 9.1 m (60 by 30 ft) and strengthen the surrounding structure. The SLS will weigh more than twice as much as the planned Ares I rocket. The Ares 1 rocket would have featured a single solid-fueled first stage, while the Space Launch System will include two large solid rocket boosters and a powerful core with up to five Space Shuttle main engines. The modifications are expected to be complete by 2016.

Future use
Early in 2016, NASA finished upgrading CT-2 to a ""Super Crawler"" for use in the Space Launch System program. CT-1 is now in the process of being modified to serve a variety of commercial spacecraft. In April 2016, Orbital ATK and NASA entered negotiations for the lease of CT-1 and one of the four Vertical Assembly Building bays.

Launch history
References
External links

Virtual tour of the Space Launch System Mobile Launcher",Category:Apollo program hardware,1
270,271,Sound trap,,Category:All articles lacking sources,1
271,272,Beatmapping,,Category:Articles with multiple maintenance issues,1
272,273,Auditory masking,"Auditory masking occurs when the perception of one sound is affected by the presence of another sound.
Auditory masking in the frequency domain is known as simultaneous masking, frequency masking or spectral masking. Auditory masking in the time domain is known as temporal masking or non-simultaneous masking.

Masked threshold
The unmasked threshold is the quietest level of the signal which can be perceived without a masking signal present. The masked threshold is the quietest level of the signal perceived when combined with a specific masking noise. The amount of masking is the difference between the masked and unmasked thresholds.

Gelfand provides a basic example. Let us say that for a given individual, the sound of a cat scratching a post in an otherwise quiet environment is first audible at a level of 10 dB SPL. However, in the presence of a masking noise (for example, a vacuum cleaner that is running simultaneously) that same individual cannot detect the sound of the cat scratching unless the level of the scratching sound is at least 26 dB SPL. We would say that the unmasked threshold for that individual for the target sound (i.e., the cat scratching) is 10 dB SPL, while the masked threshold is 26 dB SPL. The amount of masking is simply the difference between these two thresholds: 16 dB.
The amount of masking will vary depending on the characteristics of both the target signal and the masker, and will also be specific to an individual listener. While the person in the example above was able to detect the cat scratching at 26 dB SPL, another person may not be able to hear the cat scratching while the vacuum was on until the sound level of the cat scratching was increased to 30 dB SPL (thereby making the amount of masking for the second listener 20 dB).

Simultaneous masking
Simultaneous masking occurs when a sound is made inaudible by a noise or unwanted sound of the same duration as the original sound. For example, a powerful spike at 1 kHz will tend to mask out a lower-level tone at 1.1 kHz. Also, two sine tones at 440 and 450 Hz can be perceived clearly when separated. They cannot be perceived clearly when presented simultaneously.

Critical bandwidth
If two sounds of two different frequencies are played at the same time, two separate sounds can often be heard rather than a combination tone. The ability to hear frequencies separately is known as frequency resolution or frequency selectivity. When signals are perceived as a combination tone, they are said to reside in the same critical bandwidth. This effect is thought to occur due to filtering within the cochlea, the hearing organ in the inner ear. A complex sound is split into different frequency components and these components cause a peak in the pattern of vibration at a specific place on the cilia inside the basilar membrane within the cochlea. These components are then coded independently on the auditory nerve which transmits sound information to the brain. This individual coding only occurs if the frequency components are different enough in frequency, otherwise they are in the same critical band and are coded at the same place and are perceived as one sound instead of two.
The filters that distinguish one sound from another are called auditory filters, listening channels or critical bandwidths. Frequency resolution occurs on the basilar membrane due to the listener choosing a filter which is centered over the frequency they expect to hear, the signal frequency. A sharply tuned filter has good frequency resolution as it allows the center frequencies through but not other frequencies (Pickles 1982). Damage to the cochlea and the outer hair cells in the cochlea can impair the ability to tell sounds apart (Moore 1986). This explains why someone with a hearing loss due to cochlea damage would have more difficulty than a normal hearing person in distinguishing between different consonants in speech.
Masking illustrates the limits of frequency selectivity. If a signal is masked by a masker with a different frequency to the signal, then the auditory system was unable to distinguish between the two frequencies. By experimenting with conditions where one sound can mask a previously heard signal, the frequency selectivity of the auditory system can be tested.

Similar frequencies
How effective the masker is at raising the threshold of the signal depends on the frequency of the signal and the frequency of the masker. The graphs in Figure B are a series of masking patterns, also known as masking audiograms. Each graph shows the amount of masking produced at each masker frequency shown at the top corner, 250, 500, 1000 and 2000 Hz. For example, in the first graph the masker is presented at a frequency of 250 Hz at the same time as the signal. The amount the masker increases the threshold of the signal is plotted and this is repeated for different signal frequencies, shown on the X axis. The frequency of the masker is kept constant. The masking effect is shown in each graph at various masker sound levels.

Figure B shows along the Y axis the amount of masking. The greatest masking is when the masker and the signal are the same frequency and this decreases as the signal frequency moves further away from the masker frequency. This phenomenon is called on-frequency masking and occurs because the masker and signal are within the same auditory filter (Figure C). This means that the listener cannot distinguish between them and they are perceived as one sound with the quieter sound masked by the louder one (Figure D).

The amount the masker raises the threshold of the signal is much less in off-frequency masking, but it does have some masking effect because some of the masker overlaps into the auditory filter of the signal (Figure E)

Off-frequency masking requires the level of the masker to be greater in order to have a masking effect; this is shown in Figure F. This is because only a certain amount of the masker overlaps into the auditory filter of the signal and more masker is needed to cover the signal.

Lower frequencies
The masking pattern changes depending on the frequency of the masker and the intensity (Figure B). For low levels on the 1000 Hz graph, such as the 20-40 dB range, the curve is relatively parallel. As the masker intensity increases the curves separate, especially for signals at a frequency higher than the masker. This shows that there is a spread of the masking effect upward in frequency as the intensity of the masker is increased. The curve is much shallower in the high frequencies than in the low frequencies. This flattening is called upward spread of masking and is why an interfering sound masks high frequency signals much better than low frequency signals.
Figure B also shows that as the masker frequency increases, the masking patterns become increasingly compressed. This demonstrates that high frequency maskers are only effective over a narrow range of frequencies, close to the masker frequency. Low frequency maskers on the other hand are effective over a wide frequency range.

Harvey Fletcher carried out an experiment to discover how much of a band of noise contributes to the masking of a tone. In the experiment, a fixed tone signal had various bandwidths of noise centered on it. The masked threshold was recorded for each bandwidth. His research showed that there is a critical bandwidth of noise which causes the maximum masking effect and energy outside that band does not affect the masking. This can be explained by the auditory system having an auditory filter which is centered over the frequency of the tone. The bandwidth of the masker that is within this auditory filter effectively masks the tone but the masker outside of the filter has no effect (Figure G).
This is used in MP3 files to reduce the size of audio files. Parts of the signals which are outside the critical bandwidth are represented with reduced precision. The parts of the signals which are perceived by the listener are reproduced with higher fidelity.

Effects of intensity
Varying intensity levels can also have an effect on masking. The lower end of the filter becomes flatter with increasing decibel level, whereas the higher end becomes slightly steeper. Changes in slope of the high frequency side of the filter with intensity are less consistent than they are at low frequencies. At the medium frequencies (1–4 kHz) the slope increases as intensity increases, but at the low frequencies there is no clear inclination with level and the filters at high center frequencies show a small decrease in slope with increasing level. The sharpness of the filter depends on the input level and not the output level to the filter. The lower side of the auditory filter also broadens with increasing level. These observations are illustrated in Figure H.

Temporal masking
Temporal masking or non-simultaneous masking occurs when a sudden stimulus sound makes inaudible other sounds which are present immediately preceding or following the stimulus. Masking which obscures a sound immediately preceding the masker is called backward masking or pre-masking and masking which obscures a sound immediately following the masker is called forward masking or post-masking. Temporal masking's effectiveness attenuates exponentially from the onset and offset of the masker, with the onset attenuation lasting approximately 20 ms and the offset attenuation lasting approximately 100 ms.
Similar to simultaneous masking, temporal masking reveals the frequency analysis performed by the auditory system; forward masking thresholds for complex harmonic tones (e.g., a sawtooth probe with a fundamental frequency of 500 Hz) exhibit threshold peaks (i.e., high masking levels) for frequency bands centered on the first several harmonics. In fact, auditory bandwidths measured from forward masking thresholds are narrower and more accurate than those measured using simultaneous masking.
Temporal masking should not be confused with the ear's acoustic reflex, an involuntary response in the middle ear that is activated to protect the ear's delicate structures from loud sounds.

Other masking conditions
Ipsilateral (""same side"") masking is not the only condition where masking takes place. Another situation where masking occurs is called contralateral (""other side"") simultaneous masking. In this case, the instance where the signal might be audible in one ear but is deliberately taken away by applying a masker to the other ear.
The last situation where masking occurs is called central masking. This refers to the case where a masker causes a threshold elevation. This can be in the absence of, or in addition to, another effect and is due to interactions within the central nervous system between the separate neural inputs obtained from the masker and the signal.

Effects of different stimulus types
Experiments have been carried out to see the different masking effects when using a masker which is either in the form of a narrow band noise or a sinusoidal tone.
When a sinusoidal signal and a sinusoidal masker (tone) are presented simultaneously the envelope of the combined stimulus fluctuates in a regular pattern described as beats. The fluctuations occur at a rate defined by the difference between the frequencies of the two sounds. If the frequency difference is small then the sound is perceived as a periodic change in the loudness of a single tone. If the beats are fast then this can be described as a sensation of roughness. When there is a large frequency separation, the two components are heard as separate tones without roughness or beats. Beats can be a cue to the presence of a signal even when the signal itself is not audible. The influence of beats can be reduced by using a narrowband noise rather than a sinusoidal tone for either signal or masker.

Mechanisms of masking
There are many different mechanisms of masking, one being suppression. This is when there is a reduction of a response to a signal due to the presence of another. This happens because the original neural activity caused by the first signal is reduced by the neural activity of the other sound.
Combination tones are products of a signal and a masker. This happens when the two sounds interact causing new sound, which can be more audible than the original signal. This is caused by the non linear distortion that happens in the ear. For example, the combination tone of two maskers can be a better masker than the two original maskers alone.
The sounds interact in many ways depending on the difference in frequency between the two sounds. The most important two are cubic difference tones and quadratic difference tones.
Cubic difference tones are calculated by the sum
F1 – F2
(F1 being the first frequency, F2 the second) These are audible most of the time and especially when the level of the original tone is low. Hence they have a greater effect on psychoacoustic tuning curves than quadratic difference tones.
Quadratic difference tones are the result of
F2 – F1
This happens at relatively high levels hence have a lesser effect on psychoacoustic tuning curves.
Combination tones can interact with primary tones resulting in secondary combination tones due to being like their original primary tones in nature, stimulus like. An example of this is
3F1 – 2F2
Secondary combination tones are again similar to the combination tones of the primary tone.

Off frequency listening
Off frequency listening is when a listener chooses a filter just lower than the signal frequency to improve their auditory performance. This “off frequency” filter reduces the level of the masker more than the signal at the output level of the filter, which means they can hear the signal more clearly hence causing an improvement of auditory performance.

Applications
Auditory masking is used in tinnitus maskers to suppress annoying ringing, hissing, or buzzing or tinnitus often associated with hearing loss. It is also used in various kinds of audiometry, including pure tone audiometry, and the standard hearing test to test each ear unilaterally and to test speech recognition in the presence of partially masking noise.
Auditory masking is exploited to perform data compression for sound signals (MP3).

See also
Cocktail party effect
Illusory discontinuity
Psychoacoustics
Response priming

References
External links
""Addition of Simultaneous Masking"" by B. Lincoln from Stanford University
Auditory Masking & Wideband Audio Coding – video lecture by Professor E. Ambikairajah",Category:Acoustics,1
273,274,Acoustic foam,"Acoustic foam is an open celled foam used for acoustic treatment. It attenuates airborne sound waves by increasing air resistance, thus reducing the amplitude of the waves. The energy is dissipated as heat. Acoustic foam can be made in several different colors, sizes and thickness.
Acoustic foam can be attached to walls, ceilings, doors, and other features of a room to control noise levels, vibration, and echoes.
Many acoustic foam products are treated with dyes and/or fire retardants.

Uses
The objective of acoustic foam is to improve the sound quality by removing residual sound in any space.This purpose requires strategic placement of acoustic foam panels on walls, ceiling and floors, effectively eliminating resonance within the room.

Acoustic enhancement
The objective is to enhance the properties of sound by improving speech clarity and sound quality. For this reason, acoustic foam is often used in recording studios. The purpose is to reduce, but not entirely eliminate, resonance within the room. This is achieved by placing similar sized pieces of foam, often in the shape of cones or triangles, on opposite walls.

Functionality
Acoustic foam is a lightweight material made from polyurethane foam either polyether or polyester, and also extruded melamine foam. It is usually cut into tiles - often with pyramid or wedge shapes - which are suited to placing on the walls of a recording studio or a similar type of environment to act as a sound absorber, thus enhancing the sound quality within a room.
Acoustic foam reduces or eliminates echoes and background noises by controlling the reverberation that sound can make by bouncing off walls. This type of sound absorption is different from soundproofing, which is typically used to keep sound from escaping or entering a room. Therefore, acoustic foam is installed in large rooms like churches, synagogues, concert halls. These rooms have large, flat space and noise will certainly bounce around in the room. These sound absorbers are used to improve the acoustics of the room, which thereby reduces noise in the room.
Acoustic foam typically deals more with the mid and high frequencies. To deal with lower frequencies, much thicker pieces of acoustic foam are needed; large pieces of acoustic foam are often placed in the corners of a room and are called acoustic foam corner bass traps.

See also
Sorbothane
Bushing (isolator)
Anechoic chamber
Soundproofing
Vibration isolation
Polystyrene
Styrofoam
Polyurethane


== References ==",Category:All articles lacking sources,1
274,275,Echogenicity,"Echogenicity (misspelled sometimes as echogenecity) or echogeneity is the ability to bounce an echo, e.g. return the signal in ultrasound examinations. In other words, echogenicity is higher when the surface bouncing the sound echo reflects increased sound waves. Tissues that have higher echogenicity are called ""hyperechogenic"" and are usually represented with lighter colors on images in medical ultrasonography. In contrast, tissues with lower echogenicity are called ""hypoechogenic"" and are usually represented with darker colors. Areas that lack echogenicity are called ""anechogenic"" and are usually displayed as completely dark.

Microbubbles
Echogenicity can be increased by intravenously administering gas-filled microbubble contrast agent to the systemic circulation, with the procedure being called contrast-enhanced ultrasound. This is because microbubbles have a high degree of echogenicity. When gas bubbles are caught in an ultrasonic frequency field, they compress, oscillate, and reflect a characteristic echo- this generates the strong and unique sonogram in contrast-enhanced ultrasound. Gas cores can be composed of air, or heavy gases like perfluorocarbon, or nitrogen. Heavy gases are less water-soluble so they are less likely to leak out from the microbubble to impair echogenicity (McCulloch et al., 2000). Therefore, microbubbles with heavy gas cores are likely to last longer in circulation.

Reasons for higher echogenicity
During ultrasound examinations, sometimes echogenicity is higher in certain parts of body. Fatty liver could cause increased echogenicity in the liver, especially if the liver transaminases are elevated.
Women with polycystic ovary syndrome may also show an increase in stromal echogenicity.

See also
Contrast-enhanced ultrasound
Echogenic intracardiac focus
Ultrasound


== References ==",Category:Ultrasound,1
275,276,Photoacoustic effect,"The photoacoustic effect or optoacoustic effect is the formation of sound waves following light absorption in a material sample. In order to obtain this effect the light intensity must vary, either periodically (modulated light) or as a single flash (pulsed light). The photoacoustic effect is quantified by measuring the formed sound (pressure changes) with appropriate detectors, such as microphones or piezoelectric sensors. The time variation of the electric output (current or voltage) from these detectors is the photoacoustic signal. These measurements are useful to determine certain properties of the studied sample. For example, in photoacoustic spectroscopy, the photoacoustic signal is used to obtain the actual absorption of light in either opaque or transparent objects. It is useful for substances in extremely low concentrations, because very strong pulses of light from a laser can be used to increase sensitivity and very narrow wavelengths can be used for specificity. Furthermore, photoacoustic measurements serve as a valuable research tool in the study of the heat evolved in photochemical reactions (see: photochemistry), particularly in the study of photosynthesis.
Most generally, electromagnetic radiation of any kind can give rise to a photoacoustic effect. This includes the whole range of electromagnetic frequencies, from gamma radiation and X-rays to microwave and radio. Still, much of the reported research and applications, utilizing the photoacoustic effect, is concerned with the near ultraviolet/visible and infrared spectral regions.

History
The discovery of the photoacoustic effect dates back to 1880, when Alexander Graham Bell was experimenting with long-distance sound transmission. Through his invention, called ""photophone"", he transmitted vocal signals by reflecting sun-light from a moving mirror to a selenium solar cell receiver. As a byproduct of this investigation, he observed that sound waves were produced directly from a solid sample when exposed to beam of sunlight that was rapidly interrupted with a rotating slotted wheel. He noticed that the resulting acoustic signal was dependent on the type of the material and correctly reasoned that the effect was caused by the absorbed light energy, which subsequently heats the sample. Later Bell showed that materials exposed to the non-visible (ultra-violet and infra-red) portions of the solar spectrum can also produce sounds and invented a device, which he called ""spectrophone"", to apply this effect for spectral identification of materials. Bell himself and later John Tyndall and Wilhelm Röntgen extended these experiments, demonstrating the same effect in liquids and gases. However, the results were too crude, dependent on ear detection, and this technique was soon abandoned. The application of the photoacoustic effect had to wait until the development of sensitive sensors and intense light sources. In 1938 Mark Leonidovitch Veingerov revived the interest in the photoacoustic effect, being able to use it in order to measure very small carbon dioxide concentration in nitrogen gas (as low as 0.2% in volume). Since then research and applications grew faster and wider, acquiring several fold more detection sensitivity.
While the heating effect of the absorbed radiation was considered to be the prime cause of the photoacoustic effect, it was shown in 1978 that gas evolution resulting from a photochemical reaction can also cause a photoacoustic effect. Independently, considering the apparent anomalous behaviour of the photoacoustic signal from a plant leaf, which could not be explained solely by the heating effect of the exciting light, led to the cognition that photosynthetic oxygen evolution is normally a major contributor to the photoacoustic signal in this case.

Physical mechanisms
Photothermal mechanism
Although much of the literature on the subject is concerned with just one mechanism, there are actually several different mechanisms that produce the photoacoustic effect. The primary universal mechanism is photothermal, based on the heating effect of the light and the consequent expansion of the light-absorbing material. In detail, the photothermal mechanism consists of the following stages: (1) conversion of the absorbed pulsed or modulated radiation into heat energy. (2) temporal changes of the temperatures at the loci where radiation is absorbed – rising as radiation is absorbed and falling when radiation stops and the system cools. (3) expansion and contraction following these temperature changes, which are ""translated"" to pressure changes. The pressure changes, which occur in the region where light was absorbed, propagate within the sample body and can be sensed by a sensor coupled directly to it. Commonly, for the case of a condensed phase sample (liquid, solid), pressure changes are rather measured in the surrounding gaseous phase (commonly air), formed there by the diffusion of the thermal pulsations. The main physical picture, in this case, envisions the original temperature pulsations as origins of propagating temperature waves (""thermal waves""), which travel in the condensed phase, ultimately reaching the surrounding gaseous phase. The resulting temperature pulsations in the gaseous phase are the prime cause of the pressure changes there. The amplitude of the traveling thermal wave decreases strongly (exponentially) along its propagation direction, but if its propagation distance in the condensed phase is not too long, its amplitude near the gaseous phase is sufficient to create detectable pressure changes. This property of the thermal wave confers unique features to the detection of light absorption by the photoacoustic method. The temperature and pressure changes involved are minute, compared to everyday scale – typical order of magnitude for the temperature changes, using ordinary light intensities, is about micro to milli-degrees and for the resulting pressure changes is about nano to micro-bars.
The photothermal mechanism manifests itself, besides the photoacoustic effect, also by other physical changes, notably emission of infra-red radiation and changes in the refraction index. Correspondingly, it may be detected by various other means, described by terms such as ""photothermal radiometry"", ""thermal lens"" and ""thermal beam deflection"" (popularly also known as ""mirage"" effect) (see Photothermal spectroscopy. These methods parallel the photoacoustic detection. However, each method has its special range of application.

Other
While the photothermal mechanism is universal, there could exist additional other mechanisms, superimposed on the photothermal mechanism, which may contribute significantly to the photoacoustic signal. These mechanisms are generally related to photophysical processes and photochemical reactions following light absorption: (1) change in the material balance of the sample and/or the gaseous phase around the sample; (2) change in the molecular organization, which results in molecular volume changes. Most prominent examples for these two kinds of mechanisms are in photosynthesis 
The first mechanism above is mostly conspicuous in a photosynthesizing plant leaf. There, the light induced oxygen evolution causes pressure changes in the air phase, resulting in a photoacoustic signal, which is comparable in magnitude to that caused by the photothermal mechanism. This mechanism was tentatively named ""photobaric"". The second mechanism shows up in photosynthetically active sub-cell complexes in suspension (e.g. photosynthetic reaction centers). There, the electric field which is formed in the reaction center, following the light induced electron transfer process, causes a micro electrostriction effect with a change in the molecular volume. This, in turn, induces a pressure wave which propagates in the macroscopic medium. Another case for this mechanism is Bacteriorhodopsin proton pump. Here the light induced change in the molecular volume is caused by conformational changes that occur in this protein following light absorption.

Detection of the photoacoustic effect
In applying the photoacoustic effect there exist various modes of measurement. Gaseous samples or condensed phase samples, where the pressure is measured in the surrounding gaseous phase, are usually probed with a microphone. The useful applicable time-scale in this case is in the millisecond to sub-second scale. Most often, In this case, the exciting light is continuously chopped or modulated at a certain frequency (mostly in the range between ca. 10–10000 Hz) and the modulated photoacoustic signal is analyzed with a lock-in amplifier for its amplitude and phase, or for the inphase and quadrature components. When the pressure is measured within the condensed phase of the probed specimen, one utilizes piezoelectric sensors inserted into or coupled to the specimen itself. In this case the time scale is between less than nanoseconds to many microseconds  The photoacoustic signal, obtained from the various pressure sensors, depends on the physical properties of the system, the mechanism that creates the photoacoustic signal, the light-absorbing material, the dynamics of the excited state relaxation and the modulation frequency or the pulse profile of the radiation, as well as the sensor properties. This calls for appropriate procedures to (i) separate between the signals due to different mechanisms and (ii) to obtain the time dependence of the heat evolution (in the case of the photothermal mechanism) or the oxygen evolution (in the case of the photobaric mechanism in photosynthesis) or the time dependence of the volume changes, from the time dependence of the resulting photoacoustic signal.

Applications
Considering the photothermal mechanism alone, the photoacoustic signal is useful in measuring the light absorption spectrum, particularly for transparent samples where the light absorption is very small. In this case the ordinary method of absorption spectroscopy, based on difference of the intensities of a light beam before and after its passage through the sample, is not practical. In photoacoustic spectroscopy there is no such limitation. the signal is directly related to the light absorption and the light intensity. Dividing the signal spectrum by the light intensity spectrum can give a relative percent absorption spectrum, which can be calibrated to yield absolute values. This is very useful to detect very small concentrations of various materials. Photoacoustic spectroscopy is also useful for the opposite case of opaque samples, where the absorption is essentially complete. In an arrangement where a sensor is placed in a gaseous phase above the sample and the light impinges the sample from above, the photoacoustic signal results from an absorption zone close to the surface. A typical parameter which governs the signal in this case is the ""thermal diffusion length"", which depends on the material and the modulation frequency and ordinarily is in the order of several micrometers. The signal is related to the light absorbed in the small distance of the thermal diffusion length, allowing the determination of the absorption spectrum. This allows also to separately analyze a surface that is distinct from the bulk. By varying the modulation frequency and wavelength of the probing radiation one essentially varies the probed depth, which results in the possibility of depth profiling  and photoacoustic imaging, which discloses inhomogeneities within the sample. This analysis includes also the possibility to determine the thermal properties from the photoacoustic signal.
Recently, the photoacoustic approach has been utilized to quantitatively measure macromolecules, such as proteins. The photoacoustic immunoassay labels and detects target proteins using nanoparticles that can generate strong acoustic signals. The photoacoustics-based protein analysis has also been applied for point-of-care testings.
Another application of the photoacoustic effect is its ability to estimate the chemical energies stored in various steps of a photochemical reaction. Following light absorption photophysical and photochemical conversions occur, which store part of the light energy as chemical energy. Energy storage leads to less heat evolution. The resulting smaller photoacoustic signal thus gives a quantitative estimate of the extent of the energy storage. For transient species this requires the measurement of the signal in the relevant time scale and the capability to extract from the temporal part of the signal the time-dependent heat evolution, by proper deconvolution. There are numerous examples for this application. A similar application is the study of the conversion of light energy to electrical energy in solar cells. A special example is the application of the photoacoustic effect in photosynthesis research.

Photoacoustic effect in photosynthesis
Photosynthesis is a very suitable platform to be investigated by the photoacoustic effect, providing many examples to its various uses. As noted above, the photoacoustic signal from wet photosynthesizing specimens (e.g. microalgae in suspension, sea weed) is by large photothermal. The photoacoustic signal from spongy structures (leaves, lichens) is a combination of photothermal and photobaric (gas evolution or uptake) contributions. The photoacoustic signal from preparations which carry out the primary electron transfer reactions (e.g. reaction centers) is a combination of photothermal and molecular volume changes contributions. In each case, respectively, photoacoustic measurements provided information on
Energy storage (i.e. the fraction of light energy which is converted to chemical energy in the photosynthetic process;
The extent and dynamics of the gas evolution and uptake from leaves or lichens. Most usually it is photosynthetic oxygen evolution which contributes to the photoacoustic signal; Carbon dioxide uptake is a slow process and does not show up in photoacoustic measurements. Under very specific conditions, however, the photoacoustic signal becomes transiently negative, presumably reflecting oxygen uptake. However, this needs more verification;
Molecular volume changes, which occur during the primary steps of photosynthetic electron transfer.
These measurements provided information related to the mechanism of photosynthesis, as well as give indications on the intactness and health of the specimen.
Examples are: (a) the energetics of the primary electron transfer processes, obtained from the energy storage and molecular volume change measured under sub-microsecond flashes; (b) The characteristics of the 4-step oxidation cycle in photosystem II, obtained for leaves by monitoring photoacoustic pulsed signals and their oscillatory behavior under repetitive exciting light flashes; (c) the characteristics of photosystem I and photosystem II of photosynthesis (absorption spectrum, light distribution to the two photosystems) and their interactions. This is obtained by using continuously modulated light of a certain specific wavelength to excite the photoacoustic signal and measure changes in energy storage and oxygen evolution caused by background light at various chosen wavelengths.
In general, photoacoustic measurements of energy storage require a reference sample for comparison. It is a sample with exactly the same light absorption (at the given excitation wavelength) but which completely degrades all the absorbed light into heat within the time resolution of the measurement. It is lucky that photosynthetic systems are self-calibrating, providing such a reference in one sample,as follows: One compares two signals: one, which is obtained with the probing modulated/pulsed light alone and the other when a steady non-modulated light (referred to as background light), which is strong enough to drive photosynthesis into saturation, is added. The added steady light does not produce any photoacoustic effect by itself, but changes the photoacoustic response due to the modulated/pulsed probing light. The resulting signal serves as a reference to all other measurements in absence of the background light. The photothermal part of the reference signal is maximal, since at photosynthetic saturation no energy is stored. At the same time the contribution of the other mechanisms tends to zero at saturation. Thus the reference signal is proportional to the total absorbed light energy.
In order to separate and define the photobaric and photothermal contributions in spongy samples (leaves, lichens) one uses the following properties of the photoacoustic signal: (1) At low frequencies (below roughly 100 Hz) the photobaric part of the photoacoustic signal may be quite large and the total signal decreases under the background light. The photobaric signal is obtained in principle from the difference of signals (the total signal minus the reference signal, after a correction to account for the energy storage). (2) At sufficiently high frequencies, however, the photobaric signal is very much attenuated in comparison with the photothermal component and can be neglected. Also, no photobaric signal can be observed even at low frequencies in a leaf with its inner air space filled with water. This is true also in live algal thalli, suspensions of microalgae and photosynthetic bacteria. This is because the photobaric signal depends on oxygen diffusion from the photosynthetic membranes to the air phase, and is largely attenuated as the diffusion distance in the aqueous medium increases. In all the above instances when no photobaric signal is observed one may determine the energy storage by comparing the photoacoustic signal obtained with the probing light alone, to the reference signal. The parameters obtained from the above measurements are used in a variety of ways. Energy storage and the intensity of the photobaric signal are related to the efficiency of photosynthesis and can be used to monitor and follow the health of photosynthesizing organisms. They are also used to obtain mechanistic insight on the photosynthetic process: light of different wavelengths allows one to obtain the efficiency spectrum of photosynthesis, the light distribution between the two photosystems of photosynthesis and to identify different taxa of phytoplankton. The use of pulsed lasers gives thermodynamic and kinetic information on the primary electron transfer steps of photosynthesis.

See also
Microwave auditory effect


== References ==",Category:Articles lacking reliable references from February 2015,1
276,277,Savart wheel,"The Savart wheel is an acoustical device named after the French physicist Félix Savart (1791–1841), which was originally conceived and developed by the English scientist Robert Hooke (1635–1703).
A card held to the edge of a spinning toothed wheel will produce a tone whose pitch varies with the speed of the wheel. A mechanism of this sort, made using brass wheels, allowed Hooke to produce sound waves of a known frequency, and to demonstrate to the Royal Society in 1681 how pitch relates to frequency. For practical purposes Hooke's device was soon supplanted by the invention of the tuning fork.
About a century and a half after Hooke's work, the mechanism was taken up again by Savart for his investigations into the range of human hearing. In the 1830s Savart was able to construct large, finely-toothed brass wheels producing frequencies of up to 24 kHz that seem to have been the world's first artificial ultrasonic generators. In the later 19th century, Savart's wheels were also used in physiological and psychological investigations of time perception.
Nowadays, Savart wheels are commonly demonstrated in physics lectures, sometimes driven and sounded by an air hose (in place of the card mechanism).

Description
The basic device consists of a ratchet-wheel with a large number of uniformly spaced teeth. When the wheel is turned slowly while the edge of a card is held against the teeth a succession of distinct clicks can be heard. When the wheel is spun rapidly it produces a shrill tone, whereas if the wheel is allowed to turn more slowly the tone progressively decreases in pitch. Since the frequency of the tone is directly proportional to the rate at which the teeth strike the card, a Savart wheel can be calibrated to provide an absolute measure of pitch. Multiple wheels of different sizes, carrying different numbers of teeth, can also be attached so as to allow several pitches (or chords) to be produced while the axle is being turned at a constant rate.

Hooke's wheel
Hooke began work on his wheel in March 1676, in conjunction with the renowned clockmaker Thomas Tompion, following conversations with the music theorist William Holder. He had a longstanding interest in musical vibrations, and a decade earlier in 1666 had even boasted to Samuel Pepys that he could tell the rate a fly's wings were beating from the sound they made. In July 1681, he demonstrated to the Royal Society his new device for producing distinct musical tones by striking the teeth of fast-turning brass wheels. In this way, he was able to generate for the first time sound waves of known frequency, and provide an empirical demonstration of the correspondence between the human perception of pitch and the physical property of sound-wave frequency. Furthermore, by fitting different wheels alongside one another on the same axis, he was able to verify frequency ratios for musical intervals, such as perfect fifths and fourths, etc.
Hooke published his findings in 1705. Despite providing an objective measure of pitch, for everyday use his wheel was soon made irrelevant by the invention in 1711 of the tuning fork.

Savart's version
Hooke's device was not used again for study purposes for over a century. Its next documented usage was in 1830 when Savart reported his use of a system similar to Hooke's which he developed while investigating the lower range of human hearing. Savart's specific contribution was to attach a tachometer to the axis of the toothed wheel to facilitate calibration of the tooth rate. Savart used his wheel as a practical alternative to John Robison's siren, which was also being adopted at the time by Charles Cagniard de la Tour to test the range of human hearing. By 1834 Savart was constructing brass wheels with a width of 82 cm, containing as many as 720 teeth. These wheels, which could produce frequencies up to 24 kHz, have been tentatively proposed as the first artificial generators of ultrasound.

Use in time perception experiments
In the later 19th century, Savart's wheel was adapted for use in physiological and psychological investigations of the human perception of time. In 1873, the Austrian physiologist Sigmund Exner reported the auditory ability to distinguish successive clicks from the wheel (or, alternatively, rapidly snapped electric sparks) at time intervals as close as 2 milliseconds (1/500 sec). A modified wheel that produced varying numbers of clicks at different intervals was later used by the American psychologists G. Stanley Hall and Joseph Jastrow, who in 1886 reported on the limits to the human perception of acoustic discontinuities. In 1894, French electrical engineer Gustave Trouvé adapted an upright piano to play a series of 88 variously-sized Savart wheels equipping it with an electrical drive system.

Modern use
Nowadays, Savart wheels are commonly used for demonstrations during physics lectures. In one variant, the wheel can be driven by an air hose blowing on the teeth; in this case, the pitch of the sound produced will vary with the force of the air current.
The concept has also been adapted to produce an experimental musical instrument created by Bart Hopkin. This application of Savart's wheel consists of a series of 30 wooden disks of increasing size mounted on a motorized axle. Rasping vibrations are induced in a plectrum when it comes into contact with the ridges that line each disk at regular intervals, and are amplified in a styrofoam cup which acts as a sounding board. The instrument is claimed to make ""the most obtrusive, obnoxious and irritating sound ever known.""

See also
Hammond organ
Singing bird box
Tonometer

Notes and references
Notes

References

External links
""Savart's Wheel"" – musical instrument designed by Bart Hopkin",Category:Acoustics,1
277,278,Structural acoustics,"Structural acoustics is the study of the mechanical waves in structures and how they interact with and radiate into adjacent media. The field of structural acoustics is often referred to as vibroacoustics in Europe and Asia. People that work in the field of structural acoustics are known as structural acousticians. The field of structural acoustics can be closely related to a number of other fields of acoustics including noise, transduction, underwater acoustics, and physical acoustics.

Vibrations in structures
Compressional and shear waves (isotropic, homogeneous material)
Compressional waves (often referred to as longitudinal waves) expand and contract in the same direction (or opposite) as the wave motion. The wave equation dictates the motion of the wave in the x direction.

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              u
            
            
              ?
              
                x
                
                  2
                
              
            
          
        
        =
        
          
            1
            
              c
              
                L
              
              
                2
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              u
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\partial ^{2}u \over \partial x^{2}}={1 \over c_{L}^{2}}{\partial ^{2}u \over \partial t^{2}}}
  
where 
  
    
      
        u
      
    
    {\displaystyle u}
   is the displacement and 
  
    
      
        
          c
          
            L
          
        
      
    
    {\displaystyle c_{L}}
   is the longitudinal wave speed. This has the same form as the acoustic wave equation in one-dimension. 
  
    
      
        
          c
          
            L
          
        
      
    
    {\displaystyle c_{L}}
   is determined by properties (bulk modulus 
  
    
      
        B
      
    
    {\displaystyle B}
   and density 
  
    
      
        ?
      
    
    {\displaystyle \rho }
  ) of the structure according to

  
    
      
        
          
            c
            
              L
            
          
        
        =
        
          
            
              B
              ?
            
          
        
      
    
    {\displaystyle {c_{L}}={\sqrt {B \over \rho }}}
  
When two dimensions of the structure are small with respect to wavelength (commonly called a beam), the wave speed is dictated by Youngs modulus 
  
    
      
        E
      
    
    {\displaystyle E}
   instead of the 
  
    
      
        B
      
    
    {\displaystyle B}
   and are consequently slower than in infinite media.
Shear waves occur due to the shear stiffness and follows a similar equation, but with the displacement occurring in the transverse direction, perpendicular to the wave motion.

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              w
            
            
              ?
              
                x
                
                  2
                
              
            
          
        
        =
        
          
            1
            
              c
              
                s
              
              
                2
              
            
          
        
        
          
            
              
                ?
                
                  2
                
              
              w
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\partial ^{2}w \over \partial x^{2}}={1 \over c_{s}^{2}}{\partial ^{2}w \over \partial t^{2}}}
  
The shear wave speed is governed by the shear modulus 
  
    
      
        G
      
    
    {\displaystyle G}
   which is less than 
  
    
      
        E
      
    
    {\displaystyle E}
   and 
  
    
      
        B
      
    
    {\displaystyle B}
  , making shear waves slower than longitudinal waves.

Bending waves in beams and plates
Most sound radiation is caused by bending (or flexural) waves, which deform the structure transversely as they propagate. Bending waves are more complicated than compressional or shear waves and depend on material properties as well as geometric properties. They are also dispersive since different frequencies travel at different speeds.

Modeling vibrations
Finite element analysis can be used to predict the vibration of complex structures. A finite element computer program will assemble the mass, stiffness, and damping matrices based on the element geometries and material properties, and solve for the vibration response based on the loads applied.

  
    
      
        
          [
          ?
          
            ?
            
              2
            
          
          
            
              M
            
          
          +
          j
          ?
          
            
              B
            
          
          +
          (
          1
          +
          j
          ?
          )
          
            
              K
            
          
          ]
        
        
          
            
              d
            
          
          =
          
            
              F
            
          
        
      
    
    {\displaystyle {[-\omega ^{2}{\mathbf {M}}+j\omega {\mathbf {B}}+(1+j\eta ){\mathbf {K}}]}{{\mathbf {d}}={\mathbf {F}}}}

Sound-structure interaction
Fluid-structure Interaction
When a vibrating structure is in contact with a fluid, the normal particle velocities at the interface must be conserved (i.e. be equivalent). This causes some of the energy from the structure to escape into the fluid, some of which radiates away as sound, some of which stays near the structure and does not radiate away. For most engineering applications, the numerical simulation of fluid-structure interactions involved in vibro-acoustics may be achieved by coupling the Finite element method and the Boundary element method.

See also
Acoustics
Acoustic wave equation
Lamb wave
Linear elasticity
Noise control
Sound
Surface acoustic wave
Wave
Wave equation

References
Fahy F., Gardonio P. (2007). Sound Structure Interaction (2nd ed.). Academic Press. pp. 60–61. ISBN 3-540-67458-6.

External links
asa.aip.org—Website of the Acoustical Society of America",Category:Acoustics,1
278,279,Phonautograph,"The phonautograph is the earliest known device for recording sound. Previously, tracings had been obtained of the sound-producing vibratory motions of tuning forks and other objects by physical contact with them, but not of actual sound waves as they propagated through air or other media. Invented by Frenchman Édouard-Léon Scott de Martinville, it was patented on March 25, 1857. It transcribed sound waves as undulations or other deviations in a line traced on smoke-blackened paper or glass. Intended solely as a laboratory instrument for the study of acoustics, it could be used to visually study and measure the amplitude envelopes and waveforms of speech and other sounds, or to determine the frequency of a given musical pitch by comparison with a simultaneously recorded reference frequency.
Apparently, it did not occur to anyone before the 1870s that the recordings, called phonautograms, contained enough information about the sound that they could, in theory, be used to recreate it. Because the phonautogram tracing was an insubstantial two-dimensional line, direct physical playback was impossible in any case.
Several phonautograms recorded before 1861 were successfully played as sound in 2008 by optically scanning them and using a computer to process the scans into digital audio files.

Construction
Édouard-Léon Scott de Martinville, a printer and bookseller by trade, was inspired when he happened to read about the anatomy of the human ear in the course of his business. His phonautograph was constructed as an analog of the ear canal, eardrum and ossicles. Scott created several variations of the device. The functions of the ear canal and eardrum were simulated by a funnel-like horn or a small open-ended barrel with a flexible membrane of parchment or other suitable material stretched over the small end. A pig bristle or other very lightweight stylus was connected to the membrane, sometimes by an indirect linkage which roughly simulated the ossicles and served as an amplifying lever. The bristle traced a line through a thin coating of lampblack—finely divided carbon deposited by the flame of an oil or gas lamp—on a moving surface of paper or glass. The sound collected by the simulated ear and transmitted to the bristle caused the line to be modulated in accordance with the passing variations in air pressure, creating a graphic record of the sound waves.
Martinville's first patent described a flat recording surface and a weight-driven clockwork motor, but the later and more familiar form of his invention, marketed by Rudolph Koenig in 1859, recorded on a sheet of lampblack-coated paper wrapped around a cylinder which was hand-cranked. The cylinder was carried on a coarsely threaded rod so that it progressed along its axis as it rotated, producing a helical tracing. The length of the recording that could be accommodated depended on the speed of rotation, which had to be rapid in order to resolve the individual waveforms of various sounds with good detail. If only longer-term dynamics such as the cadences of speech were being studied, the cylinder could be rotated much more slowly and a longer recording could be made. Some phonautographs included a tuning fork or other means of simultaneously recording a known reference frequency.
Several other inventors subsequently produced modified versions of the phonautograph and recorded the sound-modulated line by the use of various implements and in various formats, either in attempts to improve on Scott's apparatus or to adapt it to specific applications. In at least one instance, a complete return to the device's conceptual origins was made by employing the preserved parts of an actual human ear.

Playback
By mid-April 1877, Charles Cros had realized that a phonautograph recording could be converted back into sound by photoengraving the tracing into a metal surface to create a playable groove, then using a stylus and diaphragm similar to those of the phonautograph to reverse the recording process and recreate the sound. Before he was able to put his ideas into practice, the announcement of Thomas Edison's phonograph, which recorded sound waves by indenting them into a sheet of tinfoil from which they could be played back immediately, temporarily relegated Cros's less direct method to obscurity.
Ten years later, the early experiments of Emile Berliner, the creator of the disc Gramophone, employed a recording machine which was in essence a disc form of the phonautograph. It traced a clear sound-modulated spiral line through a thin black coating on a glass disc. The photoengraving method first proposed by Cros was then used to produce a metal disc with a playable groove. Arguably, these circa 1887 experiments by Berliner were the first known reproductions of sound from phonautograph recordings.
However, as far as is known, no attempt was ever made to use this method to play any of the surviving early phonautograms made by Scott de Martinville. Possibly this was because the few images of them generally available in books and periodicals were of unpromising short bursts of sound, of fragmentary areas of longer recordings, or simply too crude and indistinct to encourage such an experiment.
Nearly 150 years after they had been recorded, promising specimens of Scott de Martinville's phonautograms, stored among his papers in France's patent office and at the Académie des Sciences, were located by American audio historians. High-quality images of them were obtained. In 2008, the team played back the recordings as sound for the first time. Modern computer-based image processing methods were used to accomplish the playback. The first results were obtained by using a specialized system developed for optically playing recordings on more conventional media which were too fragile or damaged to be played by traditional means. Later, generally available image-editing and image-to-sound conversion software, requiring only a high-quality scan of the phonautogram and an ordinary personal computer, were found to be sufficient for this application.
No matter what hardware and software are used, the basic principle involved is relatively simple. If a greatly enlarged image of a segment of a phonautograph tracing were projected as a horizontally-oriented undulating line on a sheet of graph paper, a numerical description of the line could be created by proceeding from one grid column to the next, counting the number of squares between the line and a straight horizontal reference line, and making a list of the numbers. Such a list is, in fact, a digital audio file of the simplest kind. If entered into a computer in the required format and with the required file header information, it can be played as sound. Naturally, a computer needs no projector or graph paper to convert a scanned phonautogram into a playable digital audio file by comparable procedures.
One complication is that Scott de Martinville's phonautograms were recorded on machines which were hand-cranked rather than motor-driven, resulting in unsteady rotation of the cylinder. The irregular wavering of pitch caused by playing back such recordings at a constant rate of speed can make speech much more difficult to understand and has obvious dire effects on the reproduction of music. Fortunately, several phonautograms had a separate parallel track, inscribed simultaneously with the voice track, in which a constant reference tone had been recorded. By working with short segments of the paired tracks and adjusting both so that the reference tone was held to a steady pitch, it was possible to correct the irregularity and greatly improve the results.

Recovered sounds
One phonautogram, created on April 9, 1860, was revealed to be a 20-second recording of the French folk song ""Au clair de la lune"". Due to some confusing 1860 technical terminology, it was initially played at double the original recording speed and believed to be the voice of a woman or child. At the correct speed the voice of a man, almost certainly de Martinville himself, is heard singing the song very slowly. Also recovered were two 1860 recordings of ""Vole, petite abeille"" (""Fly, Little Bee""), a lively song from a comic opera. Previously, the earliest known recording of vocal music was an 1888 Edison wax cylinder phonograph recording of a Handel choral concert.
A phonautogram containing the opening lines of Torquato Tasso's pastoral drama Aminta has also been found. Probably recorded in April or May 1860, this phonautogram is the earliest known recording of intelligible spoken words to be played back, predating Frank Lambert's 1878 talking clock recording. Earlier recordings, made in 1857, 1854, and 1853, also contain Scott's voice but are unintelligible because of their low quality, brevity and irregularity of speed.

See also
History of sound recording

References
External links
FirstSounds.org, an informal collaborative aiming to make mankind's earliest sound recordings available to all people for all time.
The Phonautographic Manuscripts of Édouard-Léon Scott de Martinville (PDF), containing French and English texts of all Scott's known writings about the phonautograph from the 1850s and 1860s.

Further reading
Koenigsberg, Allen. The Birth of the Recording Industry, adapted from ""The Seventeen-Year Itch"", delivered at the U.S. Patent Office bi-centennial in Washington, D.C. on May 9, 1990, .",Category:Sound recording technology,1
279,280,Sound transmission class,"Sound Transmission Class (or STC) is an integer rating of how well a building partition attenuates airborne sound. In the USA, it is widely used to rate interior partitions, ceilings/floors, doors, windows and exterior wall configurations (see ASTM International Classification E413 and E90). Outside the USA, the Sound Reduction Index (SRI) ISO index or its related indices are used. As of 2012, these are defined in the ISO - 140 series of standards (under revision).

The STC rating figure very roughly reflects the decibel reduction in noise that a partition can provide.

Rating methodology
The ASTM sound transmission loss test methods have changed every few years. Thus, STC results posted before 1999 may not produce the same results today, and the differences become wider as one goes further back in time–the differences in the applicable test methods between the 1970s and today being quite significant.

The STC number is derived from sound attenuation values tested at sixteen standard frequencies from 125 Hz to 4000 Hz. These Transmission Loss values are then plotted on a sound pressure level graph and the resulting curve is compared to a standard reference contour. Acoustical engineers fit these values to the appropriate TL Curve (or Transmission Loss) to determine an STC rating. The measurement is accurate for speech sounds, but much less so for amplified music, mechanical equipment noise, transportation noise, or any sound with substantial low-frequency energy below 125 Hz. Sometimes, acoustical labs will measure TL at frequencies below the normal STC boundary of 125 Hz, possibly down to 50 Hz or lower, thus giving additional valuable data to evaluate transmission loss at very low frequencies, such as a subwoofer-rich home theater system would produce. Alternatively, Outdoor-Indoor Transmission Class (OITC) is a standard used for indicating the rate of transmission of sound between outdoor and indoor spaces in a structure that considers frequencies down to 80 Hz (Aircraft/Rail/Truck traffic) and is weighted more to lower frequencies.

Sound Isolation Techniques
The following sound isolation results and methodologies are presented with data that is measured within the standard frequency range specified by appropriate ASTM standards. Although it is worthwhile to discuss the utility of sound transmission loss data that lies outside the standard frequency range (especially in the low-frequency region), for simplicity results will be primarily be presented and discussed within these standard limitations.
Typical interior walls in homes (1 sheet of 1/2? (13 mm) gypsum wallboard (drywall) on either side of a 2x4 (90 mm) wood studs spaced 16"" (406 mm) on-center with fiberglass insulation filling each stud cavity) have an STC of about 33. When asked to rate their acoustical performance, people often describe these walls as ""paper thin."" They offer little in the way of privacy. Multi-family demising partition walls are typically constructed with varying gypsum wallboard panel layers attached to both sides of double 2x4 (90 mm) wood studs spaced 16"" (406 mm) on-center and separated by a 1"" (25 mm) airspace. These double-stud walls vary in sound isolation performance from the mid STC-40s into the high STC-60s depending on the presence of insulation and the gypsum wallboard type and quantity. Commercial buildings are typically constructed using steel studs of varying widths, gauges, and on-center spacings. Each of these framing characteristics have an effect on the sound isolation of the partition to varying degrees.

Sound Absorption
Adding absorptive materials to the interior surfaces of rooms (e.g. fabric-faced fiberglass panels, thick curtains) will result in a decrease of reverberated sound energy within the room. However, absorptive interior surface treatments do not significantly improve the sound isolation from one room to another through demising partitions over the typical frequency range measured currently. Installing absorptive insulation (e.g., fiberglass batts, blow-in cellulose, mineral fiber batts) into the wall or ceiling cavities affects the sound isolation of the partition to varying degrees, depending on the framing configuration and joist or stud depth. For example, the presence of type of insulation in single 2x4 wood stud framing spaced 16"" (406 mm) on-center results in only a few STC points. In contrast, adding standard fiberglass insulation to an otherwise empty cavity in light-gauge (25-gauge or lighter) steel stud partitions can result in a nearly 10 STC-point improvement. As the stud gauge becomes heavier, the presence and type of insulation matters less.

Mass
The effect of adding multiple layers of gypsum wallboard to a frame also varies depending on the framing type and configuration. Doubling the mass of a partition does not double the STC, as the STC is calculated from a non-linear decibel sound transmission loss measurement. So, whereas installing an additional layer of gypsum wallboard to a light-gauge (25-ga. or lighter) steel stud partition will result in about a 5 STC-point increase, doing the same on single wood or single heavy-gauge steel will result in only 2 to 3 additional STC points. Adding a second additional layer (to the already 3-layer system) does not result in as drastic an STC change as the first additional layer. The effect of additional gypsum wallboard layers on double- and staggered-stud partitions is similar to that of light-gauge steel partitions. Due to increased mass, poured concrete and concrete blocks typically achieve higher STC values (in the mid STC 40s to the mid STC 50s) than equally thick framed walls. However the additional weight, added complexity of construction, and poor thermal insulation tend to limit masonry wall partitions as a viable sound isolation solution in many building construction projects. Temperate climates and hurricane- or tornado-prone areas may, however, require the use of masonry walls for structural stability.

Decoupling
Structurally decoupling the gypsum wallboard panels from the partition framing can result in a large increase in sound isolation when installed correctly. Examples of structural decoupling in building construction include resilient channels, sound isolation clips and hat channels, and staggered- or double-stud framing. The STC results of decoupling in wall and ceiling assemblies varies significantly depending on the framing type, air cavity volume, and decoupling material type. Great care must be taken in each type of decoupled partition construction, as any fastener that becomes mechanically (rigidly) coupled to the framing can short-circuit the decoupling and result in drastically lower sound isolation results.

Damping
Sound damping tapes and other materials have been used to reduce both vibration and sound transmission through materials since the early 1930s. Although the applications of sound damping was largely limited to defense and industrial applications such as naval vessels and aircraft in the past, recent research has proven the effectiveness of damping in interior sound isolation in buildings. Constrained-layer damping gypsum wallboard panels increase sound isolation in building partitions by drastically reducing the vibration of panels and, incidentally, the radiation of sound through panels. The shear loading of a highly visco-elastic interlayer sandwiched between two more rigid constraining layers causes decreased displacement due to vibration, reducing the amount of sound energy radiated through a panel between enclosures. Damped gypsum wallboard panels are effective in reducing sound transmission over a broad range of frequencies and especially useful for achieving high levels of speech privacy between partitions.

Addressing Sound Flanking
Sound isolation metrics, such as the STC, are measured in specially-isolated and designed laboratory test chambers. It is important to note that there are nearly infinite field conditions that will affect sound isolation in situ when designing or remodeling building partitions and enclosures. Partitions that are inadequately or inappropriately sealed—that contain back-to-back electrical boxes, untreated recessed lighting, and unsealed pipes to name just a few—provide flanking paths for sound. Sound flanking paths include any sound transmission path other than the wall or ceiling partition itself. Great care and caution must be applied to any acoustically-treated building partition to ensure that the field sound isolation performance more closely approaches laboratory-tested values (see data from the National Research Council of Canada.)

Legal and practical requirements
Section 1207 of International Building Code 2006 states that separation between dwelling units and between dwelling units and public and service areas must achieve STC 50 (STC 45 if field tested) for both airborne and structure-borne. However, not all jurisdictions use the IBC 2006 for their building or municipal code. In jurisdictions where IBC 2006 is used, this requirement may not apply to all dwelling units. For example, a building conversion may not need to meet this rating for all walls.
In serious cases (e.g., a bedroom adjacent to a home theater room, and an inconsiderate nocturnal neighbor, to boot) a partition to reduce sounds from high-powered home theater or stereo should ideally be STC 70 or greater, and show good attenuation at low frequencies. An STC 70 wall can require detailed design and construction and can be easily compromised by 'flanking noise', sound traveling around the partition through the contiguous frame of the structure, thus reducing the STC significantly. STC 65 to 70 walls are often designed into luxury multifamily units, dedicated home theaters, and high end hotels.
STC partition ratings taken from: ""Noise Control in Buildings: A Practical Guide for Architects and Engineers""; Cyril M. Harris, 1994

See also
Architectural acoustics
Impact insulation class
Noise
Noise control
Sound Reduction Index
Soundproofing

External links
Article ""How Is Noise Tested""
STC - diracdelta.co.uk - examples and javascript calculation from 1/3 octave values

References
Notes

Bibliography
Cyril M. Harris. ""Noise Control in Buildings: A Practical Guide for Architects and Engineers"", 1994",Category:Acoustics,1
280,281,Acoustical Intelligence,"Acoustical Intelligence (ACOUSTINT, sometimes ACINT) is an intelligence gathering discipline that collects and processes acoustic phenomena. It is a subdiscipline of MASINT (Measurement and Signature Intelligence).
This uses broadband and narrowband analysis of acquired acoustic signatures from surface ships and submarines, although it can also be used for low-flying aircraft such as helicopters. Broadband analysis concerns the overall noise created by a platform, whereas narrowband analysis examines the spectra of the received energy at a more precise level.
Broadband analysis is useful for identifying any vessel at a long range, whereas narrowband analysis is generally more useful for identifying the category, type and ideally the individual vessel name. The category might be for example differentiating between a commercial vessel and a warship; the type might be narrowing this down to an individual class and hence identifying nationality, and the individual name might identify the specific ship or submarine.
As a simple example, narrowband analysis might identify whether a subject of interest has single or multiple propeller shafts; how many blades per shaft and other salients that may help identify the platform. This may include the fundamental or harmonic emissions based on the electric services used, the gearing between shaft and engine and also the combination of gear teeth used in the ratio(s). For nuclear vessels, necessary pump frequencies can be detected. It is possible for an expert to work out how many gears, teeth and ratios are used from engine right through to propeller; this can be used to identify a particular type. Sometimes, there are machining faults - however small - introduced at the manufacturing stage. This can help to identify individual platforms.

See also
Acoustic cryptanalysis


== References ==",Category:Acoustics,1
281,282,End correction,"An acoustic pipe, such as an organ pipe, marimba, flute, or Boomwhacker resonates at a specific pitch or frequency. Longer pipes resonate at lower frequencies, producing lower-pitched sounds. The details of acoustic resonance are taught in many elementary physics classes. In an ideal tube, the wavelength of the sound produced is directly proportional to the length of the tube. A tube which is open at one end and closed at the other produces sound with a wavelength equal to four times the length of the tube. A tube which is open at both ends produces sound whose wavelength is just twice the length of the tube. Thus, when a Boomwhacker with two open ends is capped at one end, the pitch produced by the tube goes down by one octave.
The analysis above applies only to an ideal tube, of zero diameter. When designing an organ or Boomwhacker, the diameter of the tube must be taken into account. In acoustics, end correction is a short distance applied or added to the actual length of a resonance pipe, in order to calculate the precise resonance frequency of the pipe. The pitch of a real tube is lower than the pitch predicted by the simple theory. A finite diameter pipe appears to be acoustically somewhat longer than its physical length.
A theoretical basis for computation of the end correction is the radiation acoustic impedance of a circular piston. This impedance represents the ratio of acoustic pressure at the piston, divided by the flow rate induced by it. The air speed is typically assumed to be uniform across the tube end. This is a good approximation, but not exactly true in reality, since air viscosity reduces the flow rate in the boundary layer very close to the tube surface. Thus, air column inside the tube is loaded by the external fluid due to sound energy radiation. This requires an additional length to be added to the regular length for calculating the natural frequency of the pipe system.
It is denoted by 
  
    
      
        ?
        L
      
    
    {\displaystyle \Delta L}
   and sometimes by 
  
    
      
        e
      
    
    {\displaystyle e}
   . In organ pipes, an antinode is not formed exactly at the open end. Rather, antinode is formed a little distance 
  
    
      
        ?
        L
      
    
    {\displaystyle \Delta L}
   away from open end outside it.
This 
  
    
      
        ?
        L
      
    
    {\displaystyle \Delta L}
   is known as end correction, which can be calculated as,
for a closed pipe (with one opening):

  
    
      
        ?
        L
        =
        0.6
        ?
        r
        =
        0.3
        ?
        D
      
    
    {\displaystyle \Delta L=0.6\cdot r=0.3\cdot D}
  ,
where 
  
    
      
        r
      
    
    {\displaystyle r}
   is the hydraulic radius of the neck and 
  
    
      
        D
      
    
    {\displaystyle D}
   is the hydraulic diameter of the neck;
and for an open pipe (with two openings):

  
    
      
        ?
        L
        =
        1.2
        ?
        r
        =
        0.6
        ?
        D
      
    
    {\displaystyle \Delta L=1.2\cdot r=0.6\cdot D}
  .
There is no scientifically proven and accepted value for the end correction of a resonant tube, various values ranging from 0.3r to 0.6r have been suggested from numerous disparate experiments. Lord Rayleigh was the first experimenter to publish a figure, in 1871: it was 0.3r.

Notes
Sources
Mouth Correction.
Velocity of Sound in Air - Resonance in Air Columns.",Category:Fluid dynamics,1
