,ID,Title,Text,Category,Group
0,1,Double bass array,"A double bass array (DBA) is a specific layout of subwoofers within a rectangular listening space. It removes unwanted room related resonances (modes) over a wide listening area.

Preface
A DBA requires at least two subwoofers (preferably same make and model) that are placed at opposing walls in a specific layout. The subwoofer array on the back wall is inverted and delayed based on the distance to the frontal subwoofer array. This will actively ""absorb"" any reflected sound.

Design
Array layout
Modes between side walls, floor and ceiling are suppressed by arranging the subwoofer in a specific grid:

  
    
      
        
          p
          
            x
          
        
        
          =
        
        
          
            
              (
              2
              ?
              n
              +
              1
              )
              ?
              
                w
                
                  x
                
              
            
            
              2
              ?
              
                a
                
                  x
                
              
            
          
        
      
    
    {\displaystyle p_{x}{=}{\frac {(2\cdot n+1)\cdot w_{x}}{2\cdot a_{x}}}}
  

  
    
      
        
          p
          
            y
          
        
        
          =
        
        
          
            
              (
              2
              ?
              n
              +
              1
              )
              ?
              
                w
                
                  y
                
              
            
            
              2
              ?
              
                a
                
                  y
                
              
            
          
        
      
    
    {\displaystyle p_{y}{=}{\frac {(2\cdot n+1)\cdot w_{y}}{2\cdot a_{y}}}}
  

  
    
      
        
          w
          
            x
          
        
      
    
    {\displaystyle w_{x}}
  : Wall width

  
    
      
        
          w
          
            y
          
        
      
    
    {\displaystyle w_{y}}
  : Wall height

  
    
      
        
          p
          
            x
          
        
      
    
    {\displaystyle p_{x}}
  : Horizontal distance from wall (
  
    
      
        
          d
          
            x
          
        
        
          /
        
        2
      
    
    {\displaystyle d_{x}/2}
   for n=0 in picture)

  
    
      
        
          p
          
            y
          
        
      
    
    {\displaystyle p_{y}}
  : Vertical distance from wall (
  
    
      
        
          d
          
            y
          
        
        
          /
        
        2
      
    
    {\displaystyle d_{y}/2}
   for n=0 in picture)

  
    
      
        
          a
          
            x
          
        
      
    
    {\displaystyle a_{x}}
  : Number of subwoofers horizontally

  
    
      
        
          a
          
            y
          
        
      
    
    {\displaystyle a_{y}}
  : Number of subwoofers vertically
Counter 
  
    
      
        n
        =
        
          0
          ,
          1
          ,
          2
          ,
          3
          ,
          .
          .
          .
          ,
          (
          a
          ?
          1
          )
        
      
    
    {\displaystyle n={0,1,2,3,...,(a-1)}}
  

With this specific grid layout reflections from the walls act as mirror sources. Interference of the subwoofers and mirror sources create a plane wave up to a certain frequency. The more sources the higher the frequency. This cutoff frequency 
  
    
      
        
          f
          
            c
          
        
      
    
    {\displaystyle f_{c}}
   can be calculated for each dimension as follows:

  
    
      
        
          f
          
            c
          
        
        
          =
        
        
          
            c
            
              2
              ?
              d
            
          
        
      
    
    {\displaystyle f_{c}{=}{\frac {c}{2\cdot d}}}
  

  
    
      
        c
      
    
    {\displaystyle c}
  : Speed of sound

  
    
      
        d
      
    
    {\displaystyle d}
  : Distance between subwoofers
Example: A 4 × 4 array on a wall measuring 4 × 3 m would work up to 86Hz horizontally and up to 114Hz vertically.

Active Absorption
Due to the specific grid layout of the subwoofer array most modal effects are suppressed. The length modes on the other hand get fully excited. The array on the back wall will emit a polarity inverted wave at the very same moment the wave from the front wall hits the back wall. Reflection and inverted wave will interfere with each other destructively so the reflection is canceled. This is also known as ""active absorption"".
The necessary delay 
  
    
      
        
          t
          
            d
          
        
      
    
    {\displaystyle t_{d}}
   of the back array is based on the length 
  
    
      
        l
      
    
    {\displaystyle l}
   of the room:

  
    
      
        
          t
          
            d
          
        
        
          =
        
        
          
            l
            c
          
        
      
    
    {\displaystyle t_{d}{=}{\frac {l}{c}}}
  

  
    
      
        c
      
    
    {\displaystyle c}
  : Speed of sound
The necessary delay required for a DBA is available in most digital equalizers or digital crossovers. Polarity of the back array can easily be achieved by inverting the signal coming from the amps or by swapping the cables going to the driver. Most active subwoofers also offer a polarity switch.

Pros
Sound pressure level throughout the room is very even.
Modal resonances are reduced to a minimum.
Subwoofers should be placed as close as possible on the wall where they easily can be made invisible.
No bulky acoustic room treatments or even structural modifications necessary.
Simple setup.

Cons
Rooms properties and furniture can interfere with the propagation of the plane wave.
Frequency range of the plane wave depends on the number of drivers which can be quite high in large rooms.
As the back wall array is only used to cancel the energy of the frontal array the overall efficiency isn't good.
SPL throughout the room is virtually constant which might affect tonal balance with satellite speakers that fall off with listening distance.

Notes
A DBA just requires two opposing walls so it's possible to have one array on the floor and the other on the ceiling or one on the left wall and the other on the right. Low frequencies become localizable at a specific frequency though so in most cases it's probably a good idea to use the front wall and the back wall.
It largely depends on the room how good a DBA will work.
The room should be rectangular
Walls should be rigid
Measuring equipment helps to dial in the necessary delay

Weblinks
Original white paper by Anselm Goertz in German (PDF-Datei; 1,42 MB)
DBA installations with measurements
Visualize modes within a rectangular room",Category:Audio engineering,3
1,2,Sonic interaction design,"Sonic interaction design is the study and exploitation of sound as one of the principal channels conveying information, meaning, and aesthetic/emotional qualities in interactive contexts. Sonic interaction design is at the intersection of interaction design and sound and music computing. If interaction design is about designing objects people interact with, and such interactions are facilitated by computational means, in sonic interaction design, sound is mediating interaction either as a display of processes or as an input medium.

Research areas
Perceptual, cognitive, and emotional study of sonic interactions
Research in this area focuses on experimental scientific findings about human sound reception in interactive contexts.
During closed-loop interactions, the users manipulate an interface that produces sound, and the sonic feedback affects in turn the users’ manipulation. In other words, there is a tight coupling between auditory perception and action. Listening to sounds might not only activate a representation of how the sound was made: it might also prepare the listener to react to the sound. Cognitive representations of sounds might be associated with action-planning schemas, and sounds can also unconsciously cue a further reaction on the part of the listener.
Sonic interactions have the potential to influence the users’ emotions: the quality of the sounds affects the pleasantness of the interaction, and the difficulty of the manipulation influences whether the user feels in control or not.

Product sound design
Product design in the context of sonic interaction design is dealing with methods and experiences for designing interactive products having a salient sonic behaviour. Products, in this context, are either tangible and functional objects that are designed to be manipulated, or usable simulations of such objects as in virtual prototyping. Research and development in this area relies on studies from other disciplines, such as:
product sound quality;
acoustic ecology, i.e. the relationship, mediated through sound, between living beings and their environment;
film sound;
Computer and video game sound;
sound culture, i.e. the study of how the production and consumption of sound have changed throughout history and within different societies.
In design research for sonic products a set of practices have been inherited from a variety of fields. Such practices have been tested in contexts where research and pedagogy naturally intermix. Among these practices it suffices to mention:
bodystorming, especially when combined with vocal sketching, where participants produce vocal imitations to mimic the sonic behavior of objects while they are being interacted with;
theatrical practices, such as theatrical metaphors and dramatic performance;
basic design, based on demonstrations and intersubjectivity;
video prototyping with sonic overlays;
Foley artistry in filmmaking;
acting out sound dramas.

Interactive art and music
In the context of sonic interaction design, interactive art and music projects are designing and researching aesthetic experiences where sonic interaction is in the focus. The creative and expressive aspects – the aesthetics – are more important than conveying information through sound. Practices include installations, performances, public art and interactions between humans through digitally-augmented objects/environments. These often integrate elements such as embedded technology, gesture-sensitive devices, speakers or context-aware systems.
The experience is in the focus, addressing how humans are affected by the sound, and vice versa. Interactive art and music allows us to question existing paradigms and models of how we interact with technology and sound, going beyond paradigms of control (human controlling a machine). Users are part of a loop which includes action and perception.
Interactive art and music projects invite explorative actions and playful engagement. There is also a multi-sensory aspect, especially haptic-audio and audio-visual projects are popular. Amongst many other influences, this field is informed by the development of the roles of instrument-maker, composer and performer merging.
Artistic research in sonic interaction design is about productions in the interactive arts and performing arts, exploiting the role of enactive engagement with sound–augmented interactive objects.

Sonification
Sonification is the data-dependent generation of sound, if the transformation is systematic, objective and reproducible, so that it can be used as scientific method.
For sonic interaction design, sonification provides a set of methods to create interaction sounds that encode relevant data, so that the user can perceive or interpret the conveyed information. Sonification does not necessarily need to represent huge amounts of data in sound, but may only convey one or few data values in a sound. To give an example, imagine a light switch that, on activation would create a short sound that depends on the electric power consumed through the cable: more energy-wasting lamps would perhaps systematically result in more annoying switch sounds. This example shows that sonification aims to provide some information by using its systematic transformation into sound.
The integration of data-driven elements in interaction sound may serve different purposes:
to allow the users to refine their actions via auditory feedback. Example: the sonification-enhanced drilling machine which indicates by sound when a wanted orientation to the wall is reached.
to create a sonic gestalt for the interaction which allows users to compare the detailed performance on repeated interactions: for instance rowing strokes may be sonified so that the sportsmen can better synchronize their action.
to enable novel functions that would otherwise be not available (e.g. a bottle that displays by sound how much fluid is poured into glasses so that the users can more easily fill the equal amount of liquid in different glasses).
Within the field of sonification, sonic interaction design acknowledges the importance of human interaction for understanding and using auditory feedback. Within sonic interaction design, sonification can help and offer solutions, methods, and techniques to inspire and guide the design of products or interactive systems.

See also
References
Further reading
Stefano Delle Monache, Pietro Polotti, Davide Rocchesso (2010). A Toolkit for Explorations in Sonic Interaction Design. In: Proceedings of the 5th Audio Mostly Conference: A Conference on Interaction with Sound, 2010, New York (AM '10), ISBN 978-1-4503-0046-9, doi:10.1145/1859799.1859800, citation
Eoin Brazil and Mikael Fernström, (2009). Empirically Based Auditory Display Design. In: Proceedings of the SMC 2009 - 6th Sound and Music Computing Conference, 23–25 July 2009, Porto, Portugal. Available: online.
Karmen Franinovi?, Yon Visell, Daniel Hug, (2007). Sound Embodied: A Report on Sonic Interaction Design in Everyday Artifacts. In: Proceedings of the 13th International Conference on Auditory Display, Montréal, Canada, June 26–29, 2007. Available: online.
Ernest A. Edmonds, Alastair Weakley, Linda Candy, Mark Fell, Roger Knott, and Sandra Pauletto, (2005). ""The Studio as Laboratory: Combining Creative Practice and Digital Technology Research"". International Journal of Man-Machine Studies 63(4–5): 452–481. Available: online.
Ernest Edmonds, Andrew Martin, and Sandra Pauletto, (2004). Audio-visual Interfaces in Digital Art. In: Proceedings of the 2004 ACM SIGCHI International Conference on Advances in computer entertainment technology (ACE '04), Singapore, June 3–5, 2004, pp. 331–336, doi:10.1145/1067343.1067392. Available: online.
Thomas Hermann and Andy Hunt, (2004). The Importance of Interaction in Sonification. In: Proceedings of ICAD Tenth Meeting of the International Conference on Auditory Display, Sydney, Australia, July 6–9, 2004. Available: online.
Niklas Röber and Maic Masuch, (2004). Interacting With Sound: An Interaction Paradigm for Virtual Auditory Worlds. In: Proceedings of ICAD Tenth Meeting of the International Conference on Auditory Display, Sydney, Australia, July 6–9, 2004. Available: online.


== External links ==",Category:Multimodal interaction,3
2,3,Sound energy density,"Sound energy density or sound density is the sound energy per unit volume. The SI unit of sound energy density is the pascal (Pa), that is the joule per cubic metre (J/m3) in SI based units.

Mathematical definition
Sound energy density, denoted w, is defined by

  
    
      
        w
        =
        
          
            
              p
              v
            
            c
          
        
      
    
    {\displaystyle w={\frac {pv}{c}}}
  
where
p is the sound pressure;
v is the particle velocity in the direction of propagation;
c is the speed of sound.
The terms instantaneous energy density, maximum energy density, and peak energy density have meanings analogous to the related terms used for sound pressure. In speaking of average energy density, it is necessary to distinguish between the space average (at a given instant) and the time average (at a given point).

Sound energy density level
The sound energy density level gives the ratio of a sound incidence as a sound energy value in comparison to a reference level of 0 dB (DIN 45630). It is a logarithmic measure of the ratio of two sound energy densities.
The energy produced by vibrations is known as sound

  
    
      
        L
        (
        E
        )
        =
        10
        
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                E
                
                  1
                
              
              
                E
                
                  0
                
              
            
          
          )
        
        
          
            d
            B
          
        
      
    
    {\displaystyle L(E)=10\,\log _{10}\left({\frac {E_{1}}{E_{0}}}\right){\rm {dB}}}
  
where E1 and E0 are the energy densities. The unit of the sound energy density level is the decibel (dB).
If E0 is the standard reference sound energy density of

  
    
      
        
          E
          
            0
          
        
        =
        
          10
          
            ?
            12
          
        
        
          
            J
            
              m
              
                3
              
            
          
        
      
    
    {\displaystyle E_{0}=10^{-12}\mathrm {\frac {J}{m^{3}}} }

See also
Particle velocity level
Sound intensity level

References
External links
Conversion: sound intensity to sound intensity level
Ohm's law as acoustic equivalent - calculations
Relationships of acoustic quantities associated with a plane progressive acoustic sound wave - pdf",Category:Physical quantities,3
3,4,Thin-film bulk acoustic resonator,"A thin-film bulk acoustic resonator (FBAR or TFBAR) is a device consisting of a piezoelectric material sandwiched between two electrodes and acoustically isolated from the surrounding medium. FBAR devices using piezoelectric films with thicknesses ranging from several micrometres down to tenth of micrometres resonate in the frequency range of roughly 100 MHz to 10 GHz. Aluminium nitride and zinc oxide are two common piezoelectric materials used in FBARs.

Applications
A common application of FBARs is radio frequency (RF) filters for use in cell phones and other wireless applications. Such filters made from a network of resonators (either in half-ladder, full-ladder, lattice or stacked topologies) and are designed to remove unwanted frequencies from being transmitted in such devices, while allowing other specific frequencies to be received and transmitted. FBAR filters can also be found in duplexers. They have partially replaced an earlier technology based on surface acoustic wave (SAW) devices, due to smaller size and increased fabrication and operating efficiencies.
FBARs can be used by microwave oscillators.
FBARs can be used in sensor applications. For instance, when a FBAR device is put under mechanical pressure its resonance frequency will shift.
FBARs can also be integrated with power amplifiers (PA) or low noise amplifiers (LNA) to form an integrated module solution such as PA-duplexer module (PAD), or LNA-filter module.

See also
Resonance
Acoustic resonance

External links
University of Southern California explanation on the operation of FBAR's
Avago Technology product listing for FBAR duplexers


== Notes ==",Category:Sound,3
4,5,Particle velocity,"Particle velocity is the velocity of a particle (real or imagined) in a medium as it transmits a wave. The SI unit of particle velocity is the metre per second (m/s). In many cases this is a longitudinal wave of pressure as with sound, but it can also be a transverse wave as with the vibration of a taut string.
When applied to a sound wave through a medium of a fluid like air, particle velocity would be the physical speed of a parcel of fluid as it moves back and forth in the direction the sound wave is travelling as it passes.
Particle velocity should not be confused with the speed of the wave as it passes through the medium, i.e. in the case of a sound wave, particle velocity is not the same as the speed of sound. The wave moves relatively fast, while the particles oscillate around their original position with a relatively small particle velocity. Particle velocity should also not be confused with the velocity of individual molecules.
In applications involving sound, the particle velocity is usually measured using a logarithmic decibel scale called particle velocity level. Mostly pressure sensors (microphones) are used to measure sound pressure which is then propagated to the velocity field using Green's function.

Mathematical definition
Particle velocity, denoted v, is defined by

  
    
      
        
          v
        
        =
        
          
            
              ?
              
                ?
              
            
            
              ?
              t
            
          
        
      
    
    {\displaystyle \mathbf {v} ={\frac {\partial \mathbf {\delta } }{\partial t}}}
  
where ? is the particle displacement.

Progressive sine waves
The particle displacement of a progressive sine wave is given by

  
    
      
        ?
        (
        
          r
        
        ,
        
        t
        )
        =
        
          ?
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            ?
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle \delta (\mathbf {r} ,\,t)=\delta _{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}),}
  
where
?m is the amplitude of the particle displacement;

  
    
      
        
          ?
          
            ?
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{\delta ,0}}
   is the phase shift of the particle displacement;
k is the angular wavevector;
? is the angular frequency.
It follows that the particle velocity and the sound pressure along the direction of propagation of the sound wave x are given by

  
    
      
        v
        (
        
          r
        
        ,
        
        t
        )
        =
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            k
          
          ?
          
            r
          
          ?
          ?
          t
          +
          
            ?
            
              ?
              ,
              0
            
          
          +
          
            
              ?
              2
            
          
          )
        
        =
        
          v
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            v
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle v(\mathbf {r} ,\,t)={\frac {\partial \delta }{\partial t}}(\mathbf {r} ,\,t)=\omega \delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=v_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{v,0}),}
  

  
    
      
        p
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        
          c
          
            2
          
        
        
          
            
              ?
              ?
            
            
              ?
              x
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          c
          
            2
          
        
        
          k
          
            x
          
        
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            k
          
          ?
          
            r
          
          ?
          ?
          t
          +
          
            ?
            
              ?
              ,
              0
            
          
          +
          
            
              ?
              2
            
          
          )
        
        =
        
          p
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            p
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle p(\mathbf {r} ,\,t)=-\rho c^{2}{\frac {\partial \delta }{\partial x}}(\mathbf {r} ,\,t)=\rho c^{2}k_{x}\delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=p_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{p,0}),}
  
where
vm is the amplitude of the particle velocity;

  
    
      
        
          ?
          
            v
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}}
   is the phase shift of the particle velocity;
pm is the amplitude of the acoustic pressure;

  
    
      
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{p,0}}
   is the phase shift of the acoustic pressure.
Taking the Laplace transforms of v and p with respect to time yields

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          v
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle {\hat {v}}(\mathbf {r} ,\,s)=v_{\mathrm {m} }{\frac {s\cos \varphi _{v,0}-\omega \sin \varphi _{v,0}}{s^{2}+\omega ^{2}}},}
  

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          p
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\hat {p}}(\mathbf {r} ,\,s)=p_{\mathrm {m} }{\frac {s\cos \varphi _{p,0}-\omega \sin \varphi _{p,0}}{s^{2}+\omega ^{2}}}.}
  
Since 
  
    
      
        
          ?
          
            v
            ,
            0
          
        
        =
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}=\varphi _{p,0}}
  , the amplitude of the specific acoustic impedance is given by

  
    
      
        
          z
          
            
              m
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          |
        
        z
        (
        
          r
        
        ,
        
        s
        )
        
          |
        
        =
        
          |
          
            
              
                
                  
                    
                      p
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
              
                
                  
                    
                      v
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
            
          
          |
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              v
              
                
                  m
                
              
            
          
        
        =
        
          
            
              ?
              
                c
                
                  2
                
              
              
                k
                
                  x
                
              
            
            ?
          
        
        .
      
    
    {\displaystyle z_{\mathrm {m} }(\mathbf {r} ,\,s)=|z(\mathbf {r} ,\,s)|=\left|{\frac {{\hat {p}}(\mathbf {r} ,\,s)}{{\hat {v}}(\mathbf {r} ,\,s)}}\right|={\frac {p_{\mathrm {m} }}{v_{\mathrm {m} }}}={\frac {\rho c^{2}k_{x}}{\omega }}.}
  
Consequently, the amplitude of the particle velocity is related to those of the particle displacement and the sound pressure by

  
    
      
        
          v
          
            
              m
            
          
        
        =
        ?
        
          ?
          
            
              m
            
          
        
        ,
      
    
    {\displaystyle v_{\mathrm {m} }=\omega \delta _{\mathrm {m} },}
  

  
    
      
        
          v
          
            
              m
            
          
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              
                z
                
                  
                    m
                  
                
              
              (
              
                r
              
              ,
              
              s
              )
            
          
        
        .
      
    
    {\displaystyle v_{\mathrm {m} }={\frac {p_{\mathrm {m} }}{z_{\mathrm {m} }(\mathbf {r} ,\,s)}}.}

Particle velocity level
Sound velocity level (SVL) or acoustic velocity level or particle velocity level is a logarithmic measure of the effective particle velocity of a sound relative to a reference value.
Sound velocity level, denoted Lv and measured in dB, is defined by

  
    
      
        
          L
          
            v
          
        
        =
        ln
        
        
          (
          
            
              v
              
                v
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        2
        
          log
          
            10
          
        
        
        
          (
          
            
              v
              
                v
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              v
              
                v
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{v}=\ln \!\left({\frac {v}{v_{0}}}\right)\!~\mathrm {Np} =2\log _{10}\!\left({\frac {v}{v_{0}}}\right)\!~\mathrm {B} =20\log _{10}\!\left({\frac {v}{v_{0}}}\right)\!~\mathrm {dB} ,}
  
where
v is the root mean square particle velocity;
v0 is the reference particle velocity;
1 Np = 1 is the neper;
1 B = 1/2 ln 10 is the bel;
1 dB = 1/20 ln 10 is the decibel.
The commonly used reference particle velocity in air is

  
    
      
        
          v
          
            0
          
        
        =
        5
        ×
        
          10
          
            ?
            8
          
        
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle v_{0}=5\times 10^{-8}~\mathrm {m/s} .}
  
The proper notations for sound velocity level using this reference are Lv/(5 × 10?8 m/s) or Lv (re 5 × 10?8 m/s), but the notations dB SVL, dB(SVL), dBSVL, or dBSVL are very common, even if they are not accepted by the SI.

See also
Sound
Sound particle
Particle displacement
Particle acceleration

References
External links
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
The particle Velocity Can Be Directly Measured with a Microflown
Acoustic Particle-Image Velocimetry. Development and Applications",Category:Physical quantities,3
5,6,Digital recording,"In digital recording, audio signals picked up by a microphone or other transducer or video signals picked up by a camera or similar device are converted into a stream of discrete numbers, representing the changes over time in air pressure for audio, and chroma and luminance values for video, then recorded to a storage device. To play back a digital sound recording, the numbers are retrieved and converted back into their original analog waveforms so that they can be heard through a loudspeaker. To play back a digital video recording, the numbers are retrieved and converted back into their original analog waveforms so that they can be viewed on a video monitor, television or other display.

Timeline
October 3, 1938: British scientist Alec Harley Reeves files to the France Patent Office the first patent describing describing the technique known today as Pulse-code modulation (PCM). Later, Reeves filed in USA this patent in November 22, 1939.  It was first developed as a telephony technology.
1943: Bell Telephone Laboratories develops the first PCM-based digital scrambled speech transmission system, SIGSALY, in response to German interception of military telephone traffic during World War II. The twelve transmission points were retired after the war.
1957: Max Mathews of Bell develops the process to digitally record sound via computer.
1967: the first monaural PCM recorder was developed by NHK's research facilities in Japan. The 30 kHz 12-bit device used a compander (similar to DBX Noise Reduction) to extend the dynamic range, and stored the signals on a video tape recorder.
1969: NHK expands the PCM's capabilities to 2-channel stereo and 32 kHz 13-bit resolution. 
1970: American inventor James Russell patents the first digital-to-optical recording and playback system, which would later lead to the Compact Disc.
January 1971: Using NHK'S PCM recording system, engineers at Denon record the first commercial digital recordings, Something by Steve Marcus and The World Of Stomu Yamashita by Stomu Yamashta. 
1972: Denon unveils the first 8-channel digital recorder, the DN-023R, which is 47.25 kHz 13-bit PCM resolution using a 4-head open reel broadcast video tape recorder. The first recording with this new system is the Smetana Quartet performing Mozart's String Quartets K.458 and K.421, recorded in Tokyo April 24–26. Several other digitally recorded LPs follow.
1975: University of Utah professor Thomas Stockham develops a PCM digital audio recorder of his own design, using computer tape drives as the storage system. He founded the company Soundstream to offer it commercially.
1976: the prototype Soundstream 37.5 kHz, 16-bit, two channel recorder is used to record the Santa Fe Opera performing Virgil Thomson's opera The Mother of Us All for New World Records. However, the digital recorder is just a backup to the main analog multi-track recorder and the superior analog recording is used for the release by New World Records. The digital tape was presented at the 1976 AES Convention in New York, but never commercially released.
1977: Denon develops the smaller portable PCM recording system, the DN-034R. Like the DN-023R it records 8 channels at 47.25 kHz, but it uses 14-bits ""with emphasis, making it equivalent to 15.5 bits.""
August 28–31, 1977: Soundstream's PCM system runs in the background of a California direct to disc recording session by organist Virgil Fox for Crystal Records. When initially released the resulting LPs were pressed from the direct-to-disc acetate, though the later CD reissue (1987) comes from the digital backup tapes. The CD reissue was made by Bainbridge Records.
November 28, 1977: Denon brings their DN-034R to New York and records Archie Shepp's On Green Dolphin Street, making it America's first released digitally-recorded commercial album. When this is released on CD in 1984 by Nippon Columbia it also becomes one of the earliest digital-only CDs. Six other jazz albums are recorded with the DN-034R in New York before it returns to Japan in December.
April 4–5, 1978: Telarc uses Soundstream's PCM system to record Frederick Fennell and his Eastman Wind Ensemble playing Gustav Holst's Suites for Military Band and George Frideric Handel's Music for the Royal Fireworks. When released on LP this became the first US digitally-recorded classical release.
June 2, 1978: Sound 80 studios in Minneapolis records the Saint Paul Chamber Orchestra performing Aaron Copland's Appalachian Spring. This session is set up as a direct to disc recording, with an experimental 3M 50.4 kHz digital recorder in the background capturing the session. It is released on LP record as Sound80 Records S80-DLR-101 although possibly this release is taken from the direct-to-disc acetate rather than the digital backup. Later the session is re-released on Compact Disc by ProArte.
June 1978: Sound 80 records Flim and the BB's as another direct to disc recording again with the experimental 3M recorder in the background. This time the acetate is deemed not as good as the digital backup, so the digital master is used for the LP record (Sound80 Records S80-DLR-102). This makes it the first U.S. non-classical digital release. Within 6 months the hand-built 3M digital recorder is disassembled, rendering the non-standard master tape unplayable. Therefore, no Compact Disc reissue is possible.
March 8, 1979: the first Compact Disc prototype was demonstrated by Philips in Eindhoven with the respective player nicknamed ""Pinkeltje"". 
July 11, 1979: the first U.S.-recorded digital album of popular music (with vocals), Bop 'Til You Drop by guitarist Ry Cooder, was released by Warner Bros. Records. The album was recorded in Los Angeles on a 32-track digital machine built by the 3M corporation.  Also, Stevie Wonder digitally recorded his soundtrack album, Journey Through the Secret Life of Plants, three months after Cooder's album was released, followed by the Grammy-award self-titled debut album of American singer Christopher Cross. Cross' album is the first digitally recorded album to chart in the US (coincidentally also winning 5 Grammys).
1982: the first digital compact discs are marketed by Sony and Philips, and New England Digital offers the hard disk recorder (Sample-to-Disk) option on the Synclavier, the first commercial hard disk (HDD) recording system. Also that same year, Peter Gabriel releases, Security and The Nightfly released by Donald Fagen, which both were the early full digital recordings.
1984: Sony released the Sony PCM-501ES digital audio processor, which for the first time allowed consumers to make their own digital recordings, using a VHS or Betamax video tape recorder as the storage media.
1987: Sony develops Digital Audio Tape.
1990: digital radio begins in Canada, using the L-Band.
1991: Alesis Digital Audio Tape or ADAT is a tape format used for simultaneously recording eight tracks of digital audio at once, onto Super VHS magnetic tape – a format similar to that used by consumer VCRs. The product was announced in January 1991 at the NAMM convention in Anaheim, California. The first ADAT recorders shipped over a year later in February or March 1992.
1993: RADAR (audio recorder) Random Access Digital Audio Recorder or RADAR is the first single box device used for simultaneously recording 24 tracks of digital audio at once, onto hard disk drives. The product, manufactured by Creation Technologies (iZ Technology Corporation) was announced in October 1993 at the AES convention in New York, New York. The first RADAR recorders shipped in August 1994.
1996: optical discs and DVD players begin selling in Japan.

Process
Recording
The analog signal is transmitted from the input device to an analog-to-digital converter (ADC).
The ADC converts this signal by repeatedly measuring the momentary level of the analog (audio) wave and then assigning a binary number with a given quantity of bits (word length) to each measuring point.
The frequency at which the ADC measures the level of the analog wave is called the sample rate or sampling rate.
A digital audio sample with a given word length represents the audio level at one moment.
The longer the word length the more precise the representation of the original audio wave level.
The higher the sampling rate the higher the upper audio frequency of the digitized audio signal.
The ADC outputs a sequence of digital audio samples that make up a continuous stream of 0s and 1s.
These binary numbers are stored on recording media such as a hard drive, optical drive or in solid state memory.
Playback
The sequence of numbers is transmitted from storage into a digital-to-analog converter (DAC), which converts the numbers back to an analog signal by sticking together the level information stored in each digital sample, thus rebuilding the original analog wave form.
This signal is amplified and transmitted to the loudspeakers or video screen.

Recording of bits
Even after getting the signal converted to bits, it is still difficult to record; the hardest part is finding a scheme that can record the bits fast enough to keep up with the signal. For example, to record two channels of audio at 44.1 kHz sample rate with a 16 bit word size, the recording software has to handle 1,411,200 bits per second.

Techniques to record to commercial media
For digital cassettes, the read/write head moves as well as the tape in order to maintain a high enough speed to keep the bits at a manageable size.
For optical disc recording technologies such as CDs or DVDs, a laser is used to burn microscopic holes into the dye layer of the medium. A weaker laser is used to read these signals. This works because the metallic substrate of the disc is reflective, and the unburned dye prevents reflection while the holes in the dye permit it, allowing digital data to be represented.

Concerns with digital audio recording
Word size
The number of bits used to represent a sampled audio wave (the word size) directly affects the resulting noise in a recording after intentionally added dither, or the distortion of an undithered signal.
The number of possible voltage levels at the output is simply the number of levels that may be represented by the largest possible digital number (the number 2 raised to the power of the number of bits in each sample). There are no “in between” values allowed. If there are more bits in each sample the waveform is more accurately traced, because each additional bit doubles the number of possible values. The distortion is roughly the percentage that the least significant bit represents out of the average value. Distortion (as a percentage) in digital systems increases as signal levels decrease, which is the opposite of the behavior of analog systems.

Sample rate
The sample rate is just as important a consideration as the word size. If the sample rate is too low, the sampled signal cannot be reconstructed to the original sound signal.
To overcome aliasing, the sound signal (or other signal) must be sampled at a rate at least twice that of the highest frequency component in the signal. This is known as the Nyquist-Shannon sampling theorem.
For recording music-quality audio the following PCM sampling rates are the most common: 44.1, 48, 88.2, 96, 176.4, and 192 kHz.
When making a recording, experienced audio recording and mastering engineers will normally do a master recording at a higher sampling rate (i.e. 88.2, 96, 176.4 or 192 kHz) and then do any editing or mixing at that same higher frequency. High resolution PCM recordings have been released on DVD-Audio (also known as DVD-A), DAD (Digital Audio Disc—which utilizes the stereo PCM audio tracks of a regular DVD), DualDisc (utilizing the DVD-Audio layer), or Blu-ray (Profile 3.0 is the Blu-ray audio standard, although as of mid-2009 it is unclear whether this will ever really be used as an audio-only format). In addition it is nowadays also possible and common to release a high resolution recording directly as either an uncompressed WAV or lossless compressed FLAC file (usually at 24 bits) without down-converting it.
However, if a CD (the CD Red Book standard is 44.1 kHz 16 bit) is to be made from a recording, then doing the initial recording using a sampling rate of 44.1 kHz is obviously one approach. Another approach that is usually preferred is to use a higher sample rate and then downsample to the final format's sample rate. This is usually done as part of the mastering process. One advantage to the latter approach is that way a high resolution recording can be released, as well as a CD and/or lossy compressed file such as mp3—all from the same master recording.
Beginning in the 1980s, music that was recorded, mixed and mastered digitally was often labelled using the SPARS code to describe which processes were analog and which were digital.

Error rectification
One of the advantages of digital recording over analog recording is its resistance to errors.

See also
Compact discs use Reed-Solomon error correction
Cyclic redundancy check (CRC)
Digital audio workstation
Direct to disk recording
Magnetic storage
Multitrack recording
Parity Computation
Many bits are stored on RAID storage systems
4D Audio Recording system


== References ==",Category:Articles with unsourced statements from December 2015,3
6,7,AES3,"AES3 (also known as AES/EBU) is a standard for the exchange of digital audio signals between professional audio devices. AES3 was jointly developed by the Audio Engineering Society (AES) and the European Broadcasting Union (EBU). An AES3 signal can carry two channels of PCM audio over several transmission media including balanced lines, unbalanced lines, and optical fiber. The standard was first published in 1985 and has been revised in 1992 and 2003.
AES3 has been incorporated into the International Electrotechnical Commission's standard IEC 60958, and is available in a consumer-grade variant known as S/PDIF.

History and development
The development of standards for digitising analog audio, as used to interconnect both professional and domestic audio equipment, began in the late 1970s in a joint effort between the Audio Engineering Society and the European Broadcasting Union, and culminated in the publishing of AES3 in 1985. Early on, the standard was frequently known as AES/EBU. Both AES and EBU versions of the standard exist. Variants using different physical connections—essentially consumer versions of AES3 for use within the domestic ""Hi-Fi"" environment using connectors more commonly found in the consumer market—are specified in IEC 60958. These variants are commonly known as S/PDIF.
The standard has been revised in 1992 and 2003 and is published in AES and EBU versions. Worldwide, it is the most commonly used method for digitally interconnecting audio equipment.

Hardware connections
The AES3 standard parallels part 4 of the international standard IEC 60958. Of the physical interconnection types defined by IEC 60958, three are in common use.

IEC 60958 Type I—Balanced, XLR
Type I connections use balanced, 3-conductor, 110-ohm twisted pair cabling with XLR connectors. Type I connections are most often used in professional installations and are considered the AES3 standard connector. The hardware interface is usually implemented using RS-422 line drivers and receivers.

IEC 60958 Type II—Unbalanced, RCA
Type II connections use unbalanced, 2-conductor, 75-ohm coaxial cable with RCA connectors. Type II connections are used in most often in consumer audio installations and are often called coaxial S/PDIF connections.

IEC 60958 Type III Optical—Fiber, F05/TOSLINK
Type III Optical connections use optical fiber—usually plastic, but occasionally glass—with F05 connectors, which are more commonly known by their Toshiba brand name, TOSLINK. Like Type II, Type III Optical connections are also used in consumer audio installations and are often called optical S/PDIF connections.

Other connections
The AES-3id standard defines a 75-ohm BNC electrical variant of AES3. This uses the same cabling, patching and infrastructure as analogue or digital video, and is thus common in the broadcast industry.
AES3 digital audio format can also be carried over an Asynchronous Transfer Mode network. The standard for packing AES3 frames into ATM cells is AES47.
For information on the synchronization of digital audio structures, see the AES11 standard. The ability to insert unique identifiers into an AES3 bit stream is covered by the AES52 standard.

Relation to S/PDIF
The precursor of the IEC 60958 Type II specification was the Sony/Philips Digital Interface, or S/PDIF. S/PDIF and AES3 are similar in many ways and are interchangeable at the protocol level, but at the physical level they specify different electrical signaling levels and impedances, which may be significant in some applications.

Protocol
The low-level protocol for data transmission in AES3 and S/PDIF is largely identical, and the following discussion applies for S/PDIF, except as noted.
AES3 was designed primarily to support stereo PCM encoded audio in either DAT format at 48 kHz or CD format at 44.1 kHz. No attempt was made to use a carrier able to support both rates; instead, AES3 allows the data to be run at any rate, and encoding the clock and the data together using biphase mark code (BMC).
Each bit occupies one time slot.
Each audio sample (of up to 24 bits) is combined with four flag bits and a synchronisation preamble which is four time slots long to make a subframe of 32 time slots.
Two subframes (A and B, normally used for left and right audio channels) make a frame. Frames contain 64 time slots and are produced once per sample time. This determines the clock rate.
At the highest level, each 192 consecutive frames are grouped into an audio block. While samples repeat each frame time, metadata is only transmitted once per audio block.
At the default 48 kHz sample rate, there are 250 audio blocks per second, and 3,072 kilobits per second with a biphase clock of 6.144 MHz 
The 32 time slots of each subframe are assigned as follows:

Synchronisation preamble
This is a specially coded preamble that identify the subframe and its position within the audio block. They are not normal BMC-encoded data bits, although they do still have zero DC bias.
Three preambles are possible :
X (or M) : 111000102 if previous time slot was 0, 000111012 if it was 1. (Equivalently, 100100112 NRZI encoded.) Marks a word for channel A (left), other than at the start of an audio block.
Y (or W) : 111001002 if previous time slot was 0, 000110112 if it was 1. (Equivalently, 100101102 NRZI encoded.) Marks a word for channel B (right).
Z (or B) : 111010002 if previous time slot was 0, 000101112 if it was 1. (Equivalently, 100111002 NRZI encoded.) Marks a word for channel A (left) at the start of an audio block.
They are called X, Y, Z in the AES3 standard; and M, W, B in IEC 958 (an AES extension).
The 8-bit preambles are transmitted in time allocated to the first four time slots of each subframe (time slots 0 to 3). Any of the three marks the beginning of a subframe. X or Z marks the beginning of a frame, and Z marks the beginning of an audio block.

 | 0 | 1 | 2 | 3 |  | 0 | 1 | 2 | 3 | Time slots
  _____       _            _____   _
 /     \_____/ \_/  \_____/     \_/ \ Preamble X
  _____     _              ___   ___
 /     \___/ \___/  \_____/   \_/   \ Preamble Y
  _____   _                _   _____
 /     \_/ \_____/  \_____/ \_/     \ Preamble Z
  ___     ___            ___     ___ 
 /   \___/   \___/  \___/   \___/   \ All 0 bits BMC encoded
  _   _   _   _        _   _   _   _
 / \_/ \_/ \_/ \_/  \_/ \_/ \_/ \_/ \ All 1 bits BMC encoded
 
 | 0 | 1 | 2 | 3 |  | 0 | 1 | 2 | 3 | Time slots

In two-channel AES3, the preambles form a pattern of ZYXYXYXY…, but it is straightforward to extend this structure to additional channels (more subframes per frame), each with a Y preamble, as is done in the MADI protocol.

Channel status word
As stated before there is one channel status bit in each subframe, making one 192 bit word for each channel in each block. This 192 bit word is usually presented as 192/8 = 24 bytes. The contents of the channel status word are completely different between the AES3 and S/PDIF standards, although they agree that the first channel status bit (byte 0 bit 0) distinguishes between the two. In the case of AES3, the standard describes in detail how the bits have to be used. Here is a summary of the channel status word:
byte 0: basic control data: sample rate, compression, emphasis
bit 0: A value of 1 indicates this is AES3 channel status data. 0 indicates this is S/PDIF data.
bit 1: A value of 0 indicates this is linear audio PCM data. A value of 1 indicates other (usually non-audio) data.
bits 2–4: Indicates the type of signal preemphasis applied to the data. Generally set to 1002 (none).
bit 5: A value of 0 indicates that the source is locked to some (unspecified) external time sync. A value of 1 indicates an unlocked source.
Bits 6–7: Sample rate. These bits are redundant when real-time audio is transmitted (the receiver can observe the sample rate directly), but are useful if AES3 data is recorded or otherwise stored. Options are unspecified, 48 kHz (the default), 44.1 kHz, and 32 kHz.

byte 1: indicates if the audio stream is stereo, mono or some other combination.
bits 0–3: Indicates the relationship of the two channels; they might be unrelated audio data, a stereo pair, duplicated mono data, music and voice commentary, a stereo sum/difference code.
bits 4–7: Used to indicate the format of the user channel word.

byte 2: audio word length
bits 0–2: Aux bits usage. This indicates how the aux bits (time slots 4–7) are used. Generally set to 0002 (unused) or 0012 (used for 24-bit audio data).
bits 3–5: Word length. Specifies the sample size, relative to the 20- or 24-bit maximum. Can specify 0, 1, 2 or 4 missing bits. Unused bits are filled with 0, but audio processing functions such as mixing will generally fill them in with valid data without changing the effective word length.
bits 6–7: Unused

byte 3: used only for multichannel applications
byte 4: Additional sample rate information.
bits 0–1: indicate the grade of the sample rate reference, per AES11.
bit 2: reserved
bits 3–6: Extended sample rate. This indicates other sample rates, not representable in byte 0 bits 6–7. Values are assigned for 24, 96, and 192 kHz, as well as 22.05, 88.2, and 176.4 kHz.
bit 7: This ""sampling frequency scaling flag"", if set, indicates that the sample rate is multiplied by 1/1.001 to match NTSC video frame rates.

byte 5: reserved
bytes 6–9: Four ASCII characters for indicating channel origin. Widely used in large studios.
bytes 10–13: Four ASCII characters indicating channel destination, to control automatic switchers. Less often used.
bytes 14–17: 32-bit sample address, incrementing block-to-block by 192 (because there are 192 frames per block). At 48 kHz, this wraps every 24h51m18.485333s.
bytes 18–21: as above, but offset to indicate samples since midnight.
byte 22: contains information about the reliability of the channel status word.
bits 0–3: reserved
bit 4: if set, bytes 0–5 (signal format) are unreliable.
bit 5: if set, bytes 6–13 (channel labels) are unreliable.
bit 6: if set, bytes 14–17 (sample address) are unreliable.
bit 7: if set, bytes 18–21 (timestamp) are unreliable.

byte 23: CRC. This byte is used to detect corruption of the channel status word, as might be caused by switching mid-block. (Generator polynomial is x8+x4+x3+x2+1, preset to 1.)

Embedded timecode
SMPTE timecode timestamp data can be embedded within AES3 digital audio signals. It can be used for synchronization and for logging and identifying audio content. According to John Ratcliff's Timecode: A user's guide, it is embedded as a 32-bit binary word in bytes 18 to 21 of the channel status data.

See also
ADAT Lightpipe
AES-2id
MADI, an extension with many more channels.

References
Further reading
European Broadcasting Union, Specification of the Digital Audio Interface (The AES/EBU interface) Tech 3250-E third edition (2004)
Watkinson, John (2001). The Art of Digital Audio Third Edition. Focal Press. ISBN 0-240-51587-0. 
Watkinson, John (August 1989). ""The AES/EBU Digital Audio Interface"". UK Conference: AES/EBU Interface. EBU-02. 
Emmett, John (1995). ""Engineering Guidelines: The EBU/AES Digital Audio Interface"" (PDF). EBU. 
AES-2id-2006: AES information document for digital audio engineering — Guidelines for the use of the AES3 interface. (Downloaded from the AES standards web site; see external links)
Mark Yonge (June–July 2005). ""AES3 Channel Status Revisited"" (PDF). Line Up (101): 20–22. Retrieved 2013-09-01. 
""AES3 / AES-EBU channel status byte settings"".

External links
Download page for AES standards",Category:Sound,3
7,8,Category:Ultrasound,,Category:Sound,3
8,9,Test CD,"A Test CD usually refers to a compact disc containing tracks of musical and technical tests and demonstrations. Most of the tracks are made of electronic signals and pure frequencies. The purpose of these specialised compact discs is to make accurate tests and calibrate audio equipment.

See also
Audio equipment testing
Audiophile
High fidelity
Super Audio CD

Surround Sound",Category:Surround sound,3
9,10,Comic sound,,Category:All articles lacking sources,3
10,11,Category:Sound production,,Category:Sound,3
11,12,Power bandwidth,"The power bandwidth of an amplifier is sometimes taken as the frequency range (or, rarely, the upper frequency limit) for which the rated power output of an amplifier can be maintained (without excessive distortion) to at least half of the full rated power. (Some specifications may mandate 100% of the rated power; sometimes referring to as the full-power bandwidth.)
It should not be confused with ""half-power"" bandwidth only used in conjunction with filter frequency response curves, where it refers to -3dB points in the frequency response of a band-pass filter.
Operational amplifiers often use the term (full-) power bandwidth to refer to the frequency limit for the peak output voltage rather than power remaining up to the limit given for DC output voltage capability.

Specifying power bandwidth
Power bandwidth may be specified as a frequency range or as a graph.


== References ==",Category:Audio amplifier specifications,3
12,13,Decibel watt,"The decibel watt or dBW is a unit for the measurement of the strength of a signal expressed in decibels relative to one watt. It is used because of its capability to express both very large and very small values of power in a short range of number; e.g., 1 milliwatt = ?30 dBW, 1 watt = 0 dBW, 10 watts = 10 dBW, 100 watts = 20 dBW, and 1,000,000 W = 60 dBW.

  
    
      
        
          Power in dBW
        
        =
        10
        
          log
          
            10
          
        
        ?
        
          
            Power
            
              1
              
                W
              
            
          
        
      
    
    {\displaystyle {\mbox{Power in dBW}}=10\log _{10}{\frac {\mbox{Power}}{1\mathrm {W} }}}
  
and also

  
    
      
        
          Power in W
        
        =
        
          10
          
            
              Power in dBW
              10
            
          
        
      
    
    {\displaystyle {\mbox{Power in W}}=10^{\frac {\mbox{Power in dBW}}{10}}}
  
Compare dBW to dBm, which is referenced to one milliwatt (0.001 W).
A given dBW value expressed in dBm is always 30 more because 1 watt is 1,000 milliwatts, and a ratio of 1,000 (in power) is 30 dB; e.g., 10 dBm (10 mW) is equal to ?20 dBW (0.01 W).
Although the decibel (dB) is permitted for use alongside SI units, the dBW is not.

See
dBm",Category:Sound,3
13,14,A3D,"A3D (Aureal 3-Dimensional) was a technology developed by Aureal Semiconductor for use in their Vortex line of PC sound chips to deliver three-dimensional sound through headphones, two or even four speakers. The technology used head-related transfer functions (HRTF), which the human ear interprets as spatial cues indicating the location of a particular sound source. Many modern sound cards and PC games incorporated A3D via license from Aureal. Due to Aureal's acquisition (see below) the A3D technology is now part of the intellectual property of Creative Labs.
A3D differs from various forms of discrete positional audio in that it only requires two speakers, while surround sound typically requires more than four. The particular advantage of A3D is for dynamic or interactive environments such as simulations, games, video conference, and remote learning. A3D is not as effective for static productions such as movies which typically employ surround sound.
A3D uses a subset of the actual in-game 3D world data to accurately model the location of both direct (A3Dspace) and reflected (A3Dverb) sound streams (A3D 2.0 can perform up to 60 first-order reflections). EAX 1.0, the competing technology at the time promoted by Creative Labs, simulated the environment with an adjustable reverb—it didn't calculate any actual reflections off the 3D surfaces.
Creative Labs sued Aureal for patent infringement in March 1998 [1], and Aureal countersued for patent infringement and deceptive trade practices. Aureal won the lawsuit brought by Creative in December 1999. However, the cost of the legal battle caused Aureal's investors to cease funding operations, forcing Aureal into bankruptcy. Creative then acquired Aureal's assets in September 2000 through the bankruptcy court with the specific provision that Creative Labs would be released from all claims of past infringement by Creative Labs upon Aureal's A3D technology. Creative Labs has not chosen to support the A3D API.

See also
Head-related transfer function
AMD TrueAudio

References
""Aureal And Creative Engage In Legal Skirmish"". 1998-03-05. Archived from the original on 2012-12-08. Retrieved 2005-12-23.  (through Internet Archive)

External links
Aureal A3D Central

This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.",Category:Audio libraries,3
14,15,Aeolian sound,"Aeolian sound or Aeolian tone is sound that is produced by wind when it passes over or through objects.

History
Historically, Aeolus was the Greek ruler of the winds, and the Ancient Greeks believed these sounds were the voice of Aeolus.
The earliest known observations about Aeolian sounds from a ""scientific"" viewpoint were made by Athanasius Kircher in 1650.

Physical cause
An Aeolian tone is produced when air passes over an obstacle, resulting in trailing vortices with oscillatory behavior. These eddies can have strong periodic components, resulting in a steady tone. This phenomenon is the main topic of aeroacoustics;
For air moving over a cylinder, empirical data shows that an Aeolian tone will be produced with the frequency

  
    
      
        
          f
          
            Aeolian
          
        
         
        =
         
        
          
            
              ?
              v
            
            d
          
        
      
    
    {\displaystyle f_{\text{Aeolian}}\ =\ {\frac {\alpha v}{d}}}
  ,
where v is the air velocity, d is the diameter of the cylinder, and ? is the Strouhal number, which has a value of about 0.2.

Notable occurrences
Aeolian sounds can be produced in the rigging of a sail-powered ship. The vortex trails produced as the wind passes over a rope produce a sound with a frequency that varies with the velocity of the wind and the thickness of the rope. Each doubling of the wind velocity results in an octave increase in the tone, allowing up to a six octave variation in a strong, gusty wind. Ships may also carry Helmholtz resonators that amplify these sounds. Aeolian sounds can also be heard among the openings in limestone cliffs.
Some songs have been written to emulate these varying wind sounds, such as ""The Winter Wind"" by Frédéric Chopin or ""Tempest"" by Ludwig van Beethoven.

See also
Aeolian harp


== References ==",Category:Sound,3
15,16,Hearing,"Hearing, or auditory perception, is the ability to perceive sound by detecting vibrations, changes in the pressure of the surrounding medium through time, through an organ such as the ear.

Background
Sound may be heard through solid, liquid, or gaseous matter. It is one of the traditional five senses; partial or total inability to hear is called hearing loss.
In humans and other vertebrates, hearing is performed primarily by the auditory system: mechanical waves, known as vibrations are detected by the ear and transduced into nerve impulses that are perceived by the brain (primarily in the temporal lobe). Like touch, audition requires sensitivity to the movement of molecules in the world outside the organism. Both hearing and touch are types of mechanosensation.

Hearing mechanism
There are three main components of the human ear: the outer ear, the middle ear, and the inner ear.

Outer ear
The outer ear includes the pinna, the visible part of the ear, as well as the ear canal which terminates at the eardrum, also called the tympanic membrane. The pinna serves to focus sound waves through the ear canal toward the eardrum. Because of the asymmetrical character of the outer ear of most mammals, sound is filtered differently on its way into the ear depending on what vertical location it is coming from. This gives these animals the ability to localize sound vertically. The eardrum is an airtight membrane, and when sound waves arrive there, they cause it to vibrate following the waveform of the sound.

Middle ear
The middle ear consists of a small air-filled chamber that is located medial to the eardrum. Within this chamber are the three smallest bones in the body, known collectively as the ossicles which include the malleus, incus and stapes (sometimes referred to colloquially as the hammer, anvil and stirrup respectively). They aid in the transmission of the vibrations from the eardrum to the inner ear. The purpose of the middle ear ossicles is to overcome the impedance mismatch between air and water, by providing impedance matching.
Also located in the middle ear are the stapedius and tensor tympani muscles which protect the hearing mechanism through a stiffening reflex. The stapes transmits sound waves to the inner ear through the oval window, a flexible membrane separating the air-filled middle ear from the fluid-filled inner ear. The round window, another flexible membrane, allows for the smooth displacement of the inner ear fluid caused by the entering sound waves.

Inner ear
The inner ear consists of the cochlea, which is a spiral-shaped, fluid-filled tube. It is divided lengthwise by the organ of Corti, which is the main organ of mechanical to neural transduction. Inside the organ of Corti is the basilar membrane, a structure that vibrates when waves from the middle ear propagate through the cochlear fluid – endolymph. The basilar membrane is tonotopic, so that each frequency has a characteristic place of resonance along it. Characteristic frequencies are high at the basal entrance to the cochlea, and low at the apex. Basilar membrane motion causes depolarization of the hair cells, specialized auditory receptors located within the organ of Corti. While the hair cells do not produce action potentials themselves, they release neurotransmitter at synapses with the fibers of the auditory nerve, which does produce action potentials. In this way, the patterns of oscillations on the basilar membrane are converted to spatiotemporal patterns of firings which transmit information about the sound to the brainstem.

Neuronal
The sound information from the cochlea travels via the auditory nerve to the cochlear nucleus in the brainstem. From there, the signals are projected to the inferior colliculus in the midbrain tectum. The inferior colliculus integrates auditory input with limited input from other parts of the brain and is involved in subconscious reflexes such as the auditory startle response.
The inferior colliculus in turn projects to the medial geniculate nucleus, a part of the thalamus where sound information is relayed to the primary auditory cortex in the temporal lobe. Sound is believed to first become consciously experienced at the primary auditory cortex. Around the primary auditory cortex lies Wernickes area, a cortical area involved in interpreting sounds that is necessary to understand spoken words.
Disturbances (such as stroke or trauma) at any of these levels can cause hearing problems, especially if the disturbance is bilateral. In some instances it can also lead to auditory hallucinations or more complex difficulties in perceiving sound.

Hearing tests
Hearing can be measured by behavioral tests using an audiometer. Electrophysiological tests of hearing can provide accurate measurements of hearing thresholds even in unconscious subjects. Such tests include auditory brainstem evoked potentials (ABR), otoacoustic emissions (OAE) and electrocochleography (ECochG). Technical advances in these tests have allowed hearing screening for infants to become widespread.

Defense mechanism
The hearing structures of many species have defense mechanisms against injury. For example, the muscles of the middle ear (e.g. the tensor tympani muscle) in many mammals contract reflexively in reaction to loud sounds which may otherwise injure the hearing ability of the organism.

Hearing loss
There are several different types of hearing loss: Conductive hearing loss, sensorineural hearing loss and mixed types.
Conductive hearing loss
Sensorineural hearing loss
Mixed hearing loss
There are defined degrees of hearing loss:
Mild hearing loss - People with mild hearing loss have difficulties keeping up with conversations, especially in noisy surroundings. The most quiet sounds that people with mild hearing loss can hear with their better ear are between 25 and 40 dB HL.
Moderate hearing loss - People with moderate hearing loss have difficulty keeping up with conversations when they are not using a hearing aid. On average, the most quiet sounds heard by people with moderate hearing loss with their better ear are between 40 and 70 dB HL.
Severe hearing loss - People with severe hearing loss depend on powerful hearing aid. However, they often rely on lip-reading even when they are using hearing aids. The most quiet sounds heard by people with severe hearing loss with their better ear are between 70 and 95 dB HL.
Profound hearing loss - People with profound hearing loss are very hard of hearing and they mostly rely on lip-reading and sign language. The most quiet sounds heard by people with profound hearing loss with their better ear are from 95 dB HL or more.

Causes
Heredity
Congenital conditions
Presbycusis
Acquired
Noise-induced hearing loss
Ototoxic drugs and chemicals
Infection

Prevention
Hearing protection is the use of devices designed to prevent Noise-Induced Hearing Loss (NIHL), a type of post-lingual hearing impairment. The various means used to prevent hearing loss generally focus on reducing the levels of noise to which people are exposed. One way this is done is through environmental modifications such as acoustic quieting, which may be achieved with as basic a measure as lining a room with curtains, or as complex a measure as employing an anechoic chamber, which absorbs nearly all sound. Another means is the use of devices such as earplugs, which are inserted into the ear canal to block noise, or earmuffs, objects designed to cover a person's ears entirely.

Management
Hearing aids are electronic devices that enable a person with hearing loss to receive sounds at certain amplitudes. This technological development has led to the benefit of improving the sense of hearing of a person, but the usage of these devices is significantly low. Psychologically, the first time that a person realizes that he/she needs help from a professional such as an audiologist is when they feel that their hearing is severely poor. Initially, people don't like to believe that they are becoming deaf; hence it negatively affects their approach towards the use of hearing aids. Familiarity with the devices and consultation with professionals do help people feel good about using the hearing aids.

Hearing underwater
Hearing threshold and the ability to localize sound sources are reduced underwater in humans but not in aquatic animals, including whales, seals, and fishes which have ears adapted to process water borne sound.  Some research suggests underwater hearing in humans may occur through bone conduction but with poor localization. This is related to differences of the speed of sound in water vs air and the blocking of normal air conducted sound paths.

In vertebrates
Not all sounds are normally audible to all animals. Each species has a range of normal hearing for both amplitude and frequency. Many animals use sound to communicate with each other, and hearing in these species is particularly important for survival and reproduction. In species that use sound as a primary means of communication, hearing is typically most acute for the range of pitches produced in calls and speech.

Frequency range
Frequencies capable of being heard by humans are called audio or sonic. The range is typically considered to be between 20 Hz and 20,000 Hz. Frequencies higher than audio are referred to as ultrasonic, while frequencies below audio are referred to as infrasonic. Some bats use ultrasound for echolocation while in flight. Dogs are able to hear ultrasound, which is the principle of 'silent' dog whistles. Snakes sense infrasound through their jaws, and baleen whales, giraffes, dolphins and elephants use it for communication. Some fish have the ability to hear more sensitively due to a well-developed, bony connection between the ear and their swim bladder. The ""aid to the deaf"" of fishes appears in some species such as carp and herring.

In invertebrates
Vertebrates aren't the only group of animals that have hearing. Some insects have hearing organs as well (e.g. the long-horned grasshopper, lubber grasshopper and the cicada); they use sound as a form of communication.
Something widely spread among insects is body hair, that can be made to swing by sonar waves. Due to the resonance phenomenon certain hairs swing stronger when exposed to a specific sonar-frequency. This specificity depends on the stiffness and the length of the hairs. That is why certain caterpillar species have evolved hair that would resonate with the sound of buzzing wasps, thus warning them of the presence of natural enemies. Moreover, mosquitoes have hair on their antennae that resonate with the flying sound of homogeneous females, enabling the males the ability to detect potential sexual partners.
Some insects possess a tympanal organ. These are ""eardrums"", that cover air filled chambers on the legs. Similar to the hearing process with vertebrates, the eardrums react to sonar waves. Receptors that are placed on the inside translate the oscillation into electric signals and send them to the brain. Several groups of flying insects that are preyed upon by echolocating bats can perceive the ultrasound emissions this way and reflexively practice ultrasound avoidance.

Mathematics
The basilar membrane of the inner ear spreads out different frequencies: high frequencies produce a large vibration at the end near the middle ear (the ""base""), and low frequencies a large vibration at the distant end (the ""apex""). Thus the ear performs a sort of frequency analysis, roughly similar to a Fourier transform. However, the nerve pulses delivered to the brain contain both rate-versus-place and fine temporal structure information, so the similarity is not strong.

See also
Physiological
Ear
Hearing loss
Hearing test
Hear, hear

General
Auditory scene analysis
Auditory system
Bone conduction
Hearing range
Human echolocation
Listening
Neuronal encoding of sound

Test and measurement
Audiogram
Audiometry
Dichotic listening (test)
Auditory brainstem response (test)

Disorders
Auditory processing disorder
Endaural phenomena
Hearing loss
Hyperacusis
Presbycusis
Tinnitus

References
Further reading
Lopez-Poveda, Enrique A.; Palmer, A. R. (Alan R.); Meddis, Ray. (2010). The neurophysiological bases of auditory perception. New York: Springer. ISBN 978-1-4419-5685-9. OCLC 471801201. 
Peng, AW.; Salles, FT.; Pan, B.; Ricci, AJ. (2011). ""Integrating the biophysical and molecular mechanisms of auditory hair cell mechanotransduction"". Nat Commun. 2: 523. doi:10.1038/ncomms1533. PMC 3418221?. PMID 22045002.

External links
Open University - OpenLearn - Article about hearing
 The dictionary definition of hearing at Wiktionary
 Quotations related to Hearing at Wikiquote",Category:Pages with citations lacking titles,3
16,17,Directional sound,"See sound from ultrasound for a beam of ultrasound that makes audible sound in a restricted target area without a receiving set.
Directional Sound refers to the notion of using various devices to create fields of sound which spread less than most (small) traditional loudspeakers. Several techniques are available to accomplish this, and each has its benefits and drawbacks. Ultimately, choosing a directional sound device depends greatly on the environment in which it is deployed as well as the content that will be reproduced. Keeping these factors in mind will yield the best results through any evaluation of directional sound technologies.
Systems which guide evacuees during an emergency by the emission of pink noise to the exits are often also called ""directional sound"" systems.

Basic theory
In all wave-producing sources, the directivity of any source, at maximum, corresponds to the size of the source compared to the wavelengths it is generating: The larger the source is compared to the wavelength of the sound waves, the more directional beam results. The specific transduction method has no impact on the directivity of the resulting sound field; the analysis relies only on the aperture function of the source, per the Huygens–Fresnel principle.
The ultrasonic devices achieve high directivity by modulating audible sound onto high frequency ultrasound. The higher frequency sound waves have a shorter wavelength and thus don't spread out as rapidly. For this reason, the resulting directivity of these devices is far higher than physically possible with any loudspeaker system. However, they are reported to have limited low-frequency reproduction abilities. See sound from ultrasound for more information.

Speaker arrays
While a large loudspeaker is naturally more directional because of its large size, a source with equivalent directivity can be made by utilizing an array of traditional small loudspeakers, all driven together in-phase. Acoustically equal to a large speaker, this creates a larger source size compared to wavelength, and the resulting sound field is narrowed compared to a single small speaker. Large speaker arrays have been used in hundreds of arena sound systems to mitigate noise that would ordinarily travel to adjoining neighborhoods, as well as limited applications in other applications where some degree of directivity is helpful, such as museums or similar display applications that can tolerate large speaker dimensions.
Traditional speaker arrays can be fabricated in any shape or size, but a reduced physical dimension (relative to wavelength) will inherently sacrifice directivity in that dimension. The larger the speaker array, the more directional, and the smaller the size of the speaker array, the less directional it is. This is fundamental physics, and cannot be bypassed, even by using phased arrays or other signal processing methods. This is because the directivity pattern of any wave source is the Fourier Transform of the source function. Phased array design is, however, sometimes useful for beamsteering, or for sidelobe mitigation, but making these compromises necessarily reduces directivity.
Acoustically, speaker arrays are essentially the same as sound domes, which have also been available for decades; the size of the dome opening mimics the acoustic properties of a large speaker of the same diameter (or, equivalently, a large speaker array of the same diameter). Domes, however, tend to weigh much less than the weight of comparable speaker arrays (15 lbs vs. 37 lbs, per the manufacturer's websites), and are far less expensive.
Other types of large speaker panels, such as electrostatic loudspeakers, tend to be more directional than small speakers, for the same reasons as above; they are somewhat more directional only because they tend to be physically larger than most common loudspeakers. Correspondingly, an electrostatic loudspeaker the size of a small traditional speaker would be non-directional.
The directivity for various source sizes and shapes is given in. The directivity is shown to be a function only of the source size and shape, not of the specific type of transducer used.

See also
Loudspeaker
Parabolic loudspeaker
Sonic weaponry
Sound from ultrasound


== References ==",Category:Sound,3
17,18,Temp track,"A temp track is an existing piece of music or audio which is used in film production during the editing phase. It serves as a guideline for the mood or atmosphere the director is looking for in a scene.
The track is usually replaced before release by an original soundtrack composed specifically for the film. While some feel that having to follow a temp track can be limiting for a composer, it can be a useful tool in finding the right style of music for a particular scene and can be a time-saver for both the composer and director.
The temp track is sometimes also referred to as scratch music, temp score or temp music.

References
External links
The Relevance of Temp Tracks - a guide on how temp tracks can be a useful tool for finding the right music for a production",Category:Music and video,3
18,19,ITU-R 468 noise weighting,"ITU-R 468 (originally defined in CCIR recommendation 468-4; sometimes referred to as CCIR-1k) is a standard relating to noise measurement, widely used when measuring noise in audio systems. The standard, now referred to as ITU-R BS.468-4, defines a weighting filter curve, together with a quasi-peak rectifier having special characteristics as defined by specified tone-burst tests. It is currently maintained by the International Telecommunications Union who took it over from the CCIR.
It is used especially in the UK, Europe, and former countries of the British Empire such as Australia and South Africa. It is less well known in the USA where A-weighting has always been used.

Explanation
While most audio engineers are familiar with the A-weighting curve, which was based on the 40 phon equal-loudness contour derived initially by Fletcher and Munson (1933) the later CCIR-468 weighting curve, now supported as an ITU standard is less well known outside of the UK and Europe. Originally incorporated into an ANSI standard for sound level meters, A-weighting was intended for measurement of the audibility of sounds by themselves. It was never specifically intended for the measurement of the more random (near-white or pink) noise in electronic equipment, though has been used for this purpose by most microphone manufacturers since the 1970s. The human ear responds quite differently to clicks and bursts of random noise, and it is this difference that gave rise to the 468-weighting, which together with quasi-peak measurement (rather than the rms measurement used with A-weighting) became widely used by broadcasters throughout Britain, Europe, and former British Commonwealth countries, where engineers were heavily influenced by BBC test methods. Telephone companies worldwide have also used methods similar to 468 weighting with quasi-peak measurement to describe objectionable interference induced in one telephone circuit by switching transients in another.

History
Original research
Developments in the 1960s, in particular the spread of FM broadcasting and the development of the compact audio cassette with Dolby-B Noise Reduction, alerted engineers to the need for a weighting curve that gave subjectively meaningful results on the typical random noise that limited the performance of broadcast circuits, equipment and radio circuits. A-weighting was not giving consistent results, especially on FM radio transmissions and Compact Cassette recording where preemphasis of high frequencies was resulting in increased noise readings that did not correlate with subjective effect. Early efforts to produce a better weighting curve led to a DIN standard that was adopted for European Hi-Fi equipment measurement for a while.
Experiments in the BBC led to BBC Research Department Report EL-17, The Assessment of Noise in Audio Frequency Circuits, in which experiments on numerous test subjects were reported, using a variety of noises ranging from clicks to tone-bursts to pink noise. Subjects were asked to compare these with a 1 kHz tone, and final scores were then compared with measured noise levels using various combinations of weighting filter and quasi-peak detector then in existence (such as those defined in a now discontinued German DIN standard). This led to the CCIR-468 standard which defined a new weighting curve and quasi-peak rectifier.
The origin of the current ITU-R 468 weighting curve can be traced to 1956. The 1968 BBC EL-17 report discusses several weighting curves, including one identified as D.P.B. which was chosen as superior to the alternatives: A.S.A, C.C.I.F and O.I.R.T. The report's graph of the DPB curve is identical to that of the ITU-R 486 curve, except that the latter extends to slightly lower and higher frequencies. The BBC report states that this curve was given in a ""contribution by the D.B.P. (The Telephone Administration of the Federal German Republic) in the Red Book Vol. 1 1957 covering the first plenary assembly of the CCITT (Geneva 1956)"". D.B.P. is Deutsche Bundespost, the German post office which provides telephone service in Germany as the GPO does in the UK. The BBC report states ""this characteristic is based on subjective tests described by Belger."" and cites a 1953 paper by E. Belger.
Dolby Laboratories took up the new CCIR-468 weighting for use in measuring noise on their noise reduction systems, both in cinema (Dolby A) and on cassette decks (Dolby B), where other methods of measurement were failing to show up the advantage of such noise reduction. Some Hi-Fi column writers took up 468 weighting enthusiastically, observing that it reflected the roughly 10dB improvement in noise observed subjectively on cassette recordings when using Dolby B while other methods could indicate an actual worsening in some circumstances, because they did not sufficiently attenuate noise above 10 kHz.

Standards
CCIR Recommendation 468-1 was published soon after this report, and appears to have been based on the BBC work. Later versions up to CCIR468-4 differed only in minor changes to permitted tolerances. This standard was then incorporated into many other national and international standards (IEC, BSI, JIS, ITU) and adopted widely as the standard method for measuring noise, in broadcasting, professional audio, and 'Hi-Fi' specifications throughout the 1970s. When the CCIR ceased to exist, the standard was officially taken over by the ITU-R (International Telecommunication Union). Current work on this standard occurs primarily in the maintenance of IEC 60268, the international standard for sound systems.
The CCIR curve differs greatly from A-weighting in the 5 to 8 kHz region where it peaks to +12.2 dB at 6.3 kHz, the region in which we appear to be extremely sensitive to noise. While it has been said (incorrectly) that the difference is due to a requirement for assessing noise intrusiveness in the presence of programme material, rather than just loudness, the BBC report makes clear the fact that this was not the basis of the experiments. The real reason for the difference probably relates to the way in which our ears analyse sounds in terms of spectral content along the cochlea. This behaves like a set of closely spaced filters with a roughly constant Q factor, that is, bandwidths proportional to their centre frequencies. High frequency hair cells would therefore be sensitive to a greater proportion of the total energy in noise than low frequency hair cells. Though hair-cell responses are not exactly constant Q, and matters are further complicated by the way in which the brain integrates adjacent hair-cell outputs, the resultant effect appears roughly as a tilt centred on 1 kHz imposed on the A-weighting.
Dependent on spectral content, 468-weighted measurements of noise are generally about 11 dB higher than A-weighted, and this is probably a factor in the recent trend away from 468-weighting in equipment specifications as cassette tape use declines.
It is important to realise that the 468 specification covers both weighted and 'unweighted' (using a 22 Hz to 22 kHz 18 dB/octave bandpass filter) measurement and that both use a very special quasi-peak rectifier with carefully devised dynamics (A-weighting uses RMS detection for no particular reason). Rather than having a simple 'integration time' this detector requires implementation with two cascaded 'peak followers' each with different attack time-constants carefully chosen to control the response to both single and repeating tone-bursts of various durations. This ensures that measurements on impulsive noise take proper account of our reduced hearing sensitivity to short bursts. This quasi-peak measurement is also called psophometric weighting.
This was once more important because outside broadcasts were carried over 'music circuits' that used telephone lines, with clicks from Strowger and other electromechanical telephone exchanges. It now finds fresh relevance in the measurement of noise on computer 'Audio Cards' which commonly suffer clicks as drives start and stop.

Present usage of 468-weighting
468-weighting is also used in weighted distortion measurement at 1 kHz. Weighting the distortion residue after removal of the fundamental emphasises high-order harmonics, but only up to 10 kHz or so where the ears response falls off. This results in a single measurement (sometimes called distortion residue measurement) which has been claimed to correspond well with subjective effect even for power amplifiers where crossover distortion is known to be far more audible than normal THD (Total harmonic distortion) measurements would suggest.
468-weighting is still demanded by the BBC and many other broadcasters, with increasing awareness of its existence and the fact that it is more valid on random noise where pure tones do not exist.
Often both A-weighted and 468-weighted figures are quoted for noise, especially in microphone specifications.
While not intended for this application, the 468 curve has also been used (offset to place the 0 dB point at 2 kHz rather than 1 kHz) as ""M weighting"" in standards such as ISO 21727 intended to gauge loudness or annoyance of cinema soundtracks. This application of the weighting curve does not include the quasi-peak detector specified in the ITU standard.

Summary of specification
Note: this is not the full definitive standard.

Weighting curve specification ( Weighted measurement)
The weighting curve is specified by both a circuit diagram of a weighting network and a table of amplitude responses.

Above is the ITU-R 468 Weighting Filter Circuit Diagram. The source and sink impedances are both 600 ohms (resistive), as shown in the diagram. The values are taken directly from the ITU-R 468 specification. Note that since this circuit is purely passive, it cannot create the additional 12dB gain required; any results must be corrected by a factor of 8.1333, or +18.2dB.
Table of amplitude responses:

The values of the amplitude response table slightly differ from those resulting from the circuit diagram, e.g. because of the finite resolution of the numerical values. In the standard it is said that the 33.06 nF capacitor may be adjusted or an active filter may be used.
Modeling at hand the circuit above and some calculus give this formula to get the amplitude response in dB for any given frequency value :

  
    
      
        
          R
          
            I
            T
            U
          
        
        (
        f
        )
        =
        
          
            
              1.246332637532143
              ?
              
                10
                
                  ?
                  4
                
              
              
              f
            
            
              (
              
                h
                
                  1
                
              
              (
              f
              )
              
                )
                
                  2
                
              
              +
              (
              
                h
                
                  2
                
              
              (
              f
              )
              
                )
                
                  2
                
              
            
          
        
      
    
    {\displaystyle R_{ITU}(f)={\frac {1.246332637532143\cdot 10^{-4}\,f}{\sqrt {(h_{1}(f))^{2}+(h_{2}(f))^{2}}}}}
  

  
    
      
        I
        T
        U
        (
        f
        )
        =
        18.2
        +
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              I
              T
              U
            
          
          (
          f
          )
          )
        
      
    
    {\displaystyle ITU(f)=18.2+20\log _{10}\left(R_{ITU}(f)\right)}
  
where

  
    
      
        
          h
          
            1
          
        
        (
        f
        )
        =
        ?
        4.737338981378384
        ?
        
          10
          
            ?
            24
          
        
        
        
          f
          
            6
          
        
        +
        2.043828333606125
        ?
        
          10
          
            ?
            15
          
        
        
        
          f
          
            4
          
        
        ?
        1.363894795463638
        ?
        
          10
          
            ?
            7
          
        
        
        
          f
          
            2
          
        
        +
        1
      
    
    {\displaystyle h_{1}(f)=-4.737338981378384\cdot 10^{-24}\,f^{6}+2.043828333606125\cdot 10^{-15}\,f^{4}-1.363894795463638\cdot 10^{-7}\,f^{2}+1}
  

  
    
      
        
          h
          
            2
          
        
        (
        f
        )
        =
        1.306612257412824
        ?
        
          10
          
            ?
            19
          
        
        
        
          f
          
            5
          
        
        ?
        2.118150887518656
        ?
        
          10
          
            ?
            11
          
        
        
        
          f
          
            3
          
        
        +
        5.559488023498642
        ?
        
          10
          
            ?
            4
          
        
        
        f
      
    
    {\displaystyle h_{2}(f)=1.306612257412824\cdot 10^{-19}\,f^{5}-2.118150887518656\cdot 10^{-11}\,f^{3}+5.559488023498642\cdot 10^{-4}\,f}

Tone-burst response requirements
5 kHz single bursts:

Repetitive tone-burst response
5 ms, 5 kHz bursts at repetition rate:

Unweighted measurement
Uses 22 Hz HPF and 22 kHz LPF 18dB/decade or greater (Tables to be added)

See also
Weighting filter
Equal-loudness contour
Noise weighting
A-weighting
Audio system measurements

References
Audio Engineer's Reference Book, 2nd Ed 1999, edited Michael Talbot Smith, Focal Press
An Introduction to the Psychology of Hearing 5th ed, Brian C.J.Moore, Elsevier Press

External links
AES pro audio reference definition of ITU-R 468-weighting
Weighting Filter Set Circuit diagrams",Category:Audio engineering,3
19,20,Micro perforated plate,"A micro perforated plate (MPP) is a device used to absorb sound, reducing its intensity. It consists of a thin flat plate, made from one of several different materials, with small holes punched in it. An MPP offers an alternative to traditional sound absorbers made from porous materials.

Structure
An MPP is normally 0.5 - 2 mm thick. The holes typically cover 0.5 to 2% of the plate, depending on the application and the environment in which the MPP is to be mounted. Hole diameter is usually less than 1 millimeter, typically 0.05 to 0.5 mm. They are usually made using the microperforation process.

Operating principle
The goal of a sound absorber is to convert acoustical energy into heat. In a traditional absorber, the sound wave propagates into the absorber. Because of the proximity of the porous material, the oscillating air molecules inside the absorber lose their acoustical energy due to friction.
A MPP works in almost the same way. When the oscillating air molecules penetrate the MPP, the friction between the air in motion and the surface of the MPP dissipates the acoustical energy.

Comparison with other materials
Traditional sound absorbers are porous materials such as mineral wool, glass or polyester fibres. It is not possible to use these materials in harsh environments such as engine compartments. Traditional absorbers have many drawbacks, including pollution, the risk of fire, and problems with the useful lifetime of the absorbing material.
The main reason why Micro Perforates have become so popular among acousticians is that they have a good absorption performance but without the disadvantages of a porous material. Furthermore, an MPP is also preferable from an aesthetic point of view.

History
For a while, perforated metal panels with holes in the 1 – 10 mm range have been used as a cage for sound-absorbing glass-fiber bats where large holes let the sound waves reach into the absorbent fiber. Another use has been the creation of narrowband Helmholtz absorbers which can be tuned by hole size and the dimensions of the hole distance and air gap behind the panel. However, when the hole dimensions are in the region of 0.05 - 0.5 mm, the narrow absorption peaks become much wider, making the additional fiber absorber more or less unnecessary, while still maintaining a very high absorption factor. By varying geometrical and material parameters, the acoustical performance can be tailored to meet a multitude of specifications in various applications.
One early contributor to the theory of micro perforated plates as sound absorbers was Professor Daa-You Maa. Further possibilities aiming to improve the accuracy of Maa’s original model are currently being investigated. One other major phenomenon that currently being investigated is the nonlinear effect i.e. an MPP behaves differently depending on the magnitude of the incident sound wave.

References
External links
Acoustical Society of America",Category:Sound,3
20,21,Sound map,"Sound maps are digital geographical maps that put emphasis on the sonic representation of a specific location. Sound maps are created by associating landmarks (streets in a city, train stations, stores, pathways, factories, oil pumps, etc.) and soundscapes.
The term “soundscape” refers to the sonic environment of a specific locale. It may also refer to actual environments, or to abstract constructions such as musical compositions and tape montages, particularly when considered as an artificial environment. The objective of sound maps is to represent a specific environment using its soundscape as primary references as opposed to visual cues. Sound maps are in many ways the most effective auditory archive of an environment. Sound maps are similar to sound walks which are a form of active participation in the soundscape. Soundwalks and indeed, sound maps encourage the participants to listen discriminatively, and moreover, to make critical judgments about the sounds heard and their contribution to the balance or imbalance of the sonic environment. However, soundwalks will plot out a route for the user to follow and give guidance as to what the user may be hearing at each checkpoint. Sound maps, on the other hand, have specific soundscapes recorded that users can listen to at each checkpoint.

History / Background
The theoretical framework upon which sound maps are based derive from earlier research on acoustic ecology and soundscapes, the later being a term first coined by researcher and music composer R. Murray Schafer in the 1960s. Looking to challenge traditional ideas of recording reality, Schafer, along with several college music composers such as Barry Truax and Hildegard Westerkamp, funded the World Soundscape Project, an ambitious sound recording project that led the team based in Simon Fraser University to travel within Canada and out in Europe to collect data on local soundscapes. The sounds that they recorded were used to build a database of locales not based on the visual, but on their acoustic particularities. The result of the project had been released to the public in the form of a series books entitled The Music of the Environment series which included narrative accounts of the soundscape recording activity (European Sound Diary) and soundscape analysis (Five Village Soundscapes). However, when those works were first published, the recordings were not available for the public to listen to as the project mainly aimed at building a database of sound over a long period of time. The World Soundscape Project also birthed major theoretical framework for future studies of acoustic ecology and soundscapes, among them R. Murray Schafer’s The Tuning of the World in which the idea of soundscape studies were first introduced as well as Barry Truax’s The World Soundscape Project's Handbook for Acoustic Ecology that presented the foundational terminology for research in the field.
Sound maps make use of new computer locative technologies to achieve the similar purpose of preserving the soundscape of specific locales, but differs in the way of presenting the sound database. Through digital technologies such as mapping software and audio file encoding, the objective of using sound maps is partly that of making a soundscape database available to the public in a comprehensive fashion by uploading each site-specific soundscape onto a digital map as well as making the end product available for public collaboration. Users are able to pull up a map of the city and click on the sound clip icons in order to hear the soundscape for that location. Some sound maps are crowd-sourced and therefore allow the public to record their own soundscapes and upload them onto the digital map provided by the site hosting the sound map. Therefore, the soundscape database is built by the public and made available to the public for use.

Applications
The Sound Around You Project
The Sound Around You project began as a soundscape research project at the University of Salford, UK in 2007. The project allows people across the world to use their iPhone (or any other audio recorder) to record clips or sonic postcards of around 30 seconds in length from different sound environments, or ‘soundscapes’ from a family car journey to a busy shopping centre, and to upload them to the virtual map, along with their opinions of them and why they chose to record it. Sound Around You aims to raise awareness of how our soundscape influences us and could have far reaching implications for professions and social groups ranging from urban planners to house buyers.

New York Sound Map
The NYSoundmap is a project of The New York Society for Acoustic Ecology (NYSAE), a New York metropolitan chapter of the American Society for Acoustic Ecology, an organization dedicated to exploring the role of sound in natural habitats and human societies, and promoting public dialog concerning the identification, preservation, and restoration of natural and cultural sound environments. The NYSAE's purpose is to explore and create an ongoing dialog regarding aural experience specific to New York City. The NYSoundmap project is the direct result of the NYSAE's interest in collecting and disseminating the city's aural experiences to the general public. We are artists, architects, sound engineers, philosophers and designers. Our relationship to sound as a vital and key component of urban living is made manifest by our desire to create and share this map with and for friends, neighbors and fellow citizens of the city of New York. Through the NYSoundmap project, the NYSAE aims to facilitate a dialogue between people from a wide variety of communities and backgrounds - from beginners to professional sound artists and musicians.

Stanley Park Soundmap
The Stanley Park Soundmap is a web-based document of the sonic attributes of one of North America's largest urban parks located in Vancouver, British Columbia, Canada. Using a GPS unit, and a compact digital audio recorder 13 positions in the park were documented on a cool sunny day on Thursday, March 12, 2009. The location data and sound recordings were then linked to a map created in a Geographic Information Systems (GIS) based desktop application.

Montreal Sound map
The Montréal Sound Map is a web-based soundscape project that allows users to upload field recordings to a Google Map of Montréal. The soundscape is constantly changing, and this project acts as a sonic time capsule with the goal of preserving sounds before they disappear.

Sonoteca Bahia Blanca
Sonoteca Bahia Blanca is a virtual platform that aims to provide a common space for the collection, concentration, sharing and distribution of sound through its georeferencing and organization in a database, from a collaborative, supportive cultural practice and community status. The project seek to enhance the sound heritage of the city, to rediscover and disseminate it, as a means of its multiple identities. Sound Map: www.sonotecabahiablanca.com/mapa

Sound map with the MOMA studio
Sound and space are closely linked. Our ears help define our surroundings by picking up on spatial clues in reflected sound waves. This innate ability to situate ourselves in our soundscape was probably more overtly useful in the days before electricity, when we had to rely on our ears to alert us to danger our eyes could not detect. There is, however, a movement in the visually impaired community to cultivate this ability to help them navigate in the world and participate in sports, and artists such as Janet Cardiff use sound and spatiality as integral parts of their work (see The Forty Part Motet).

Significance / Importance
Sound maps give people a new way to look at geography and the world around them. They allow users to reconnect with their immediate environment which the current generation seldom does anymore (think about how often we see others with headphones in or on the phone instead of keeping their ears open). Sound maps also have a historical significance in that they will give future generations an idea of what a specific place sounded like, at a specific time. Indeed, as the Montreal Sound Map project pointed out: sound maps can be used as “sonic time capsules” which preserve the sounds of a place before they disappear. Currently, we possess historical maps and pictures that can tell us how past societies lived. However, we have no idea what those societies sounded like. Sound maps give us an opportunity to have access to this vital historical significance.

See also
Soundscape
Ambient music
Biomusic
Biophony
Field recording
Noise map
Sound art
Sound sculpture
Space music
R. Murray Schafer

Further reading
Schafer, Raymond Murray (ed.) (1977). European Sound Diary. Vancouver : A.R.C. Publications : A.R.C. the Aesthetic Research Centre ; Burnaby, B.C. : World Soundscape Project.
Schafer, Raymond Murray (ed.) (2009). Five Villages Soundscape (2nd edition) Joensuu: Tampereen Ammattikorkeakoulu University of Applied Sciences (1st edition 1977).
Smith, J. Susan (1994). ""Soundscape"". Area, Vol. 26, No.3, pp. 232–240. The Royal Geography Society.
Waldock, Jacqueline (2011).""SOUNDMAPPING: Critiques And Reflections On This New Publicly Engaging Medium"". Journal of Sonic Studies, volume 1, nr. 1.
2006 The West Meets the East in Acoustic Ecology (Tadahiko Imada, Kozo Hiramatsu et al. Eds), Japanese Association for Sound Ecology & Hirosaki University International Music Centre ISBN 4-9903332-1-7

Sound file
Wind Portlandreginal(2011) by Scott Smallwood

Artcar(2011) by Scott Smallwood

References
External links
Sound Mapping 1998
New York Sound Map
Sound Map of Budapest
Sound Map of Bratislava
Cerrado Ambisônico
Sound Map of Krakow
Sound Map of Wroclaw
Belgrade Sound map
Rabeca.org
Santorini Sound map
Stanley Park Sound map
Sonoteca Bahia Blanca Sound Map
Montréal Sound map
the MoMa Studio",Category:Pages using citations with accessdate and no URL,3
21,22,Sound intensity,"Sound intensity also known as acoustic intensity is defined as the power carried by sound waves per unit area in a direction perpendicular to that area. The SI unit of intensity, which includes sound intensity, is the watt per square meter (W/m2). One application is the noise measurement of sound intensity in the air at a listener's location as a sound energy quantity.
Sound intensity is not the same physical quantity as sound pressure. Hearing is directly sensitive to sound pressure which is related to sound intensity. In consumer audio electronics, the level differences are called ""intensity"" differences, but sound intensity is a specifically defined quantity and cannot be sensed by a simple microphone. The rate at which sound energy passes through a unit area held perpendicular to the direction of propagation of sound waves is called intensity of sound.

Mathematical definition
Sound intensity, denoted I, is defined by

  
    
      
        
          I
        
        =
        p
        
          v
        
      
    
    {\displaystyle \mathbf {I} =p\mathbf {v} }
  
where
p is the sound pressure;
v is the particle velocity.
Both I and v are vectors, which means that both have a direction as well as a magnitude. The direction of sound intensity is the average direction in which energy is flowing.
The average sound intensity during time T is given by

  
    
      
        ?
        
          I
        
        ?
        =
        
          
            1
            T
          
        
        
          ?
          
            0
          
          
            T
          
        
        p
        (
        t
        )
        
          v
        
        (
        t
        )
        
        
          d
        
        t
        .
      
    
    {\displaystyle \langle \mathbf {I} \rangle ={\frac {1}{T}}\int _{0}^{T}p(t)\mathbf {v} (t)\,\mathrm {d} t.}
  
Also,
Intensity of Sound = 2?²n²A²?v
Where,
n is frequency of sound, A is the Amplitude of sound wave, v is velocity of sound, and ? is density of medium in which sound is traveling

Inverse-square law
For a spherical sound wave, the intensity in the radial direction as a function of distance r from the centre of the sphere is given by

  
    
      
        I
        (
        r
        )
        =
        
          
            P
            
              A
              (
              r
              )
            
          
        
        =
        
          
            P
            
              4
              ?
              
                r
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle I(r)={\frac {P}{A(r)}}={\frac {P}{4\pi r^{2}}},}
  
where
P is the sound power;
A(r) is the area of a sphere of radius r.
Thus sound intensity decreases as 1/r2 from the centre of the sphere:

  
    
      
        I
        (
        r
        )
        ?
        
          
            1
            
              r
              
                2
              
            
          
        
        .
      
    
    {\displaystyle I(r)\propto {\frac {1}{r^{2}}}.}
  
This relationship is an inverse-square law.

Sound intensity level
Sound intensity level (SIL) or acoustic intensity level is the level (a logarithmic quantity) of the intensity of a sound relative to a reference value.
It is denoted LI, expressed in dB, and defined by

  
    
      
        
          L
          
            I
          
        
        =
        
          
            1
            2
          
        
        ln
        
        
          (
          
            
              I
              
                I
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        
          log
          
            10
          
        
        
        
          (
          
            
              I
              
                I
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              I
              
                I
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{I}={\frac {1}{2}}\ln \!\left({\frac {I}{I_{0}}}\right)\!~\mathrm {Np} =\log _{10}\!\left({\frac {I}{I_{0}}}\right)\!~\mathrm {B} =10\log _{10}\!\left({\frac {I}{I_{0}}}\right)\!~\mathrm {dB} ,}
  
where
I is the sound intensity;
I0 is the reference sound intensity;
1 Np = 1 is the neper;
1 B = (1/2) ln(10) is the bel;
1 dB = (1/20) ln(10) is the decibel.
The commonly used reference sound intensity in air is

  
    
      
        
          I
          
            0
          
        
        =
        1
         
        
          p
          W
          
            /
          
          
            m
            
              2
            
          
        
        .
      
    
    {\displaystyle I_{0}=1~\mathrm {pW/m^{2}} .}
  
The proper notations for sound intensity level using this reference are LI /(1 pW/m2) or LI (re 1 pW/m2), but the notations dB SIL, dB(SIL), dBSIL, or dBSIL are very common, even if they are not accepted by the SI.
The reference sound intensity I0 is defined such that a progressive plane wave has the same value of sound intensity level (SIL) and sound pressure level (SPL), since

  
    
      
        I
        ?
        
          p
          
            2
          
        
        .
      
    
    {\displaystyle I\propto p^{2}.}
  
The equality of SIL and SPL requires that

  
    
      
        
          
            I
            
              I
              
                0
              
            
          
        
        =
        
          
            
              p
              
                2
              
            
            
              p
              
                0
              
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle {\frac {I}{I_{0}}}={\frac {p^{2}}{p_{0}^{2}}},}
  
where p0 = 20 ?Pa is the reference sound pressure.
For a progressive spherical wave,

  
    
      
        
          
            p
            v
          
        
        =
        
          z
          
            0
          
        
        ,
      
    
    {\displaystyle {\frac {p}{v}}=z_{0},}
  
where z0 is the characteristic specific acoustic impedance. Thus,

  
    
      
        
          I
          
            0
          
        
        =
        
          
            
              
                p
                
                  0
                
                
                  2
                
              
              I
            
            
              p
              
                2
              
            
          
        
        =
        
          
            
              
                p
                
                  0
                
                
                  2
                
              
              p
              v
            
            
              p
              
                2
              
            
          
        
        =
        
          
            
              p
              
                0
              
              
                2
              
            
            
              z
              
                0
              
            
          
        
        .
      
    
    {\displaystyle I_{0}={\frac {p_{0}^{2}I}{p^{2}}}={\frac {p_{0}^{2}pv}{p^{2}}}={\frac {p_{0}^{2}}{z_{0}}}.}
  
In air at ambient temperature, z0 = 410 Pa·s/m, hence the reference value I0 = 1 pW/m2.
In an anechoic chamber, which approximates a free field (no reflection), the SIL can be taken as being equal to the SPL. This fact is exploited to measure sound power in anechoic conditions.

Measurement
One method of sound intensity measurement involves the use of two microphones located close to each other, normal to the direction of sound energy flow. A signal analyser is used to compute the crosspower between the measured pressures and the sound intensity is derived from (proportional to) the imaginary part of the crosspower.

References
External links
How Many Decibels Is Twice as Loud? Sound Level Change and the Respective Factor of Sound Pressure or Sound Intensity
Acoustic Intensity
Conversion: Sound Intensity Level to Sound Intensity and Vice Versa
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
Table of Sound Levels. Corresponding Sound Intensity and Sound Pressure
What Is Sound Intensity Measurement and Analysis?",Category:Physical quantities,3
22,23,Acoustic ecology,"Acoustic ecology, sometimes called ecoacoustics or soundscape studies, is a discipline studying the relationship, mediated through sound, between human beings and their environment. Acoustic ecology studies started in the late 1960s with R. Murray Schafer and his team at Simon Fraser University (Vancouver, British Columbia, Canada) as part of the World Soundscape Project. The original WSP team included Barry Truax and Hildegard Westerkamp, Bruce Davies and Peter Huse, among others. The first study produced by the WSP was titled The Vancouver Soundscape. The interest in this area grew enormously after this pioneer and innovative study and the area of acoustic ecology raised the interest of researchers and artists all over the world. In 1993, the members of the by now large and active international acoustic ecology community formed the World Forum of Acoustic Ecology.
Every three years since the WFAE's founding at Banff, Canada in 1993, an international symposium has taken place. Stockholm, Amsterdam, Devon, Peterborough, and Melbourne followed. In November 2006, the WFAE meeting took place in Hirosaki, Japan. Koli, Finland, was the meeting place of the latest WFAE world conference.
From its roots in the sonic sociology and radio art of Schafer and his colleagues, acoustic ecology has found expression in many different fields. While most have taken some inspiration from Schafer's writings, in recent years there have also been healthy divergences from the initial ideas. Among the expanded expressions of acoustic ecology are increasing attention to the sonic impacts of road and airport construction, widespread networks of ""phonographers"" exploring the world through sound, the broadening of bioacoustics (the use of sound by animals) to consider the subjective and objective responses of animals to human noise, including increasing use of the idea of ""acoustic ecology"" in the literature, and a popular in the effects of human noise on animals, with ocean noise capturing the most attention. Acoustic ecology finds expression in many different fields, including niches as unique as historical soundscapes and psychosonography.

Bioacoustics
Noise is generally a by-product of increased urbanization and development. Noise can alter the acoustic environment of aquatic and terrestrial habitats. Bird diversity has shown to decline because of chronic noise levels in cities and along roadways. Some species such as the urban great tits have changed the frequency of there calls to adapt. In terms of evolution, man-made noise is a much more recent phenomenon. Scientific research has shown that it has potential to change behavior, alter physiology and even restructure animal communities.

List of compositional works
""Dominion"" by Barry Truax
""Dominion"" takes listeners on an acoustic journey across Canada. The work begins with the firing of the Noon Gun in St. John's harbour in Newfoundland and continues westward, recording sounds such as the Peace Tower bell in Ottawa and the O Canada Horn in Vancouver, along the way. A 12-piece orchestra, representing the 10 provinces and then two territories, carries listeners through the work, along with the whistle of a Canadian Pacific Railway train, representing the railroad that first connected Canada over a century ago.

Acoustic Ecological Archeology
Marc E. Moglen (2007) recreated pre-historical Soundscapes (Acoustic Ecology) at University of California, Berkeley's Department of Anthropology, combining compositional techniques with site recordings for a non-diegetic piece in the virtual world of Second Life, on ""Okapi Island"". At the Center for New Media the acoustic ecological setting of the former jazz scene in Oakland, CA was developed for a virtual world setting.

""Soundmarks of Canada"" by Peter Huse
""A composition recreating the acoustic profile of community sounds unique to Canadian locales, coast to coast"". Source: Soundscapes of Canada.

See also
Biophony
Bernie Krause
Lombard effect
Marine mammals and sonar
Fisheries acoustics
Noise map
Soundscape

References
Bibliography
Marcello Sorce Keller, “The Windmills of my Mind – Musings about Haydn, Kant, Sonic Ecology, and Hygiene”, in Gisa Jähnichen and Chinthaka Meddegoda (eds.), Music – Dance and Environment. Serdang: Universiti Putra Malaysia Press, 2013, 1–31.

External links
Acoustic Ecology and the Soundscape Bibliography created by Maksymilian Kapelanski for Leonardo/ISAST
Bazilchuk, Nancy. 2007. Choral Reefs: An inexpensive device monitors ocean health through sound. Conservation 8(1).
""An Introduction to Acoustic Ecology"" by Kendall Wrightson
""Science of sound"" Canadian Geographic
Mailman, Joshua B. 2012. ""Seven Metaphors for (Music) Listening: DRAMaTIC"" in Journal of Sonic Studies v.2.",Category:Sound,3
23,24,Immersion (virtual reality),"Immersion into virtual reality is a perception of being physically present in a non-physical world. The perception is created by surrounding the user of the VR system in images, sound or other stimuli that provide an engrossing total environment.
The name is a metaphoric use of the experience of submersion applied to representation, fiction or simulation. Immersion can also be defined as the state of consciousness where a ""visitor"" (Maurice Benayoun) or ""immersant"" (Char Davies)'s awareness of physical self is transformed by being surrounded in an artificial environment; used for describing partial or complete suspension of disbelief, enabling action or reaction to stimulations encountered in a virtual or artistic environment. The degree to which the virtual or artistic environment faithfully reproduces reality determines the degree of suspension of disbelief. The greater the suspension of disbelief, the greater the degree of presence achieved.

Types
According to Ernest W. Adams, author and consultant on game design, immersion can be separated into three main categories:
Tactical immersion
Tactical immersion is experienced when performing tactile operations that involve skill. Players feel ""in the zone"" while perfecting actions that result in success.
Strategic immersion
Strategic immersion is more cerebral, and is associated with mental challenge. Chess players experience strategic immersion when choosing a correct solution among a broad array of possibilities.
Narrative immersion
Narrative immersion occurs when players become invested in a story, and is similar to what is experienced while reading a book or watching a movie.
Staffan Björk and Jussi Holopainen, in Patterns In Game Design, divide immersion into similar categories, but call them sensory-motoric immersion, cognitive immersion and emotional immersion, respectively. In addition to these, they add a new category:
Spatial immersion
Spatial immersion occurs when a player feels the simulated world is perceptually convincing. The player feels that he or she is really ""there"" and that a simulated world looks and feels ""real"".

Presence
Presence, a term derived from the shortening of the original ""telepresence"", is a phenomenon enabling people to interact with and feel connected to the world outside their physical bodies via technology. It is defined as a person's subjective sensation of being there in a scene depicted by a medium, usually virtual in nature. Most designers focus on the technology used to create a high-fidelity virtual environment; however, the human factors involved in achieving a state of presence must be taken into account as well. It is the subjective perception, although generated by and/or filtered through human-made technology, that ultimately determines the successful attainment of presence.
Virtual reality glasses can produce a visceral feeling of being in a simulated world, a form of spatial immersion called Presence. According to Oculus VR, the technology requirements to achieve this visceral reaction are low-latency and precise tracking of movements.
Michael Abrash gave a talk on VR at Steam Dev Days in 2014. According to the VR research team at Valve, all of the following are needed to establish presence.
A wide field of view (80 degrees or better)
Adequate resolution (1080p or better)
Low pixel persistence (3 ms or less)
A high enough refresh rate (>60 Hz, 95 Hz is enough but less may be adequate)
Global display where all pixels are illuminated simultaneously (rolling display may work with eye tracking.)
Optics (at most two lenses per eye with trade-offs, ideal optics not practical using current technology)
Optical calibration
Rock-solid tracking – translation with millimeter accuracy or better, orientation with quarter degree accuracy or better, and volume of 1.5 meter or more on a side
Low latency (20 ms motion to last photon, 25 ms may be good enough)

Immersive virtual reality
Immersive virtual reality is a hypothetical future technology that exists today as virtual reality art projects, for the most part. It consists of immersion in an artificial environment where the user feels just as immersed as they usually feel in consensus reality.

Direct interaction of the nervous system
The most considered method would be to induce the sensations that made up the virtual reality in the nervous system directly. In functionalism/conventional biology we interact with consensus reality through the nervous system. Thus we receive all input from all the senses as nerve impulses. It gives your neurons a feeling of heightened sensation. It would involve the user receiving inputs as artificially stimulated nerve impulses, the system would receive the CNS outputs (natural nerve impulses) and process them allowing the user to interact with the virtual reality. Natural impulses between the body and central nervous system would need to be prevented. This could be done by blocking out natural impulses using nanorobots which attach themselves to the brain wiring, whilst receiving the digital impulses of which describe the virtual world, which could then be sent into the wiring of the brain. A feedback system between the user and the computer which stores the information would also be needed. Considering how much information would be required for such a system, it is likely that it would be based on hypothetical forms of computer technology.

Requirements
Understanding of the nervous system
A comprehensive understanding of which nerve impulses correspond to which sensations, and which motor impulses correspond to which muscle contractions will be required. This will allow the correct sensations in the user, and actions in the virtual reality to occur. The Blue Brain Project is the current, most promising research with the idea of understanding how the brain works by building very large scale computer models.
Ability to manipulate CNS
The nervous system would obviously need to be manipulated. Whilst non-invasive devices using radiation have been postulated, invasive cybernetic implants are likely to become available sooner and be more accurate. Manipulation could occur at any stage of the nervous system – the spinal cord is likely to be simplest; as all nerves pass through here, this could be the only site of manipulation. Molecular Nanotechnology is likely to provide the degree of precision required and could allow the implant to be built inside the body rather than be inserted by an operation.
Computer hardware/software to process inputs/outputs
A very powerful computer would be necessary for processing virtual reality complex enough to be nearly indistinguishable from consensus reality and interacting with central nervous system fast enough.

Immersive digital environments
An immersive digital environment is an artificial, interactive, computer-created scene or ""world"" within which a user can immerse themselves.
Immersive digital environments could be thought of as synonymous with virtual reality, but without the implication that actual ""reality"" is being simulated. An immersive digital environment could be a model of reality, but it could also be a complete fantasy user interface or abstraction, as long as the user of the environment is immersed within it. The definition of immersion is wide and variable, but here it is assumed to mean simply that the user feels like they are part of the simulated ""universe"". The success with which an immersive digital environment can actually immerse the user is dependent on many factors such as believable 3D computer graphics, surround sound, interactive user-input and other factors such as simplicity, functionality and potential for enjoyment. New technologies are currently under development which claim to bring realistic environmental effects to the players' environment – effects like wind, seat vibration and ambient lighting.

Perception
To create a sense of full immersion, the 5 senses (sight, sound, touch, smell, taste) must perceive the digital environment to be physically real. Immersive technology can perceptually fool the senses through:
Panoramic 3D displays (visual)
Surround sound acoustics (auditory)
Haptics and force feedback (tactile)
Smell replication (olfactory)
Taste replication (gustation)

Interaction
Once the senses reach a sufficient belief that the digital environment is real (it is interaction and involvement which can never be real), the user must then be able to interact with the environment in a natural, intuitive manner. Various immersive technologies such as gestural controls, motion tracking, and computer vision respond to the user's actions and movements. Brain control interfaces (BCI) respond to the user's brainwave activity.

Examples and applications
Training and rehearsal simulations run the gamut from part task procedural training (often buttonology, for example: which button do you push to deploy a refueling boom) through situational simulation (such as crisis response or convoy driver training) to full motion simulations which train pilots or soldiers and law enforcement in scenarios that are too dangerous to train in actual equipment using live ordinance.
Computer games from simple arcade to massively multiplayer online game and training programs such as flight and driving simulators. Entertainment environments such as motion simulators that immerse the riders/players in a virtual digital environment enhanced by motion, visual and aural cues. Reality simulators, such as one of the Virunga Mountains in Rwanda that takes you on a trip through the jungle to meet a tribe of mountain gorillas. Or training versions such as one which simulates taking a ride through human arteries and the heart to witness the buildup of plaque and thus learn about cholesterol and health.
In parallel with scientist, artists like Knowbotic Research, Donna Cox, Rebecca Allen, Robbie Cooper, Maurice Benayoun, Char Davies, and Jeffrey Shaw use the potential of immersive virtual reality to create physiologic or symbolic experiences and situations.
Other examples of immersion technology include physical environment / immersive space with surrounding digital projections and sound such as the CAVE, and the use of virtual reality headsets for viewing movies, with head-tracking and computer control of the image presented, so that the viewer appears to be inside the scene. The next generation is VIRTSIM, which achieves total immersion through motion capture and wireless head mounted displays for teams of up to thirteen immersants enabling natural movement through space and interaction in both the virtual and physical space simultaneously.

Use in medical care
New fields of studies linked to the immersive virtual reality emerges every day. Researchers see a great potential in virtual reality tests serving as complementary interview methods in psychiatric care. Immersive virtual reality have in studies also been used as an educational tool in which the visualization of psychotic states have been used to get increased understanding of patients with similar symptoms. New treatment methods are available for schizophrenia and other newly developed research areas where immersive virtual reality is expected to achieve melioration is in education of surgical procedures, rehabilitation program from injuries and surgeries and reduction of phantom limb pain.

Applications in the built environment
In the domain of architectural design and building science, immersive virtual environments are adopted to facilitate architects and building engineers to enhance the design process through assimilating their sense of scale, depth, and spatial awareness. Such platforms integrate the use of virtual reality models and mixed reality technologies in various functions of building science research, construction operations, personnel training, end-user surveys, performance simulations and BIM visualization. Head-mounted displays (with both 3 Degrees of freedom and 6 degrees of freedom systems) and CAVE platforms are used for spatial visualization and Building information modeling (BIM) navigations for different design and evaluation purposes. Clients, architects and building owners use derived applications from game engines to navigate 1:1 scale BIM models, allowing a virtual walkthrough experience of future buildings. For such use cases, the performance improvement of space navigation between virtual reality headsets and 2D desktop screens has been investigated in various studies, with some suggesting significant improvement in virtual reality headsets while others indicate no significant difference. Architects and building engineers can also use immersive design tools to model various building elements in virtual reality CAD interfaces, and apply property modifications to Building information modeling (BIM) files through such environments.
In the building construction phase, immersive environments are used to improve site preparations, on site communication and collaboration of team members, safetyand logistics. For training of construction workers, virtual environments have shown to be highly effective in skill transfer with studies showing similar performance results to training in real environments.Moreover, virtual platforms are also used in the operation phase of buildings to interact and visualize data with Internet of Things (IoT) devices available in buildings, process improvement and also resource management.
Occupant and end-user studies are performed through immersive environments. Virtual immersive platforms engage future occupants in the building design process by providing a sense of presence to users with integrating pre-construction mock-ups and BIM models for the evaluation of alternative design options in the building model in a timely and cost efficient manner. Studies conducting human experiments have shown users perform similarly in daily office activities (object identification, reading speed and comprehension) within immersive virtual environments and benchmarked physical environments. In the field of lighting, virtual reality headsets have been used investigate the influence of façade patterns on the perceptual impressions and satisfaction of a simulated daylit space. Moreover, artificial lighting studies have implemented immersive virtual environments to evaluate end-users lighting preferences of simulated virtual scenes with the controlling of the blinds and artificial lights in the virtual environment.
For structural engineering and analysis, immersive environments enable the user to focus on structural investigations without getting too distracted to operate and navigate the simulation tool. Virtual and Augmented Reality applications have been designed for finite element analysis of shell structures. Using stylus and data gloves as input devices, the user can create, modify mesh, and specify boundary conditions. For a simple geometry, real-time color-coded results are obtained by changing loads on the model. Studies have used artificial neural networks (ANN) or approximation methods to achieve real-time interaction for the complex geometry, and to simulate its impact via haptic gloves. Large scale structures and bridge simulation have also been achieved in immersive virtual environments. The user can move the loads acting on the bridge, and finite element analysis results are updated immediately using an approximate module.

Detrimental effects
Simulation sickness, or simulator sickness, is a condition where a person exhibits symptoms similar to motion sickness caused by playing computer/simulation/video games (Oculus Rift is working to solve simulator sickness).
Motion sickness due to virtual reality is very similar to simulation sickness and motion sickness due to films. In virtual reality, however, the effect is made more acute as all external reference points are blocked from vision, the simulated images are three-dimensional and in some cases stereo sound that may also give a sense of motion. Studies have shown that exposure to rotational motions in a virtual environment can cause significant increases in nausea and other symptoms of motion sickness.
Other behavioural changes such as stress, addiction, isolation and mood changes are also discussed to be side-effects caused by immersive virtual reality.

See also
Footnotes
References
External links
Annual Summit on Immersive Technology
[1] pdf download of Joseph Nechvatal's text book: Immersive Ideals / Critical Distances. LAP Lambert Academic Publishing. 2009
Audio and Game Immersion PhD thesis about game audio (the IEZA Framework) and immersion.
""Improvising Synesthesia: Comprovisation of Generative Graphics and Music"" by Joshua B. Mailman, in Leonardo Electronic Almanac v.19 no.3, Live Visuals, 2013, pp. 352–84, about two immersive systems for improvising music and graphics through dance-like motion detected by an infrared video camera and other sensors.",Category:Emerging technologies,3
24,25,Music without sound,"Music without sound can refer to music that falls outside the range of human hearing (typically 20 Hz–20 kHz) or to compositions, such as visual music, that are analogous to or suggestive of conventional music but in media other than sound, such as color, shape, position, motion and literature (see Discursive music below). It is commonly taken for granted that music is wont to be performed or recorded, but some sound works simply won't fit on a disc or on stage, being either extremely discreet (like Robin Minard's Silent Music) or incomplete (Varèse's Unfinished music). Additionally, silence can be regarded as the via negativa of music and has induced long lasting fascination to music composers of all kinds. A composer deals with the absence of sound as much as they deal with sounds. Therefore, this article includes several examples of apophasis in music (like Algorithmic music or Gesture Music).

Gesture music
Sofia Gubaidulina
Silence in music happens when the music stops during a performance. It is sometimes replaced by gesture music. In his Sofia Gubaidulina biography, Michael Kurtz mentions the silent solo performance by the conductor included in ...Stimmenn... verstummen..., an orchestral work from 1986.
Milan Knizak
From 1960, the International Fluxus Movement created a number of Events or Verbal Pieces, whose temporal structures were typically vague so as to be sometimes without beginning nor end, with or without sound, with or without music. A remarkable example is that of Czech artist Milan Knizak's 1965 Snowstorm N°1 whose score states: Paper gliders are distributed to an idle and waiting audience.
Helmut LachenmannLachenmann composed Salut Für Caldwell for 2 guitars in 1977. The piece includes silent moments when « the players silent motions and gestures created a space of unheard music » [Michael Kurtz]
Takehisa Kosugi
In 1963 Takehisa Kosugi composed for Fluxus 1 a musical piece called Theatre Music in the form of a rectangle of cardstock that bore the trace of a spiral of moving feet. This was paired with the instructions: ""Keep walking intently"".
Juan María Solare
His work Gestenstücke (2008) is a collection of five pieces for 4 performers in which a musical structure is used to put order in non-sounding elements, concretely gestures. For instance, the first piece of the cycle is a canon of gestures. Premiere: University Bremen, Ensemble Neues Musiktheater, June 12, 2008.
Another non-sounding piece is his conceptual work called Tense Atmosphere a graphic score which consists of a silence with a sforzato sign (2013).

Algorithmic music
A sequence of finite instructions according to Wikipedia, an algorithm relates to computation, which ultimately relates to music. Eighteenth century algorithmic music is a contemporary of automaton machines, like Jacques de Vaucanson's duck or Wolfgang von Kempelen's chess player, or Ada Lovelace and Charles Babbage's 1822 difference engine. In 1787, W.A. Mozart (1756–1791) devised an aleatoric system called Musikalisches Würfelspiel (Musical Dice) published 1793 by J.J. Hummel in Berlin-Amsterdam, to compose waltzes with a pair of dice and a set of written bars on paper cards. The combination of all the 16 cards and transitions, of which there are theoretically 
  
    
      
        
          10
          
            29
          
        
      
    
    {\displaystyle 10^{29}}
   combinations, constitutes a minuet.
Austrian composer Maximilian Johann Karl Dominik Stadler, also known as Maximilian Stadler (1748–1833), created a table to compose minuets and trios with a pair of dice. In the case of the minuet version, there are 16 cards with one bar each and preconceived transitions between certain musical measures. Mathematical games columnist Martin Gardner once remarked in an article about automated music composition. ""If you fail to preserve it, it will be a waltz that will probably never be heard again.""
The method of pure aleatoric music was used in the twentieth century by US composer Lejaren Hiller.

The optophonic piano
Wladimir Baranoff-Rossiné started building his optophonic piano in the 1915s. A set of painted glass discs are rotated via the small keyboard. Light is projected through the discs onto the wall. The player can control intensity of light and speed of rotation. 6 or 8 keys of the 3-octaves keyboard are devoted to colored discs. It is not clear what kind of sound the keyboard is able to produce. Oscillator frequencies (as stated in some articles) are rather unlikely between 1915-1920. More probably the light show was performed with piano accompaniment, maybe performed on the reduced keyboard of the Optosonic Piano, akin to a toy piano.

Unfinished/aborted music
At one point, the music exists in the composer's mind. In 1928, Edgar Varèse started working on an opera called L'Astronome based on North American Indian legends, a project he never completed and destroyed the drafts. In 1932, he asked Antonin Artaud to write the libretto of a large scale oratorio, Il n'ya plus de firmament (There is no longer any firmament). In his book Phantasmatic Radio Allen S. Weiss translates the beginning of Artaud's text:
The piece was never completed and Varèse turned to other projects, including a radiophonic work involving various synchronised choirs located in different places of the world. He never found the technology for it. In 1948, Artaud insisted on having noise sounds included in his Pour en finir avec le judgement de Dieu (To have done with the judgment of God). This was done in the Radiodiffusion Television Française studios where Pierre Schaeffer was working at the time.

Silent music
Silent Music (1994) is an installation work by Canadian artist Robin Minard (b1953) with several hundred wall-mounted piezo loudspeakers and four-channel audio. The miniature loudspeakers are displayed in plant-like shapes on the walls of public spaces. They reproduce the minuscule ambient sounds of the public space where they are installed or play random synthetic sounds at barely audible volume. Minard deals with remote sounds best suited to intimate listening. In 2006 he created 'A voir en silence', a small artist book with loudspeakers and hand-written text.

Discursive music
Marcel Proust
The Vinteuil Sonata (French: Sonate de Vinteuil) is an imaginary violin and piano sonata by fictitious composer Vinteuil recurring several times in Marcel Proust's A La Recherche Du Temps Perdu (In Search of Lost Time), particularly in Un Amour De Swann (1913). In the latter volume, Charles Swann associates strong emotions and memories to the melody composed by Vinteuil. The French composer Reynaldo Hahn noticed how much Marcel Proust ""vivait la musique de son temps"" (experienced contemporary music). For example, Proust immediately praised and enjoyed Debussy's 1902 Pelléas et Mélisande opera. Critics disagree on which composer inspired the Sonata. Possibly Gabriel Fauré or César Franck. In Les Plaisirs et les Jours (1896), Proust focusses on Hans Sachs's monologue from Wagner's Die Meistersinger von Nürnberg, Act II. In Jean Santeuil (1952), a Camille Saint-Saëns Sonata for Violin and Piano (op. 75, 1885) plays a key role and is presumably the model for the Sonate de Vinteuil. In Un Amour De Swann, the Vinteuil Sonata is played during evenings at the Verdurins' by pianist Dechambre. The main character's emotions are mirrored by Proust's musical reminiscences.
Walter Marchetti
A good example of imaginary music can be found in a Walter Marchetti poem where he mentions a Juan Hidalgo imaginary composition (both Hidalgo and Marchetti were members of the Spanish Zaj Group of Madrid in the late 1950s).

See also
4?33?: John Cage's silent composition - four minutes thirty-three seconds of silence
List of silent musical compositions


== Footnotes ==",Category:New media art,3
25,26,Category:Sound recording,,Category:Sound,3
26,27,Category:Noise,,Category:Sound,3
27,28,Category:Acoustics,,Category:Sound,3
28,29,Insert (effects processing),"In audio processing and sound reinforcement, an insert is an access point built into the mixing console, allowing the user to add external line level devices into the signal flow between the microphone preamplifier and the mix bus.
Common usages include gating, compressing, equalizing and for reverb effects that are specific to that channel or group. Inserts can be used as an alternate way to route signals such as for multitrack recording output or line level direct input.

Insert jacks
Inserts can be balanced or unbalanced. Typically, higher-end mixers will have balanced inserts and entry level mixers will have unbalanced inserts. Balanced inserts appear as a pair of jacks, one serving as the send (out from the mixer) and the other serving as the return (back to the mixer.) Balanced insert jacks can be XLR, 1/4"" TRS phone connector or Bantam TT.
Unbalanced inserts can also be a pair of jacks such as RCA or 1/4"" TS (Tip Sleeve) phone connector. Again, one jack serves as send and the other serves as return.
Most modern entry level and medium format mixers use a single TRS phone jack for both Send and Return. This dual-purpose insert jack only has three conductors, and balanced lines need at least two conductors. Because two lines share the same three-conductor insert jack, its architecture is necessarily unbalanced, with the two circuits sharing a common ground. Of the mixers using this kind of dual-purpose insert jack, most are designed with Tip Send, Ring Return, though many can still be found with Ring Send, Tip Return. A very few mixers have both architectures present on the same mixer; Tip Send for input channels and Tip Return for mix groups.
Insert jacks are often normalized so that signal is passed through the jack if nothing is inserted but is interrupted when the jack is holding a plug. Inserts with two separate jacks will have normalizing such that the Return jack interrupts signal but the Send jack doesn't. The Send jack can always be counted on to send signal out to external devices. A refinement of the normalization of jacks is the presence on the mixer of an insert ON/OFF button which allows the user to patch into or around the inserted devices at will without having to physically disconnect the insert cables.
Unbalanced TRS phone inserts are normalized as well. The presence of a plug in the jack breaks normal internal signal flow, sending signal out to external devices and returning this signal to the channel. TRS phone jacks can be specially wired with Tip and Ring connected at the insert end, and both conductors going to Tip at the distant end. This allows for tapping the insert point for its signal without interrupting signal flow inside the mixer. A less reliable method to achieve the same end is to insert a TS or TRS phone plug halfway into the insert until there is a springy ""click"" feeling, at which point the plug is contacting the signal within the insert jack, but isn't breaking the normalized contact. The ""half-click"" method works fine until the insert cable is jarred or wiggled, causing noise or a loss of signal within the channel.
Because of the combination of balanced external devices and unbalanced insert jacks, the process of inserting involves finding out which devices have which kinds of output configurations. Full electronic balancing needs a different cabling style than transformer balancing, which in turn needs a different cabling style than impedance balancing. Mistakes in the interconnection may make the inserted signal drop in level by 6 dB or add hum and buzz or even overheat a balanced output circuit on the external device, decreasing its usable life.
Insert jacks themselves can be the source of intermittent signal problems. Internal jack contacts may get too loose over time and they may oxidize, impeding electrical conduction. Regular use of the jack helps keep oxidization down. The manufacturer using high quality jacks and good assembly practices helps reduce failures over time.
Another problem with TRS, TS and TT jacks that come in Send/Return pairs is that the Send jack and plug look just like the Return jack and plug. Cross-patching mistakes are possible, resulting in no signal passing through the insert.

Mixer Implementation
Inserts on analog mixers appear in various locations in the signal flow, depending on the vision of the designer. Most inserts tap the signal after the microphone preamplifier and after the HPF (if present.) Others tap the signal after the channel EQ and before the fader. A few tap the signal after the fader and before the mix buses. Many consoles offer a choice between two, three or four of the possible insert points by a combination of internal jumpers or links that a skilled technician can modify.
Digital consoles are often designed to allow the user to move the virtual insert point before or after the channel EQ and some allow the insert point to be placed after the fader and before the mix buses. These are ""soft"" changes; the options depend largely upon the design of the mixer's user interface and the breadth of processing power devoted to the insert function.
Inserted devices can be connected in series to create a string of inserted devices. For instance, one could connect a gate, a compressor and an equalizer in series through the same channel's insert.
Some digital mixers allow multiple effects to be inserted virtually, still others allow multiple third party plugins to be used as virtual inserts.
Inserts might be found on monoaural mixer inputs, monoaural and stereo subgroups, auxiliary inputs, main outputs and matrix outputs. They're rarely found on stereo line level inputs. EQs are commonly inserted on monitor mixer output mixes so that the monitor engineer can use his own wedge and the PFL/Solo bus to hear what the artist's wedge sounds like without having to climb on stage to check.

Signal Levels
Similar to input preamplifiers and outputs, insert points are found at a variety of signal levels. Most are designed to handle a nominal -10 dBV consumer line level or +4 dBu professional line level, although variations may be found. Most balanced inserts are at +4 dBu nominal level. Both analog and digital designs feature headroom, allowing transients exceeding the nominal level to be handled without distortion. For example, a digital console might design their inserts such as a +4 dBu signal corresponds to a -20 dBFS digital representation, effectively yielding 20 dB of headroom. For optimum gain staging and the least amount of system hiss, inserted devices should be chosen with regard to the signal level they can handle and the signal level the mixer can handle. Best gain staging is achieved when both insert and inserted device match in level.

See also
Balanced
Patch bay


== External links ==",Category:Sound,3
29,30,Programme level,"Programme level refers to the signal level that an audio source is transmitted or recorded at, and is important in audio if listeners of Compact Discs (CDs), radio and television are to get the best experience, without excessive noise in quiet periods or distortion of loud sounds. Programme level is often measured using a peak programme meter or a VU meter.
The level of an audio signal is among the most basic of measurements, and yet widespread misunderstanding and disagreement about programme levels has become arguably the greatest single obstacle to high quality sound reproduction.

How it works
Live sound covers an enormous range of levels, but this is not something that can be demonstrated with a conventional sound level meter. Sound level meters respond quite slowly, even on a ""fast"" setting: they use a root mean square (RMS) rectifier which by definition must take a slow running average of the square of the input voltage. Music is complex, and constantly varying, with brief peaks originating from many sources including the initial impact of sticks on cymbals and drums. A loud band might measure 100 dB SPL on a sound level meter, yet have peaks reaching 130 dB SPL or higher.
A recording system must handle these peaks; they can be measured using a peak responding meter with an integration time of 0.5 ms or less (not a standard IEC type PPM which has a longer integration time).
The sound level meter is useless for properly assessing noise levels, since the commonly used A-weighting is based on equal-loudness contours for pure tones, and is not valid for the random noise.
The subjective loudness of noise is best measured using a noise-meter to the ITU-R 468 noise weighting standard. The chart below shows, on this basis, the real range of live music, and then the level capabilities of various stages in the audio chain, from microphone to loudspeaker.

Analysing programme levels
This chart is based on the assumption that what goes in should come out—true high-fidelity—and so an Alignment Level (AL) corresponding to 100 dB SPL has been assumed throughout. Any lower level would imply severe clipping at the first stage; the master recording. Top quality microphones do not present a problem; most will handle 130 dB SPL without severe distortion, and a few manage more than 140 dB SPL.
The master recording process, using current 24-bit techniques, offers around 99 dB of ""true"" dynamic range (based on the ITU-R 468 noise weighting standard); identical to the dynamic range of a good studio microphone, though very few recordings will use just one microphone, and so the noise on most recordings is likely to be the sum of several microphones after mixing, and probably at least 6 dB worse than shown.

See also
Audio system measurements
Noise measurement
Weighting filter
Equal-loudness contour
Fletcher-Munson curves

External links
EBU Recommendation R68-2000
AES Preprint 4828 - Levels in Digital Audio Broadcasting by Neil Gilchrist (not free)
EBU Recommendation R117-2006 (against loudness war)
AES Convention Paper 5538 On Levelling and Loudness Problems at Broadcast Studios
EBU R89-1997 on CD-R levels",Category:All articles lacking sources,3
30,31,Sound pressure,"Sound pressure or acoustic pressure is the local pressure deviation from the ambient (average or equilibrium) atmospheric pressure, caused by a sound wave. In air, sound pressure can be measured using a microphone, and in water with a hydrophone. The SI unit of sound pressure is the pascal (Pa).

Mathematical definition
A sound wave in a transmission medium causes a deviation (sound pressure, a dynamic pressure) in the local ambient pressure, a static pressure.
Sound pressure, denoted p, is defined by

  
    
      
        
          p
          
            
              t
              o
              t
              a
              l
            
          
        
        =
        
          p
          
            
              s
              t
              a
              t
            
          
        
        +
        p
        ,
      
    
    {\displaystyle p_{\mathrm {total} }=p_{\mathrm {stat} }+p,}
  
where
ptotal is the total pressure;
pstat is the static pressure.

Sound measurements
Sound intensity
In a sound wave, the complementary variable to sound pressure is the particle velocity. Together, they determine the sound intensity of the wave.Sound intensity, denoted I and measured in W·m?2 in SI units, is defined by

  
    
      
        
          I
        
        =
        p
        
          v
        
        ,
      
    
    {\displaystyle \mathbf {I} =p\mathbf {v} ,}
  
where
p is the sound pressure;
v is the particle velocity.

Acoustic impedance
Acoustic impedance, denoted Z and measured in Pa·m?3·s in SI units, is defined by

  
    
      
        Z
        (
        s
        )
        =
        
          
            
              
                
                  
                    p
                    ^
                  
                
              
              (
              s
              )
            
            
              
                
                  
                    Q
                    ^
                  
                
              
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle Z(s)={\frac {{\hat {p}}(s)}{{\hat {Q}}(s)}},}
  
where

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {p}}(s)}
   is the Laplace transform of sound pressure;

  
    
      
        
          
            
              Q
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {Q}}(s)}
   is the Laplace transform of sound volume flow rate.
Specific acoustic impedance, denoted z and measured in Pa·m?1·s in SI units, is defined by

  
    
      
        z
        (
        s
        )
        =
        
          
            
              
                
                  
                    p
                    ^
                  
                
              
              (
              s
              )
            
            
              
                
                  
                    v
                    ^
                  
                
              
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle z(s)={\frac {{\hat {p}}(s)}{{\hat {v}}(s)}},}
  
where

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {p}}(s)}
   is the Laplace transform of sound pressure;

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        s
        )
      
    
    {\displaystyle {\hat {v}}(s)}
   is the Laplace transform of particle velocity.

Particle displacement
The particle displacement of a progressive sine wave is given by

  
    
      
        ?
        (
        
          r
        
        ,
        
        t
        )
        =
        
          ?
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            ?
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle \delta (\mathbf {r} ,\,t)=\delta _{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}),}
  
where

  
    
      
        
          ?
          
            
              m
            
          
        
      
    
    {\displaystyle \delta _{\mathrm {m} }}
   is the amplitude of the particle displacement;

  
    
      
        
          ?
          
            ?
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{\delta ,0}}
   is the phase shift of the particle displacement;
k is the angular wavevector;
? is the angular frequency.
It follows that the particle velocity and the sound pressure along the direction of propagation of the sound wave x are given by

  
    
      
        v
        (
        
          r
        
        ,
        
        t
        )
        =
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        
          v
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            v
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle v(\mathbf {r} ,\,t)={\frac {\partial \delta }{\partial t}}(\mathbf {r} ,\,t)=\omega \delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=v_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{v,0}),}
  

  
    
      
        p
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        
          c
          
            2
          
        
        
          
            
              ?
              ?
            
            
              ?
              x
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          c
          
            2
          
        
        
          k
          
            x
          
        
        
          ?
          
            
              m
            
          
        
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        
          p
          
            
              m
            
          
        
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            p
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle p(\mathbf {r} ,\,t)=-\rho c^{2}{\frac {\partial \delta }{\partial x}}(\mathbf {r} ,\,t)=\rho c^{2}k_{x}\delta _{\mathrm {m} }\cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=p_{\mathrm {m} }\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{p,0}),}
  
where
vm is the amplitude of the particle velocity;

  
    
      
        
          ?
          
            v
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}}
   is the phase shift of the particle velocity;
pm is the amplitude of the acoustic pressure;

  
    
      
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{p,0}}
   is the phase shift of the acoustic pressure.
Taking the Laplace transforms of v and p with respect to time yields

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          v
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle {\hat {v}}(\mathbf {r} ,\,s)=v_{\mathrm {m} }{\frac {s\cos \varphi _{v,0}-\omega \sin \varphi _{v,0}}{s^{2}+\omega ^{2}}},}
  

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          p
          
            
              m
            
          
        
        
          
            
              s
              cos
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\hat {p}}(\mathbf {r} ,\,s)=p_{\mathrm {m} }{\frac {s\cos \varphi _{p,0}-\omega \sin \varphi _{p,0}}{s^{2}+\omega ^{2}}}.}
  
Since 
  
    
      
        
          ?
          
            v
            ,
            0
          
        
        =
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}=\varphi _{p,0}}
  , the amplitude of the specific acoustic impedance is given by

  
    
      
        
          z
          
            
              m
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        
          |
        
        z
        (
        
          r
        
        ,
        
        s
        )
        
          |
        
        =
        
          |
          
            
              
                
                  
                    
                      p
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
              
                
                  
                    
                      v
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
            
          
          |
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              v
              
                
                  m
                
              
            
          
        
        =
        
          
            
              ?
              
                c
                
                  2
                
              
              
                k
                
                  x
                
              
            
            ?
          
        
        .
      
    
    {\displaystyle z_{\mathrm {m} }(\mathbf {r} ,\,s)=|z(\mathbf {r} ,\,s)|=\left|{\frac {{\hat {p}}(\mathbf {r} ,\,s)}{{\hat {v}}(\mathbf {r} ,\,s)}}\right|={\frac {p_{\mathrm {m} }}{v_{\mathrm {m} }}}={\frac {\rho c^{2}k_{x}}{\omega }}.}
  
Consequently, the amplitude of the particle displacement is related to that of the acoustic velocity and the sound pressure by

  
    
      
        
          ?
          
            
              m
            
          
        
        =
        
          
            
              v
              
                
                  m
                
              
            
            ?
          
        
        ,
      
    
    {\displaystyle \delta _{\mathrm {m} }={\frac {v_{\mathrm {m} }}{\omega }},}
  

  
    
      
        
          ?
          
            
              m
            
          
        
        =
        
          
            
              p
              
                
                  m
                
              
            
            
              ?
              
                z
                
                  
                    m
                  
                
              
              (
              
                r
              
              ,
              
              s
              )
            
          
        
        .
      
    
    {\displaystyle \delta _{\mathrm {m} }={\frac {p_{\mathrm {m} }}{\omega z_{\mathrm {m} }(\mathbf {r} ,\,s)}}.}

Inverse-proportional law
When measuring the sound pressure created by an object, it is important to measure the distance from the object as well, since the sound pressure of a spherical sound wave decreases as 1/r from the centre of the sphere (and not as 1/r2, like the sound intensity):

  
    
      
        p
        (
        r
        )
        ?
        
          
            1
            r
          
        
        .
      
    
    {\displaystyle p(r)\propto {\frac {1}{r}}.}
  
This relationship is an inverse-proportional law.
If the sound pressure p1 is measured at a distance r1 from the centre of the sphere, the sound pressure p2 at another position r2 can be calculated:

  
    
      
        
          p
          
            2
          
        
        =
        
          
            
              r
              
                1
              
            
            
              r
              
                2
              
            
          
        
        
        
          p
          
            1
          
        
        .
      
    
    {\displaystyle p_{2}={\frac {r_{1}}{r_{2}}}\,p_{1}.}
  
The inverse-proportional law for sound pressure comes from the inverse-square law for sound intensity:

  
    
      
        I
        (
        r
        )
        ?
        
          
            1
            
              r
              
                2
              
            
          
        
        .
      
    
    {\displaystyle I(r)\propto {\frac {1}{r^{2}}}.}
  
Indeed,

  
    
      
        I
        (
        r
        )
        =
        p
        (
        r
        )
        v
        (
        r
        )
        =
        p
        (
        r
        )
        [
        p
        ?
        
          z
          
            ?
            1
          
        
        ]
        (
        r
        )
        ?
        
          p
          
            2
          
        
        (
        r
        )
        ,
      
    
    {\displaystyle I(r)=p(r)v(r)=p(r)[p*z^{-1}](r)\propto p^{2}(r),}
  
where

  
    
      
        ?
      
    
    {\displaystyle *}
   is the convolution operator;
z ?1 is the convolution inverse of the specific acoustic impedance,
hence the inverse-proportional law:

  
    
      
        p
        (
        r
        )
        ?
        
          
            1
            r
          
        
        .
      
    
    {\displaystyle p(r)\propto {\frac {1}{r}}.}
  
The sound pressure may vary in direction from the centre of the sphere as well, so measurements at different angles may be necessary, depending on the situation. An obvious example of a sound source whose spherical sound wave varies in level in different directions is a bullhorn.

Sound pressure level
Sound pressure level (SPL) or acoustic pressure level is a logarithmic measure of the effective pressure of a sound relative to a reference value.
Sound pressure level, denoted Lp and measured in dB, is defined by

  
    
      
        
          L
          
            p
          
        
        =
        ln
        
        
          (
          
            
              p
              
                p
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        2
        
          log
          
            10
          
        
        
        
          (
          
            
              p
              
                p
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              p
              
                p
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{p}=\ln \!\left({\frac {p}{p_{0}}}\right)\!~\mathrm {Np} =2\log _{10}\!\left({\frac {p}{p_{0}}}\right)\!~\mathrm {B} =20\log _{10}\!\left({\frac {p}{p_{0}}}\right)\!~\mathrm {dB} ,}
  
where
p is the root mean square sound pressure;
p0 is the reference sound pressure;
1 Np is the neper;
1 B = (1/2 ln 10) Np is the bel;
1 dB = (1/20 ln 10) Np is the decibel.
The commonly used reference sound pressure in air is

  
    
      
        
          p
          
            0
          
        
        =
        20
         
        
          ?
          P
          a
        
        ,
      
    
    {\displaystyle p_{0}=20~\mathrm {\mu Pa} ,}
  
which is often considered as the threshold of human hearing (roughly the sound of a mosquito flying 3 m away). The proper notations for sound pressure level using this reference are Lp/(20 ?Pa) or Lp (re 20 ?Pa), but the suffix notations dB SPL, dB(SPL), dBSPL, or dBSPL are very common, even if they are not accepted by the SI.
Most sound level measurements will be made relative to this reference, meaning 1 Pa will equal an SPL of 94 dB. In other media, such as underwater, a reference level of 1 ?Pa is used. These references are defined in ANSI S1.1-1994.

Examples
The lower limit of audibility is defined as SPL of 0 dB, but the upper limit is not as clearly defined. While 1 atm (194 dB Peak or 191 dB SPL) is the largest pressure variation an undistorted sound wave can have in Earth's atmosphere, larger sound waves can be present in other atmospheres or other media such as under water, or through the Earth.

Ears detect changes in sound pressure. Human hearing does not have a flat spectral sensitivity (frequency response) relative to frequency versus amplitude. Humans do not perceive low- and high-frequency sounds as well as they perceive sounds between 3,000 and 4,000 Hz, as shown in the equal-loudness contour. Because the frequency response of human hearing changes with amplitude, three weightings have been established for measuring sound pressure: A, B and C. A-weighting applies to sound pressures levels up to 55 dB, B-weighting applies to sound pressures levels between 55 dB and 85 dB, and C-weighting is for measuring sound pressure levels above 85 dB.
In order to distinguish the different sound measures a suffix is used: A-weighted sound pressure level is written either as dBA or LA. B-weighted sound pressure level is written either as dBB or LB, and C-weighted sound pressure level is written either as dBC or LC. Unweighted sound pressure level is called ""linear sound pressure level"" and is often written as dBL or just L. Some sound measuring instruments use the letter ""Z"" as an indication of linear SPL.

Distance
The distance of the measuring microphone from a sound source is often omitted when SPL measurements are quoted, making the data useless. In the case of ambient environmental measurements of ""background"" noise, distance need not be quoted as no single source is present, but when measuring the noise level of a specific piece of equipment the distance should always be stated. A distance of one metre (1 m) from the source is a frequently used standard distance. Because of the effects of reflected noise within a closed room, the use of an anechoic chamber allows for sound to be comparable to measurements made in a free field environment.
According to the inverse proportional law, when sound level Lp1 is measured at a distance r1, the sound level Lp2 at the distance r2 is

  
    
      
        
          L
          
            
              p
              
                2
              
            
          
        
        =
        
          L
          
            
              p
              
                1
              
            
          
        
        +
        20
        
          log
          
            10
          
        
        
        
          (
          
            
              
                r
                
                  1
                
              
              
                r
                
                  2
                
              
            
          
          )
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{p_{2}}=L_{p_{1}}+20\log _{10}\!\left({\frac {r_{1}}{r_{2}}}\right)\!~\mathrm {dB} .}

Multiple sources
The formula for the sum of the sound pressure levels of n incoherent radiating sources is

  
    
      
        
          L
          
            ?
          
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              
                
                  
                    
                      p
                      
                        1
                      
                    
                  
                  
                    2
                  
                
                +
                
                  
                    
                      p
                      
                        2
                      
                    
                  
                  
                    2
                  
                
                +
                …
                +
                
                  
                    
                      p
                      
                        n
                      
                    
                  
                  
                    2
                  
                
              
              
                
                  
                    p
                    
                      0
                    
                  
                
                
                  2
                
              
            
          
          )
        
        
         
        
          d
          B
        
        =
        10
        
          log
          
            10
          
        
        
        
          [
          
            
              
                (
                
                  
                    
                      p
                      
                        1
                      
                    
                    
                      p
                      
                        0
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            
              
                (
                
                  
                    
                      p
                      
                        2
                      
                    
                    
                      p
                      
                        0
                      
                    
                  
                
                )
              
              
                2
              
            
            +
            …
            +
            
              
                (
                
                  
                    
                      p
                      
                        n
                      
                    
                    
                      p
                      
                        0
                      
                    
                  
                
                )
              
              
                2
              
            
          
          ]
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{\Sigma }=10\log _{10}\!\left({\frac {{p_{1}}^{2}+{p_{2}}^{2}+\ldots +{p_{n}}^{2}}{{p_{0}}^{2}}}\right)\!~\mathrm {dB} =10\log _{10}\!\left[\left({\frac {p_{1}}{p_{0}}}\right)^{2}+\left({\frac {p_{2}}{p_{0}}}\right)^{2}+\ldots +\left({\frac {p_{n}}{p_{0}}}\right)^{2}\right]\!~\mathrm {dB} .}
  
Inserting the formulas

  
    
      
        
          
            (
            
              
                
                  p
                  
                    i
                  
                
                
                  p
                  
                    0
                  
                
              
            
            )
          
          
            2
          
        
        =
        
          10
          
            
              
                L
                
                  i
                
              
              
                10
                
                
                  d
                  B
                
              
            
          
        
        ,
        
        i
        =
        1
        ,
        
        2
        ,
        
        …
        ,
        
        n
        ,
      
    
    {\displaystyle \left({\frac {p_{i}}{p_{0}}}\right)^{2}=10^{\frac {L_{i}}{10\,\mathrm {dB} }},\quad i=1,\,2,\,\ldots ,\,n,}
  
in the formula for the sum of the sound pressure levels yields

  
    
      
        
          L
          
            ?
          
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              10
              
                
                  
                    L
                    
                      1
                    
                  
                  
                    10
                    
                    
                      d
                      B
                    
                  
                
              
            
            +
            
              10
              
                
                  
                    L
                    
                      2
                    
                  
                  
                    10
                    
                    
                      d
                      B
                    
                  
                
              
            
            +
            …
            +
            
              10
              
                
                  
                    L
                    
                      n
                    
                  
                  
                    10
                    
                    
                      d
                      B
                    
                  
                
              
            
          
          )
        
        
         
        
          d
          B
        
        .
      
    
    {\displaystyle L_{\Sigma }=10\log _{10}\!\left(10^{\frac {L_{1}}{10\,\mathrm {dB} }}+10^{\frac {L_{2}}{10\,\mathrm {dB} }}+\ldots +10^{\frac {L_{n}}{10\,\mathrm {dB} }}\right)\!~\mathrm {dB} .}

Examples of sound pressure
*All values listed are the effective sound pressure unless otherwise stated.

See also
Acoustics
Phon (unit)
Loudness
Sone (unit)
Sound level meter
Stevens' power law
Weber–Fechner law, especially The case of sound

References
General
Beranek, Leo L., Acoustics (1993), Acoustical Society of America, ISBN 0-88318-494-X.
Daniel R. Raichel, The Science and Applications of Acoustics (2006), Springer New York, ISBN 1441920803.

External links
Sound Pressure and Sound Power, Effect and Cause
Conversion of Sound Pressure to Sound Pressure Level and Vice Versa
Table of Sound Levels, Corresponding Sound Pressure and Sound Intensity
Ohm's Law as Acoustic Equivalent, Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
Sound Pressure and Sound Power, Two Commonly Confused Characteristics of Sound
How Many Decibels Is Twice as Loud? Sound Level Change and the Respective Factor of Sound Pressure or Sound Intensity
Decibel (Loudness) Comparison Chart",Category:Physical quantities,3
31,32,A-weighting,"A-weighting is the most commonly used of a family of curves defined in the International standard IEC 61672:2003 and various national standards relating to the measurement of sound pressure level. A-weighting is applied to instrument-measured sound levels in an effort to account for the relative loudness perceived by the human ear, as the ear is less sensitive to low audio frequencies. It is employed by arithmetically adding a table of values, listed by octave or third-octave bands, to the measured sound pressure levels in dB. The resulting octave band measurements are usually added (logarithmic method) to provide a single A-weighted value describing the sound; the units are written as dB(A). Other weighting sets of values – B, C, D and now Z – are discussed below.
The curves were originally defined for use at different average sound levels, but A-weighting, though originally intended only for the measurement of low-level sounds (around 40 phon), is now commonly used for the measurement of environmental noise and industrial noise, as well as when assessing potential hearing damage and other noise health effects at all sound levels; indeed, the use of A-frequency-weighting is now mandated for all these measurements, although it is badly suited for these purposes, being only applicable to low levels so that it tends to devalue the effects of low frequency noise in particular. It is also used when measuring low-level noise in audio equipment, especially in the United States. In Britain, Europe and many other parts of the world, broadcasters and audio engineers more often use the ITU-R 468 noise weighting, which was developed in the 1960s based on research by the BBC and other organizations. This research showed that our ears respond differently to random noise, and the equal-loudness curves on which the A, B and C weightings were based are really only valid for pure single tones.

History
A-weighting began with work by Fletcher and Munson which resulted in their publication, in 1933, of a set of equal-loudness contours. Three years later these curves were used in the first American standard for sound level meters. This ANSI standard, later revised as ANSI S1.4-1981, incorporated B-weighting as well as the A-weighting curve, recognising the unsuitability of the latter for anything other than low-level measurements. But B-weighting has since fallen into disuse. Later work, first by Zwicker and then by Schomer, attempted to overcome the difficulty posed by different levels, and work by the BBC resulted in the CCIR-468 weighting, currently maintained as ITU-R 468 noise weighting, which gives more representative readings on noise as opposed to pure tones.

Deficiencies of A-weighting
A-weighting is only really valid for relatively quiet sounds and for pure tones as it is based on the 40-phon Fletcher–Munson curves which represented an early determination of the equal-loudness contour for human hearing.
Because of perceived discrepancies between early and more recent determinations, the International Organization for Standardization (ISO) recently revised its standard curves as defined in ISO 226, in response to the recommendations of a study coordinated by the Research Institute of Electrical Communication, Tohoku University, Japan. The study produced new curves by combining the results of several studies, by researchers in Japan, Germany, Denmark, UK, and USA. (Japan was the greatest contributor with about 40% of the data.) This has resulted in the recent acceptance of a new set of curves standardized as ISO 226:2003. The report comments on the surprisingly large differences, and the fact that the original Fletcher–Munson contours are in better agreement with recent results than the Robinson-Dadson, which appear to differ by as much as 10–15 dB especially in the low-frequency region, for reasons that are not explained. Fortuitously, the 40-phon Fletcher–Munson curve is particularly close to the modern ISO 226:2003 standard.
Nevertheless, it will be noted that A-weighting would be a better match to the loudness curve if it fell much more steeply above 10 kHz, and it is likely that this compromise came about because steep filters were difficult to construct in the early days of electronics. Nowadays, no such limitation need exist, as demonstrated by the ITU-R 468 curve. If A-weighting is used without further band-limiting it is possible to obtain different readings on different instruments when ultrasonic, or near ultrasonic noise is present. Accurate measurements therefore require a 20 kHz low-pass filter to be combined with the A-weighting curve in modern instruments. This is defined in IEC 61012 as AU weighting and while very desirable, is rarely fitted to commercial sound level meters.

B-, C-, D- and Z-weightings
A-frequency-weighting is mandated by the international standard IEC 61672 to be fitted to all sound level meters. The old B- and D-frequency-weightings have fallen into disuse, but many sound level meters provide for C frequency-weighting and its fitting is mandated — at least for testing purposes — to precision (Class one) sound level meters. D-frequency-weighting was specifically designed for use when measuring high level aircraft noise in accordance with the IEC 537 measurement standard. The large peak in the D-weighting curve is not a feature of the equal-loudness contours, but reflects the fact that humans hear random noise differently from pure tones, an effect that is particularly pronounced around 6 kHz. This is because individual neurons from different regions of the cochlea in the inner ear respond to narrow bands of frequencies, but the higher frequency neurons integrate a wider band and hence signal a louder sound when presented with noise containing many frequencies than for a single pure tone of the same pressure level. Following changes to the ISO standard, D-frequency-weighting should now only be used for non-bypass engines and as these are not fitted to commercial aircraft — but only to military ones — A-frequency-weighting is now mandated for light civilian aircraft measurements, while a more accurate loudness-corrected weighting EPNdB is required for certification of large transport aircraft
Z- or ZERO frequency-weighting was introduced in the International Standard IEC 61672 in 2003 and was intended to replace the ""Flat"" or ""Linear"" frequency weighting often fitted by manufacturers. This change was needed as each sound level meter manufacturer could choose their own low and high frequency cut-offs (–3 dB) points, resulting in different readings, especially when peak sound level was being measured. As well, the C-frequency-weighting, with –3 dB points at 31.5 Hz and 8 kHz did not have a sufficient bandpass to allow the sensibly correct measurement of true peak noise (Lpk)
B- and D-frequency-weightings are no longer described in the body of the standard IEC 61672 : 2003, but their frequency responses can be found in the older IEC 60651, although that has been formally withdrawn by the International Electro-technical Commission in favour of IEC 61672 : 2003. The frequency weighting tolerances in IEC 61672 have been tightened over those in the earlier standards IEC 179 and IEC 60651 and thus instruments complying with the earlier specifications should no longer be used for legally required measurements.

Environmental and other noise measurements
A-weighted decibels are abbreviated dB(A) or dBA. When acoustic (calibrated microphone) measurements are being referred to, then the units used will be dB SPL referenced to 20 micropascals = 0 dB SPL. dBrn adjusted is not a synonym for dBA, but for dBa (in telecommunications dBa denotes ""decibels adjusted"" i.e. weighted absolute noise power, which has nothing to do with A-weighting).
The A-weighting curve has been widely adopted for environmental noise measurement, and is standard in many sound level meters. The A-weighting system is used in any measurement of environmental noise (examples of which include roadway noise, rail noise, aircraft noise). A-weighting is also in common use for assessing potential hearing damage caused by loud noise.
A-weighted SPL measurements of noise level are increasingly found on sales literature for domestic appliances such as refrigerators, freezers and computer fans. In Europe, the A-weighted noise level is used for instance for normalizing the noise of tires on cars.
The A-weighting is also used for noise dose measurements at work. A noise level of more than 85 dB(A) each day increases the risk factor for hearing damage.
Noise exposure for visitors of venues with loud music is usually also expressed in dB(A), although the presence of high levels of low frequency noise does not justify this.

Audio reproduction and broadcasting equipment
Although the A-weighting curve, in widespread use for noise measurement, is said to have been based on the 40-phon Fletcher-Munson curve, research in the 1960s demonstrated that determinations of equal-loudness made using pure tones are not directly relevant to our perception of noise. This is because the cochlea in our inner ear analyses sounds in terms of spectral content, each 'hair-cell' responding to a narrow band of frequencies known as a critical band. The high-frequency bands are wider in absolute terms than the low frequency bands, and therefore 'collect' proportionately more power from a noise source. However, when more than one critical band is stimulated, the outputs of the various bands are summed by the brain to produce an impression of loudness. For these reasons equal-loudness curves derived using noise bands show an upwards tilt above 1 kHz and a downward tilt below 1 kHz when compared to the curves derived using pure tones.
This enhanced sensitivity to noise in the region of 6 kHz became particularly apparent in the late 1960s with the introduction of compact cassette recorders and Dolby-B noise reduction. A-weighted noise measurements were found to give misleading results because they did not give sufficient prominence to the 6 kHz region where the noise reduction was having greatest effect, and did not sufficiently attenuate noise around 10 kHz and above (a particular example is with the 19 kHz pilot tone on FM radio systems which, though usually inaudible is not sufficiently attenuated by A-weighting, so that sometimes one piece of equipment would even measure worse than another and yet sound better, because of differing spectral content.
ITU-R 468 noise weighting was therefore developed to more accurately reflect the subjective loudness of all types of noise, as opposed to tones. This curve, which came out of work done by the BBC Research Department, and was standardised by the CCIR and later adopted by many other standards bodies (IEC, BSI) and, as of 2006, is maintained by the ITU. It became widely used in Europe, especially in broadcasting, and was adopted by Dolby Laboratories who realised its superior validity for their purposes when measuring noise on film soundtracks and compact cassette systems. Its advantages over A-weighting is less understood in the US, where the use of A-weighting still predominates. It is universally used by broadcasters in Britain, Europe, and former countries of the British Empire such as Australia and South Africa.

Function realisation of some common weightings
The standard defines weightings (
  
    
      
        A
        (
        f
        )
        ,
        C
        (
        f
        )
      
    
    {\displaystyle A(f),C(f)}
  ) in dB units by tables with tolerance limits (to allow a variety of implementations). Additionally the underlying weighting functions 
  
    
      
        
          R
          
            X
          
        
        (
        f
        )
      
    
    {\displaystyle R_{X}(f)}
   to calculate the weightings are described in the standard. The weighting function 
  
    
      
        
          R
          
            X
          
        
        (
        f
        )
      
    
    {\displaystyle R_{X}(f)}
   is applied to the amplitude spectrum (not the intensity spectrum) of the unweighted sound level. Appropriate weighting functions are:

A
R
          
            A
          
        
        (
        f
        )
        =
        
          
            
              
                12194
                
                  2
                
              
              ?
              
                f
                
                  4
                
              
            
            
              (
              
                f
                
                  2
                
              
              +
              
                20.6
                
                  2
                
              
              )
              
              
                
                  (
                  
                    f
                    
                      2
                    
                  
                  +
                  
                    107.7
                    
                      2
                    
                  
                  )
                  
                  (
                  
                    f
                    
                      2
                    
                  
                  +
                  
                    737.9
                    
                      2
                    
                  
                  )
                
              
              
              (
              
                f
                
                  2
                
              
              +
              
                12194
                
                  2
                
              
              )
            
          
        
         
        ,
      
    
    {\displaystyle R_{A}(f)={12194^{2}\cdot f^{4} \over (f^{2}+20.6^{2})\quad {\sqrt {(f^{2}+107.7^{2})\,(f^{2}+737.9^{2})}}\quad (f^{2}+12194^{2})}\ ,}
  

  
    
      
        A
        (
        f
        )
        =
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              A
            
          
          (
          f
          )
          )
        
        +
        2.00
      
    
    {\displaystyle A(f)=20\log _{10}\left(R_{A}(f)\right)+2.00}

B
R
          
            B
          
        
        (
        f
        )
        =
        
          
            
              
                12194
                
                  2
                
              
              ?
              
                f
                
                  3
                
              
            
            
              (
              
                f
                
                  2
                
              
              +
              
                20.6
                
                  2
                
              
              )
              
              
                
                  (
                  
                    f
                    
                      2
                    
                  
                  +
                  
                    158.5
                    
                      2
                    
                  
                  )
                
              
              
              (
              
                f
                
                  2
                
              
              +
              
                12194
                
                  2
                
              
              )
            
          
        
         
        ,
      
    
    {\displaystyle R_{B}(f)={12194^{2}\cdot f^{3} \over (f^{2}+20.6^{2})\quad {\sqrt {(f^{2}+158.5^{2})}}\quad (f^{2}+12194^{2})}\ ,}
  

  
    
      
        B
        (
        f
        )
        =
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              B
            
          
          (
          f
          )
          )
        
        +
        0.17
      
    
    {\displaystyle B(f)=20\log _{10}\left(R_{B}(f)\right)+0.17}

C
R
          
            C
          
        
        (
        f
        )
        =
        
          
            
              
                12194
                
                  2
                
              
              ?
              
                f
                
                  2
                
              
            
            
              (
              
                f
                
                  2
                
              
              +
              
                20.6
                
                  2
                
              
              )
              
              (
              
                f
                
                  2
                
              
              +
              
                12194
                
                  2
                
              
              )
            
          
        
         
        ,
      
    
    {\displaystyle R_{C}(f)={12194^{2}\cdot f^{2} \over (f^{2}+20.6^{2})\quad (f^{2}+12194^{2})}\ ,}
  

  
    
      
        C
        (
        f
        )
        =
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              C
            
          
          (
          f
          )
          )
        
        +
        0.06
      
    
    {\displaystyle C(f)=20\log _{10}\left(R_{C}(f)\right)+0.06}
  

The offsets (approximately ?2.0, ?0.17 and ?0.06 for A, B and C, respectively) ensure the normalisation to 0 dB at 1000 Hz. Precisely, the respective offset values are 
  
    
      
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              A
            
          
          (
          1000
          )
          )
        
      
    
    {\displaystyle 20\log _{10}\left(R_{A}(1000)\right)}
  , 
  
    
      
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              B
            
          
          (
          1000
          )
          )
        
      
    
    {\displaystyle 20\log _{10}\left(R_{B}(1000)\right)}
  , and 
  
    
      
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              C
            
          
          (
          1000
          )
          )
        
      
    
    {\displaystyle 20\log _{10}\left(R_{C}(1000)\right)}
  .

D
R
          
            D
          
        
        (
        f
        )
        =
        
          
            f
            
              6.8966888496476
              ?
              
                10
                
                  ?
                  5
                
              
            
          
        
        ?
        
          
            
              
                h
                (
                f
                )
              
              
                (
                
                  f
                  
                    2
                  
                
                +
                79919.29
                )
                
                (
                
                  f
                  
                    2
                  
                
                +
                1345600
                )
              
            
          
        
      
    
    {\displaystyle R_{D}(f)={\frac {f}{6.8966888496476\cdot 10^{-5}}}\cdot {\sqrt {\frac {h(f)}{(f^{2}+79919.29)\,(f^{2}+1345600)}}}}
  

  
    
      
        D
        (
        f
        )
        =
        20
        
          log
          
            10
          
        
        ?
        
          (
          
            R
            
              D
            
          
          (
          f
          )
          )
        
        ,
      
    
    {\displaystyle D(f)=20\log _{10}\left(R_{D}(f)\right),}
  
where

  
    
      
        h
        (
        f
        )
        =
        
          
            
              (
              1037918.48
              ?
              
                f
                
                  2
                
              
              
                )
                
                  2
                
              
              +
              1080768.16
              
              
                f
                
                  2
                
              
            
            
              (
              9837328
              ?
              
                f
                
                  2
                
              
              
                )
                
                  2
                
              
              +
              11723776
              
              
                f
                
                  2
                
              
            
          
        
         
        .
      
    
    {\displaystyle h(f)={\frac {(1037918.48-f^{2})^{2}+1080768.16\,f^{2}}{(9837328-f^{2})^{2}+11723776\,f^{2}}}\ .}

Transfer function equivalent
The gain curves can be realised by the following s-domain transfer functions. They are not defined in this way though, being defined by tables of values with tolerances in the standards documents, thus allowing different realisations:

A
H
          
            A
          
        
        (
        s
        )
        =
        
          
            
              
                k
                
                  A
                
              
              ?
              
                s
                
                  4
                
              
            
            
              (
              s
              +
              129.4
              
                )
                
                  2
                
              
              
              (
              s
              +
              676.7
              )
              
              (
              s
              +
              4636
              )
              
              (
              s
              +
              76655
              
                )
                
                  2
                
              
            
          
        
      
    
    {\displaystyle H_{A}(s)={k_{A}\cdot s^{4} \over (s+129.4)^{2}\quad (s+676.7)\quad (s+4636)\quad (s+76655)^{2}}}
  
kA ? 7.39705×109

B
H
          
            B
          
        
        (
        s
        )
        =
        
          
            
              
                k
                
                  B
                
              
              ?
              
                s
                
                  3
                
              
            
            
              (
              s
              +
              129.4
              
                )
                
                  2
                
              
              
              (
              s
              +
              995.9
              )
              
              (
              s
              +
              76655
              
                )
                
                  2
                
              
            
          
        
      
    
    {\displaystyle H_{B}(s)={k_{B}\cdot s^{3} \over (s+129.4)^{2}\quad (s+995.9)\quad (s+76655)^{2}}}
  
kB ? 5.99185×109

C
H
          
            C
          
        
        (
        s
        )
        =
        
          
            
              
                k
                
                  C
                
              
              ?
              
                s
                
                  2
                
              
            
            
              (
              s
              +
              129.4
              
                )
                
                  2
                
              
              
              (
              s
              +
              76655
              
                )
                
                  2
                
              
            
          
        
      
    
    {\displaystyle H_{C}(s)={k_{C}\cdot s^{2} \over (s+129.4)^{2}\quad (s+76655)^{2}}}
  
kC ? 5.91797×109

D
H
          
            D
          
        
        (
        s
        )
        =
        
          
            
              
                k
                
                  D
                
              
              ?
              s
              ?
              (
              
                s
                
                  2
                
              
              +
              6532
              s
              +
              4.0975
              ×
              
                10
                
                  7
                
              
              )
            
            
              (
              s
              +
              1776.3
              )
              
              (
              s
              +
              7288.5
              )
              
              (
              
                s
                
                  2
                
              
              +
              21514
              s
              +
              3.8836
              ×
              
                10
                
                  8
                
              
              )
            
          
        
      
    
    {\displaystyle H_{D}(s)={k_{D}\cdot s\cdot (s^{2}+6532s+4.0975\times 10^{7}) \over (s+1776.3)\quad (s+7288.5)\quad (s^{2}+21514s+3.8836\times 10^{8})}}
  
kD ? 91104.32
The k values are constants which are used to normalize the function to a gain of 1 (0 dB). The values listed above normalize the functions to 0 dB at 1 kHz, as they are typically used. (This normalization is shown in the image.)

See also
Noise
Signal noise
Psophometric weighting
Audio quality measurement
Noise pollution
Noise regulation
Headroom
Rumble measurement
Weighting filter
Weighting curve
Luminosity function, the light equivalent

References
Audio Engineer's Reference Book, 2nd Ed 1999, edited Michael Talbot Smith, Focal Press
An Introduction to the Psychology of Hearing 5th ed, Brian C. J. Moore, Elsevier Press

External links
Noise Measurement Briefing. Archived from the original on 2013-02-25.
A-weighting filter circuit for audio measurements
Weighting Filter Set Circuit diagrams
AES pro audio reference definition of ""weighting filters""
Frequency Weighting Equations
A-weighting in detail
A-Weighting Equation and online calculation
Researches in loudness measurement by CBS using noise bands, 1966 IEEE Article
Evaluation of Loudness-level weightings and LLSEL JASA
Comparison of some loudness measures for loudspeaker listening tests (Aarts, JAES, 1992) PDF containing algorithm for ABCD filters",Category:Audio engineering,3
32,33,Sound power,"Sound power or acoustic power is the rate at which sound energy is emitted, reflected, transmitted or received, per unit time. The SI unit of sound power is the watt (W). It is the power of the sound force on a surface of the medium of propagation of the sound wave. For a sound source, unlike sound pressure, sound power is neither room-dependent nor distance-dependent. Sound pressure is a measurement at a point in space near the source, while the sound power of a source is the total power emitted by that source in all directions. Sound power passing through an area is sometimes called sound flux or acoustic flux through that area.

Sound power level LWA
Regulations control the maximum sound power level LWA that a device (e.g. vacuum cleaner) is allowed to produce. The A-weighting scale is used in the calculation as the regulation is concerned with the loudness as perceived by the human ear. Measurements are taken at several defined points around the device.
The test environment can be located indoors or outdoors. The ideal environment is on the ground in a large open space or hemi-anechoic chamber (free-field over a reflecting plane ). To account for undesired reflections from nearby objects, walls, and the ceiling, and for any residual background noises, measurement corrections are applied.

Table of selected sound sources
Here is a table of some examples.

Mathematical definition
Sound power, denoted P, is defined by

  
    
      
        P
        =
        
          f
        
        ?
        
          v
        
        =
        A
        p
        
        
          u
        
        ?
        
          v
        
        =
        A
        p
        v
      
    
    {\displaystyle P=\mathbf {f} \cdot \mathbf {v} =Ap\,\mathbf {u} \cdot \mathbf {v} =Apv}
  
where
f is the sound force of unit vector u;
v is the particle velocity of projection v along u;
A is the area;
p is the sound pressure.
In a medium, the sound power is given by

  
    
      
        P
        =
        
          
            
              A
              
                p
                
                  2
                
              
            
            
              ?
              c
            
          
        
        cos
        ?
        ?
        ,
      
    
    {\displaystyle P={\frac {Ap^{2}}{\rho c}}\cos \theta ,}
  
where
A is the area of the surface;
? is the mass density;
c is the sound velocity;
? is the angle between the direction of propagation of the sound and the normal to the surface.
For example, a sound at SPL = 85 dB or p = 0.356 Pa in air (? = 1.2 kg·m?3 and c = 343 m·s?1) through a surface of area A = 1 m2 normal to the direction of propagation (? = 0 °) has a sound energy flux P = 0.3 mW.
This is the parameter one would be interested in when converting noise back into usable energy, along with any losses in the capturing device.

Relationships with other quantities
Sound power is related to sound intensity:

  
    
      
        P
        =
        A
        I
        ,
      
    
    {\displaystyle P=AI,}
  
where
A is the area;
I is the sound intensity.
Sound power is related sound energy density:

  
    
      
        P
        =
        A
        c
        w
        ,
      
    
    {\displaystyle P=Acw,}
  
where
c is the speed of sound;
w is the sound energy density.

Sound power level definition
Sound power level (SWL) or acoustic power level is a logarithmic measure of the power of a sound relative to a reference value.
Sound power level, denoted LW and measured in dB, is defined by

  
    
      
        
          L
          
            W
          
        
        =
        
          
            1
            2
          
        
        ln
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          N
          p
        
        =
        
          log
          
            10
          
        
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          B
        
        =
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              P
              
                P
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{W}={\frac {1}{2}}\ln \!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {Np} =\log _{10}\!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {B} =10\log _{10}\!\left({\frac {P}{P_{0}}}\right)\!~\mathrm {dB} ,}
  
where
P is the sound power;
P0 is the reference sound power;
1 Np = 1 is the neper;
1 B = 1/2 ln 10 is the bel;
1 dB = 1/20 ln 10 is the decibel.
The commonly used reference sound power in air is

  
    
      
        
          P
          
            0
          
        
        =
        1
         
        
          p
          W
        
        .
      
    
    {\displaystyle P_{0}=1~\mathrm {pW} .}
  
The proper notations for sound power level using this reference are LW/(1 pW) or LW (re 1 pW), but the suffix notations dB SWL, dB(SWL), dBSWL, or dBSWL are very common, even if they are not accepted by the SI.
The reference sound power P0 is defined as the sound power with the reference sound intensity I0 = 1 pW/m2 passing through a surface of area A0 = 1 m2:

  
    
      
        
          P
          
            0
          
        
        =
        
          A
          
            0
          
        
        
          I
          
            0
          
        
        ,
      
    
    {\displaystyle P_{0}=A_{0}I_{0},}
  
hence the reference value P0 = 1 pW.

Relationship with sound pressure level
The generic calculation of sound power from sound pressure is as follows:

  
    
      
        
          L
          
            W
          
        
        =
        
          L
          
            p
          
        
        +
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              
                A
                
                  S
                
              
              
                A
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{W}=L_{p}+10\log _{10}\!\left({\frac {A_{S}}{A_{0}}}\right)\!~\mathrm {dB} ,}
  
where: 
  
    
      
        
          
            A
            
              S
            
          
        
      
    
    {\displaystyle {A_{S}}}
   defines the area of a surface that wholly encompasses the source. This surface may be any shape, but it must fully enclose the source.
In the case of a sound source located in free field positioned over a reflecting plane (i.e. the ground), in air at ambient temperature, the sound power level at distance r from the sound source is approximately related to sound pressure level (SPL) by

  
    
      
        
          L
          
            W
          
        
        =
        
          L
          
            p
          
        
        +
        10
        
          log
          
            10
          
        
        
        
          (
          
            
              
                2
                ?
                
                  r
                  
                    2
                  
                
              
              
                A
                
                  0
                
              
            
          
          )
        
        
         
        
          d
          B
        
        ,
      
    
    {\displaystyle L_{W}=L_{p}+10\log _{10}\!\left({\frac {2\pi r^{2}}{A_{0}}}\right)\!~\mathrm {dB} ,}
  
where
Lp is the sound pressure level;
A0 = 1 m2;

  
    
      
        
          2
          ?
          
            r
            
              2
            
          
        
        ,
      
    
    {\displaystyle {2\pi r^{2}},}
   defines the surface area of a hemisphere; and
r must be sufficient that the hemisphere fully encloses the source.
Derivation of this equation:

  
    
      
        
          
            
              
                
                  L
                  
                    W
                  
                
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      P
                      
                        P
                        
                          0
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        A
                        I
                      
                      
                        
                          A
                          
                            0
                          
                        
                        
                          I
                          
                            0
                          
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      I
                      
                        I
                        
                          0
                        
                      
                    
                  
                  )
                
                +
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      A
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}L_{W}&={\frac {1}{2}}\ln \!\left({\frac {P}{P_{0}}}\right)\\&={\frac {1}{2}}\ln \!\left({\frac {AI}{A_{0}I_{0}}}\right)\\&={\frac {1}{2}}\ln \!\left({\frac {I}{I_{0}}}\right)+{\frac {1}{2}}\ln \!\left({\frac {A}{A_{0}}}\right)\!.\end{aligned}}}
  
For a progressive spherical wave,

  
    
      
        
          z
          
            0
          
        
        =
        
          
            p
            v
          
        
        ,
      
    
    {\displaystyle z_{0}={\frac {p}{v}},}
  

  
    
      
        A
        =
        4
        ?
        
          r
          
            2
          
        
        ,
      
    
    {\displaystyle A=4\pi r^{2},}
   (the surface area of sphere)
where z0 is the characteristic specific acoustic impedance.
Consequently,

  
    
      
        I
        =
        p
        v
        =
        
          
            
              p
              
                2
              
            
            
              z
              
                0
              
            
          
        
        ,
      
    
    {\displaystyle I=pv={\frac {p^{2}}{z_{0}}},}
  
and since by definition I0 = p02/z0, where p0 = 20 ?Pa is the reference sound pressure,

  
    
      
        
          
            
              
                
                  L
                  
                    W
                  
                
              
              
                
                =
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        p
                        
                          2
                        
                      
                      
                        p
                        
                          0
                        
                        
                          2
                        
                      
                    
                  
                  )
                
                +
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        4
                        ?
                        
                          r
                          
                            2
                          
                        
                      
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                ln
                
                
                  (
                  
                    
                      p
                      
                        p
                        
                          0
                        
                      
                    
                  
                  )
                
                +
                
                  
                    1
                    2
                  
                
                ln
                
                
                  (
                  
                    
                      
                        4
                        ?
                        
                          r
                          
                            2
                          
                        
                      
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
              
            
            
              
              
                
                =
                
                  L
                  
                    p
                  
                
                +
                10
                
                  log
                  
                    10
                  
                
                
                
                  (
                  
                    
                      
                        4
                        ?
                        
                          r
                          
                            2
                          
                        
                      
                      
                        A
                        
                          0
                        
                      
                    
                  
                  )
                
                
                 
                
                  d
                  B
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}L_{W}&={\frac {1}{2}}\ln \!\left({\frac {p^{2}}{p_{0}^{2}}}\right)+{\frac {1}{2}}\ln \!\left({\frac {4\pi r^{2}}{A_{0}}}\right)\\&=\ln \!\left({\frac {p}{p_{0}}}\right)+{\frac {1}{2}}\ln \!\left({\frac {4\pi r^{2}}{A_{0}}}\right)\\&=L_{p}+10\log _{10}\!\left({\frac {4\pi r^{2}}{A_{0}}}\right)\!~\mathrm {dB} .\end{aligned}}}
  
The sound power estimated practically does not depend on distance. The sound pressure used in the calculation may be affected by distance due to viscous effects in the propagation of sound unless this is accounted for.

References
External links
Sound power and Sound pressure. Cause and Effect
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave
NIOSH Powertools Database
Sound Power Testing",Category:Physical quantities,3
33,34,Ambient noise level,"In atmospheric sounding and noise pollution, ambient noise level (sometimes called background noise level, reference sound level, or room noise level) is the background sound pressure level at a given location, normally specified as a reference level to study a new intrusive sound source.
Ambient sound levels are often measured in order to map sound conditions over a spatial regime to understand their variation with locale. In this case the product of the investigation is a sound level contour map. Alternatively ambient noise levels may be measured to provide a reference point for analyzing an intrusive sound to a given environment. For example, sometimes aircraft noise is studied by measuring ambient sound without presence of any overflights, and then studying the noise addition by measurement or computer simulation of overflight events. Or roadway noise is measured as ambient sound, prior to introducing a hypothetical noise barrier intended to reduce that ambient noise level.
Ambient noise level is measured with a sound level meter. It is usually measured in dB relative to a reference pressure of 0.00002 Pa, i.e., 20 ?Pa (micropascals) in SI units. A pascal is a newton per square meter. The centimeter-gram-second system of units, the reference sound pressure for measuring ambient noise level is 0.0002 dyn/cm2. Most frequently ambient noise levels are measured using a frequency weighting filter, the most common being the A-weighting scale, such that resulting measurements are denoted dB(A), or decibels on the A-weighting scale.

See also
A-weighting
Background noise
Environmental noise
Noise barrier
Noise health effects
Noise pollution
Noise regulation

References
 This article incorporates public domain material from the General Services Administration document ""Federal Standard 1037C"" (in support of MIL-STD-188).",Category:Noise,3
34,35,Sound baffle,"A sound baffle is a construction or device which reduces the strength (level) of airborne sound. Sound baffles are a fundamental tool of noise mitigation, the practice of minimizing noise pollution or reverberation. An important type of sound baffle is the noise barrier constructed along highways to reduce sound levels at properties in the vicinity. Sound baffles are also applied to walls and ceilings in building interiors to absorb sound energy and thus lessen reverberation.

Highway noise barriers
The technology for accurate prediction of the effects of noise barrier design using a computer model to analyze roadway noise has been available since the early 1970s. The earliest published scientific design of a noise barrier may have occurred in Santa Clara County, California in 1970 for a section of the Foothill Expressway in Los Altos, California. The county used a computer model to predict the effects of sound propagation from roadways, with variables consisting of vehicle speed, ratio of trucks to automobiles, road surface type, roadway geometrics, micro-meteorology and the design of proposed soundwalls.

Interior sound baffle design
Since the early 1900s, scientists have been aware of the utility of certain types of interior coatings or baffles to improve the acoustics of concert halls, theaters, conference rooms and other spaces where sound quality is important. By the mid-1950s, Bolt, Beranek and Newman and a few other U.S. research organizations were developing technology to address sound quality's design challenges. This design field draws on several disciplines including acoustical science, computer modeling, architecture and materials science. Sound baffles are also used in speaker cabinets to absorb energy from the pressure created by the speakers, thus reducing cabinet resonance.
In 1973, Pearl P. Randolph, a school bus driver in Virginia, won a new school bus in a national contest held by Wayne Corporation for the suggestion that sound baffles be installed in the ceiling of school buses. In 1981, they were first made mandatory by the state of California.

Vehicle exhaust sound baffles
Baffles are also found in the exhaust pipes of vehicles, particularly motorcycles.

See also
Noise pollution
Noise health effects


== References ==",Category:Noise pollution,3
35,36,Category:Soundscape ecology,,Category:Ecological techniques,3
36,37,Nominal level,"Nominal level is the operating level at which an electronic signal processing device is designed to operate. The electronic circuits that make up such equipment are limited in the maximum signal they can handle and the low-level internally generated electronic noise they add to the signal. The difference between the internal noise and the maximum level is the device's dynamic range. The nominal level is the level that these devices were designed to operate at, for best dynamic range and adequate headroom. When a signal is chained with improper gain staging through many devices, the dynamic range of the signal is reduced.
In audio, a related measurement, signal-to-noise ratio, is usually defined as the difference between the nominal level and the noise floor, leaving the headroom as the difference between nominal and maximum output. It is important to realize that the measured level is a time average, meaning that the peaks of audio signals regularly exceed the measured average level. The headroom measurement defines how far the peak levels can stray from the nominal measured level before clipping. The difference between the peaks and the average for a given signal is the crest factor.
There is some confusion over the use of the term ""nominal"", which is often used incorrectly to mean ""average or typical"". The relevant definition in this case is ""as per design""; gain is applied to make the average signal level correspond to the designed, or nominal, level.

Standards
VU meters are designed to represent the perceived loudness of a passage of music, measuring in volume units. The product is designed so that the best signal quality is obtained when the meter rarely goes above nominal. The markings are often in dB instead of ""VU"", and the reference level is defined in the product's manual. In professional recording and sound reinforcement gear, the nominal level is 0 VU = +4 dBu. In consumer level equipment, the nominal level varies, but some standardize to 0 VU = ?10 dBV. The difference between consumer and pro equipment revolves around the cost required to create larger power supplies and output higher levels.
In broadcasting equipment, this is termed the Maximum Permitted Level, which is defined by European Broadcasting Union standards. These devices use peak programme meters instead of VU meters, which gives the reading a different meaning.
""Mic level"" is sometimes defined as ?60 dBV, though levels from microphones vary widely.
In video systems, nominal levels are 1 VP-P for synched systems, such as baseband composite video, and 0.7 VP-P for systems without sync. Note that these levels are measured peak-to-peak, while audio levels are time averages.

See also
Alignment level
Programme levels
Transmission level point

References
External links
Nominal Level — Sweetwater glossary
Level Headed — Nominal Level (explained) plus an SV-3700 modification",Category:Articles with unsourced statements from November 2016,3
37,38,AES-2id,,Category:Sound,3
38,39,Category:Sounds by type,,Category:Sound,3
39,40,Gunshot,"A gunshot is a single discharge of a gun, typically a man-portable firearm, producing a visible flash, a powerful and loud shockwave and often chemical gunshot residue. The term can also refer to a ballistic wound caused by such a discharge.
Multiple discharges of one or more firearms are referred to as gunfire. The word can connote either the sound of a gun firing, the projectiles that were fired, or both. For example, the statement ""gunfire came from the next street"" could either mean the sound of discharge, or it could mean the bullets that were discharged. It is better to be a bit more specific while writing however. ""The sound of gunfire"" or ""we came under gunfire"" would be more descriptive and prevent confusion. In the latter phrase, in particular, ""fire"" is more commonly used (i.e. ""under fire""), as both words hold the same general meaning within the proper context.

Gunfire characteristics
There are three primary attributes that characterize gunfire and hence enable the detection and location of gunfire and similar weapon discharges:
A muzzle flash that occurs when superheated gases and incompletely combusted propellant residue are secondarily ignited upon contact with fresh ambient oxygen after being expelled out the gun barrel
A muzzle blast that occurs when high-pressure gases within the barrel is suddenly released and rapidly expands when the projectile exits the muzzle and the bullet-bore contact that maintained the seal is removed. A typical muzzle blast generates a shock wave with a sound pressure level (SPL) of 140 dB or louder.
A whip-like “snap” or “crack” caused by the sonic boom that occurs as a projectile moves through the air at supersonic speeds.
Gunfire can be confused with other noises that can sound similar, such as firework explosions and cars backfiring. Gunfire noise propagation is anisotropic. The sounds may be heard at greater distances in the direction of bullet travel than behind or beside the gun.
Urban areas typically exhibit diurnal noise patterns where background noise is higher during the daytime and lower at night, where the noise floor directly correlated to urban activity (e.g., automobile traffic, airplane traffic, construction, and so on). Firearm muzzle blast may be masked by ambient noise during daytime; but may be detected at greater distances during the quieter hours of darkness. A popular urban gunfire locator system typically uses six to ten audio sensors per square mile for trilateration. (two or three per square kilometer)
A suppressor can be attached to the muzzle of a firearm to decrease the audio signature of high-velocity gases released from the muzzle when firing the weapon. The sound of firing is only decreased and is still considerable. Suppressors attached to the muzzle will not reduce the sound of high velocity gas released from other locations like the gap between the cylinder and barrel of a revolver. A muzzle suppressor is similarly ineffective in reducing the snap of a supersonic bullet, or the noise produced by the mechanical action of a self-loading firearm. Use of suppressors is rare in United States crimes. A 2007 study estimated unlawful suppressor possession was involved in only 0.05 percent (1 in 2,000) federal criminal prosecutions; and the suppressor was unused, but simply in the possession of the defendant for 92% of prosecutions involving a suppressor.

See also
Ballistic trauma
Gun crime
Gunfight
Gunshot residue
Hydrostatic shock
Stopping power
Terminal ballistics
Warning shot
Wound ballistics

References
External links
Introduction to Firearm Suppressor Technology
Detailed measurements of the sound of a .308 rifle firing",Category:Firearms,3
40,41,Category:Unidentified sounds,,Category:Sound production,3
41,42,Constant spectrum melody,"A constant timbre at a constant pitch is characterized by a spectrum. Along a piece of music, the spectrum measured within a narrow time window varies with the melody and the possible effects of instruments. Therefore, it may seem paradoxical that a constant spectrum can be perceived as a melody rather than a stamp.

The paradox is that the ear is not an abstract spectrograph: it ""calculates"" the Fourier transform of the sound signal in a narrow time window, but the slower variations are seen as temporal evolution and not as pitch.
However, the example of paradoxical melody above contains no infrasound (i.e. pure tone of period slower than the time window). The second paradox is that when two pitches are very close, they create a beat. If the period of this beat is longer than the integration window, it is seen as a sinusoidal variation in the average rating: sin(2?(f+?)t) + sin(2?(f-?)t) = sin(2?ft)cos(2??t), where 1/? is the slow period.

The present spectrum is made of multiple frequencies beating together, resulting in a superimposition of various pitches fading in and out at different moments and pace, thus forming the melody.

MATLAB/Scilab/Octave code
Here is the program used to generate the paradoxical melody:

n=10; length=20; harmon=10; df=0.1; 
t=(1:length*44100)/44100; 
y=0; 
for i = 0:n, 
  for j = 1:harmon, 
    y=y+sin(2*3.1415927*(55+i*df)*j*t); 
  end; 
end;
sound(y/(n*harmon),44100);

References
See also

Shepard-Risset tone, forever increasing pitch
File:Risset accelerando beat1 MCLD.ogg: forever accelerating beat
Spectral music
Auditory illusion
Musical acoustics",Category:Perception,3
42,43,Anthropophony,"The term, anthropophony, consists of the Greek prefix, anthropo, meaning human, and the suffix, phon, meaning sound. The term refers to all sound produced by humans, whether coherent, such as music, theatre, and language, or incoherent and chaotic such as random signals generated primarily by electromechanical means.
The term was first used to describe certain soundscape phenomena recorded as part of a bioacoustic study in 2001–2002 commissioned by the National Park Service, and done in Sequoia/King's Canyon National Park. Anthropophony is one of three terms used by Drs. Stuart Gage and Bernie Krause to define the general sources of human sounds/noise that occur within a soundscape. The other two non-human, but natural sound sources include biophony and geophony.

See also
Nature sounds

References

Hull J. ""The Noises of Nature"". Idea Lab. New York Times Magazine, 18 February 2008. 
Krause B (January–February 2008). ""Anatomy of the Soundscape"". Journal of the Audio Engineering Society. 56 (1/2). 
Bryan C. Pijanowski, Luis J. Villanueva-Rivera, Sarah L. Dumyahn, Almo Farina, Bernie L. Krause, Brian M. Napoletano, Stuart H. Gage, and Nadia Pieretti,Soundscape Ecology: The Science of Sound in the Landscape, BioScience, March, 2011, vol. 61 no. 3, 203–216
Bernie Krause, Stuart H. Gage, Wooyeong Joo, Measuring and interpreting the temporal variability in the soundscape at four places in Sequoia National Park, Landscape Ecology, DOI 10.1007/s10980-011-9639-6, August 2011",Category:Sound,3
43,44,Sound-on-film,"Sound-on-film is a class of sound film processes where the sound accompanying picture is physically recorded onto photographic film, usually, but not always, the same strip of film carrying the picture. Sound-on-film processes can either record an analog sound track or digital sound track, and may record the signal either optically or magnetically. Earlier technologies were sound-on-disc, meaning the film's soundtrack would be on a separate phonograph record.

History
Analog sound-on-film recording
The most prevalent current method of recording analogue sound on a film print is by stereo variable-area (SVA) recording, a technique first used in the mid-1970s as Dolby Stereo. A two-channel audio signal is recorded as a pair of lines running parallel with the film's direction of travel through the projector. The lines change area (grow broader or narrower) depending on the magnitude of the signal. The projector shines light from a small lamp, called an exciter, through a perpendicular slit onto the film. The image on the small slice of exposed track modulates the intensity of the light, which is collected by a photosensitive element: a photocell, a photodiode or CCD.
In the early years of the 21st century distributors changed to using cyan dye optical soundtracks on color stocks instead of applicated tracks, which use environmentally unfriendly chemicals to retain a silver (black-and-white) soundtrack. Because traditional incandescent exciter lamps produce copious amounts of infra-red light, and cyan tracks do not absorb infra-red light, this change has required theaters to replace the incandescent exciter lamp with a complementary colored red LED or laser. These LED or laser exciters are backwards-compatible with older tracks.
Earlier processes, used on 70 mm film prints and special presentations of 35 mm film prints, recorded sound magnetically on ferric oxide tracks bonded to the film print, outside the sprocket holes. 16 mm and Super 8 formats sometimes used a similar magnetic track on the camera film, bonded to one side of the film on which the sprocket holes had not been punched (""single perforated"") for the purpose. Film of this form is no longer manufactured, but single-perforated film without the magnetic track (allowing an optical sound track) or, in the case of 16 mm, utilising the soundtrack area for a wider picture (Super 16 format) is readily available.

Digital sound-on-film formats
Three different digital soundtrack systems for 35 mm cinema release prints were introduced during the 1990s. They are: Dolby Digital, which is stored between the perforations on the sound side; SDDS, stored in two redundant strips along the outside edges (beyond the perforations); and DTS, in which sound data is stored on separate compact discs synchronized by a timecode track on the film just to the right of the analog soundtrack and left of the frame (Sound-on-disc). Because these soundtrack systems appear on different parts of the print, one movie can contain all of them, allowing broad distribution without regard for the sound system installed at individual theatres.

Sound-on-film formats
Almost all sound formats used with motion-picture film have been sound-on-film formats, including:

Optical analog formats
Fox/Western Electric (Westrex) Movietone, are variable-density formats of sound film. (No longer used, but still playable on modern 35 mm projectors.)
RCA Photophone, a variable-area format now universally used for optical analog soundtracks—since the late 1970s, usually with a Dolby encoding matrix.
Tri-Ergon, the patent of this Berlin based company was bought by Fox in 1926.

Encoding matrices
Dolby Stereo
Dolby SR
Ultra Stereo

Optical digital formats
Dolby Digital
Sony Dynamic Digital Sound

Obsolete formats
Cinema Digital Sound, an optical format which was the first commercial digital sound format, used between 1990 and 1992
Fantasound. This was a system developed by RCA and Disney Studios with a multi-channel soundtrack recorded on a separate strip of film from the picture. It was used for the initial release of Walt Disney's Fantasia (1940)
Phonofilm, patented by Lee De Forest in 1919, defunct by 1929

See also
Charles A. Hoxie
List of film formats
List of film sound systems
Movietone sound system
Optigan
Phonofilm
RCA Photophone
Eugène Lauste
Joseph Tykoci?ski-Tykociner

References
External links
Multichannel Film Sound (MKPE)",Category:Articles using small message boxes,3
44,45,Roughness (psychophysics),"Roughness is studied by examining how textures are perceived and encoded by an individual's somatosensory system. In an experiment to measure and compare the roughness of different sounds, listeners are presented with different sounds and asked to rate their roughness, for example on a rating scale. Recent research has displayed that there are two different codes, at least, for roughness: a vibrotactile code used for fine surfaces, and a spatial code used for coarse to medium surfaces.
According to psychophysical theory, the roughness of a complex sound (a sound comprising many partials or pure tone components) depends on the distance between the partials measured in critical bandwidths. Any simultaneous pair of partials of about the same amplitude that is less than a critical bandwidth apart produces roughness associated with the inability of the basilar membrane to separate them clearly.
Roughness is physiologically determined and therefore universal, but it is appraised differently in different musical styles. Some musical styles deliberately create large amounts of roughness for aesthetic effect (for example some polyphonic styles in the Balkans in which singers favor simultaneous second intervals) while others try to avoid roughness as much as possible or treat rough sounds in special ways (for example most tonal western music).
In terms of psychophysics, several studies have been done involving a person’s ability to detect the differences between the weight and roughness of objects. A syndrome called Verger-Dejerine syndrome has been known to affect these somatosensory abilities. Patients with this somatosensory cortical loss syndrome commonly display damage to their parietal lobe and it was eventually concluded that it may be that the brain has some form of an asymmetrical organization, as performance in a normal subject shows oblique differences depending on their hand use. However, these patients still exhibit normal or minimally reduced peripheral sensitivity to cold, heat, pain, touch and deep pressure.
Roughness perception is one of the multidimensional scaling of texture perception, which is the judgment of the substance and quality of an object. The studies of roughness perception demonstrate that it is unidimensional, it depends on element height, diameter, shape, compliance, and density; and that the relationship between roughness perception and the physical properties of a surface is complex and nonlinear. Also, there were early observations stated that scanning velocity and contact force between the finger and a surface have minor or no effect on roughness magnitude judgments. The physical determinants of roughness perception are complex, but the evidence is that the neural mechanisms are simple. Furthermore, research performed at the University of North Carolina revealed that scanning velocity did not have an effect on relative roughness because the roughness for all surfaces increase by the same amount as scanning velocity increases in accordance with Weber's Law.

See also
Auditory masking
Consonance and dissonance
Psychoacoustics (Masking effects)

References
Further reading
Plomp, R. & Levelt, W.J.M. (1965). Tonal consonance and critical bandwidth. Journal of the Acoustical Society of America, Vol. 38, pp. 548–560.
Terhardt, E. (1974). On the perception of periodic sound fluctuations (roughness). Acustica.",Category:Sound,3
45,46,Speech transmission index,"Speech Transmission Index (STI) is a measure of speech transmission quality. The absolute measurement of speech intelligibility is a complex science. The STI measures some physical characteristics of a transmission channel (a room, electro-acoustic equipment, telephone line, etc.), and expresses the ability of the channel to carry across the characteristics of a speech signal. STI is a well-established objective measurement predictor of how the characteristics of the transmission channel affect speech intelligibility.
The influence that a transmission channel has on speech intelligibility is dependent on:
the speech level
frequency response of the channel
non-linear distortions
background noise level
quality of the sound reproduction equipment
echos (reflections with delay > 100ms)
the reverberation time
psychoacoustic effects (masking effects)

History
The STI was introduced by Tammo Houtgast and Herman Steeneken in 1971, and was accepted by Acoustical Society of America in 1980. Steeneken and Houtgast decided to develop the Speech Transmission Index because they were tasked to carry out a very lengthy series of dull speech intelligibility measurements for the Netherlands Armed Forces. Instead, they spent the time developing a much quicker objective method (which was actually the predecessor to the STI).
Houtgast and Steeneken developed the Speech Transmission Index while working at The Netherlands Organisation of Applied Scientific Research TNO. Their team at TNO kept supporting and developing the STI, improving the model and developing hardware and software for measuring the STI, until 2010. In that year, the TNO research group responsible for the STI spun out of TNO and continued its work as a privately owned company named Embedded Acoustics. Embedded Acoustics now continues to support development of the STI, with Herman Steeneken (now formally retired from TNO) still acting as a senior consultant.
In the early years (until approx. 1985) the use of the STI was largely limited to a relatively small international community of speech researchers. The introduction of the RASTI (""Room Acoustics STI"") made the STI method available to a larger population of engineers and consultants, especially when Bruel & Kjaer introduced their RASTI measuring device (which was based on the earlier RASTI system developed by Steeneken and Houtgast at TNO). RASTI was designed to be much faster than the original (""full"") STI, taking less than 30 seconds instead of 15 minutes for a measuring point. However, RASTI was only intended (as the name says) for pure room acoustics, not electro-acoustics. Application of RASTI to transmission chains featuring electro-acoustic components (such as loudspeakers and microphones) became fairly common, and led to complaints about inaccurate results. The use of RASTI was even specified by some application standards (such as CAA specification 15 for aircraft cabin PA systems) for applications featuring electro-acoustics, simply because it was the only feasible method at the time. The inadequacies of RASTI were sometimes simply accepted for lack of a better alternative. TNO did produce and sell instruments for measuring full STI and various other STI derivatives, but these devices were relatively expensive, large and heavy.
Around the year 2000, the need for an alternative to RASTI that could also be applied safely to Public Address (PA) systems had become fully apparent. At TNO, Jan Verhave and Herman Steeneken started work on a new STI method, that would later become known as STIPA (STI for Public Address systems). The first device to include STIPA measurements available for sale to the general public was made by Gold-Line. At this time, STIPA measuring instruments are available from various manufacturers.
RASTI was standardized internationally in 1988, in IEC-60268-16. Since then, IEC-60268-16 was revised three times, the latest revisions (rev.4) appearing in 2011. Each revision included updates of the STI methodology that had become accepted in the STI research community over time, such as the inclusion of redundancy between adjacent octave bands (rev.2), level-dependent auditory masking (rev.3) and various methods for applying the STI to specific populations such as non-natives and the hearing impaired (rev.4). An IEC maintenance team is currently working on rev. 5.
RASTI was declared obsolete by the IEC in June 2011, with the appearance of rev. 4 of IEC-602682-16. At this time, this simplified STI derivative was still stipulated as a standard method in some industries. STIPA is now seen as the successor to RASTI for almost every application.

Scale
STI is a numeric representation measure of communication channel characteristics whose value varies from 0 = bad to 1 = excellent. On this scale, an STI of at least .5 is desirable for most applications.
Barnett (1995, 1999) proposed to use a reference scale, the Common Intelligibility Scale (CIS), based on a mathematical relation with STI (CIS = 1 + log (STI)).

STI predicts the likelihood of syllables, words and sentences being comprehended. As an example, for native speakers, this likelihood is given by:
If non-native speakers, people with speech disorders or hard-of-hearing people are involved, other probabilities hold.
It is interesting but not astonishing that STI prediction is independent of the language spoken – not astonishing, as the ability of the channel to transport patterns of physical speech is measured.
Another method is defined for computing a physical measure that is highly correlated with the intelligibility of speech as evaluated by speech perception tests given a group of talkers and listeners. This measure is called the Speech Intelligibility Index, or SII.

Nominal qualification bands for STI
The IEC 60268-16 ed4 2011 Standard defines a qualification scale in order to provide flexibility for different applications. The values of this alpha-scale run from ""U"" to ""A+"".

Standards
STI has gained international acceptance as the quantifier of channel influence on speech intelligibility. The International Electrotechnical Commission Objective rating of speech intelligibility by speech transmission index, as prepared by the TC 100 Technical Committee, defines the international standard.
Further the following standards have, as part of the requirements to be fulfilled, integrated testing the STI and realisation of a minimal speech transmission index:
International Organization for Standardization (ISO) standard for sound system loudspeakers in Fire detection and fire alarm systems
National Fire Protection Association Alarm Code
British Standards Institution Fire detection and alarm systems for buildings
German Institute for Standardization Sound Systems for Emergency Purposes

STIPA
STIPA (Speech Transmission Index for Public Address Systems) is a version of the STI using a simplified method and test signal. Within the STIPA signal, each octave band is modulated simultaneously with two modulation frequencies. The modulation frequencies are spread among the octave bands in a balanced way, making it possible to obtain a reliable STI measurement based on a sparsely sampled Modulation Transfer Function matrix. Although initially designed for Public Address systems (and similar installations, such as Voice Evacuation Systems and Mass Notification Systems), STIPA can also be used for a variety of other applications. The only situation in which RASTI is currently considered inferior to full STI is in the presence of strong echoes.
A single STIPA measurement generally takes between 15 and 25 seconds, combining the speed of RASTI with (nearly) the wide scope of applicability and reliability of full STI.
Since STIPA has become widely available, and given the fact that RASTI has several disadvantages and no benefits over STIPA, RASTI is now considered obsolete.
Although the STIPA test signal does not resemble speech to the human ear, in terms of frequency content as well as intensity fluctuations it is a signal with speech-like characteristics.
Speech can be described as noise that is intensity-modulated by low-frequency signals. The STIPA signal contains such intensity modulations at 14 different modulation frequencies, spread across 7 octave bands. At the receiving end of the communication system, the depth of modulation of the received signal is measured and compared with that of the test signal in each of a number of frequency bands. Reductions in the modulation depth are associated with loss of intelligibility.

Indirect method
An alternative Impulse response method, also known as the ""indirect method,"" assumes that the channel is linear and requires stricter synchronization of the sound source to the measurement instrument. The main benefit of the indirect method over the direct method (based on modulated test signals) is that the full MTF matrix is measured, covering all relevant modulation frequencies in all octave bands. In very large spaces (such as cathedrals), where echoes are likely to occur, the indirect method is usually preferred over direct method (e.g. using modulated STIPA signals). In general, the indirect method is often the best option when studying speech intelligibility based on ""pure room acoustics,"" when no electro-acoustic components are present within the transmission path.
However, the requirement that the channel must be linear implies that the indirect method cannot be used reliably in many real-life applications: whenever the transmission chain features components that might exhibit non-linear behaviour (such as loudspeakers), indirect measurements may yield incorrect results. Also, depending on the type of impulse response measurement that is used, the influence of background noise present during measurements may not be dealt with correctly. This means that the indirect method should only be used with great care when measuring Public Address systems and Voice Evacuation systems. IEC-60268-16 rev. 4 does not disallow the indirect method for such applications, but issues the following words of warning: ""Critical analysis is therefore required of how the impulse response is obtained and potentially influenced by non-linearities in the transmission system, particularly as in practice, system components can be operated at the limits of their performance range."" In practice, verification of the validity of the linearity assumption is often too complex for everyday use, making the (direct) STIPA method the preferred method whenever loudspeakers are involved.
Although many measuring tools based on the indirect method offer STIPA as well as ""full STI"" options, the sparse Modulation Transfer Function matrix inherent to STIPA offers no advantages when using the indirect method. Impulse response based STIPA measurements must not be confused with direct STIPA measurements, as the validity of the result still depends on whether or not the channel is linear.

List of manufacturers of STI measuring instruments
STI measuring instruments are (and have been) made by various manufacturers. Below is a list of brands under which STI measuring instruments have been sold, in alphabetical order.
Audio Precision [2]. Offers an STI Plug-in option for use with APx500 Series audio analyzers.
Audiomatica [3]. Offers an STI (including STIPA) tool in CLIO 11 system that is compliant with the latest version of the standard (IEC-60268-16 rev. 4). CLIO 12 system is capable of both indirect STI/STIPA and direct STIPA measurements.
Bedrock Audio [4]. This is the brand under which Embedded Acoustics sells their STIPA hardware, such as the SM50.
Brüel & Kjær [5]. Offers handheld as well as software based solutions.
Gold Line [6]. First to offer STIPA measuring solutions (DSP2 and DSP30), but currently not offering any tools that comply with the latest standards (IEC-60268-16 rev. 4).
HEAD acoustics [7]. Offers STI options (including STIPA, STITEL, and RASTI) for both the Artemis Suite [8] and ACQUA [9] test systems.
Ivie [10]. Offers STIPA-capable acoustic measuring tools such as the IE-45.
Norsonic [11]. Norsonic was early to adopt STIPA and offer STIPA modules on their instruments (Nor-140). Appears not to be sold in the US.
NTi Audio [12]. Offers STIPA modules with their AL1 and XL2 line of acoustic measuring instruments as well as a Talkbox and other peripherals. Apparent market leader at this moment (2013).
Quest [13]. Now part of 3M, Quest produces tools such as the Quest Verifier.
Svantek [14]
TNO. Not currently marketing any products, but sold (among others) the STIDAS series of measuring instruments before.
The market for STI measuring solution is still developing, so the above list is subject to change as manufacturers enter or leave the market. The list does not include software producers that produce STI-capable acoustic measuring and simulation software. Mobile apps for STIPA measurements (such as the ones sold by Studio Six Digital [15] and Embedded Acoustics [16]) are also excluded from the list.

See also
Mean opinion score

References
External links
Intelligibility Conversion: %ALcons = Articulation Loss of Consonants in % to STI = Speech Transmission Index and vice versa
Background information on the STI and links to STI resources
Speech Intelligibility Papers IV
STI explained for the non sound specialist",Category:Sound,3
46,47,Matrix decoder,"Matrix decoding is an audio technology where a finite number of discrete audio channels (e.g., 2) are decoded into a larger number of channels on play back (e.g., 5). The channels are generally, but not always, arranged for transmission or recording by an encoder, and decoded for playback by a decoder.
The function is to allow multichannel audio, such as quadraphonic sound or surround sound to be encoded in a stereo signal, and thus played back as stereo on stereo equipment, and as surround on surround equipment – this is ""compatible"" multichannel audio.

Process
Matrix encoding does not allow one to encode several channels in fewer channels without losing information: one cannot fit 5 channels into 2 (or even 3 into 2) without losing information, as this loses dimensions: the decoded signals are not independent. The idea is rather to encode something that will both be an acceptable approximation of the surround sound when decoded, and acceptable (or even superior) stereo.

Notation
The notation for matrix encoding consists of the number of original discrete audio channels separated by a colon from the number of encoded and decoded channels. For example, four channels encoded into two discrete channels and decoded back to four-channels would be notated:

4:2:4

A simpler situation would be to derive extra channels from the existing ones, but with no special encoding at the origin. For example, five discrete channels decoded to six channels would be notated:

5:5:6

Many matrix decoders take advantage of the Haas effect, as well as audio cues inherent in the source channels.
The various encoding matrixes are described below.

Dynaquad matrix (2:2:4) / (4:2:4)
Mostly used as a simple method for deriving back channels out of normal stereo recording (2:2:4), this matrix was also used for a specific encoding of 4 sound channels in some albums (4:2:4).

Encoding matrix
Decoding matrix
Electro-Voice Stereo-4 matrix (2:2:4) / (4:2:4)
The first matrix system on the market, it was invented by Leonard Feldman and Jon Fixler, and sold by Electro-Voice. This matrix was used for specific encoding of 4 sound channels in many albums (4:2:4).

Encoding matrix
Decoding matrix
SQ matrix, ""Stereo Quadraphonic"", CBS SQ (4:2:4)
j
        =
        +
        
          90
          
            ?
          
        
      
    
    {\displaystyle j=+90^{\circ }}
   phase-shift, 
  
    
      
        k
        =
        ?
        
          90
          
            ?
          
        
      
    
    {\displaystyle k=-90^{\circ }}
   phase-shift
The basic SQ matrix had mono/stereo anomalies as well as encoding/decoding problems, heavily criticized by Michael Gerzon and others.
An attempt to improve the system lead to the use of other encoders or sound capture techniques, yet the decoding matrix remained unchanged.

Position Encoder
An N/2 encoder that encoded every position in a 360° circle - it had 16 inputs and each could be dialed to the exact direction desired, generating an optimized encode.

Forward-Oriented encoder
j
        =
        +
        
          90
          
            ?
          
        
      
    
    {\displaystyle j=+90^{\circ }}
   phase-shift, 
  
    
      
        k
        =
        ?
        
          90
          
            ?
          
        
      
    
    {\displaystyle k=-90^{\circ }}
   phase-shift
The Forward-Oriented encoder caused Center Back to be encoded as Center Front and was recommended for live broadcast use for maximum mono compatibility - it also encoded Center Left/Center Right and both diagonal splits in the optimal manner. Could be used to modify existing 2-channel stereo recordings and create 'synthesized SQ' that when played through a Full-Logic or Tate DES SQ decoder, exhibited a 180° or 270° synthesized quad effect. Many stereo FM radio stations broadcasting SQ in the 1970s used their Forward-Oriented SQ encoder for this. For SQ decoders, CBS designed a circuit that produced the 270° enhancement using the 90° phase shifters in the decoder. Sansui's QS Encoders and QS Vario-Matrix Decoders had a similar capability.

Backwards-Oriented encoder
j
        =
        +
        
          90
          
            ?
          
        
      
    
    {\displaystyle j=+90^{\circ }}
   phase-shift, 
  
    
      
        k
        =
        ?
        
          90
          
            ?
          
        
      
    
    {\displaystyle k=-90^{\circ }}
   phase-shift
The Backwards-Oriented Encoder was the reverse of the Forward-Oriented Encoder - it allowed sounds to be placed optimally in the back half of the room, but mono-compatibility was sacrificed. When used with standard stereo recordings it created ""extra wide"" stereo with sounds outside the speakers.
Some encoding mixers had channel strips switchable between forward-oriented and backwards-oriented encoding.

London Box
It encoded the Center Back in such a way that it didn't cancel in mono playback, thus its output was usually mixed with that of a Position Encoder or a Forward Oriented encoder. After 1972, the vast majority of SQ Encoded albums were mixed with either the Position Encoder or the Forward-Oriented encoder.

Ghent microphone
In addition, CBS created the SQ Ghent Microphone, which was a spatial microphone system using the Neumann QM-69 mic. The signals from the QM-69 were differenced, and then phase-matrixed into 2-channel SQ. With the Ghent Microphone, SQ was transformed from a Matrix into a Kernel and an additional signal could be derived to provide N:3:4 performance.

Universal SQ
In 1976, Ben Bauer integrated matrix and discrete systems into USQ, or Universal SQ. It was a hierarchical 4-4-4 discrete matrix that used the SQ matrix as the baseband for discrete quadraphonic FM broadcasts using additional difference signals called ""T"" and ""Q"". For a USQ FM broadcast, the additional ""T"" modulation was placed at 38 kHz in quadrature to the standard stereo difference signal and the ""Q"" modulation was placed on a carrier at 76 kHz. For standard 2-channel SQ Matrix broadcasts, CBS recommended that an optional pilot-tone be placed at 19 kHz in quadrature to the regular pilot-tone to indicate SQ encoded signals and activate the listeners Logic decoder. CBS argued that the SQ system should be selected as the standard for quadraphonic FM because, in FCC listening tests of the various four channel broadcast proposals, the 4:2:4 SQ system, decoded with a CBS Paramatrix decoder, outperformed 4:3:4 (without logic) as well as all other 4:2:4 (with logic) systems tested, approaching the performance of a discrete master tape within a very slight margin. At the same time, the SQ ""fold"" to stereo and mono was preferred to the stereo and mono ""fold"" of 4:4:4, 4:3:4 and all other 4:2:4 encoding systems.

Tate DES decoder
The Directional Enhancement System, also known as the Tate DES, was an advanced decoder that enhanced the directionality of the basic SQ matrix.
It first matrixed the four outputs of the SQ decoder to derive additional signals, then compared their envelopes to detect the predominant direction and degree of dominance. A processor section, implemented outside of the Tate IC chips, applied variable attack/decay timing to the control signals and determined the coefficients of the ""B"" (Blend) matrices needed to enhance the directionality. These were acted upon by true analog multipliers in the Matrix Multiplier IC's, to multiply the incoming matrix by the ""B"" matrices and produce outputs in which the directionality of all predominant sounds were enhanced.
Since the DES could recognize all three directions of the Energy Sphere simultaneously, and enhance the separation, it had a very open and 'discrete' sounding soundfield.
In addition, the enhancement was done with sufficient additional complexity that all non-dominant sounds were kept at their proper levels.
Dolby used the Tate DES IC's in their theater processors until around 1986, when they developed the Pro Logic system. Unfortunately, delays and problems kept the Tate DES IC's from the market until the late-1970s and only two consumer decoders were ever made that employed them, the Audionics Space & Image Composer and the Fosgate Tate II 101A. The Fosgate used a faster, updated version of the IC, called the Tate II, and additional circuitry that provided for separation enhancement around the full 360 soundfield. Unlike the earlier Full Wave-matching Logic decoders for SQ, that varied the output levels to enhance directionality, the Tate DES cancelled SQ signal crosstalk as a function of the predominant directionality, keeping non-dominant sounds and reverberation in its proper spatial locations at their correct level.

QS matrix, ""Regular Matrix"", ""Quadraphonic Sound"" (4:2:4)
j
        =
        +
        
          90
          
            ?
          
        
      
    
    {\displaystyle j=+90^{\circ }}
   phase-shift, 
  
    
      
        k
        =
        ?
        
          90
          
            ?
          
        
      
    
    {\displaystyle k=-90^{\circ }}
   phase-shift

Matrix H (4:2:4)
j = 20° phase-shift k = 25° phase-shift l = 55° phase-shift m = 115° phase-shift

Ambisonic UHJ kernel (3:2:4 or more)
j
        =
        +
        
          90
          
            ?
          
        
      
    
    {\displaystyle j=+90^{\circ }}
   phase-shift, 
  
    
      
        k
        =
        ?
        
          90
          
            ?
          
        
      
    
    {\displaystyle k=-90^{\circ }}
   phase-shift

Dolby Stereo and Dolby Surround (matrix 4:2:4)
Dolby Stereo and Dolby Surround are also known as Dolby MP, Dolby SVA and Pro Logic.
Dolby SVA matrix is the original name of the Dolby Stereo 4:2:4 encoding matrix.
The term ""Dolby Surround"" refers to both the encoding and decoding in the home environment, while in the theater it is known ""Dolby Stereo"", ""Dolby Motion Picture matrix"" or ""Dolby MP"". ""Pro Logic"" refers to the decoder used, there is no special Pro Logic encoding matrix.
The Ultra Stereo system, developed by different company, is compatible and uses similar matrixes to Dolby Stereo.
The Dolby Stereo Matrix is straightforward: the four original channels: Left (L), Center (C), Right (R), and Surround (S), are combined into two, known as Left-total (LT) and Right-total (RT) by this formula:
where j = 90° phase-shift
The center channel information is carried by both LT and RT in phase, and surround channel information by both LT and RT but out of phase. The surround channel is a single limited frequency-range (7 kHz low-pass filtered) mono rear channel, dynamically compressed and placed with a lower volume than the rest. This allows for better separation of signals.
This gives good compatibility with both mono playback, which reproduces L, C and R from the mono speaker with C at a level 3 dB higher than L or R, but surround information cancels out. It also gives good compatibility with two-channel stereo playback where C is reproduced from both left and right speakers to form a phantom center and surround is reproduced from both speakers but in a diffuse manner.
A simple 4-channel decoder could simply send the sum signal (L+R) to the center speaker, and the difference signal (L-R) to the surrounds. But such a decoder would provide poor separation between adjacent speaker channels, thus anything intended for the center speaker would also reproduce from left and right speakers only 3 dB below the level in the center speaker. Similarly anything intended for the left speaker would be reproduced from both the center and surround speakers, again only 3 dB below the level in the left speaker. There is, however complete separation between left and right, and between center and surround channels.
To overcome this problem the cinema decoder uses so-called ""logic"" circuitry to improve the separation. The logic circuitry decides which speaker channel has the highest signal level and gives it priority, attenuating the signals fed to the adjacent channels. Because there already is complete separation between opposite channels there is no need to attenuate those, in effect the decoder switches between L and R priority and C and S priority. This places some limitations on mixing for Dolby Stereo and to ensure that sound mixers mixed soundtracks appropriately they would monitor the sound mix via a Dolby Stereo encoder and decoder in tandem. In addition to the logic circuitry the surround channel is also fed via a delay, adjustable up to 100 ms to suit auditoria of differing sizes, to ensure that any leakage of program material intended for left or right speakers into the surround channel is always heard first from the intended speaker. This exploits the ""Precedence effect"" to localize the sound to the intended direction.

Dolby Pro Logic II matrix (5:2:5)
j
        =
        +
        
          90
          
            ?
          
        
      
    
    {\displaystyle j=+90^{\circ }}
   phase-shift, 
  
    
      
        k
        =
        ?
        
          90
          
            ?
          
        
      
    
    {\displaystyle k=-90^{\circ }}
   phase-shift
The Pro Logic II matrix provides for stereo full frequency back channels. Normally a sub-woofer channel is driven by simply filtering and redirecting the existing bass frequencies of the original stereo track.

References
See also
Ambisonic UHJ format
Dolby Digital
Dolby Pro Logic
Haas effect
Quadraphonic sound
Surround sound",Category:Surround sound,3
47,48,Minnaert resonance,"The Minnaert resonance is the acoustic resonance frequency of a single bubble in an infinite domain of water (neglecting the effects of surface tension and viscous attenuation). The resonant frequency is given by

  
    
      
        f
        =
        
          
            
              
                
              
              
                
                  1
                
              
            
            
              
                
              
              
                
                  2
                  ?
                  a
                
              
            
          
        
        
          
            (
            
              
                
                  
                    
                  
                  
                    
                      3
                      ?
                       
                      
                        p
                        
                          A
                        
                      
                    
                  
                
                
                  
                    
                  
                  
                    
                      ?
                    
                  
                
              
            
            )
          
          
            1
            
              /
            
            2
          
        
      
    
    {\displaystyle f={\cfrac {1}{2\pi a}}\left({\cfrac {3\gamma ~p_{A}}{\rho }}\right)^{1/2}}
  
where 
  
    
      
        a
      
    
    {\displaystyle a}
   is the radius of the bubble, 
  
    
      
        ?
      
    
    {\displaystyle \gamma }
   is the polytropic coefficient, 
  
    
      
        
          p
          
            A
          
        
      
    
    {\displaystyle p_{A}}
   is the ambient pressure, and 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   is the density of water. This formula can also be used to find the resonant frequency of a bubble cloud with 
  
    
      
        a
      
    
    {\displaystyle a}
   as the radius of the cloud and 
  
    
      
        ?
      
    
    {\displaystyle \rho }
   the difference between the density of water and the bulk density of the cloud. For a bubble in water at standard pressure 
  
    
      
        (
        
          p
          
            A
          
        
        =
        100
         
        
          
            k
            P
            a
          
        
        ,
         
        ?
        =
        1000
         
        
          
            k
            g
            
              /
            
            
              m
              
                3
              
            
          
        
        )
      
    
    {\displaystyle (p_{A}=100~{\rm {kPa}},~\rho =1000~{\rm {kg/m^{3}}})}
  , this equation reduces to 
  
    
      
        f
        a
        ?
        3.26
         
        
          
            m
            
              /
            
            s
          
        
      
    
    {\displaystyle fa\approx 3.26~{\rm {m/s}}}
  , where 
  
    
      
        f
         
      
    
    {\displaystyle f~}
   is the resonant frequency of the bubble.

References
External links
Low-Frequency Resonant Scattering of Bubble Clouds by Paul A. Hwang and William J. Teague, 2000, Journal of Atmospheric and Oceanic Technology, vol. 17, no. 6, pp. 847-853.",Category:Bubbles,3
48,49,Category:Sound by country,,Category:Commons category with local link different than on Wikidata,3
49,50,Growling,"Growling is a low, guttural vocalization produced by predatory animals as a warning to others, as a sign of aggression, or to express anger. Low or dull rumbling noises may also be emitted by human beings when they are discontent with something or they are angry, although this human sound is often termed ""groaning"".
Animals that growl include felines, any kind of bear, canines, alligators, and crocodiles. The animals most commonly known for growling are canines and felines.
Grrr /??????/ is an onomatopoeic word which imitates the growling sound of predatory animals, and is often used with other related meanings. It is one of the rare pronounceable words of the English language that consists solely of consonants. Its most simple use is by children imitating animals. An example would be: ""Mommy! Look at me! I'm a polar bear! Grrr!"" This word is also widely used in various titles to express growling when written.

Growling anatomy
An animals ability to growl has a physiological basis, just like any other activity an animal does. The growl is emitted from the larynx, also known as the voice box, which is located at the top of the throat. It is made up of both cartilage and soft tissue, with an opening in the center to allow the passage of air. Similar to how humans learn to speak, animals learn to growl through vibration of their vocal cords that occurs when air enters the larynx and passes over them.
Growling usually first appears in dogs when puppies are about 24 days of age during play fights, emitting a pitch of up to 450 Hz with great variation in consistency. By 9 weeks old, puppies produce a growl of around 300 Hz, with no variation in consistency. This is the final development of the dogs growl, and it will remain consistent through its life, although may vary in pitch between individuals. In other animals, growling can occur for various reasons. Most commonly is fear, aggression, territoriality, or like in alligators, for mating.

Growling in canines
Dogs are one of the most common animals known to growl. Dogs growl as a form of communication, most often when they are angry or showing signs of aggression. Dogs can also growl when they are playing with other canines/humans, growling over their possessions, are in pain, or during territorial displays. Human interpretation of dogs and other canines growling is often context dependent. If the growl is isolated as a audio clip, generally humans are unable to determine if the growl is playful, angry, or otherwise. When the growl is elicited directly from the dog, humans are often able to use other physical cues, as well as the length and volume/tone of the growl to interpret its meaning. Humans who are more frequently in the presence of canines are more accurately able to interpret the meaning of growls.
Growling in dogs is generally seen as unfavorable, and there are various methods to deal with this behaviour including therapy, training and temperament testing. The therapy approach to fear based or aggressive growling in dogs seems to work the most favourably, having a strong emphasis on owner-dog communication and understanding, as well as a strong reward system. Food-related aggression in dogs also elicits a growling response, and often occurs in many shelter dogs. This behaviour can have an adverse effect on their adoption rates, even though there is a high probability the food-related aggression will stop in the adopted home. Proper understanding of dog growling behaviours increases the likelihood of adoption in dogs with growling problems that are housed in shelters.

Function of aggressive growling
Food protection in canines tends to elicit a longer growl than average, and can be directed at humans, other canines, or other animals. Some fish, such as gurnards elicit a growling noise when attempting to grab prey fish, and have been shown to have a higher success rate at obtaining prey than non-growling fish. Growling in gurnards gives an advantage when there is limited food resources.This growl lasts up the 3 seconds and consists of up to 3 sound pulses, and is the only vocalization produced by this fish and is one of their two main feeding strategies.
In bears almost all vocalizations can be wrongly classified as a growl. Unlike cats and dogs, bears seldom truly growl and instead the fear-moans of a trapped or treed bear are often mistaken as a threatening growl. When bears are being intentionally aggressive, as in when hunting or when threatened, they will tend to remain silent, or make short blowing noises.
Felines such as jaguars and tigers also growl to signal territorial aggression, eliciting anti-predator responses from animals such as elephants. Similar to human interpretation of growling, elephants are able to distinguish the threat level based on the individual growl and will respond accordingly; elephants will retreat from tigers, but defend against jaguars.. Domestic house cats also growl, sounding like ""brrrrrooowwww"", usually followed by the typical hissing sound. In domestic cats, growling is a warning noise, implying unhappiness, annoyance, fear or other forms of aggression, and is a signal to back off. Cats may growl,similar to dogs, in the presence of other cats or dogs to establish dominance or to indicate they do not wish to interact with that individual

See also
Death growl
Roar (utterance)
Dog attack
Bark (sound)
Alveolar trill
Snarl


== References ==",Category:Sound,3
50,51,NICAM,"Near Instantaneous Companded Audio Multiplex (NICAM) is an early form of lossy compression for digital audio. It was originally developed in the early 1970s for point-to-point links within broadcasting networks. In the 1980s, broadcasters began to use NICAM compression for transmissions of stereo TV sound to the public.

History
Near-instantaneous companding
The idea was first described in 1964. In this, the 'ranging' was to be applied to the analogue signal before the analogue-to-digital converter (ADC) and after the digital-to-analogue converter (DAC). The application of this to broadcasting, in which the companding was to be done entirely digitally after the ADC and before the DAC, was described in a 1972 BBC Research Report.

Point-to-point links
NICAM was originally intended to provide broadcasters with six high-quality audio channels within a total bandwidth of 2048 kbit/s. This figure was chosen to match the E1 primary multiplex rate, and systems using this rate could make use of the planned PDH national and international telecommunications networks.
Several similar systems had been developed in various countries, and in about 1977/78 the BBC Research Department conducted listening tests to evaluate them. The candidates were:
A RAI system which used A-law companding to compress 14-bit linear PCM samples into 10 bits (14:10)
A NICAM-type system proposed by Télédiffusion de France (14:9)
NICAM-1 (13:10)
NICAM-2 (14:11)
NICAM-3 (14:10)
It was found that NICAM-2 provided the best sound quality, but reduced programme-modulated noise to an unnecessarily low level at the expense of bit rate. NICAM-3, which had been proposed during the test to address this, was selected as the winner.
Audio is encoded using 14 bit pulse-code modulation at a sampling rate of 32 kHz.

Broadcasts to the public
NICAM's second role – transmission to the public – was developed in the 80s by the BBC. This variant was known as NICAM-728, after the 728 kbit/s bitstream it is sent over. It uses the same audio coding parameters as NICAM-3.
The first NICAM digital stereo programme was broadcast on BBC2 in 1986, though programmes were not advertised as being broadcast in stereo on the BBC until some five years later, when the majority of the country's transmitters had been upgraded to broadcast NICAM, and a large number of BBC programmes were being made in stereo.
The BBC publicly launched their NICAM stereo service in the United Kingdom on Saturday 31 August 1991 (see 1991 in television) though other UK broadcasters ITV and Channel 4 advertised this capability some months earlier. Channel 4 began tests much earlier in February 1989 via the Crystal Palace transmitter in London.
It has been standardized as ETS EN 300 163.

Nations using NICAM public broadcasts
Several European countries (as well as one Middle East country) have implemented NICAM with the PAL and SECAM TV systems

Some Asia-Pacific nations/regions have implemented NICAM
Hong Kong (commonly used for dual language for programming containing both Cantonese and English/Mandarin/Japanese/Korean soundtracks; planned to cease by 2020 when digital TV broadcasting transition is complete and analogue TV transmissions will be stopped)
Singapore
Macau
Guangzhou
South Africa (SABC1, SABC2, etv)
Malaysia
Used by TV1, TV2, ntv7, 8TV, and TV9 around Klang Valley. TV3 also uses NICAM on their VHF transmission frequency (Channel 12) in the Klang Valley, but uses Zweikanalton on their UHF transmission frequency (Channel 29).

New Zealand (Full switchover to DVB-T complete by 1 December 2013. NICAM became historical from that date.)
Indonesia
Television stations in Indonesia use NICAM Stereo for analogue television. Full switchover to DVB-T2 is expected to complete by 2020 by which all analogue broadcasting have ceased.

Some other countries use Zweikanalton analogue stereo instead. Analogue stereo conversion thus begins.

How NICAM works
In order to provide mono ""compatibility"", the NICAM signal is transmitted on a subcarrier alongside the sound carrier. This means that the FM or AM regular mono sound carrier is left alone for reception by monaural receivers.
A NICAM-based stereo-TV infrastructure can transmit a stereo TV programme as well as the mono ""compatibility"" sound at the same time, or can transmit two or three entirely different sound streams. This latter mode could be used to transmit audio in different languages, in a similar manner to that used for in-flight movies on international flights. In this mode, the user can select which soundtrack to listen to when watching the content by operating a ""sound-select"" control on the receiver.

This is the spectrum of NICAM on the PAL system. On the SECAM L system, the NICAM sound carrier is at 5.85 MHz, before the AM sound carrier, and the video bandwidth is reduced from 6.5 MHz to 5.5 MHz.
NICAM currently offers the following possibilities. The mode is automatically selected by the inclusion of a 3-bit type field in the data stream.
One digital stereo sound channel.
Two completely different digital mono sound channels.
One digital mono sound channel and a 352 kbit/s data channel.
One 704 kbit/s data channel.
The four other options could be implemented at a later date. Only the first two of the ones listed are known to be in general use however.

NICAM packet transmission
The NICAM packet (except for the header) is scrambled with a nine-bit pseudo-random bit-generator before transmission.
The topology of this pseudo-random generator yields a bitstream with a repetition period of 511 bits.
The pseudo-random generator's polynomial is: 
  
    
      
        
          x
          
            9
          
        
        +
        
          x
          
            4
          
        
        +
        1.
      
    
    {\displaystyle x^{9}+x^{4}+1.}
  
The pseudo-random generator is initialized with: 
  
    
      
        111111111.
      
    
    {\displaystyle 111111111.}
  
Making the NICAM bitstream look more like white noise is important because this reduces signal patterning on adjacent TV channels.
The NICAM header is not subject to scrambling. This is necessary so as to aid in locking on to the NICAM data stream and resynchronisation of the data stream at the receiver.
At the start of each NICAM packet the pseudo-random bit generator's shift register is reset to all ones.

NICAM transmission issues
There are some latent issues involved with the processing of NICAM audio in the transmission chain.
NICAM (unlike the Compact Disc standard) samples 14-bit audio at 32 kHz.
The upper frequency limit of a NICAM sound channel is 15 kHz due to anti-aliasing filters at the encoder.
The original 14-bit PCM audio samples are companded digitally to 10 bits for transmission.
NICAM audio samples are divided into blocks of 32. If all the samples in a block are quiet, such that the most significant bits are all zeros, these bits can be discarded at no loss.
On louder samples some of the least significant bits are truncated, with the hope that they will be inaudible.
A 3-bit control signal for each block records which bits were discarded.
Digital companding (using a CCITT J.17 pre-emphasis curve) ensures that the encoding and decoding algorithms can track perfectly.

NICAM carrier power
ITU (and CCITT) standards specify that the power level of the NICAM signal should be at -20 dB with respect to the power of the vision carrier.
The level of the FM mono sound carrier must be at least -13 dB.
Measuring the modulation level of the NICAM signal is difficult because the QPSK NICAM carrier waveform (unlike AM or FM modulated carrier waveforms) is not emitted at a discrete frequency.
When measured with spectrum analyser the actual level of the carrier (L) can be calculated using the following formula:
L(NICAM) = L(Measured) + 10 log (R/BWAnalyser) + K
L(NICAM) = actual level of the NICAM carrier [dB?V]
L(Measured) = measured level of the NICAM carrier [dB?V]
R = -3 dB bandwidth of the signal [kHz]
BWAnalyser = bandwidth of the spectrum analyser [kHz]
K = logarithmic form factor of the spectrum analyser ~2 dB
note: if BWAnalyser is greater than R, the formula becomes L(NICAM) = L(Measured) + K

NICAM's unusual features
NICAM sampling is not standard PCM sampling, as commonly employed with the Compact Disc or at the codec level in MP3, AAC or Ogg audio devices. NICAM sampling more closely resembles Adaptive Differential Pulse Code Modulation, or A-law companding with an extended, rapidly modifiable dynamic range.

Two's complement signing
The two's complement method of signing the samples is used, so that:
01111111111111 represents positive full-scale
10000000000000 represents negative full-scale

±0 V has three binary representations
00000000000001 represents 0 V, with no +/- distinction. This may have originated as a method to reduce the emergence of DC patterns from transmission of silent material.
00000000000000 represents 0 V, with no +/- distinction
11111111111111 represents 0 V, with no +/- distinction

Parity checking limited to only 6 of 10 bits
In order to strengthen parity protection for the sound samples, the parity bit is calculated on only the top six bits of each NICAM sample. Early BBC NICAM research showed that uncorrected errors in the least significant four bits were preferable to the reduced overall protection offered by parity-protecting all ten bits.

Recording of NICAM audio
VCR
VHS and Betamax home videocassette recorders (""VCR""s) initially only recorded the audio tracks using a fixed linear recording head, which was inadequate for recording NICAM audio; this significantly limited their sound quality. Many VCRs later included high quality stereo audio recording as an additional feature, in which the incoming high quality stereo audio source (typically FM radio or NICAM TV) was frequency modulated and then recorded in addition to the usual audio and video VCR tracks, using the same high-bandwidth helical scanning technique used for the video signal. Full size VCRs already made full use of the tape, so the high quality audio signal was recorded diagonally under the video signal, using additional helical scan heads and depth multiplexing. The mono audio track (and on some machines, a non-NICAM, non-Hi-Fi stereo track) was also recorded on the linear track, as before, to ensure backwards-compatibility of recordings made on Hi-Fi machines when played on non-Hi-Fi VCRs.
Such devices were often described as ""HiFi audio"", ""Audio FM"" / ""AFM"" (FM standing for ""Frequency Modulation""), and sometimes informally as ""Nicam"" VCRs (due to their use in recording the Nicam broadcast audio signal). They remained compatible with non-HiFi VCR players since the standard audio track was also recorded, and were at times used as an alternative to audio cassette tapes due to their exceptional bandwidth, frequency range, and extremely flat frequency response.

DVD
While recording in video mode (compatible with DVD-Video), most DVD recorders can only record one of the three channels (Digital I, Digital II, Analogue mono) allowed by the standard. Newer standard such as DVD-VR allows recording all the digital channels (in both stereo and bilingual mode), whereas the mono channel will be lost.

Flash memory and computer multimedia
Codecs for digital media on computers will often convert NICAM to another digital audio format to compress drive space.

See also
Multichannel television sound
Sound-in-Syncs
Zweikanalton A2

References
Further reading
Osborne, D.W. and Croll, M.G. (1973), Digital sound signals: Bit-rate reduction using an experimental digital compandor. BBC Research Department Report 1973/41.
Croll, M.G., Osborne, D.W. and Reid, D.F. (1973), Digital sound signals: Multiplexing six high-quality sound channels for transmission at a bit-rate of 2.048 Mbit/s. BBC Research Department Report 1973/42.
Reid, D.F. and Croll, M.G. (1974), Digital sound signals: The effect of transmission errors in a near-instantaneous digitally companded system. BBC Research Department Report 1974/24.
Reid, D.F. and Gilchrist, N.H.C. (1977), Experimental 704 kbit/s multiplex equipment for two 15 kHz sound channels. BBC Research Department Report 1977/38.
Kalloway, M.J. (1978), An experimental 4-phase d.p.s.k. stereo sound system: the effect of multipath propagation. BBC Research Department Report 1978/15.

External links
Related websites or technical explanations
A technical description of NICAM
The BBC's information page on NICAM
Overview of Television Broadcasting Systems
MATLAB NICAM function",Category:Television technology,3
51,52,High-resolution audio,"High-resolution audio, also known as High-definition audio or HD audio, is a marketing term used by some recorded-music retailers and high-fidelity sound reproduction equipment vendors. It refers to higher than 44.1 kHz sample rate and/or higher than 16-bit linear bit depth. It usually means 96 kHz (or even much higher), sometimes informally written as ""96k"", meaning a Nyquist frequency of 48 kHz, which is outside of the hearing range of any human ear.

Definitions
There is no standard definition for what constitutes high-resolution audio, but it is generally used to describe audio signals with bandwidth and/or dynamic range greater than that of Compact Disc Digital Audio (CD-DA, informally CDs). This includes pulse-code modulation (PCM) encoded audio with sampling rates greater than 44,100 Hz and with bit-depths greater than 16, or their equivalents using other encoding techniques such as pulse-density modulation (PDM).
Although there is no firm definition, Sony describes high-resolution audio devices as those that deliver audio that’s clearer, sharper and more complex than other music sources and closer to the original.
File formats capable of storing high-resolution audio include FLAC, ALAC, WAV, AIFF and DSD, the format used by Super Audio Compact Discs (SACD).

History
One of the first attempts to market high-resolution audio was High Definition Compatible Digital in 1995. This was followed by three more optical disc formats claiming sonic superiority over CD-DA: DAD in 1998, SACD in 1999, and DVD-Audio in 2000. None of these achieved widespread adoption.
Following the rise in online music retailing at the start of the 21st century, high-resolution audio downloads were introduced by HDtracks starting in 2008.
Further attempts to market high-resolution audio on optical disc followed with Pure Audio Blu-ray in 2009, and High Fidelity Pure Audio in 2013. Competition in online high-resolution audio retail stepped-up in 2014 with the announcement of Neil Young's Pono service.
Consumer audio products that came with the ""Hi-Res AUDIO"" logo indicate that the product meet the specification required for a high resolution audio product, as defined by Japan Electronics and Information Technology Industries Association (JEITA).
Most recently, Sony has reaffirmed its commitment towards the development of high resolution audio segment by offering a slew of Hi-Res Audio products as it seeks to re-establish its leadership in consumer audio products.

Controversy
Whether there is any benefit to high-resolution audio over CD-DA is controversial, with some sources claiming sonic superiority:
""The DSD process used for producing SACDs captures more of the nuances from a performance and reproduces them with a clarity and transparency not possible with CD.—The Mariinsky record label of the Mariinsky Ballet (formerly Kirov Ballet), St. Petersburg, Russia, that sells Super Audio CDs (SACDs)
""the main claimed benefit of high-resolution audio files is superior sound quality [..] 24-bit/96k or 24-bit/192kHz files should therefore more closely replicate the sound quality that the musicians and engineers were working with in the studio. [..] As always, though, there are some people who can't hear a difference. So, if you can't see or hear a difference, save your money…""—What Hi-Fi?
and with other opinions ranging from skeptical to highly critical:
""If they [the music business] cared about sound quality in the first place, they would make all of the releases sound great in every format they sell: MP3, FLAC, CD, iTunes, or LP.""—cnet
""Impractical overkill that nobody can afford""—Gizmodo
""A solution to a problem that doesn't exist, a business model based on willful ignorance and scamming people.""—Xiph.org
Business magazine Bloomberg Businessweek suggests that caution is in order with regard to high-resolution audio: ""There is reason to be wary, given consumer electronics companies’ history of pushing advancements whose main virtue is to require everyone to buy new gadgets.""
High resolution files that are downloaded from niche websites that cater to ""audiophile"" listeners often include different mastering in the release –  thus many comparisons of CD to ""special"" releases are evaluating differences in mastering, rather than bit depth.
Most early papers using blind listening tests concluded that differences are not audible by the sample of listeners taking the test. Blind tests have shown that musicians and composers are unable to distinguish higher resolutions from 16-bits 48 kHz One 2014 paper showed that dithering using outdated methods (rectangular unshaped dither, rather than the industry standard triangular dither) produces audible artifacts in blind listening tests.


== Notes ==",Category:Sound,3
52,53,Sound-in-Syncs,"Sound-in-Syncs is a method of multiplexing sound and video signals into a channel designed to carry video, in which data representing the sound is inserted into the line synchronising pulse of an analogue television waveform. This is used on point-to-point links within broadcasting networks, including studio/transmitter links (STL). It is not used for broadcasts to the public.

History
The technique was first developed by the BBC in the late 1960s. In 1966, The corporation's Research Department made a feasibility study of the use of pulse-code modulation (PCM) for transmitting television sound during the synchronising period of the video signal. This had several advantages: it removed the necessity for a separate sound link and offered improved sound quality and reliability.

Awards
Sound-in-Syncs and its R&D engineers have won several awards, including:
The Royal Television Society's Geoffrey Parr Award in 1972
A Queen's Award for Enterprise in 1974
In 1999, a Technology & Engineering Emmy Award

Versions
Original mono S-i-S
In the original system, as applied to 625 line analogue TV, the audio signal was sampled twice during each television line and each sample converted to 10-bit PCM. Two such samples were inserted into the next line synchronising pulse. At the destination, the audio samples were converted back to analogue form and the video waveform restored to normal. Compandors operating on the signal before encoding and after decoding enabled the required signal-to-noise ratio to be achieved. As the PCM noise was predominantly high-pitched, the compandor only needed to operate on the high frequencies. Also, the compandor only operated at high audio levels, so that modulation of the noise by the companding would be masked by the relatively loud high-frequency audio components. A pilot tone at half the sampling frequency was transmitted to enable the expander to track the gain adjustment applied by the compressor, even when the latter was limiting.
Following successful trials with the BBC, in 1971 Pye TVT started to make and sell the S-i-S equipment under licence. The largest quantities went to the BBC itself, to the EBU and to Canada. Smaller numbers went to other countries including South Africa, Australia and Japan.

Ruggedised S-i-S
A ruggedised version of the system was developed, which provided about 7 kHz audio bandwidth, for use over noisy or difficult microwave paths, such as those often encountered for outside broadcasts.

Stereo S-i-S
Later systems, developed in the 1980s, used 14-bit linear PCM samples, digitally companded into 10-bit samples by means of NICAM-3 lossy compression. These were capable of carrying two audio channels and were known as stereo Sound-in-Syncs.

ITV S-i-S
The ITV network used coders and encoders produced by RE of Denmark. The two variations of Sound-in-Syncs used by the BBC and ITV were not compatible. The terms DCSIS or DSIS was commonly used in ITV to describe dual channel Sound-in-Syncs. Very often material carried was dual mono and not stereo.

Notes and references
Further reading
Waveform Specification of the BBC Sound-in-Syncs Equipment, EBU Review, 121A, June 1970.
Chorley, J.M. and Shorter, D.E.L. (1970), P.C.M. Sound-in-Syncs: Operational Systems for Video Distribution and Contribution Networks, IEE Conference Publication, No. 69, 1970 International Broadcasting Convention
Dalton, C.J. (1968), The Distribution of Television Sound Signals by PCM Signals incorporated in the Vision Waveform, IEE International Broadcasting Convention, September 1968, Vol. 46, Part 1
Sanders, J.R. (1967), Pulse sound: a system of television sound broadcasting using pulses in the video waveform, BBC Engineering Monograph, No. 67, May 1967
Sanders, J.R. (1968) Pulse-code modulation for high-quality sound signal distribution: Incorporation of the sound signal in the video signal. BBC Research Department report (PDF)
Shorter, D.E.L., Chew, J.R., Howarth, D. and Sanders, J.R. (1968), Pulse code modulation for high-quality sound signal distribution, BBC Engineering Monograph, No. 75, December 1968
Shorter, D.E.L. (1969), The Distribution of Television Sound by Pulse-code Modulation Signals incorporated in the Video Waveform, EBU Review, No. 113A, February 1969",Category:Television technology,3
53,54,Birth of public radio broadcasting,"The birth of public radio broadcasting is credited to Lee de Forest who transmitted the world’s first public broadcast in New York City on January 13, 1910. This broadcast featured the voices of Enrico Caruso and other Metropolitan Opera stars. Members of the public and the press used earphones to listen to the broadcast in several locations throughout the city. This marked the beginning of what would become nearly universal wireless radio communication.

First public broadcast
Date
A 1907 Lee De Forest company advertisement said,

It will soon be possible to distribute grand opera music from transmitters placed on the stage of the Metropolitan Opera House by a Radio Telephone station on the roof to almost any dwelling in Greater New York and vicinity ... The same applies to large cities. Church music, lectures, etc., can be spread abroad by the Radio Telephone.

Several years later, on January 13, 1910, the first public radio broadcast was an experimental transmission of a live Metropolitan Opera House performance by several famous opera singers. This transmission was arranged by Lee de Forest.

Performers
The wireless radio broadcast consisted of performances of Cavalleria Rusticana and Pagliacci. Riccardo Martin performed as Turridu, Emmy Destinn as Santuzza, and Enrico Caruso as Canio. The conductor was Egisto Tango. This event is regarded as the birth of public radio broadcasting.
The New York Times reported on January 14, 1910:

Opera broadcast in part from the stage of the New York City Metropolitan Opera Company was heard on January 13, 1910, when Enrico Caruso and Emmy Destinn sang arias from Cavalleria Rusticana and I Pagliacci, which were ""trapped and magnified by the dictograph directly from the stage and borne by wireless Hertzian waves over the turbulent waters of the sea to transcontinental and coastwise ships and over the mountainous peaks and undulating valleys of the country."" The microphone was connected by telephone wire to the laboratory of Dr. Lee De Forest.

Equipment
Receivers
The few radio receivers able to pick up this first-ever ""outside broadcast"" were those at the De Forest Radio Laboratory, on board ships in New York Harbor, in large hotels on Times Square and at New York city locations where members of the press were stationed at receiving sets. Public receivers with earphones had been set up in several well-advertised locations throughout New York City. There were members of the press stationed at various receiving sets throughout the city and the public was invited to listen to the broadcast.
The experiment was considered mostly unsuccessful. The microphones of the day were of poor quality and could not pick up most of the singing on stage. Only off-stage singers singing directly into a microphone could be heard clearly. The New York Times reported the next day that static and interference ""kept the homeless song waves from finding themselves"".
Lee De Forest's Radio Telephone Company manufactured and sold the first commercial radios in the demonstration room at the Metropolitan Life Building in New York City for this public event.

Transmitter
The wireless transmitter had 500 watts of power. It is reported that this broadcast was heard 20 km away on a ship at sea. The broadcast was also heard in Bridgeport, Connecticut.

Other broadcasts
Early music transmission
The very first transmission of music by radio is credited to one Dr. Nussbaumer of the University of Graz in 1904, however it was not to the public. He yodeled an Austrian folk song into an experimental transmitter which was received in the next room at the university where he worked. He does not show in any standard scientific reference works.
Lee De Forest produced a program broadcasting opera phonograph records from the Eiffel Tower in Paris in 1908. This was just an experimental stunt for other nearby hobbyists and not considered a public broadcast as the public had no access to receivers at the time. At one point, when testing the radiotelephone for the Navy, Lee de Forest played patriotic phonograph music as the ships entered the harbor.

See also
History of broadcasting
Oldest radio station
Oldest television station
Women in early radio
History of radio
Radio broadcasting
History of telecommunication
History of television
Metropolitan Opera radio broadcasts

References
Notes
Bibliography
Chase's 2000 Calendar of Events, NTC/Contemporary Publishing Group, Inc. 2000, ISBN 0-8092-2776-2
Kane, Joseph Nathan, Famous First Facts, Fourth Edition Revised and Expanded, New York: The H.W. Wilson Company, 1981, ISBN 0-8242-0661-4",Category:Sound,3
54,55,Kosten unit,"The Kosten unit (Ke) is a commonly used aggregate measure for aircraft noise in the Netherlands, developed by the Kosten Committee 1963. A yearly average which represents outdoor noise levels.

References
Adviescommissie Geluidhinder door Vliegtuigen, 1967. Adviescommissie Geluidhinder door Vliegtuigen, Geluidhinder door vliegtuigen., TNO, Delft (1967).",Category:Orphaned articles from February 2009,3
55,56,Reflection phase change,"A phase change sometimes occurs when a wave is reflected. Such reflections occur for many types of wave, including light waves, sound waves, and waves on strings.

Optics
Light waves change phase by 180° when they reflect from the surface of a medium with higher refractive index than that of the medium in which they are travelling. A light wave travelling in air that is reflected by a glass barrier will undergo a 180° phase change, while light travelling in glass will not undergo a phase change if it is reflected by a boundary with air. For this reason, optical boundaries are normally specified as an ordered pair (air-glass, glass-air); indicating which material the light is moving out of, and in to, respectively.
""Phase"" here is the phase of the electric field oscillations, not the magnetic field oscillations. Also, this is referring to near-normal incidence—for p-polarized light reflecting off glass at glancing angle, beyond the Brewster angle, the phase change is 0°.
The phase changes that take place upon reflection play an important part in thin film interference.

Sound waves
Sound waves in a solid experience a phase reversal (a 180° change) when they reflect from a boundary with air. Sound waves in air do not experience a phase change when they reflect from a solid, but they do exhibit a 180° change when reflecting from a region with lower acoustic impedance. An example of this is when a sound wave in a hollow tube encounters the open end of the tube. The phase change on reflection is important in the physics of wind instruments.

Strings
A wave on a string experiences a 180° phase change when it reflects from a point where the string is fixed. Reflections from the free end of a string exhibit no phase change. The phase change when reflecting from a fixed point contributes to the formation of standing waves on strings, which produce the sound from stringed instruments.

Electrical transmission lines
Reflections of signals on conducting lines can exhibit a phase change from the incident signal. The voltage wave reflection on a line terminated with a short circuit is 180° phase shifted. This is analogous (by the mobility analogy) to a string where the end is fixed in position, or a sound wave in a tube with a blocked off end. The current wave, on the other hand, is not phase shifted. A transmission line terminated with an open circuit is the dual case; the voltage wave is shifted by 0° and the current wave is shifted by 180°. In both cases the full amplitude of the wave is reflected.
A transmission line terminated with a pure capacitance or inductance will also give rise to a phase shifted wave at full amplitude. The voltage phase shift is given by

  
    
      
        ?
        =
        2
        
          tan
          
            ?
            1
          
        
        ?
        
          
            
              Z
              
                0
              
            
            X
          
        
      
    
    {\displaystyle \varphi =2\tan ^{-1}{Z_{0} \over X}}
  
where
Z0 is the characteristic impedance of the line
X is the susceptance of the inductance or capacitance, given respectively by ?L or ?1/?C
L and C are, respectively, inductance and capacitance, and
? is the angular frequency.
The phase shift will be between 0 and 180° and will be respectively positive or negative for inductors and capacitors. The phase shift will be exactly 90° when X = Z0.
For the general case when the line is terminated with some arbitrary impedance, Z, the reflected wave is generally less than the incident wave. The full expression for phase shift needs to be used,

  
    
      
        ?
        =
        
          tan
          
            ?
            1
          
        
        ?
        
          (
          
            
              
                2
                
                  |
                
                Z
                
                  |
                
                sin
                ?
                (
                ?
                Z
                )
                
                  Z
                  
                    0
                  
                
              
              
                
                  Z
                  
                    2
                  
                
                ?
                
                  Z
                  
                    0
                  
                  
                    2
                  
                
              
            
          
          )
        
      
    
    {\displaystyle \varphi =\tan ^{-1}\left({\frac {2|Z|\sin(\angle Z)Z_{0}}{Z^{2}-Z_{0}^{2}}}\right)}
  
This expression assumes the characteristic impedance is purely resistive.


== References ==",Category:Wave mechanics,3
56,57,Sound from ultrasound,"Sound from ultrasound is the name given here to the generation of audible sound from modulated ultrasound without using an active receiver. This happens when the modulated ultrasound passes through a nonlinear medium which acts, intentionally or unintentionally, as a demodulator.

Parametric array
Since the early 1960s, researchers have been experimenting with creating directive low-frequency sound from nonlinear interaction of an aimed beam of ultrasound waves produced by a parametric array using heterodyning. Ultrasound has much shorter wavelengths than audible sound, so that it propagates in a much narrower beam than any normal loudspeaker system using audio frequencies. Most of the work was performed in liquids (for underwater sound use).
The first modern device for air acoustic use was created in 1998, and is now known by the trademark name ""Audio Spotlight"", a term first coined in 1983 by the Japanese researchers who abandoned the technology as infeasible in the mid-1980s.
A transducer can be made to project a narrow beam of modulated ultrasound that is powerful enough, at 100 to 110 dBSPL, to substantially change the speed of sound in the air that it passes through. The air within the beam behaves nonlinearly and extracts the modulation signal from the ultrasound, resulting in sound that can be heard only along the path of the beam, or that appears to radiate from any surface that the beam strikes. This technology allows a beam of sound to be projected over a long distance to be heard only in a small well-defined area; for a listener outside the beam the Sound pressure decreases substantially. This effect cannot be achieved with conventional loudspeakers, because sound at audible frequencies cannot be focused into such a narrow beam.
There are some limitations with this approach. Anything that interrupts the beam will prevent the ultrasound from propagating, like interrupting a spotlight's beam. For this reason, most systems are mounted overhead, like lighting.

Applications
Military
There has been speculation about military sonic weapons that emit highly-directional high-intensity sound; however, these devices do not use ultrasound, although sometimes thought to do so. Wikileaks has published technical specifications of such sound weapons.

Commercial advertising
A sound signal can be aimed so that only a particular passer-by, or somebody very close, can hear it. In commercial applications, it can target sound to a single person without the peripheral sound and related noise of a loudspeaker.

Personal audio
It can be used for personal audio, either to have sounds audible to only one person, or that which a group wants to listen to. The navigation instructions for example are only interesting for the driver in a car, not for the passengers. Another possibility are future applications for true stereo sound, where one ear does not hear what the other is hearing.

Train Signaling Device
Directional audio train signaling may be accomplished through the use of an ultrasonic beam which will warn of the approach of a train while avoiding the nuisance of loud train signals on surrounding homes and businesses.

History
This technology was originally developed by the US Navy and Soviet Navy for underwater sonar in the mid-1960s, and was briefly investigated by Japanese researchers in the early 1980s, but these efforts were abandoned due to extremely poor sound quality (high distortion) and substantial system cost. These problems went unsolved until a paper published by Dr. F. Joseph Pompei of the Massachusetts Institute of Technology in 1998 fully described a working device that reduced audible distortion essentially to that of a traditional loudspeaker.

Products
As of 2014 there were known to be five devices which have been marketed that use ultrasound to create an audible beam of sound.

Audio Spotlight
F. Joseph Pompei of MIT developed technology he calls the ""Audio Spotlight"", and made it commercially available in 2000 by his company Holosonics, which according to their website claims to have sold ""thousands"" of their ""Audio Spotlight"" systems. Disney was amongst the first major corporations to adopt it for use at the Epcot Center, and many other application examples are shown on the Holosonics website.
Audio Spotlight is a narrow beam of sound that can be controlled with similar precision to light from a spotlight. It uses a beam of ultrasound as a ""virtual acoustic source"", enabling control of sound distribution. The ultrasound has wavelengths only a few millimeters long which are much smaller than the source, and therefore naturally travel in an extremely narrow beam. The ultrasound, which contains frequencies far outside the range of human hearing, is completely inaudible. But as the ultrasonic beam travels through the air, the inherent properties of the air cause the ultrasound to change shape in a predictable way. This gives rise to frequency components in the audible band, which can be predicted and controlled.

HyperSonic Sound
Elwood ""Woody"" Norris, founder and Chairman of American Technology Corporation (ATC), announced he had successfully created a device which achieved ultrasound transmission of sound in 1996. This device used piezoelectric transducers to send two ultrasonic waves of differing frequencies toward a point, giving the illusion that the audible sound from their interference pattern was originating at that point. ATC named and trademarked their device as ""HyperSonic Sound"" (HSS). In December 1997, HSS was one of the items in the Best of What's New issue of Popular Science. In December 2002, Popular Science named HyperSonic Sound the best invention of 2002. Norris received the 2005 Lemelson-MIT Prize for his invention of a ""hypersonic sound"". ATC (now named LRAD Corporation) spun off the technology to Parametric Sound Corporation in September 2010 to focus on their Long Range Acoustic Device products (LRAD), according to their quarterly reports, press releases and executive statements.

Mitsubishi Electric Engineering Corporation
Mitsubishi apparently offers a sound from ultrasound product named the ""MSP-50E"" but commercial availability has not been confirmed.

AudioBeam
German audio company Sennheiser Electronic once listed their ""AudioBeam"" product for about $4,500. There is no indication that the product has been used in any public applications. The product has since been discontinued.

Soundlazer
Started as a Kickstarter project in 2012, Richard Haberkern developed the Soundlazer SL-01 for use by the general public. The SL-01 is currently available for sale on the company's website as the consumer version or as a developer's kit. A new model, named the Soundlazer Snap, is expected to deliver in early 2015, following a crowdfunding campaign on Kickstarter. The Snap was designed primarily for hobbyists and requires some assembly.

Literature survey
The first experimental systems were built over 30 years ago, although these first versions only played simple tones. It was not until much later (see above) that the systems were built for practical listening use.

Experimental ultrasonic nonlinear acoustics
A chronological summary of the experimental approaches taken to examine Audio Spotlight systems in the past will be presented here. At the turn of the millennium working versions of an Audio Spotlight capable of reproducing speech and music could be bought from Holosonics, a company founded on Dr. Pompei's work in the MIT Media Lab.
Related topics were researched almost 40 years earlier in the context of underwater acoustics.
The first article consisted of a theoretical formulation of the half pressure angle of the demodulated signal.
The second article provided an experimental comparison to the theoretical predictions.
Both articles were supported by the U.S. Office of Naval Research, specifically for the use of the phenomenon for underwater sonar pulses. The goal of these systems was not high directivity per se, but rather higher usable bandwidth of a typically band-limited transducer.
The 1970s saw some activity in experimental airborne systems, both in air and underwater. Again supported by the U.S. Office of Naval Research, the primary aim of the underwater experiments was to determine the range limitations of sonar pulse propagation due to nonlinear distortion. The airborne experiments were aimed at recording quantitative data about the directivity and propagation loss of both the ultrasonic carrier and demodulated waves, rather than developing the capability to reproduce an audio signal.
In 1983 the idea was again revisited experimentally but this time with the firm intent to analyze the use of the system in air to form a more complex base band signal in a highly directional manner. The signal processing used to achieve this was simple DSB-AM with no precompensation, and because of the lack of precompensation applied to the input signal, the THD Total harmonic distortion levels of this system would have probably been satisfactory for speech reproduction, but prohibitive for the reproduction of music. An interesting feature of the experimental set up used in was the use of 547 ultrasonic transducers to produce a 40 kHz ultrasonic sound source of over 130db at 4 m, which would demand significant safety considerations. Even though this experiment clearly demonstrated the potential to reproduce audio signals using an ultrasonic system, it also showed that the system suffered from heavy distortion, especially when no precompensation was used.

Theoretical ultrasonic nonlinear acoustics
The equations that govern nonlinear acoustics are quite complicated and unfortunately they do not have general analytical solutions. They usually require the use of a computer simulation. However, as early as 1965, Berktay performed an analysis under some simplifying assumptions that allowed the demodulated SPL to be written in terms of the amplitude modulated ultrasonic carrier wave pressure Pc and various physical parameters. Note that the demodulation process is extremely lossy, with a minimum loss in the order of 60 dB from the ultrasonic SPL to the audible wave SPL. A precompensation scheme can be based from Berktay's expression, shown in Equation 1, by taking the square root of the base band signal envelope E and then integrating twice to invert the effect of the double partial time derivative. The analogue electronic circuit equivalents of a square root function is simply an op-amp with feedback, and an equalizer is analogous to an integration function. However these topic areas lie outside the scope of this project.

  
    
      
        
          p
          
            2
          
        
        (
        x
        ,
        t
        )
        =
        K
        ?
        
          P
          
            c
          
          
            2
          
        
        ?
        
          
            
              ?
              
                2
              
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        
          E
          
            2
          
        
        (
        x
        ,
        t
        )
      
    
    {\displaystyle p_{2}(x,t)=K\cdot P_{c}^{2}\cdot {\frac {\partial ^{2}}{\partial t^{2}}}E^{2}(x,t)}
  
Where

  
    
      
        
          p
          
            2
          
        
        (
        x
        ,
        t
        )
        =
        
      
    
    {\displaystyle p_{2}(x,t)=\,}
   Audible secondary pressure wave

  
    
      
        K
        =
        
      
    
    {\displaystyle K=\,}
   misc. physical parameters

  
    
      
        
          P
          
            c
          
        
        =
        
      
    
    {\displaystyle P_{c}=\,}
   SPL of the ultrasonic carrier wave

  
    
      
        E
        (
        x
        ,
        t
        )
        =
        
      
    
    {\displaystyle E(x,t)=\,}
   Envelope function (such as DSB-AM)
This equation says that the audible demodulated ultrasonic pressure wave (output signal) is proportional to the twice differentiated, squared version of the envelope function (input signal). Precompensation refers to the trick of anticipating these transforms and applying the inverse transforms on the input, hoping that the output is then closer to the untransformed input.
By the 1990s, it was well known that the Audio Spotlight could work but suffered from heavy distortion. It was also known that the precompensation schemes placed an added demand on the frequency response of the ultrasonic transducers. In effect the transducers needed to keep up with what the digital precompensation demanded of them, namely a broader frequency response. In 1998 the negative effects on THD of an insufficiently broad frequency response of the ultrasonic transducers was quantified with computer simulations by using a precompensation scheme based on Berktay's expression. In 1999 Pompei's article discussed how a new prototype transducer met the increased frequency response demands placed on the ultrasonic transducers by the precompensation scheme, which was once again based on Berktay's expression. In addition impressive reductions in the THD of the output when the precompensation scheme was employed were graphed against the case of using no precompensation.
In summary, the technology that originated with underwater sonar 40 years ago has been made practical for reproduction of audible sound in air by Pompei's paper and device, which, according to his AES paper (1998), demonstrated that distortion had been reduced to levels comparable to traditional loudspeaker systems.

Modulation scheme
The nonlinear interaction mixes ultrasonic tones in air to produce sum and difference frequencies. A DSB-AM modulation scheme with an appropriately large baseband DC offset, to produce the demodulating tone superimposed on the modulated audio spectra, is one way to generate the signal that encodes the desired baseband audio spectra. This technique suffers from extremely heavy distortion as not only the demodulating tone interferes, but also all other frequencies present interfere with one another. The modulated spectra is convolved with itself, doubling its bandwidth by the length property of the convolution. The baseband distortion in the bandwidth of the original audio spectra is inversely proportional to the magnitude of the DC offset (demodulation tone) superimposed on the signal. A larger tone results in less distortion.
Further distortion is introduced by the second order differentiation property of the demodulation process. The result is a multiplication of the desired signal by the function -?² in frequency. This distortion may be equalized out with the use of preemphasis filtering (increase amplitude of high frequency signal).
By the time convolution property of the fourier transform, multiplication in the time domain is a convolution in the frequency domain. Convolution between a baseband signal and a unity gain pure carrier frequency shifts the baseband spectra in frequency and halves its magnitude, though no energy is lost. One half-scale copy of the replica resides on each half of the frequency axis. This is consistent with Parseval's theorem.
The modulation depth m is a convenient experimental parameter when assessing the total harmonic distortion in the demodulated signal. It is inversely proportional to the magnitude of the DC offset. THD increases proportionally with m1².
These distorting effects may be better mitigated by using another modulation scheme that takes advantage of the differential squaring device nature of the nonlinear acoustic effect. Modulation of the second integral of the square root of the desired baseband audio signal, without adding a DC offset, results in convolution in frequency of the modulated square-root spectra, half the bandwidth of the original signal, with itself due to the nonlinear channel effects. This convolution in frequency is a multiplication in time of the signal by itself, or a squaring. This again doubles the bandwidth of the spectra, reproducing the second time integral of the input audio spectra. The double integration corrects for the -?² filtering characteristic associated with the nonlinear acoustic effect. This recovers the scaled original spectra at baseband.
The harmonic distortion process has to do with the high frequency replicas associated with each squaring demodulation, for either modulation scheme. These iteratively demodulate and self-modulate, adding a spectrally smeared out and time exponentiated copy of the original signal to baseband and twice the original center frequency each time, with one iteration corresponding to one traversal of the space between the emitter and target. Only sound with parallel collinear phase velocity vectors interfere to produce this nonlinear effect. Even-numbered iterations will produce their modulation products, baseband and high frequency, as reflected emissions from the target. Odd-numbered iterations will produce their modulation products as reflected emissions off the emitter.
This effect still holds when the emitter and the reflector are not parallel, though due to diffraction effects the baseband products of each iteration will originate from a different location each time, with the originating location corresponding to the path of the reflected high frequency self-modulation products.
These harmonic copies are largely attenuated by the natural losses at those higher frequencies when propagating through air.

Attenuation of ultrasound in air
The Figure provided in provided an estimation of the attenuation that the ultrasound would suffer as it propagated through air. The figures from this graph correspond to completely linear propagation, and the exact effect of the nonlinear demodulation phenomena on the attenuation of the ultrasonic carrier waves in air was not considered. There is an interesting dependence on humidity. Nevertheless, a 50 kHz wave can be seen to suffer an attenuation level in the order of 1 dB per meter at one atmosphere of pressure.

Safe use of high-intensity ultrasound
For the nonlinear effect to occur, relatively high intensity ultrasonics are required. The SPL involved was typically greater than 100 dB of ultrasound at a nominal distance of 1 m from the face of the ultrasonic transducer. Exposure to more intense ultrasound over 140 dB near the audible range (20–40 kHz) can lead to a syndrome involving manifestations of nausea, headache, tinnitus, pain, dizziness and fatigue, but this is around 100 times the 100 dB level cited above, and is generally not a concern. Dr Joseph Pompei of Audio Spotlight has published data showing that their product generates ultrasonic sound pressure levels around 130 dB (at 60 kHz) measured at 3 meters.
The UK's independent Advisory Group on Non-ionising Radiation (AGNIR) produced a 180-page report on the health effects of human exposure to ultrasound and infrasound in 2010. The UK Health Protection Agency (HPA) published their report, which recommended an exposure limit for the general public to airborne ultrasound sound pressure levels (SPL) of 100 dB (at 25 kHz and above).
OSHA specifies a safe ceiling value of ultrasound as 145 dB SPL exposure at the frequency range used by commercial systems in air, as long as there is no possibility of contact with the transducer surface or coupling medium (i.e. submerged). This is several times the highest levels used by commercial Audio Spotlight systems, so there is a significant margin for safety. In a review of international acceptable exposure limits Howard et al. (2005) noted the general agreement amongst standards organizations, but expressed concern with the decision by United States of America’s Occupational Safety and Health Administration (OSHA) to increase the exposure limit by an additional 30 dB under some conditions (equivalent to a factor of 1000 in intensity).
For frequencies of ultrasound from 25 to 50 kHz, a guideline of 110 dB has been recommended by Canada, Japan, the USSR, and the International Radiation Protection Agency, and 115 dB by Sweden in the late 1970s to early 1980s, but these were primarily based on subjective effects. The more recent OSHA guidelines above are based on ACGIH (American Conference of Governmental Industrial Hygienists) research from 1987.
Lawton(2001) reviewed international guidelines for airborne ultrasound in a report published by the United Kingdom’s Health and Safety Executive, this included a discussion of the guidelines issued by the American Conference of Governmental Industrial Hygienists (ACGIH), 1988. Lawton states “This reviewer believes that the ACGIH has pushed its acceptable exposure limits to the very edge of potentially injurious exposure"". The ACGIH document also mentioned the possible need for hearing protection.

See also
Directional sound
Infrasound
Parametric array

Further resources
USS Patent 6778672 filed on 17 August 2004 describes an HSS system for using ultrasound to:-
Direct distinct 'in-car entertainment' directly to passengers in different positions.
Shape the airwaves in the vehicle to deaden unwanted noises.

References


== External links ==",Category:Webarchive template wayback links,3
57,58,EIAJ MTS,"EIAJ MTS is a multichannel television sound standard created by the EIAJ.
Bilingual and stereo sound television programs started being broadcast in Japan in October 1978 using a system developed by NHK Technical Research Labs. This system was modified and standardized by the EIAJ in January 1979.
The original version M TV standard has a monaural FM transmission at 4.5 MHz. For Japanese multichannel television sound a second channel, or sub-channel, is added to the original signal by using an FM sub-carrier at twice the line frequency (Fh, or 15374 Hz). In order to identify the different modes (mono, stereo, or dual sound) a pilot tone is also added on an AM carrier at 3.5 times the line frequency. The pilot tone frequencies are 982.5 Hz for stereo and 922.5 Hz for dual sound. Contrary to Zweikanalton these pilot tones are not coupled to the line frequency but were instead chosen to allow use of filters already employed in the Pocket Bell pager system.

See also
Multichannel television sound (3 additional audio channels in an NTSC-format audio carrier.)
NICAM
Zweikanalton A2


== References ==",Category:Sound,3
58,59,Intelligibility (communication),"In speech communication, intelligibility is a measure of how comprehensible speech is in given conditions. Intelligibility is affected by the level (loud but not too loud) and quality of the speech signal, the type and level of background noise, reverberation (some reflections but not too many), and, for speech over communication devices, the properties of the communication system. The concept of speech intelligibility is relevant to several fields, including phonetics, human factors, acoustical engineering, and audiometry.

Noise levels and reverberation
Intelligibility is negatively impacted by background noise and too much reverberation. The relationship between sound and noise levels is generally described in terms of a signal-to-noise ratio. With a background noise level between 35 and 100 dB, the threshold for 100% intelligibility is usually a signal-to-noise ratio of 12 dB. 12 dB means that the signal should be roughly 4 times louder that the background noise. The speech signal ranges from about 200–8000 Hz, while human hearing ranges from about 20-20,000 Hz, so the effects of masking depend on the frequency range of the masking noise. Additionally, different speech sounds make use of different parts of the speech frequency spectrum, so a continuous background noise such as white or pink noise will have a different effect on intelligibility than a variable or modulated background noise such as competing speech, multi-talker or ""cocktail party"" babble, or industrial machinery.
Reverberation also affects the speech signal by blurring speech sounds over time. This has the effect of enhancing vowels with steady states, while masking stops, glides and vowel transitions, and prosodic cues such as pitch and duration.
The fact that background noise compromises intelligibility is exploited in audiometric testing involving spoken speech and some linguistic perception experiments as a way to compensate for the ceiling effect by making listening tasks more difficult.

Intelligibility standards
Word articulation remains high even when only 1–2% of the wave is unaffected by distortion.

Intelligibility with different types of speech
Lombard speech
The human brain automatically changes speech made in noise through a process called the Lombard effect. Such speech has increased intelligibility compared to normal speech. It is not only louder but the frequencies of its phonetic fundamental are increased and the durations of its vowels are prolonged. People also tend to make more noticeable facial movements.

Screaming
Shouted speech is less intelligible than Lombard speech because increased vocal energy produces decreased phonetic information. However, ""infinite peak clipping of shouted speech makes it almost as intelligible as normal speech.""

Clear speech
Clear speech is used when talking to a person with a hearing impairment. It is characterized by a slower speaking rate, more and longer pauses, elevated speech intensity, increased word duration, ""targeted"" vowel formants, increased consonant intensity compared to adjacent vowels, and a number of phonological changes (including fewer reduced vowels and more released stop bursts).

Infant-directed speech
Infant-directed speech—or baby talk—uses a simplified syntax and a small and easier-to-understand vocabulary than speech directed to adults Compared to adult directed speech, it has a higher fundamental frequency, exaggerated pitch range, and slower rate.

Citation speech
Citation speech occurs when people engage self-consciously in spoken language research. It has a slower tempo and fewer connected speech processes (e.g., shortening of nuclear vowels, devoicing of word-final consonants) than normal speech.

Hyperspace speech
Hyperspace speech, also known as the hyperspace effect, occurs when people are misled about the presence of environment noise. It involves modifying the F1 and F2 of phonetic vowel targets to ease perceived difficulties on the part of the listener in recovering information from the acoustic signal.

Notes
External links
Intelligibility conversion ALcons to STI and vice versa
Speech Quality and Evaluation (a chapter from a Master Thesis)",Category:Sound,3
59,60,Mix-minus,"Relative to audio engineering, a mix-minus or clean feed is a particular setup of a mixing console or matrix mixer, such that each output of the mixer contains everything except the associated input. Mix-minus, which is technically accomplished via a wiring and patching configuration, prevents echoes or feedback from reverberating or howling and squealing through the broadcast or sound reinforcement system.

Examples of mix-minus configurations
The most common example of mix-minus is when hooking up a telephone hybrid to a console, at a radio station. The person on the telephone hears everything playing, including the DJ, except that the caller does not hear his own voice.
Mix-minus is also often used together with IFB systems in electronic news gathering (ENG) for television news reporters and interview subjects speaking to a host from a remote outside broadcast (OB) location. Because of the delay that is introduced in most means of transmission (including satellite feeds and audio over IP connections), the remote subject's voice has to be removed from his earpiece. Otherwise, the subject would hear himself with a slight (but very distracting) delay.
Another common example is in the field of sound reinforcement. Consider a room with sound stations for multiple users, each station containing a microphone and a loudspeaker. Such a room might be used in a government house of parliament. The microphone in station #1 would feed the loudspeakers in every other station except station #1. In other words, station #1 receives a mix of all microphones minus the station #1 microphone. This enables all participants to hear each other clearly but minimizes problems with acoustic feedback.
Mix-minus is also used with VoIP communication when recording for podcasts: mix-minus removes the caller's voice from the VoIP call, but allows them to hear all other channels available at the mixing console (mixer).
Some broadcast mixing desks, notably those designed in house by the BBC, maintain a separate mix bus for clean feeds. This is technically different, and arguably superior to most implementations of mix-minus, but the end result is the same.


== References ==",Category:Audio engineering,3
60,61,Phonetic reversal,"Phonetic reversal is the process of reversing the phonemes or phones of a word or phrase. When the reversal is identical to the original, the word or phrase is called a phonetic palindrome. Phonetic reversal is not entirely identical to backmasking, which is specifically the reversal of recorded sound. This is because pronunciation in speech causes a reversed diphthong to sound different in either direction (e.g. eye [a?] becoming yah [j??]), or differently articulate a consonant depending on where it lies in a word, hence creating an imperfect reversal.
Backmasking involves not only the reversal of the order of phonemes or phones, but the reversal of the allophonic realizations of those phonemes. Strictly speaking, a reversal of phonemes will still result in allophones appropriate for the new position; for example, if a word with a final /t/ is reversed so that the /t/ is initial, the initial /t/ will be aspirated in line with the conventional allophonic patterns of English phonology.
According to proponents of reverse speech, phonetic reversal occurs unknowingly during normal speech.

ExamplesEdit
In 1974 album Rock Bottom, the track Little Red Riding Hood Hit The Road presented the chord progression along with Robert Wyatt's singing being both phonetically reversed at one middle point of the song, which turned the track's harmonics to be reversed from the beginning although Robert Wyatt restarted to sing normally, causing an original and disturbing effect.
In 1982, John Wright of NoMeansNo sang phonetically reversed lyrics on the backing vocal to the ""Rich Guns"" track on the band's first album, Mama.
In the 1984 American film Amadeus, lead character Wolfgang Mozart claims to Constanze Weber that ""[in Salzburg] everything goes backwards"". He then proceeds to deliver a series of phonetically reversed phrases, many of them vulgar, which she must guess by reversing them out loud.
In the television drama Twin Peaks, the Man from Another Place's character's speech was phonetically reversed, as well as other characters.
The Simpsons used the technique to parody Twin Peaks in the episode ""Who Shot Mr. Burns? (Part Two)"".
Singer/songwriter/multi-instrumentalist Jim Ure is better known by his phonetically reversed name Midge Ure.
Kate Bush used phonetic reversal in her songs ""Watching You Without Me"" (1985) and ""Leave it Open"" (1982).
The English rock band Radiohead used the effect on the song ""Like Spinning Plates"", released on their 2001 album Amnesiac. Singer Thom Yorke sang the lyrics backwards; this recording was in turn reversed to create ""backwards-sounding"" vocals.
A specific recording of the phrase ""In the mix"" exists that is a phonetic palindrome, and is often used by Turntablist DJs for this reason.
In the 2008 monster film Cloverfield, after the credits, a broken sound recording can be heard of Rob saying ""...help us..."", as at the end, he and his girlfriend were trapped under a bridge. If reversed, it sounds like Rob saying ""...it’s still alive...""
In 2007, backwards speaking radio sensation ""Backwards Dave"" (David Klempfner) beat the Guinness World Record Holder, David Fuhrer aka ""Mr Backwards"" in a backwards-speaking competition live on Triple J. In 2008, Dave performed on Australia's Got Talent saying ""Does Australia Have Talent?"" in phonetic reversal. In 2012 Backwards Dave appeared on Channel 7's ""Sunrise"", an Australian TV show.
During the opening theme song for Gravity Falls, a whisper is heard saying, ""I'm still here."" However, if one reverses it, it says, ""Three Letters Back"", which is a clue to the ending credits code. The whisper is changed to ""Switch the A with Z"" in Double Dipper, ""26 Letters"" in Bottomless Pit, ""Key Vigenere"" in Scary-Oke, and ""Not What He Seems"" in the episode with this title (while still using the Vigenere cipher).

ReferencesEdit

EV: Audio reversal in popular culture
Script for Amadeus
Boy can talk fluently backward - Yahoo!7 (November 13, 2013)",Category:Phonetics,3
61,62,Soundwalk,"A soundwalk is a walk with a focus on listening to the environment. The term was first used by members of the World Soundscape Project under the leadership of composer R. Murray Schafer in Vancouver in the 1970s. Hildegard Westerkamp, from the same group of artists, defines soundwalking as ""... any excursion whose main purpose is listening to the environment. It is exposing our ears to every sound around us no matter where we are."" 
Other terms closely related to soundwalking and used by Schafer include:
Keynote: typically ambient sounds which are not perceived, not because they are inaudible but because they are filtered out cognitively, such as a highway or air-condition hum)
Soundmark: a sonic landmark; a sound which is characteristic of a place)
Sound signal: a foreground sound; e.g. a dog, an alarm clock; messages/meaning is usually carried through sound signals.
Sound object: the smallest possible recognizable sonic entity (recognizable by its amplitude envelope)
Acousmatic: a description for sounds whose sources are out of sight or unknown. This also relates to acousmatic music.
Schafer was particularly interested in the implications of the changes in soundscapes in industrial societies in children, and children's relationship to the world through sound. He was a proponent of ear-cleaning (cleaning one's ears cognitively), and he saw soundwalking as an important part of this process of re-engaging our aural senses in finding our place in the world. 
Soundwalking has been used as artistic medium by visual artists and documentary makers, such as Janet Cardiff.

See also
Soundscape ecology
Acousmatic music
Sound art
Shinrin-yoku
Soundwalk (collective)


== References ==",Category:Pages using citations with accessdate and no URL,3
62,63,Diffusion (acoustics),"Diffusion, in acoustics and architectural engineering, is the efficacy by which sound energy is spread evenly in a given environment. A perfectly diffusive sound space is one that has certain key acoustic properties which are the same anywhere in the space. A non-diffuse sound space would have considerably different reverberation time as the listener moved around the room. Virtually all spaces are non-diffuse. Spaces which are highly non-diffuse are ones where the acoustic absorption is unevenly distributed around the space, or where two different acoustic volumes are coupled. The diffusiveness of a sound field can be measured by taking reverberation time measurements at a large number of points in the room, then taking the standard deviation on these decay times. Alternately, the spatial distribution of the sound can be examined. Small sound spaces generally have very poor diffusion characteristics at low frequencies due to room modes.

Diffusor
Diffusors (or diffusers) are used to treat sound aberrations, such as echoes, in rooms. They are an excellent alternative or complement to sound absorption because they do not remove sound energy, but can be used to effectively reduce distinct echoes and reflections while still leaving a live sounding space. Compared to a reflective surface, which will cause most of the energy to be reflected off at an angle equal to the angle of incidence, a diffusor will cause the sound energy to be radiated in many directions, hence leading to a more diffusive acoustic space. It is also important that a diffusor spreads reflections in time as well as spatially. Diffusors can aid sound diffusion, but this is not why they are used in many cases; they are more often used to remove coloration and echoes.
Diffusors come in many shapes and materials. The birth of modern diffusors was marked by Manfred R. Schroeders' invention of number-theoretic diffusors in the 1970s.

Maximum length sequence diffusors
Maximum length sequence based diffusors are made of strips of material with two different depths. The placement of these strips follows an MLS. The width of the strips is smaller than or equal to half the wavelength of the frequency where the maximum scattering effect is desired. Ideally, small vertical walls are placed between lower strips, improving the scattering effect in the case of tangential sound incidence. The bandwidth of these devices is rather limited; at one octave above the design frequency, diffusor efficiency drops to that of a flat surface.

Quadratic-residue diffusors
MLS based diffusors are superior to geometrical diffusors in many respects; they have limited bandwidth. The new goal was to find a new surface geometry that would combine the excellent diffusion characteristics of MLS designs with wider bandwidth. A new design was discovered, called a quadratic-residue diffusor. Today the quadratic residue diffusor or Schroeder diffusor is still widely used. Quadratic-Residue Diffusors can be designed to diffuse sound in either one or two directions. They too suffer from ""flat plate"" frequencies, but at a higher frequencies than MLS diffusors. Fractal constructions can be used to extend bandwidth.

Primitive-root diffusors
Primitive-root diffusors are based on a number theoretic sequence based on primitive roots. Although they produce a notch in the scattering response, in reality the notch is over too narrow a bandwidth to be useful. In terms of performance, they are very similar to Quadratic-Residue Diffusors.

Optimized diffusors
By using numerical optimisation, it is possible to increase the number of theoretical designs, especially for diffusors with a small number of wells per period. But the big advantage of optimisation is that arbitrary shapes can be used which can blend better with architectural forms.

Two-dimensional (""hemispherical"") diffusors
Designed, like most diffusors, to create ""a big sound in a small room,"" unlike other diffusors, two-dimensional diffusors scatter sound in a hemispherical pattern. This is done by the creation of a grid, whose cavities have wells of varying depth, according to the matrix addition of two quadratic sequences equal or proportionate to those of a regular diffusor. These diffusors are very helpful for controlling the direction of the diffusion, particularly in studios and control rooms  .

Manufacturers
Yukon Acoustics

See also
Sound baffle

References
Further reading
T. J. Cox and P. D'Antonio, ""Acoustic Absorbers and Diffusors - Theory, Design and Application"" Spon press.",Category:Sound,3
63,64,Stridulation,"Stridulation is the act of producing sound by rubbing together certain body parts. This behavior is mostly associated with insects, but other animals are known to do this as well, such as a number of species of fish, snakes and spiders. The mechanism is typically that of one structure with a well-defined lip, ridge, or nodules (the ""scraper"" or plectrum) being moved across a finely-ridged surface (the ""file"" or stridulitrum—sometimes called the pars stridens) or vice versa, and vibrating as it does so, like the dragging of a phonograph needle across a vinyl record. Sometimes it is the structure bearing the file which resonates to produce the sound, but in other cases it is the structure bearing the scraper, with both variants possible in related groups. Common onomatopoeic words for the sounds produced by stridulation include chirp and chirrup.

Arthropod stridulation
Insects and other arthropods stridulate by rubbing together two parts of the body. These are referred to generically as the stridulatory organs.
The mechanism is best known in crickets, mole crickets, and grasshoppers, but other insects which stridulate include Curculionidae (weevils and bark beetles), Cerambycidae (longhorned beetles), Mutillidae (""velvet ants""), Reduviidae (assassin bugs), Buprestidae (metallic wood-boring beetles), Hydrophilidae (water scavenger beetles), Cicindelinae (tiger beetles), Scarabaeidae (scarab beetles), Glaresidae (""enigmatic scarabs""), larval Lucanidae (stag beetles), Passalidae (Bessbugs), Geotrupidae (earth-boring dung beetles), Alydidae (broad-headed bugs), Miridae (leaf bugs), Corixidae (water boatmen), notably Micronecta scholtzi, various ants (including the Black imported fire ant, Solenopsis richteri), and some species of Agromyzidae (leaf-mining flies). Stridulation is also known in a few tarantulas (Arachnida), some pill millipedes (Diplopoda, Oniscomorpha), and stick insects such as Pterinoxylus spinulosus.
The anatomical parts used to produce sound are quite varied: the most common system is that seen in grasshoppers and many other insects, where a hind leg scraper is rubbed against the adjacent forewing (in beetles and true bugs the forewings are hardened); in crickets and katydids a file on one wing is rubbed by a scraper on the other wing; in longhorned beetles, the back edge of the pronotum scrapes against a file on the mesonotum; in various other beetles, the sound is produced by moving the head—up/down or side-to-side—while in others the abdominal tergites are rubbed against the elytra; in assassin bugs, the tip of the mouthparts scrapes along a ridged groove in the prosternum; in velvet ants the back edge of one abdominal tergite scrapes a file on the dorsal surface of the following tergite.
Most spiders are silent, but some tarantula species are known to stridulate. When disturbed, Theraphosa blondi, the Goliath tarantula, can produce a rather loud hissing noise by rubbing together the bristles on its legs. This is said to be audible to a distance of up to 15 feet (4.5 m). One of the wolf spiders, Schizocosa stridulans Stratton, produces low-frequency sounds by flexing its abdomen (tremulation, rather than stridulation) or high-frequency stridulation by using the cymbia on the ends of its pedipalps.
Stridulation in several of these examples is for attracting a mate, or as a form of territorial behaviour, but can also be a warning signal (acoustic aposematism, as in velvet ants and tarantulas). This kind of communication was first described by Slovenian biologist Ivan Regen (1868–1947).

Vertebrate stridulation
Some species of venomous snakes stridulate as part of a threat display. They arrange their body into a series of parallel C-shaped (counterlooped) coils that they rub together to produce a sizzling sound, rather like water on a hot plate. The best-known examples are members of the genus Echis (saw-scaled vipers), although those of the genus Cerastes (North African desert vipers) and at least one bush viper species, Atheris desaixi, do this as well. A bird species, the club-winged manakin, has a dedicated stridulation apparatus, while a species of mammal, the lowland streaked tenrec, (Hemicentetes semispinosus) produces a high-pitched noise by rubbing together specialised quills on its back.

References
External links
The British Library Sound Archive contains over 150,000 recordings of animal sounds and natural atmospheres from around the world.",Category:Articles containing video clips,3
64,65,Zoom H2n Handy Recorder,"The Zoom H2n is a portable digital sound recording device manufactured by Zoom. It is the successor of the Zoom H2 recorder. The Zoom H2n has 5 mic capsules built inside it. Musical applications for the H2N include the ability to use the device as a stereo or multi-track(4 channel) recorder; the device also includes built-in editor for some minor editing works with in the device.

Operation
The Zoom H2n is capable of more than 20 hours of operation using two standard AA batteries. The device weighs 130g without batteries and has a reference speaker for listening the recordings.

Key Features
It can record in WAV format up to 24-bit/96 kHz and MP3 up to 320kbit/s. It has additional functions like Lo-cut Filter, Compressor/Limiter, Auto Gain, Pre-Rec, Auto-Rec, Tuner, Metronome, Variable Speed Playback built into it.

Recording modes
The device can record in MS(mid-side), XY, MS+XY (2ch) and MS+XY (4ch) modes. If the 2.00 firmware update has been applied, it can also record in the 360-degree ambisonic format, along a horizontal plane. It can record in Directional (XY, MS mid mics) Bidirectional (MS side mic) types.

References
External links
Official website
Zoom Gear & Home Recording Forum",Category:Sound,3
65,66,Wow and flutter measurement,"Measurement of wow and flutter is carried out on audio tape machines, cassette recorders and players, and other analog recording and reproduction devices with rotary components (e.g. movie projectors, turntables (vinyl recording), etc.) This measurement quantifies the amount of 'frequency wobble' (caused by speed fluctuations) present in subjectively valid terms. Turntables tend to suffer mainly slow wow. In digital systems, which are locked to crystal oscillators, variations in clock timing are referred to as wander or jitter, depending on speed.
While the terms wow and flutter used to be used separately (for wobbles at a rate below and above 4 Hz respectively), they tend to be combined now that universal standards exist for measurement which take both into account simultaneously. Listeners find flutter most objectionable when the actual frequency of wobble is 4 Hz, and less audible above and below this rate. This fact forms the basis for the weighting curve shown here. The weighting curve is misleading, inasmuch as it presumes inaudibility of flutters above 200 Hz, when actually faster flutters are quite damaging to the sound. A flutter of 200 Hz at a level of -50db will create 0.3% intermodulation distortion, which would be considered unacceptable in a preamp or amplifier.

Measurement techniques
Measuring instruments use a frequency discriminator to translate the pitch variations of a recorded tone into a flutter waveform, which is then passed through the weighting filter, before being full-wave rectified to produce a slowly varying signal which drives a meter or recording device. The maximum meter indication should be read as the flutter value.
The following standards all specify the weighting filter shown above, together with a special slow-quasi-peak full-wave rectifier designed to register any brief speed excursions. As with many audio standards these are identical derivatives of a common specification.
IEC 386
DIN45507
BS4847
CCIR 409-3
AES6-2008
Measurement is usually made on a 3.15 kHz (or sometimes 3 kHz) tone, a frequency chosen because it is high enough to give good resolution, but low enough not to be affected by drop-outs and high-frequency losses. Ideally, flutter should be measured using a pre-recorded tone free from flutter. Record-replay flutter will then be around twice as high, because worst case variations will add from time to time. When a recording is played back on the same machine it was made on, a very slow change from low to high flutter will often be observed, because any cyclic flutter caused by capstan rotation may go from adding to cancelling as the tape slips slightly out of synchronism. A good technique is to stop the tape from time to time and start it again, as this will often result in different readings as the correlation between record and playback flutter shifts. On high-end or most professional machines in a well maintained state, one may expect that it is impractical to not possible to find a tape made on a better machine. Therefore, a record-playback test using the stop-start technique, is, for practical purposes, the best that can be accomplished.

Audible effects
Wow and flutter are particularly audible on music with oboe, string, guitar, flute, brass, or piano solo playing. While wow is perceived clearly as pitch variation, flutter can alter the sound of the music differently, making it sound ‘cracked’ or ‘ugly’. There is an interesting reason for this. A recorded 1 kHz tone with a small amount of flutter (around 0.1%) can sound fine in a ‘dead’ listening room, but in a reverberant room constant fluctuations will often be clearly heard. These are the result of the current tone ‘beating’ with its echo, which since it originated slightly earlier, has a slightly different pitch. What is heard is quite pronounced amplitude variation, which the ear is very sensitive to. This probably explains why piano notes sound ‘cracked’. Because they start loud and then gradually tail off, piano notes leave an echo that can be as loud as the dying note that it beats with, resulting in a level that varies from complete cancellation to double-amplitude at a rate of a few Hz: instead of a smoothly dying note we hear a heavily modulated one. Oboe notes may be particularly affected because of their harmonic structure. Another way that flutter manifests is as a truncation of reverb tails. This may be due to the persistence of memory with regard to spatial location based on early reflections and comparison of Doppler effects over time. The auditory system may become distracted by pitch shifts in the reverberation of a signal that should be of fixed and solid pitch.

Equipment performance
Professional tape machines can achieve a weighted flutter figure of around 0.02%, which is considered inaudible.
High end cassette decks struggle to manage around 0.08% weighted, which is still audible under some conditions.
Digital music players such as CD, DAT, or MP3 use electronic clocks to deliver samples, which suffer from an analogous effect called jitter.
The linear sound track on VCR video recorders has much higher wow and flutter than the VHS-HiFi high fidelity track which is contained within the video signal.
The term ""flutter echo"" is used in relation to a particular form of reverberation that flutters in amplitude. It has no direct connection with flutter as described here, though the mechanism of modulation through cancellation may have something in common with that described above.

Absolute speed
Absolute speed error causes a change in pitch, and it is useful to know that a semitone in music represents a 6% frequency change. This is because Western music uses the ‘equal temperament scale' based on a constant geometric ratio between twelve notes; and the twelfth root of 2 is 1.05946. Anyone with a good musical ear can detect a pitch change of around 1%, though an error of up to 3% is likely to go unnoticed, except by those few with ‘absolute pitch’. Most ‘movie’ films shown on European television are sped up by 4.166% because they were shot at 24 frames per second, but are scanned at 25 frames per second to match the PAL standard of 25 frame/s 50 field/s. This causes a noticeable increase in pitch on voices, which often brings surprised comment from the actors themselves when they hear their performance on video. It can also frustrate attempts to play along with film music, which is closer to a semitone sharp than its intended pitch. Recently, digital pitch correction has been applied to some films, which corrects the pitch without altering lip-sync, by adding in extra cycles of sound. This has to be regarded as a form of distortion, as there is no way to change the pitch of a sound without also slowing it down that does not change the waveform itself.

Flutter correction
Novel DSP processes have been developed that correct wow and flutter by tracking various spuriae on the tape or film which can be re-purposed as timing references. Several recent (2006) DVD releases have utilized a system developed by Plangent Processes that substantially reduces wow and flutter of very high rates to extremely low levels, with a substantial improvement in quality, and without adding distortion or extra cycles of sound. The software Capstan by Celemony analyzes the already digitalized musical material and uses varispeed playback to eliminate wow and flutter.

Scrape flutter
High-frequency flutter, above 100 Hz, can sometimes result from tape vibrating as it passes over a head (or other non-rotating element in the tape path), as a result of rapidly interacting stretching in the tape and stick-slip at the head. This is termed 'scrape flutter'. It adds a roughness to the sound that is not typical of wow & flutter, and damping devices or heavy rollers are sometimes employed on professional tape machines to reduce or prevent it. Scrape flutter measurement requires special techniques, often using a 10 kHz tone.

See also
Audio quality measurement
Noise measurement
Headroom
Rumble measurement
ITU-R 468 noise weighting
A-weighting
Weighting filter
Equal-loudness contour
Fletcher-Munson curves
Flutter (electronics and communication)
Wow (recording)

References
External links
http://www.manquen.net/audio/",Category:Audio engineering,3
66,67,Precedence effect,"The precedence effect or law of the first wavefront is a binaural psychoacoustic effect. When a sound is followed by another sound separated by a sufficiently short time delay (below the listener's echo threshold), listeners perceive a single fused auditory image; its perceived spatial location is dominated by the location of the first-arriving sound (the first wave front). The lagging sound also affects the perceived location. However, its effect is suppressed by the first-arriving sound.
The Haas effect is a psychoacoustic effect, described in 1949 by Helmut Haas in his Ph.D. thesis. It is often equated with the underlying precedence effect.

History
The ""law of the first wavefront"" was described and named in 1948 by Lothar Cremer.
The ""precedence effect"" was described and named in 1949 by Wallach et al. They showed that when two identical sounds are presented in close succession they will be heard as a single fused sound. In their experiments, fusion occurred when the lag between the two sounds was in the range 1 to 5 ms for clicks, and up to 40 ms for more complex sounds such as speech or piano music. When the lag was longer, the second sound was heard as an echo.
Additionally, Wallach et al. demonstrated that when successive sounds coming from sources at different locations were heard as fused, the apparent location of the perceived sound was dominated by the location of the sound that reached the ears first (i.e. the first-arriving wavefront). The second-arriving sound had only a very small (albeit measurable) effect on the perceived location of the fused sound. They designated this phenomenon as the precedence effect, and noted that it explains why sound localization is possible in the typical situation where sounds reverberate from walls, furniture and the like, thus providing multiple, successive stimuli. They also noted that the precedence effect is an important factor in the perception of stereophonic sound.
Wallach et al. did not systematically vary the intensities of the two sounds, although they cited research by Langmuir et al. which suggested that if the second-arriving sound is at least 15 dB louder than the first, the precedence effect breaks down.
The ""Haas effect"" derives from a 1951 paper by Helmut Haas. In 1951 Haas examined how the perception of speech is affected in the presence of a single, coherent sound reflection. To create anechoic conditions, the experiment was carried out on the rooftop of a freestanding building. Another test was carried out in a room with a reverberation time of 1.6 ms. The test signal (recorded speech) was emitted from two similar loudspeakers at locations 45° to the left and to the right in 3 m distance to the listener.
Haas found that humans localize sound sources in the direction of the first arriving sound despite the presence of a single reflection from a different direction. A single auditory event is perceived. A reflection arriving later than 1 ms after the direct sound increases the perceived level and spaciousness (more precisely the perceived width of the sound source). A single reflection arriving within 5 to 30 ms can be up to 10 dB louder than the direct sound without being perceived as a secondary auditory event (echo). This time span varies with the reflection level. If the direct sound is coming from the same direction the listener is facing, the reflection's direction has no significant effect on the results. A reflection with attenuated higher frequencies expands the time span that echo suppression is active. Increased room reverberation time also expands the time span of echo suppression.

Appearance
The precedence effect appears if the subsequent wave fronts arrive between 2 ms and about 50 ms later than the first wave front. This range is signal dependent. For speech the precedence effect disappears for delays above 50 ms, but for music the precedence effect can also appear for delays of some 100 ms.
In two-click lead–lag experiments, localization effects include aspects of summing localization, localization dominance, and lag discrimination suppression. The last two are generally considered to be aspects of the precedence effect:
Summing localization: for time delays below 2 ms, listeners only perceive one sound; its direction is between the locations of the lead and lag sounds. An application for summing localization is the intensity stereophony, where two loudspeakers emit the same signal with different levels, resulting in the localized sound direction between both loudspeakers. The localized direction depends on the level difference between the loudspeakers.
Localization dominance: for delays between 2 and 5 ms, listeners also perceive one sound; its location is determined by the location of the leading sound.
Lag discrimination suppression: for short time delays, listeners are less capable of discriminating the location of the lagging sound.
For time delays above 50 ms (for speech) or some 100 ms (for music) the delayed sound is perceived as an echo of the first-arriving sound. Both sound directions are localized correctly. The time delay for perceiving echoes depends on the signal characteristics. For signals with impulse characteristics, echoes are perceived for delays above 50 ms. For signals with a nearly constant amplitude, the echo threshold can be enhanced up to time differences of 1 to 2 seconds.
A special appearance of the precedence effect is the Haas effect. Haas showed that the precedence effect appears even if the level of the delayed sound is up to 10 dB higher than the level of the first wave front. In this case, the range of delays, where the precedence effect works, is reduced to delays between 10 and 30 ms.

Applications
The precedence effect is important for the hearing in enclosed rooms. With the help of this effect, it remains possible to determine the direction of a sound source (e.g. the direction of a speaker) even in the presence of wall reflections.

Sound reinforcement systems
Haas' findings can be applied to sound reinforcement systems and public address systems. The signal for loudspeakers placed at distant locations from a stage may be delayed electronically by an amount equal to the time sound takes to travel through the air from the stage to the distant location, plus about 10 to 20 milliseconds and played at a level up to 10 dB louder than sound emanating from the stage. The first arrival of sound from the source on stage determines perceived localization whereas the slightly later sound from delayed loudspeakers simply increases the perceived sound level without negatively affecting localization. In this configuration, the listener will localize all sound from the direction of the direct sound, but he will benefit from the higher sound level, which has been enhanced by the loudspeakers.

Ambience extraction
The precedence effect can be employed to increase the perception of ambience during the playback of stereo recordings. If two speakers are placed to the left and right of the listener (in addition to the main speakers), and fed with the program material delayed by 10 to 20 milliseconds, the random-phase ambience components of the sound will become sufficiently decorrelated that they cannot be localized. This effectively extracts the recording's existing ambience, while leaving its foreground ""direct"" sounds still appearing to come from the front.

Multichannel audio decoding
The effect was taken into account and exploited in the psychoacoustics of the Fosgate Tate 101A SQ decoder, developed by Jim Fosgate in consultation with Peter Scheiber and Martin Willcocks, to produce much better spatiality and directionality in matrix decoding of 4-2-4 (SQ quadraphonic) audio.

Haas kicker
Many older LEDE (""live end, dead end"") control room designs featured so-called ""Haas kickers"" – reflective panels placed at the rear to create specular reflections which were thought to provide a wider stereo listening area or raise intelligibility. However, what is beneficial for one type of sound is detrimental to others, so Haas kickers, like compression ceilings, are no longer commonly found in control rooms.

See also
Binaural fusion
Franssen effect

References
Further reading
Floyd Toole ""Sound Reproduction"", Focal Press (July 25, 2008), Chapter 6
Blauert ""Spatial Hearing - Revised Edition: The Psychophysics of Human Sound Localization"", The MIT Press; Rev Sub edition (October 2, 1996)
Litovsky et al. (1999), ""The precedence effect"" J. Acoustic. Soc. Am., Vol. 106, No. 4",Category:Audio engineering,3
67,68,Sonology,,Category:Sound,3
68,69,Category:Music,,Category:Sound,3
69,70,World Soundscape Project,"The World Soundscape Project (WSP) is an international research project founded by Canadian composer R. Murray Schafer in the late 1960s at Simon Fraser University. The project initiated the modern study of acoustic ecology. Its ultimate goal is ""to find solutions for an ecologically balanced soundscape where the relationship between the human community and its sonic environment is in harmony."" The practical manifestations of this goal include education about the soundscape and noise pollution, in addition to the recording and cataloguing of international soundscapes with a focus on preservation of soundmarks and dying sounds and sound environments. Publications which emerged from the project include The Book of Noise (1968) and The Tuning of the World (1977), both by Schafer, as well as the Handbook for Acoustic Ecology (1978) by Barry Truax. The project has thus far resulted in two major tours, in Canada and Europe, the results of which comprise the World Soundscape Library. Notable members included Howard Broomfield, Bruce Davis, Peter Huse, Barry Truax and Hildegard Westerkamp.

Early History - The Vancouver Soundscape
The project emerged from Schafer's reaction to the increasing degradation of Vancouver's developing soundscape, as well as work as a professor at Simon Fraser University where he taught a course in noise pollution in attempt to draw attention to the sonic environment. The project quickly attracted a small group of young composers and communications students, and after receiving support from the Donner Canadian Foundation it began its first project in 1972: a detailed study of the soundscape of Vancouver. The study resulted in a recording, The Vancouver Soundscape which was published in 1973. It consisted of phonographical recordings of local soundscapes and soundmarks as well as a spoken documentary recording by R. Murray Schafer discussing notions of acoustic design, providing analysis of examples of good and bad acoustic design in Vancouver, illustrated with further recordings. This recording is notable for its phonographic approach and is distinct from later WSP recordings in that it features no on-site narration. However, some of the recordings do bear the beginnings of more compositional approaches to soundscape recording, evidenced particularly in the track, 'Entrance to the Harbour' in which a 30-minute boat journey is condensed into a 7-minute montage in an effort to highlight the important sound events. The recordists of this publication were Howard Broomfield, Bruce Davis, Peter Huse and Colin Miles.

Soundscapes of Canada
Following the success of The Vancouver Soundscape, two members, Peter Huse and Bruce Davis, embarked on a tour across Canada in 1973, in an effort to document the changing soundscape and preserve dying sounds becoming obsolete due to new technology. The types of sounds recorded on this project included natural ambiences, signifiers such as bells, chimes and foghorns, as well as mechanical and industrial sounds. The recordings are typically long uninterrupted takes and do not attempt to mask the presence of the recordists. Much of the recordings consist of Huse and Davis asking for directions or interacting with the people they encounter in the sound environments they documented. This approach of transparency marks a shift in aesthetic from the previous venture. The recordings were mixed into a documentary radio series on CBC Ideas called Soundscapes of Canada. The series consisted of ten one-hour programmes which were broadcast in 1974.

Five Village Soundscapes
In 1975, a larger group of members, including Schafer, embarked on a tour of Europe. The tour included workshops and lectures in several major cities, spreading the educational and theoretical aspects of the Project, as well as detailed analysis and recording projects of five European Villages, one of each in Sweden (Skruv), Germany (Bissingen), Italy (Cembra), France (Lesconil) and Scotland (Dollar). The recording projects were similar in style and purpose to the Canadian tour, but with special focus on rural areas and soundscapes which were still well-preserved. The tour also resulted in two publications, a narrative account of the trip called European Sound Diary and a detailed soundscape analysis called Five Village Soundscapes.
In 2009, a group of Finnish researchers led by Helmi Järviluoma, Heikki Uimonen, and Noora Vikman in collaboration with Barry Truax revisited the same five villages to analyze how their soundscapes had changed over 30 years due to urbanization.

Soundscape Vancouver 1996
The project slowed down during the 1980s and direct activity by the original members under the WSP banner dwindled. However, the aftereffects left by their work in the 1970s helped Acoustic Ecology emerge internationally as a field of study and the methodologies of the World Soundscape Project served as the basis or inspiration for subsequent international endeavours.
In the 1990s, the project reemerged with the digitization of much of the recordings library onto the DAT format, and new digital recordings made in Vancouver. These recordings formed the basis of the Soundscape Vancouver '96 project which culminated in a publication intended to revisit the Vancouver Soundscape project of 1973. Instead of phonographic recordings this release was compositionally oriented, with the recordings used as source material for soundscape compositions by a host of prominent international electroacoustic composers. Featured composers included Darren Copeland, Sabine Breitsameter, Hans Ulrich Werner, Barry Truax and Claude Schryer. It also featured one track of phonographic ambience and a narrated documentary by Barry Truax and Hildegard Westerkamp similar to the final track on the 1973 publication by R. Murray Schafer. It was released as a double-CD with the original 1973 project.

Present day
The entire library has been digitized and a new project has begun to bring its contents online for student access. The online catalogue has been updated to include photographs from many of the recording sites. While the project has not embarked on any recent studies or tours, its impact on the international acoustic ecology community has been dramatic and there are many groups in different countries now devoted to similar projects inspired by the efforts of the World Soundscape Project. In 1993, the members of the international acoustic ecology community, including important contributors to the World Soundscape Project, formed the World Forum of Acoustic Ecology.

See also
Sound culture

References
External links
World Soundscape Project
Sonic Research Studio SFU - includes catalogue of WSP library.",Category:Sound,3
70,71,Freesound,"Freesound is a collaborative repository of Creative Commons licensed audio samples with more than 230,000 sounds and 4 million registered users (as of February 2015). Sounds are uploaded to the website by its users, and cover a wide range of subjects, from field recordings to synthesized sound effects. Audio content in the repository can be tagged and browsed by folksonomic means as well as standard text-based search. Audio content in the repository is also analysed using the open-source audio analysis tool Essentia, which powers the similarity search functionality of the site. Freesound has a RESTful API through which third-party applications can access and retrieve audio content and its metadata.

Licensing
Freesound originally used the Creative Commons Sampling Plus license for all samples, but has since switched to using CC0, CC BY, and CC BY-NC. Older samples remain under Sampling Plus unless their uploaders have relicensed them.
All of these licenses allow use and distribution of the samples (modified or unmodified) for non-commercial purposes, and in the case of CC0 and CC BY also for commercial purposes. The Sampling Plus license does not allow unmodified samples to be distributed commercially, and therefore, like CC BY-NC, fails the Definition of Free Cultural Works and the Debian Free Software Guidelines, and is not free enough to be used on Wikipedia, free software programs, and other projects requiring free content.

History
The Freesound Project was officially launched on April 5, 2005 in the context of the 2005 International Computer Music Conference. It is a project of the Music Technology Group of Universitat Pompeu Fabra, Barcelona. Bram de Jong and the other members of the Freesound Team are responsible for developing and administrating the Freesound website.
Children of Men was the first major motion picture known to legally use a sample from Freesound in its production. The sound used was ""male_Thijs_loud_scream.aiff"" posted by the user thanvannispen, and the film properly attributes the sample in the credits.

Features
Metadata tags
GeoTagging
Sample packs
Remix tree
Similarity search
Sound Bookmarks
Forum
Follow users and tags
Waveform display
HTML5/Flash preview
RESTful API
Creative Commons licensing
The features of Freesound are designed to make the sound files on the website easy to index, search, and browse. Since the licenses used (except CC0) stipulate that the authors of the original work must be credited in the derivative work, the site is capable of automatically generating an attribution list to make this easier.

Software architecture
The Freesound platform is based on technology developed by the Music Technology Group of the Pompeu Fabra University, Barcelona.
Similarity searches based on automatically extracted audio features using Essentia audio analysis tool.
Spectral centroid waveform display
The following web technologies are also used:
Python scripting
PostgreSQL database
Apache Solr text search

References
External links
Official website
Creative Commons interview with Bram de Jong
Using Freesound with ccMixter",Category:Sound,3
71,72,Category:Silence,,Category:Sound,3
72,73,Delayed Auditory Feedback,"Delayed Auditory Feedback (DAF), also called delayed sidetone, is a type of altered auditory feedback that consists of extending the time between speech and auditory perception. It can consist of a device that enables a user to speak into a microphone and then hear his or her voice in headphones a fraction of a second later. Some DAF devices are hardware; DAF computer software is also available. Most delays that produce a noticeable effect are between 50-200ms. DAF usage (with a 175 millisecond delay) has been shown to induce mental stress.
It is a type of altered auditory feedback that—along with frequency-altered feedback and white noise masking—is used to treat stuttering; it has also demonstrated interesting discoveries about the auditory feedback system when used with non-stuttering individuals. It is most effective when used in both ears. Delayed auditory feedback devices are used in speech perception experiments in order to demonstrate the importance of auditory feedback in speech perception as well as in speech production.
Delayed auditory feedback has been used with a directional microphone and speaker to create a device intended to silence an individual speaker using the mental stress induced in people not used to the effect.
There are now also different mobile apps available that use DAF in phone calls.

Effects in people who stutter
Electronic fluency devices use delayed auditory feedback and have been used as a technique to aid with stuttering. Early investigators suggested and have continually been proven correct in assuming that those who stutter had an abnormal speech–auditory feedback loop that was corrected or bypassed while speaking under DAF. In stutterers with atypical auditory anatomy, DAF improves fluency, but not in those with typical anatomy. DAF is also used with clutterers. Its effects are slowing of speech which can result in increased fluency for clutterers and also syllable awareness.

Effects in normal speakers
Studies that are more recent have looked at the effects of DAF in non-stutterers to see what it can prove about the structure of the auditory and verbal pathways in the brain.
Indirect effects of delayed auditory feedback in non-stutterers include reduction in rate of speech, increase in intensity, and increase in fundamental frequency in order to overcome the effects of the feedback. Direct effects include repetition of syllables, mispronunciations, omissions, and omitted word endings. These direct effects are often referred to as “artificial stuttering” 
In a normal individual, auditory feedback speech sounds are directed to the inner ear with a .001 second delay. In delayed auditory feedback, the delay is artificially disrupted.
Studies have found that In children age 4-6 there is less disturbance of speech than children age 7-9 under a delay of 200ms. Younger children are maximally disrupted around 500ms while older children around 400ms. A 200ms delay produces maximum disruption for adults. As the data collected from these studies indicates, the delay required for maximum disruption decreases with age. However, it increases again for older adults, to 400ms.
Sex differences in DAF show no difference or indicate that men are generally more affected than women indicating that the feedback subsystems in the vocal monitor process could be different between the sexes.
In general, more rapid, fluent speakers are less affected by DAF than slower, less fluent speakers. Also, more rapid fluent speakers are maximally disrupted by a shorter delay time, while slower speakers are maximally disrupted under longer delay times.
Studies using computational modeling and functional magnetic resonance imaging (fMRI) have shown that the temporo-parietal regions function as a conscious self-monitoring system to support an automatic speech production system and that projections from auditory error cells in the posterior superior temporal cortex that go to motor correction cells in right frontal cortex mediate auditory feedback control of speech.

Effects in non-humans
Continuous delayed auditory feedback in Zebra finch songbirds caused them to change their song syllable timing, indicating that DAF can change the motor program of syllable timing generation during short periods of time in zebra finches, similar to the effects observed in humans.


== References ==",Category:Sound,3
73,74,Auditory scene analysis,"In psychophysics, auditory scene analysis (ASA) is a proposed model for the basis of auditory perception. This is understood as the process by which the human auditory system organizes sound into perceptually meaningful elements. The term was coined by psychologist Albert Bregman. The related concept in machine perception is computational auditory scene analysis (CASA), which is closely related to source separation and blind signal separation.
The three key aspects of Bregman's ASA model are: segmentation, integration, and segregation.

Background
Sound reaches the ear and the eardrum vibrates as a whole. This signal has to be analyzed (in some way). The model proposes that sounds will either be heard as ""integrated"" (heard as a whole – much like harmony in music), or ""segregated"" into individual components (which leads to counterpoint). For example, a bell can be heard as a 'single' sound (integrated), or some people are able to hear the individual components – they are able to segregate the sound. This can be done with chords where it can be heard as a 'color', or as the individual notes.
In many circumstances the segregated elements can be linked together in time, producing an auditory stream. This ability of auditory streaming can be demonstrated by the so-called cocktail party effect. Up to a point, with a number of voices speaking at the same time or with background sounds, one is able to follow a particular voice even though other voices and background sounds are present. In this example, the ear is segregating this voice from other sounds (which are integrated), and the mind ""streams"" these segregated sounds into an auditory stream. This is a skill which is highly developed by musicians, notably conductors who are able to listen to one, two, three or more instruments at the same time (segregating them), and following each as an independent line through auditory streaming. Organists also develop this skill having to stream up to five or more voices [parts] at a time.
Natural sounds, such as the human voice, musical instruments, or cars passing in the street, are made up of many frequencies, which contribute to the perceived quality (or timbre) of the sounds. When two or more natural sounds occur at once, all the components of the simultaneously active sounds are received at the same time, or overlapped in time, by the ears of listeners. This presents their auditory systems with a problem: which parts of the sound should be grouped together and treated as parts of the same source or object? Grouping them incorrectly can cause the listener to hear non-existent sounds built from the wrong combinations of the original components.

Grouping and streams
A number of grouping principles appear to underlie ASA, many of which are related to principles of perceptual organization discovered by the school of Gestalt psychology. These can be broadly categorized into sequential grouping cues (those that operate across time – segregated) and simultaneous grouping cues (those that operate across frequency – integrated). In addition, schemas (learned patterns) play an important role.
Errors in simultaneous grouping can lead to the blending of sounds that should be heard as separate, the blended sounds having different perceived qualities (such as pitch or timbre) to any of the sounds actually received.
Errors in sequential grouping can lead, for example, to hearing a word created out of syllables originating from two different voices. The job of ASA is to group incoming sensory information to form an accurate mental representation of the individual sounds.
When sounds are grouped by the auditory system into a perceived sequence, distinct from other co-occurring sequences, each of these perceived sequences is called an ""auditory stream"". Normally, a stream corresponds to a distinct environmental sound pattern that persists over time, such as a person talking, a piano playing, or a dog barking, but perceptual errors and illusions are possible under unusual circumstances. One example of this is the laboratory phenomenon of streaming, also called ""stream segregation"". If two sounds, A and B, are rapidly alternated in time, after a few seconds the perception may seem to ""split"" so that the listener hears two rather than one stream of sound, each stream corresponding to the repetitions of one of the two sounds, for example, A-A-A-A-, etc. accompanied by B-B-B-B-, etc. The tendency towards segregation into separate streams is favored by differences in the acoustical properties of sounds A and B. Among the differences that favor segregation are those of frequency (for pure tones), fundamental frequency (for rich tones), frequency composition, spatial position, and speed of the sequence (faster sequences segregate more readily). An interactive web page illustrating this streaming and the importance of frequency separation and speed can be found here.

Experimental basis
Many experiments have studied the segregation of more complex patterns of sound, such as a sequence of high notes of different pitches, interleaved with low ones. In such sequences, the segregation of co-occurring sounds into distinct streams has a profound effect on the way they are heard. Perception of a melody is formed more easily if all its notes fall in the same auditory stream. We tend to hear the rhythms among notes that are in the same stream, excluding those that are in other streams. Judgments of timing are more precise between notes in the same stream than between notes in separate streams. Even perceived spatial location and perceived loudness can be affected by sequential grouping.
While the initial research on this topic was done on human adults, recent studies have shown that some ASA capabilities are present in newborn infants, showing that they are built-in, rather than learned through experience. Other research has shown that non-human animals also display ASA. Currently, scientists are studying the activity of neurons in the auditory regions of the cerebral cortex to discover the mechanisms underlying ASA.

See also
Illusory discontinuity
Theory of indispensable attributes


== References ==",Category:Articles needing expert attention with no reason or talk parameter,3
74,75,Supersonic speed,"Supersonic travel is a rate of travel of an object that exceeds the speed of sound (Mach 1). For objects traveling in dry air of a temperature of 20 °C (68 °F) at sea level, this speed is approximately 343 m/s, 1,125 ft/s, 768 mph, 667 knots, or 1,235 km/h. Speeds greater than five times the speed of sound (Mach 5) are often referred to as hypersonic. Flights during which only some parts of the air surrounding an object, such as the ends of rotor blades, reach supersonic speeds are called transonic. This occurs typically somewhere between Mach 0.8 and Mach 1.23.
Sounds are traveling vibrations in the form of pressure waves in an elastic medium. In gases, sound travels longitudinally at different speeds, mostly depending on the molecular mass and temperature of the gas, and pressure has little effect. Since air temperature and composition varies significantly with altitude, Mach numbers for aircraft may change despite a constant travel speed. In water at room temperature supersonic speed can be considered as any speed greater than 1,440 m/s (4,724 ft/s). In solids, sound waves can be polarized longitudinally or transversely and have even higher velocities.
Supersonic fracture is crack motion faster than the speed of sound in a brittle material.

Early meaning
At the beginning of the 20th century, the term ""supersonic"" was used as an adjective to describe sound whose frequency is above the range of normal human hearing. The modern term for this meaning is ""ultrasonic"".

Supersonic objects
The tip of a bullwhip is thought to be the first man-made object to break the sound barrier, resulting in the telltale ""crack"" (actually a small sonic boom). The wave motion traveling through the bullwhip is what makes it capable of achieving supersonic speeds.
Most modern fighter aircraft are supersonic aircraft, but there have been supersonic passenger aircraft, namely Concorde and the Tupolev Tu-144. Both these passenger aircraft and some modern fighters are also capable of supercruise, a condition of sustained supersonic flight without the use of an afterburner. Due to its ability to supercruise for several hours and the relatively high frequency of flight over several decades, Concorde spent more time flying supersonically than all other aircraft combined by a considerable margin. Since Concorde's final retirement flight on November 26, 2003, there are no supersonic passenger aircraft left in service. Some large bombers, such as the Tupolev Tu-160 and Rockwell B-1 Lancer are also supersonic-capable.
Most modern firearm bullets are supersonic, with rifle projectiles often travelling at speeds approaching and in some cases well exceeding Mach 3.
Most spacecraft, most notably the Space Shuttle are supersonic at least during portions of their reentry, though the effects on the spacecraft are reduced by low air densities. During ascent, launch vehicles generally avoid going supersonic below 30 km (~98,400 feet) to reduce air drag.
Note that the speed of sound decreases somewhat with altitude, due to lower temperatures found there (typically up to 25 km). At even higher altitudes the temperature starts increasing, with the corresponding increase in the speed of sound.
When an inflated balloon is burst, the torn pieces of latex contract at supersonic speed, which contributes to the sharp and loud popping noise.

Supersonic flight
Supersonic aerodynamics is simpler than subsonic aerodynamics because the airsheets at different points along the plane often can't affect each other. Supersonic jets and rocket vehicles require several times greater thrust to push through the extra aerodynamic drag experienced within the transonic region (around Mach 0.85–1.2). At these speeds aerospace engineers can gently guide air around the fuselage of the aircraft without producing new shock waves, but any change in cross area farther down the vehicle leads to shock waves along the body. Designers use the Supersonic area rule and the Whitcomb area rule to minimize sudden changes in size.

However, in practical applications, a supersonic aircraft must operate stably in both subsonic and supersonic profiles, hence aerodynamic design is more complex.
One problem with sustained supersonic flight is the generation of heat in flight. At high speeds aerodynamic heating can occur, so an aircraft must be designed to operate and function under very high temperatures. Duralumin, the traditional aircraft material, starts to lose strength and go into plastic deformation at relatively low temperatures, and is unsuitable for continuous use at speeds above Mach 2.2 to 2.4. Materials such as titanium and stainless steel allow operations at much higher temperatures. For example, the Lockheed SR-71 Blackbird jet could fly continuously at Mach 3.1 while some parts were above 315 °C (600 °F).
Another area of concern for sustained high-speed flight is engine operation. Jet engines create thrust by increasing the temperature of the air they ingest, and as the aircraft speeds up, friction and compression heat this air before it reaches the engines. The maximum allowable temperature of the exhaust is determined by the materials in the turbine at the rear of the engine, so as the aircraft speeds up, the difference in intake and exhaust temperature that the engine can create decreases, and the thrust along with it. Air cooling the turbine area to allow operations at higher temperatures was a key solution, one that continued to improve through the 1950s and on to this day.
Intake design was also a major issue. Normal jet engines can only ingest subsonic air, so for supersonic operation the air must be slowed down. Ramps or cones in the intake are used to create shock waves that slow the airflow before it reaches the engine. Doing so removes energy from the airflow, causing drag. The key to reducing this drag is to use multiple small oblique shock waves, but this was difficult because the angle they make inside the intake changes with Mach number. In order to efficiently operate across a range of speeds, the shock waves have to be ""tuned.""
An aircraft able to operate for extended periods at supersonic speeds has a potential range advantage over a similar design operating subsonically. Most of the drag an aircraft sees while speeding up to supersonic speeds occurs just below the speed of sound, due to an aerodynamic effect known as wave drag. An aircraft that can accelerate past this speed sees a significant drag decrease, and can fly supersonically with improved fuel economy. However, due to the way lift is generated supersonically, the lift-to-drag ratio of the aircraft as a whole drops, leading to lower range, offsetting or overturning this advantage.
The key to having low supersonic drag is to properly shape the overall aircraft to be long and thin, and close to a ""perfect"" shape, the von Karman ogive or Sears-Haack body. This has led to almost every supersonic cruising aircraft looking very similar to every other, with a very long and slender fuselage and large delta wings, cf. SR-71, Concorde, etc. Although not ideal for passenger aircraft, this shaping is quite adaptable for bomber use.

History of supersonic flight
Aviation research during World War II led to the creation of the first rocket- and jet-powered aircraft. Several claims of breaking the sound barrier during the war subsequently emerged. However, the first recognized flight exceeding the speed of sound by a manned aircraft in controlled level flight was performed on October 14, 1947 by the experimental Bell X-1 research rocket plane piloted by Charles ""Chuck"" Yeager. The first production plane to break the sound barrier was an F-86 Canadair Sabre with the first 'supersonic' woman pilot, Jacqueline Cochran, at the controls. According to David Masters, the DFS 346 prototype captured in Germany by the Soviets, after being released from a B-29 at 32800 ft (10000 m), reached 683 mph (1100 km/h) late in 1945, which would have exceeded Mach 1 at that height. The pilot in these flights was the German Wolfgang Ziese.
On August 21, 1961, a Douglas DC-8-43 (registration N9604Z) exceeded Mach one in a controlled dive during a test flight at Edwards Air Force Base. The crew were William Magruder (pilot), Paul Patten (copilot), Joseph Tomich (flight engineer), and Richard H. Edwards (flight test engineer). This is the first and only supersonic flight by a civilian airliner, other than Concorde or the Tu-144.

See also
Area rule
Hypersonic speed
Transonic speed
Sonic boom
Supersonic aircraft
Supersonic airfoils
Vapor cone
Prandtl–Glauert singularity

References
External links
""Can We Ever Fly Faster Speed of Sound"", October 1944, Popular Science one of the earliest articles on shock waves and flying the speed of sound
""Britain Goes Supersonic"", January 1946, Popular Science 1946 article trying to explain supersonic flight to the general public
MathPages - The Speed of Sound
Supersonic sound pressure levels",Category:Aerospace engineering,3
75,76,Amplitude,"The amplitude of a periodic variable is a measure of its change over a single period (such as time or spatial period). There are various definitions of amplitude (see below), which are all functions of the magnitude of the difference between the variable's extreme values. In older texts the phase is sometimes called the amplitude.

Definitions
Peak-to-peak amplitude
Peak-to-peak amplitude is the change between peak (highest amplitude value) and trough (lowest amplitude value, which can be negative). With appropriate circuitry, peak-to-peak amplitudes of electric oscillations can be measured by meters or by viewing the waveform on an oscilloscope. Peak-to-peak is a straightforward measurement on an oscilloscope, the peaks of the waveform being easily identified and measured against the graticule. This remains a common way of specifying amplitude, but sometimes other measures of amplitude are more appropriate.

Peak amplitude
In audio system measurements, telecommunications and other areas where the measurand is a signal that swings above and below a reference value but is not sinusoidal, peak amplitude is often used. If the reference is zero, this is the maximum absolute value of the signal; if the reference is a mean value (DC component), the peak amplitude is the maximum absolute value of the difference from that reference.

Semi-amplitude
Semi-amplitude means half the peak-to-peak amplitude. Some scientists use amplitude or peak amplitude to mean semi-amplitude.
It is the most widely used measure of orbital wobble in astronomy and the measurement of small radial velocity semi-amplitudes of nearby stars is important in the search for exoplanets (see Doppler spectroscopy).

Root mean square amplitude
Root mean square (RMS) amplitude is used especially in electrical engineering: the RMS is defined as the square root of the mean over time of the square of the vertical distance of the graph from the rest state; i.e. the RMS of the AC waveform (with no DC component).
For complicated waveforms, especially non-repeating signals like noise, the RMS amplitude is usually used because it is both unambiguous and has physical significance. For example, the average power transmitted by an acoustic or electromagnetic wave or by an electrical signal is proportional to the square of the RMS amplitude (and not, in general, to the square of the peak amplitude).
For alternating current electric power, the universal practice is to specify RMS values of a sinusoidal waveform. One property of root mean square voltages and currents is that they produce the same heating effect as direct current in a given resistance.
The peak-to-peak value is used, for example, when choosing rectifiers for power supplies, or when estimating the maximum voltage that insulation must withstand. Some common voltmeters are calibrated for RMS amplitude, but respond to the average value of a rectified waveform. Many digital voltmeters and all moving coil meters are in this category. The RMS calibration is only correct for a sine wave input since the ratio between peak, average and RMS values is dependent on waveform. If the wave shape being measured is greatly different from a sine wave, the relationship between RMS and average value changes. True RMS-responding meters were used in radio frequency measurements, where instruments measured the heating effect in a resistor to measure current. The advent of microprocessor controlled meters capable of calculating RMS by sampling the waveform has made true RMS measurement commonplace.

Ambiguity
In general, the use of peak amplitude is simple and unambiguous only for symmetric periodic waves, like a sine wave, a square wave, or a triangular wave. For an asymmetric wave (periodic pulses in one direction, for example), the peak amplitude becomes ambiguous. This is because the value is different depending on whether the maximum positive signal is measured relative to the mean, the maximum negative signal is measured relative to the mean, or the maximum positive signal is measured relative to the maximum negative signal (the peak-to-peak amplitude) and then divided by two. In electrical engineering, the usual solution to this ambiguity is to measure the amplitude from a defined reference potential (such as ground or 0 V). Strictly speaking, this is no longer amplitude since there is the possibility that a constant (DC component) is included in the measurement.

Pulse amplitude
In telecommunication, pulse amplitude is the magnitude of a pulse parameter, such as the voltage level, current level, field intensity, or power level.
Pulse amplitude is measured with respect to a specified reference and therefore should be modified by qualifiers, such as average, instantaneous, peak, or root-mean-square.
Pulse amplitude also applies to the amplitude of frequency- and phase-modulated waveform envelopes.

Formal representation
In this simple wave equation

  
    
      
        x
        =
        A
        sin
        ?
        (
        ?
        [
        t
        ?
        K
        ]
        )
        +
        b
         
        ,
      
    
    {\displaystyle x=A\sin(\omega [t-K])+b\ ,}
  

  
    
      
        A
      
    
    {\displaystyle A}
   is the amplitude (or peak amplitude),

  
    
      
        x
      
    
    {\displaystyle x}
   is the oscillating variable,

  
    
      
        ?
      
    
    {\displaystyle \omega }
   is angular frequency,

  
    
      
        t
      
    
    {\displaystyle t}
   is time,

  
    
      
        K
      
    
    {\displaystyle K}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   are arbitrary constants representing time and displacement offsets respectively.

Units
The units of the amplitude depend on the type of wave, but are always in the same units as the oscillating variable. A more general representation of the wave equation is more complex, but the role of amplitude remains analogous to this simple case.
For waves on a string, or in medium such as water, the amplitude is a displacement.
The amplitude of sound waves and audio signals (which relates to the volume) conventionally refers to the amplitude of the air pressure in the wave, but sometimes the amplitude of the displacement (movements of the air or the diaphragm of a speaker) is described. The logarithm of the amplitude squared is usually quoted in dB, so a null amplitude corresponds to ?? dB. Loudness is related to amplitude and intensity and is one of the most salient qualities of a sound, although in general sounds can be recognized independently of amplitude. The square of the amplitude is proportional to the intensity of the wave.
For electromagnetic radiation, the amplitude of a photon corresponds to the changes in the electric field of the wave. However, radio signals may be carried by electromagnetic radiation; the intensity of the radiation (amplitude modulation) or the frequency of the radiation (frequency modulation) is oscillated and then the individual oscillations are varied (modulated) to produce the signal.

Waveform and envelope
The amplitude as defined above is a constant and the wave is said to be wavelength continuous. If this condition does not hold, amplitude alike variations with time and/or position may be quantified in terms of the envelope of the wave.

See also
Complex amplitude
Waves and their properties:
Frequency
Wavelength
Crest factor

Amplitude modulation


== Notes ==",Category:Wave mechanics,3
76,77,Phonophobia,"Phonophobia, also called ligyrophobia or sonophobia, is a fear of or aversion to loud sounds—a type of specific phobia. It can also mean a fear of voices, or a fear of one's own voice. It is a very rare phobia which is often the symptom of hyperacusis. Sonophobia can refer to the hypersensitivity of a patient to sound and can be part of the diagnosis of a migraine. Occasionally it is called acousticophobia.
The term phonophobia comes from Greek ???? - ph?n?, ""sound"" and ????? - phobos, ""fear"".
Ligyrophobics may be fearful of devices that can suddenly emit loud sounds, such as computer speakers or fire alarms. When operating a device such as a home theater system, computer, television, or CD player, they may wish to have the volume turned down all the way before doing anything that would cause the speakers to emit sound, so that once the command to produce sound is given, the user can raise the volume of the speakers to a comfortable listening level. They may avoid parades and carnivals due to the loud instruments such as drums. As festive occasions are accompanied by music of over 120 decibels, many phobics develop agoraphobia. Other ligyrophobics also steer clear of any events in which fireworks are to be let off .
Another example is watching someone blow up a balloon beyond its normal capacity. This is often an unsettling, even disturbing thing for a person with ligyrophobia to observe, as he or she anticipates a loud sound when the balloon pops. When balloons pop, two types of reactions are heavy breathing and panic attacks. The sufferer becomes anxious to get away from the source of the loud sound and may get headaches.
It may also be related to, caused by, or confused with ""hyperacusis"", extreme sensitivity to loud sounds Phonophobia also refers to an extreme form of misophonia.

See also
Astraphobia – fear of thunder
Misophonia
List of phobias


== References ==",Category:Noise,3
77,78,Audio frequency,"An audio frequency (abbreviation: AF) or audible frequency is characterized as a periodic vibration whose frequency is audible to the average human. The SI unit of audio frequency is the hertz (Hz). It is the property of sound that most determines pitch.
The generally accepted standard range of audible frequencies is 20 to 20,000 Hz, although the range of frequencies individuals hear is greatly influenced by environmental factors. Frequencies below 20 Hz are generally felt rather than heard, assuming the amplitude of the vibration is great enough. Frequencies above 20,000 Hz can sometimes be sensed by young people. High frequencies are the first to be affected by hearing loss due to age and/or prolonged exposure to very loud noises.

Frequencies and descriptions
See also
Absolute threshold of hearing
Loudspeaker
Musical acoustics
Piano key frequencies
Scientific pitch notation
Whistle register


== References ==",Category:Audio engineering,3
78,79,Sound localization,"Sound localization is a listener's ability to identify the location or origin of a detected sound in direction and distance. It may also refer to the methods in acoustical engineering to simulate the placement of an auditory cue in a virtual 3D space (see binaural recording, wave field synthesis).
The sound localization mechanisms of the mammalian auditory system have been extensively studied. The auditory system uses several cues for sound source localization, including time- and level-differences (or intensity-difference) between both ears, spectral information, timing analysis, correlation analysis, and pattern matching.
These cues are also used by other animals, but there may be differences in usage, and there are also localization cues which are absent in the human auditory system, such as the effects of ear movements. Animals with the ability to localize sound have a clear evolutionary advantage.

How sound reaches the brain
Sound is the perceptual result of mechanical vibrations traveling through a medium such as air or water. Through the mechanisms of compression and rarefaction, sound waves travel through the air, bounce off the pinna and concha of the exterior ear, and enter the ear canal. The sound waves vibrate the tympanic membrane (ear drum), causing the three bones of the middle ear to vibrate, which then sends the energy through the oval window and into the cochlea where it is changed into a chemical signal by hair cells in the organ of corti, which synapse onto spiral ganglion fibers that travel through the cochlear nerve into the brain.

Neural interactions
In vertebrates, inter-aural time differences are known to be calculated in the superior olivary nucleus of the brainstem. According to Jeffress, this calculation relies on delay lines: neurons in the superior olive which accept innervation from each ear with different connecting axon lengths. Some cells are more directly connected to one ear than the other, thus they are specific for a particular inter-aural time difference. This theory is equivalent to the mathematical procedure of cross-correlation. However, because Jeffress' theory is unable to account for the precedence effect, in which only the first of multiple identical sounds is used to determine the sounds' location (thus avoiding confusion caused by echoes), it cannot be entirely used to explain the response. Furthermore, a number of recent physiological observations made in the midbrain and brainstem of small mammals have shed considerable doubt on the validity of Jeffress' original ideas 
Neurons sensitive to inter-aural level differences (ILDs) are excited by stimulation of one ear and inhibited by stimulation of the other ear, such that the response magnitude of the cell depends on the relative strengths of the two inputs, which in turn, depends on the sound intensities at the ears.
In the auditory midbrain nucleus, the inferior colliculus (IC), many ILD sensitive neurons have response functions that decline steeply from maximum to zero spikes as a function of ILD. However, there are also many neurons with much more shallow response functions that do not decline to zero spikes.

The cone of confusion
Most mammals are adept at resolving the location of a sound source using interaural time differences and interaural level differences. However, no such time or level differences exist for sounds originating along the circumference of circular conical slices, where the cone's axis lies along the line between the two ears.
Consequently, sound waves originating at any point along a given circumference slant height will have ambiguous perceptual coordinates. That is to say, the listener will be incapable of determining whether the sound originated from the back, front, top, bottom or anywhere else along the circumference at the base of a cone at any given distance from the ear. Of course, the importance of these ambiguities are vanishingly small for sound sources very close to or very far away from the subject, but it is these intermediate distances that are most important in terms of fitness.
These ambiguities can be removed by tilting the head, which can introduce a shift in both the amplitude and phase of sound waves arriving at each ear. This translates the vertical orientation of the interaural axis horizontally, thereby leveraging the mechanism of localization on the horizontal plane. Moreover, even with no alternation in the angle of the interaural axis (i.e. without tilting one's head) the hearing system can capitalize on interference patterns generated by pinnae, the torso, and even the temporary re-purposing of a hand as extension of the pinna (e.g., cupping one's hand around the ear).
As with other sensory stimuli, perceptual disambiguation is also accomplished through integration of multiple sensory inputs, especially visual cues. Having localized a sound within the circumference of a circle at some perceived distance, visual cues serve to fix the location of the sound. Moreover, prior knowledge of the location of the sound generating agent will assist in resolving its current location.

Sound localization by the human auditory system
Sound localization is the process of determining the location of a sound source. Objectively speaking, the major goal of sound localization is to simulate a specific sound field, including the acoustic sources, the listener, the media and environments of sound propagation. The brain utilizes subtle differences in intensity, spectral, and timing cues to allow us to localize sound sources. In this section, to more deeply understand the human auditory mechanism, we will briefly discuss about human ear localization theory.

General Introduction
Localization can be described in terms of three-dimensional position: the azimuth or horizontal angle, the elevation or vertical angle, and the distance (for static sounds) or velocity (for moving sounds).
The azimuth of a sound is signaled by the difference in arrival times between the ears, by the relative amplitude of high-frequency sounds (the shadow effect), and by the asymmetrical spectral reflections from various parts of our bodies, including torso, shoulders, and pinnae.
The distance cues are the loss of amplitude, the loss of high frequencies, and the ratio of the direct signal to the reverberated signal.
Depending on where the source is located, our head acts as a barrier to change the timbre, intensity, and spectral qualities of the sound, helping the brain orient where the sound emanated from. These minute differences between the two ears are known as interaural cues.
Lower frequencies, with longer wavelengths, diffract the sound around the head forcing the brain to focus only on the phasing cues from the source.
Helmut Haas discovered that we can discern the sound source despite additional reflections at 10 decibels louder than the original wave front, using the earliest arriving wave front. This principle is known as the Haas effect, a specific version of the precedence effect. Haas measured down to even a 1 millisecond difference in timing between the original sound and reflected sound increased the spaciousness, allowing the brain to discern the true location of the original sound. The nervous system combines all early reflections into a single perceptual whole allowing the brain to process multiple different sounds at once. The nervous system will combine reflections that are within about 35 milliseconds of each other and that have a similar intensity.

Duplex Theory
To determine the lateral input direction (left, front, right), the auditory system analyzes the following ear signal information:

Duplex Theory
In 1907, Lord Rayleigh utilized tuning forks to generate monophonic excitation and studied the lateral sound localization theory on a human head model without auricle. He first presented the interural clue difference based sound localization theory, which is known as Duplex Theory. Human ears are on the different sides of the head, thus they have different coordinates in space. As shown in fig. 2, since the distances between the acoustic source and ears are different, there are time difference and intensity difference between the sound signals of two ears. We call those kinds of differences as Interaural Time Difference (ITD) and Interaural Intensity Difference (IID) respectively.

ITD and IID
From fig.2 we can see that no matter for source B1 or source B2, there will be a propagation delay between two ears, which will generate the ITD. Simultaneously, human head and ears may have shadowing effect on high frequency signals, which will generate IID.
Interaural Time Difference (ITD) Sound from the right side reaches the right ear earlier than the left ear. The auditory system evaluates interaural time differences from: (a) Phase delays at low frequencies and (b) group delays at high frequencies.
Massive experiments demonstrate that ITD relates to the signal frequency f. Suppose the angular position of the acoustic source is ?, the head radius is r and the acoustic velocity is c, the function of ITD is given by:
  
    
      
        I
        T
        D
        =
        
          
            {
            
              
                
                  300
                  ×
                  
                    r
                  
                  ×
                  sin
                  ?
                  ?
                  
                    /
                  
                  
                    c
                  
                  ,
                
                
                  
                    if 
                  
                  f
                  ?
                  
                    4000Hz 
                  
                
              
              
                
                  200
                  ×
                  
                    r
                  
                  ×
                  sin
                  ?
                  ?
                  
                    /
                  
                  
                    c
                  
                  ,
                
                
                  
                    if 
                  
                  f
                  >
                  
                     4000Hz
                  
                
              
            
            
          
        
      
    
    {\displaystyle ITD={\begin{cases}300\times {\text{r}}\times \sin \theta /{\text{c}},&{\text{if }}f\leq {\text{4000Hz }}\\200\times {\text{r}}\times \sin \theta /{\text{c}},&{\text{if }}f>{\text{ 4000Hz}}\end{cases}}}
  . In above closed form, we assumed that the 0 degree is in the right ahead of the head and counter-clockwise is positive.
Interaural Intensity Difference (IID) or Interaural Level Difference (ILD) Sound from the right side has a higher level at the right ear than at the left ear, because the head shadows the left ear. These level differences are highly frequency dependent and they increase with increasing frequency. Massive theoretical researches demonstrate that IID relates to the signal frequency f and the angular position of the acoustic source ?. The function of IID is given by: 
  
    
      
        I
        I
        D
        =
        1.0
        +
        (
        f
        
          /
        
        1000
        
          )
          
            0.8
          
        
        ×
        sin
        ?
        ?
      
    
    {\displaystyle IID=1.0+(f/1000)^{0.8}\times \sin \theta }
  
For frequencies below 1000 Hz, mainly ITDs are evaluated (phase delays), for frequencies above 1500 Hz mainly IIDs are evaluated. Between 1000 Hz and 1500 Hz there is a transition zone, where both mechanisms play a role.
Localization accuracy is 1 degree for sources in front of the listener and 15 degrees for sources to the sides. Humans can discern interaural time differences of 10 microseconds or less.

Evaluation for low frequencies
For frequencies below 800 Hz, the dimensions of the head (ear distance 21.5 cm, corresponding to an interaural time delay of 625 µs) are smaller than the half wavelength of the sound waves. So the auditory system can determine phase delays between both ears without confusion. Interaural level differences are very low in this frequency range, especially below about 200 Hz, so a precise evaluation of the input direction is nearly impossible on the basis of level differences alone. As the frequency drops below 80 Hz it becomes difficult or impossible to use either time difference or level difference to determine a sound's lateral source, because the phase difference between the ears becomes too small for a directional evaluation.

Evaluation for high frequencies
For frequencies above 1600 Hz the dimensions of the head are greater than the length of the sound waves. An unambiguous determination of the input direction based on interaural phase alone is not possible at these frequencies. However, the interaural level differences become larger, and these level differences are evaluated by the auditory system. Also, group delays between the ears can be evaluated, and is more pronounced at higher frequencies; that is, if there is a sound onset, the delay of this onset between the ears can be used to determine the input direction of the corresponding sound source. This mechanism becomes especially important in reverberant environments. After a sound onset there is a short time frame where the direct sound reaches the ears, but not yet the reflected sound. The auditory system uses this short time frame for evaluating the sound source direction, and keeps this detected direction as long as reflections and reverberation prevent an unambiguous direction estimation. The mechanisms described above cannot be used to differentiate between a sound source ahead of the hearer or behind the hearer; therefore additional cues have to be evaluated.

Pinna Filtering Effect Theory
Motivations
Duplex theory clearly points out that ITD and IID play significant roles in sound localization but they can only deal with lateral localizing problems. For example, based on duplex theory, if two acoustic sources are symmetrically located on the right front and right back of the human head, they will generate equal ITDs and IIDs, which is called as cone model effect. However, human ears can actually distinguish this set of sources. Besides that, in natural sense of hearing, only one ear, which means no ITD or IID, can distinguish the sources with a high accuracy. Due to the disadvantages of duplex theory, researchers proposed the pinna filtering effect theory. The shape of human pinna is very special. It is concave with complex folds and asymmetrical no matter horizontally or vertically. The reflected waves and the direct waves will generate a frequency spectrum on the eardrum, which is related to the acoustic sources. Then auditory nerves localize the sources by this frequency spectrum. Therefore, a corresponding theory was proposed and called as pinna filtering effect theory.

Math Model
These spectrum clue generated by pinna filtering effect can be presented as Head-Related Transfer Functions (HRTF). The corresponding time domain expressions are called as Head-Related Impulse Response (HRIR). HRTF is also called as the transfer function from the free field to a specific point in the ear canal. We usually recognize HRTFs as LTI systems:

  
    
      
        
          H
          
            L
          
        
        =
        
          H
          
            L
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        =
        
          P
          
            L
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        
          /
        
        
          P
          
            0
          
        
        (
        r
        ,
        ?
        )
      
    
    {\displaystyle H_{L}=H_{L}(r,\theta ,\varphi ,\omega ,\alpha )=P_{L}(r,\theta ,\varphi ,\omega ,\alpha )/P_{0}(r,\omega )}
  

  
    
      
        
          H
          
            R
          
        
        =
        
          H
          
            R
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        =
        
          P
          
            R
          
        
        (
        r
        ,
        ?
        ,
        ?
        ,
        ?
        ,
        ?
        )
        
          /
        
        
          P
          
            0
          
        
        (
        r
        ,
        ?
        )
      
    
    {\displaystyle H_{R}=H_{R}(r,\theta ,\varphi ,\omega ,\alpha )=P_{R}(r,\theta ,\varphi ,\omega ,\alpha )/P_{0}(r,\omega )}
  ,
where L and R represent the left ear and right ear respectively. 
  
    
      
        
          P
          
            L
          
        
      
    
    {\displaystyle P_{L}}
   and 
  
    
      
        
          P
          
            R
          
        
      
    
    {\displaystyle P_{R}}
   represent the amplitude of sound pressure at entrances of left and right ear canal. 
  
    
      
        
          P
          
            0
          
        
      
    
    {\displaystyle P_{0}}
   is the amplitude of sound pressure at the center of the head coordinate when listener does not exist. In general, HRTFs 
  
    
      
        
          H
          
            L
          
        
      
    
    {\displaystyle H_{L}}
   and 
  
    
      
        
          H
          
            R
          
        
      
    
    {\displaystyle H_{R}}
   are functions of source angular position 
  
    
      
        ?
      
    
    {\displaystyle \theta }
  , elevation angle 
  
    
      
        ?
      
    
    {\displaystyle \varphi }
  , distance between source and center of the head 
  
    
      
        r
      
    
    {\displaystyle r}
  , the angular velocity 
  
    
      
        ?
      
    
    {\displaystyle \omega }
   and the equivalent dimension of the head 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
  .

HRTF Database
At present, the main institutes that work on measuring HRTF database includes CIPIC International Lab, MIT Media Lab, The Graduate School in Psychoacoustics at the University of Oldenburg, Neurophysiology Lab in University of Wisconsin-Madison and Ames Lab of NASA. They carefully measures the HRIRs from both humans and animals and share the results on Internet for people who want to study.

Other Cues for 3D Space Localization
Monaural cues
The human outer ear, i.e. the structures of the pinna and the external ear canal, form direction-selective filters. Depending on the sound input direction in the median plane, different filter resonances become active. These resonances implant direction-specific patterns into the frequency responses of the ears, which can be evaluated by the auditory system (directional bands) for vertical sound localization. Together with other direction-selective reflections at the head, shoulders and torso, they form the outer ear transfer functions. These patterns in the ear's frequency responses are highly individual, depending on the shape and size of the outer ear. If sound is presented through headphones, and has been recorded via another head with different-shaped outer ear surfaces, the directional patterns differ from the listener's own, and problems will appear when trying to evaluate directions in the median plane with these foreign ears. As a consequence, front–back permutations or inside-the-head-localization can appear when listening to dummy head recordings, or otherwise referred to as binaural recordings. It has been shown that human subjects can monaurally localize high frequency sound but not low frequency sound. Binaural localization, however, was possible with lower frequencies. This is likely due to the pinna being small enough to only interact with sound waves of high frequency. It seems that people can only accurately localize the elevation of sounds that are complex and include frequencies above 7,000 Hz, and a pinna must be present.

Dynamic binaural cues
When the head is stationary, the binaural cues for lateral sound localization (interaural time difference and interaural level difference) do not give information about the location of a sound in the median plane. Identical ITDs and ILDs can be produced by sounds at eye level or at any elevation, as long as the lateral direction is constant. However, if the head is rotated, the ITD and ILD change dynamically, and those changes are different for sounds at different elevations. For example, if an eye-level sound source is straight ahead and the head turns to the left, the sound becomes louder (and arrives sooner) at the right ear than at the left. But if the sound source is directly overhead, there will be no change in the ITD and ILD as the head turns. Intermediate elevations will produce intermediate degrees of change, and if the presentation of binaural cues to the two ears during head movement is reversed, the sound will be heard behind the listener. Hans Wallach artificially altered a sound’s binaural cues during movements of the head. Although the sound was objectively placed at eye level, the dynamic changes to ITD and ILD as the head rotated were those that would be produced if the sound source had been elevated. In this situation, the sound was heard at the synthesized elevation. The fact that the sound sources objectively remained at eye level prevented monaural cues from specifying the elevation, showing that it was the dynamic change in the binaural cues during head movement that allowed the sound to be correctly localized in the vertical dimension. The head movements need not be actively produced; accurate vertical localization occurred in a similar setup when the head rotation was produced passively, by seating the blindfolded subject in a rotating chair. As long as the dynamic changes in binaural cues accompanied a perceived head rotation, the synthesized elevation was perceived.

Distance of the sound source
The human auditory system has only limited possibilities to determine the distance of a sound source. In the close-up-range there are some indications for distance determination, such as extreme level differences (e.g. when whispering into one ear) or specific pinna (the visible part of the ear) resonances in the close-up range.
The auditory system uses these clues to estimate the distance to a sound source:
Direct/ Reflection ratio: In enclosed rooms, two types of sound are arriving at a listener: The direct sound arrives at the listener's ears without being reflected at a wall. Reflected sound has been reflected at least one time at a wall before arriving at the listener. The ratio between direct sound and reflected sound can give an indication about the distance of the sound source.
Loudness: Distant sound sources have a lower loudness than close ones. This aspect can be evaluated especially for well-known sound sources.
Sound spectrum : High frequencies are more quickly damped by the air than low frequencies. Therefore, a distant sound source sounds more muffled than a close one, because the high frequencies are attenuated. For sound with a known spectrum (e.g. speech) the distance can be estimated roughly with the help of the perceived sound.
ITDG: The Initial Time Delay Gap describes the time difference between arrival of the direct wave and first strong reflection at the listener. Nearby sources create a relatively large ITDG, with the first reflections having a longer path to take, possibly many times longer. When the source is far away, the direct and the reflected sound waves have similar path lengths.
Movement: Similar to the visual system there is also the phenomenon of motion parallax in acoustical perception. For a moving listener nearby sound sources are passing faster than distant sound sources.
Level Difference: Very close sound sources cause a different level between the ears.

Signal processing
Sound processing of the human auditory system is performed in so-called critical bands. The hearing range is segmented into 24 critical bands, each with a width of 1 Bark or 100 Mel. For a directional analysis the signals inside the critical band are analyzed together.
The auditory system can extract the sound of a desired sound source out of interfering noise. This allows the listener to concentrate on only one speaker if other speakers are also talking (the cocktail party effect). With the help of the cocktail party effect sound from interfering directions is perceived attenuated compared to the sound from the desired direction. The auditory system can increase the signal-to-noise ratio by up to 15 dB, which means that interfering sound is perceived to be attenuated to half (or less) of its actual loudness.

Localization in enclosed rooms
In enclosed rooms not only the direct sound from a sound source is arriving at the listener's ears, but also sound which has been reflected at the walls. The auditory system analyses only the direct sound, which is arriving first, for sound localization, but not the reflected sound, which is arriving later (law of the first wave front). So sound localization remains possible even in an echoic environment. This echo cancellation occurs in the Dorsal Nucleus of the Lateral Lemniscus (DNLL).
In order to determine the time periods, where the direct sound prevails and which can be used for directional evaluation, the auditory system analyzes loudness changes in different critical bands and also the stability of the perceived direction. If there is a strong attack of the loudness in several critical bands and if the perceived direction is stable, this attack is in all probability caused by the direct sound of a sound source, which is entering newly or which is changing its signal characteristics. This short time period is used by the auditory system for directional and loudness analysis of this sound. When reflections arrive a little bit later, they do not enhance the loudness inside the critical bands in such a strong way, but the directional cues become unstable, because there is a mix of sound of several reflection directions. As a result, no new directional analysis is triggered by the auditory system.
This first detected direction from the direct sound is taken as the found sound source direction, until other strong loudness attacks, combined with stable directional information, indicate that a new directional analysis is possible. (see Franssen effect)

Specific techniques with applications
Auditory transmission stereo system
This kind of sound localization technique provides us the real virtual stereo system. It utilizes ""smart"" manikins, such as KEMAR, to glean signals or use DSP methods to simulate the transmission process from sources to ears. After amplifying, recording and transmitting, the two channels of received signals will be reproduced through earphones or speakers. This localization approach uses electroacoustic methods to obtain the spatial information of the original sound field by transferring the listener's auditory apparatus to the original sound field. The most considerable advantages of it would be that its acoustic images are lively and natural. Also, it only needs two independent transmitted signal to reproduce the acoustic image of a 3D system.

3D para-virtualization stereo system
The representatives of this kind of system are SRS Audio Sandbox, Spatializer Audio Lab and Qsound Qxpander. They use HRTF to simulate the received acoustic signals at the ears from different directions with common binary-channel stereo reproduction. Therefore, they can simulate reflected sound waves and improve subjective sense of space and envelopment. Since they are para-virtualization stereo systems, the major goal of them is to simulate stereo sound information. Traditional stereo systems use sensors that are quite different from human ears. Although those sensors can receive the acoustic information from different directions, they do not have the same frequency response of human auditory system. Therefore, when binary-channel mode is applied, human auditory systems still cannot feel the 3D sound effect field. However, the 3D para-virtualization stereo system overcome such disadvantages. It uses HRTF principles to glean acoustic information from the original sound field then produce a lively 3D sound field through common earphones or speakers.

Multichannel stereo virtual reproduction
Since the multichannel stereo systems require lots of reproduction channels, some researchers adopted the HRTF simulation technologies to reduce the number of reproduction channels. They use only two speakers to simulate multiple speakers in a multichannel system. This process is called as virtual reproduction. Essentially, such approach uses both interaural difference principle and pinna filtering effect theory. Unfortunately, this kind of approach cannot perfectly substitute the traditional multichannel stereo system, such as 5.1 surround sound system. That is because when the listening zone is relatively larger, simulation reproduction through HRTFs may cause invert acoustic images at symmetric positions.

Animals
Since most animals have two ears, many of the effects of the human auditory system can also be found in other animals. Therefore, interaural time differences (interaural phase differences) and interaural level differences play a role for the hearing of many animals. But the influences on localization of these effects are dependent on head sizes, ear distances, the ear positions and the orientation of the ears.

Lateral information (left, ahead, right)
If the ears are located at the side of the head, similar lateral localization cues as for the human auditory system can be used. This means: evaluation of interaural time differences (interaural phase differences) for lower frequencies and evaluation of interaural level differences for higher frequencies. The evaluation of interaural phase differences is useful, as long as it gives unambiguous results. This is the case, as long as ear distance is smaller than half the length (maximal one wavelength) of the sound waves. For animals with a larger head than humans the evaluation range for interaural phase differences is shifted towards lower frequencies, for animals with a smaller head, this range is shifted towards higher frequencies.
The lowest frequency which can be localized depends on the ear distance. Animals with a greater ear distance can localize lower frequencies than humans can. For animals with a smaller ear distance the lowest localizable frequency is higher than for humans.
If the ears are located at the side of the head, interaural level differences appear for higher frequencies and can be evaluated for localization tasks. For animals with ears at the top of the head, no shadowing by the head will appear and therefore there will be much less interaural level differences, which could be evaluated. Many of these animals can move their ears, and these ear movements can be used as a lateral localization cue.

Odontocetes
Dolphins (and other odontocetes) rely on echolocation to aid in detecting, identifying, localizing, and capturing prey. Dolphin sonar signals are well suited for localizing multiple, small targets in a three-dimensional aquatic environment by utilizing highly directional (3 dB beamwidth of about 10 deg), broadband (3 dB bandwidth typically of about 40 kHz; peak frequencies between 40 kHz and 120 kHz), short duration clicks (about 40 ?s). Dolphins can localize sounds both passively and actively (echolocation) with a resolution of about 1 deg. Cross-modal matching (between vision and echolocation) suggests dolphins perceive the spatial structure of complex objects interrogated through echolocation, a feat that likely requires spatially resolving individual object features and integration into a holistic representation of object shape. Although dolphins are sensitive to small, binaural intensity and time differences, mounting evidence suggests dolphins employ position-dependent spectral cues derived from well-developed head-related transfer functions, for sound localization in both the horizontal and vertical planes. A very small temporal integration time (264 ?s) allows localization of multiple targets at varying distances. Localization adaptations include pronounc",Category:Sound,3
79,80,Sound barrier,"The sound barrier or sonic barrier is a popular term for the sudden increase in aerodynamic drag and other effects experienced by an aircraft or other object when it approaches supersonic speed. When aircraft first began to be able to reach close to supersonic speed, these effects were seen as constituting a barrier making supersonic speed very difficult or impossible.
In dry air at 20 °C (68 °F), the sound barrier is reached when an object moves at a speed of 343 metres per second (about 767 mph, 1234 km/h or 1,125 ft/s). The term came into use in this sense during World War II, when a number of aircraft started to encounter the effects of compressibility, a number of unrelated aerodynamic effects that ""struck"" their aircraft, seemingly impeding further acceleration. By the 1950s, new aircraft designs routinely ""broke"" the sound barrier.

History
Some common whips such as the bullwhip or stockwhip are able to move faster than sound: the tip of the whip breaks the sound barrier and causes a sharp crack—literally a sonic boom. Firearms made after the 19th century have generally had a supersonic muzzle velocity.
The sound barrier may have been first breached by living beings some 150 million years ago. Some paleobiologists report that, based on computer models of their biomechanical capabilities, certain long-tailed dinosaurs such as Apatosaurus and Diplodocus may have possessed the ability to flick their tails at supersonic speeds, possibly used to generate an intimidating booming sound. This finding is theoretical and disputed by others in the field. Meteorites entering the Earth's atmosphere usually, if not always, descend faster than sound.

Early problems
The tip of the propeller on many early aircraft may reach supersonic speeds, producing a noticeable buzz that differentiates such aircraft. This is particularly noticeable on the Stearman (released in 1934), and noticeable on the North American T-6 Texan when it enters a sharp-breaking turn. This is undesirable, as the transonic air movement creates disruptive shock waves and turbulence. It is due to these effects that propellers are known to suffer from dramatically decreased performance as they approach the speed of sound. It is easy to demonstrate that the power needed to improve performance is so great that the weight of the required engine grows faster than the power output of the propeller can compensate. This problem was one that led to early research into jet engines, notably by Frank Whittle in England and Hans von Ohain in Germany, who were led to their research specifically in order to avoid these problems in high-speed flight.
Nevertheless, propeller aircraft were able to approach the speed of sound in a dive. Unfortunately, doing so led to numerous crashes for a variety of reasons. Most infamously, in the Mitsubishi Zero, pilots flew full power into the terrain because the rapidly increasing forces acting on the control surfaces of their aircraft overpowered them. In this case, several attempts to fix it only made the problem worse. Likewise, the flexing caused by the low torsional stiffness of the Supermarine Spitfire's wings caused them, in turn, to counteract aileron control inputs, leading to a condition known as control reversal. This was solved in later models with changes to the wing. Worse still, a particularly dangerous interaction of the airflow between the wings and tail surfaces of diving Lockheed P-38 Lightnings made ""pulling out"" of dives difficult; however, the problem was later solved by the addition of a ""dive flap"" that upset the airflow under these circumstances. Flutter due to the formation of shock waves on curved surfaces was another major problem, which led most famously to the breakup of de Havilland Swallow and death of its pilot, Geoffrey de Havilland, Jr. in 1946. A similar problem is thought to have been the cause of the 1943 crash of the BI-1 rocket aircraft in the Soviet Union.
All of these effects, although unrelated in most ways, led to the concept of a ""barrier"" making it difficult for an aircraft to exceed the speed of sound.

Early claims
During WWII and immediately thereafter, a number of claims were made that the sound barrier had been broken in a dive. The majority of these purported events can be dismissed as instrumentation errors. The typical airspeed indicator (ASI) uses air pressure differences between two or more points on the aircraft, typically near the nose and at the side of the fuselage, to produce a speed figure. At high speed, the various compression effects that lead to the sound barrier also cause the ASI to go non-linear and produce inaccurately high or low readings, depending on the specifics of the installation. This effect became known as ""Mach jump"". Before the introduction of Mach meters, accurate measurements of supersonic speeds could only be made externally, normally using ground-based instruments. Many claims of supersonic speeds were found to be far below this speed when measured in this fashion.
In 1942, Republic Aviation issued a press release stating that Lts. Harold E. Comstock and Roger Dyar had exceeded the speed of sound during test dives in the P-47 Thunderbolt. It is widely agreed that this was due to inaccurate ASI readings. In similar tests, the North American P-51 Mustang, a higher performance aircraft, demonstrated limits at Mach 0.85, with every flight over M0.84 causing the aircraft to be damaged by vibration.

One of the highest recorded instrumented Mach numbers attained for a propeller aircraft is the Mach 0.891 for a Spitfire PR XI, flown during dive tests at the Royal Aircraft Establishment, Farnborough in April 1944. The Spitfire, a photo-reconnaissance variant, the Mark XI, fitted with an extended 'rake type' multiple pitot system, was flown by Squadron Leader J. R. Tobin to this speed, corresponding to a corrected true airspeed (TAS) of 606 mph. In a subsequent flight, Squadron Leader Anthony Martindale achieved Mach 0.92, but it ended in a forced landing after over- revving damaged the engine.
In the 1990s, Hans Guido Mutke claimed to have broken the sound barrier on 9 April 1945 in the Messerschmitt Me 262 jet aircraft. He states that his ASI pegged itself at 1,100 kilometres per hour (680 mph). Mutke reported not just transonic buffeting but the resumption of normal control once a certain speed was exceeded, then a resumption of severe buffeting once the Me 262 slowed again. He also reported engine flame out.
This claim is widely disputed, even by pilots in his unit. All of the effects he reported are known to occur on the Me 262 at much lower speeds, and the ASI reading is simply not reliable in the transonic. Further, a series of tests made by Karl Doetsch at the behest of Willy Messerschmitt found that the plane became uncontrollable above Mach 0.86, and at Mach 0.9 would nose over into a dive that could not be recovered from. Post-war tests by the RAF confirmed these results, with the slight modification that the maximum speed using new instruments was found to be Mach 0.84, rather than Mach 0.86.
In 1999, Mutke enlisted the help of Professor Otto Wagner of the Munich Technical University to run computational tests to determine whether the aircraft could break the sound barrier. These tests do not rule out the possibility, but are lacking accurate data on the coefficient of drag that would be needed to make accurate simulations. Wagner stated ""I don't want to exclude the possibility, but I can imagine he may also have been just below the speed of sound and felt the buffeting, but did not go above Mach-1.""
One bit of evidence presented by Mutke is on page 13 of the ""Me 262 A-1 Pilot's Handbook"" issued by Headquarters Air Materiel Command, Wright Field, Dayton, Ohio as Report No. F-SU-1111-ND on January 10, 1946:

Speeds of 950 km/h (590 mph) are reported to have been attained in a shallow dive 20° to 30° from the horizontal. No vertical dives were made. At speeds of 950 to 1,000 km/h (590 to 620 mph) the air flow around the aircraft reaches the speed of sound, and it is reported that the control surfaces no longer affect the direction of flight. The results vary with different airplanes: some wing over and dive while others dive gradually. It is also reported that once the speed of sound is exceeded, this condition disappears and normal control is restored.

The comments about restoration of flight control and cessation of buffeting above Mach 1 are very significant in a 1946 document. However, it is not clear where these terms came from, as it does not appear the US pilots carried out such tests.
In his 1990 book Me-163, former Messerschmitt Me 163 ""Komet"" pilot Mano Ziegler claims that his friend, test pilot Heini Dittmar, broke the sound barrier while diving the rocket plane, and that several people on the ground heard the sonic booms. He claims that on 6 July 1944, Dittmar, flying Me 163B V18, bearing the Stammkennzeichen alphabetic code VA+SP, was measured traveling at a speed of 1,130 km/h (702 mph). However, no evidence of such a flight exists in any of the materials from that period, which were captured by Allied forces and extensively studied. Dittmar had been officially recorded at 1,004.5 km/h (623.8 mph) in level flight on 2 October 1941 in the prototype Me 163A V4. He reached this speed at less than full throttle, as he was concerned by the transonic buffeting. Dittmar himself does not make a claim that he broke the sound barrier on that flight, and notes that the speed was recorded only on the AIS. He does, however, take credit for being the first pilot to ""knock on the sound barrier.""
The Luftwaffe test pilot Lothar Sieber (April 7, 1922 – March 1, 1945) may have inadvertently become the first man to break the sound barrier on 1 March 1945. This occurred while he was piloting a Bachem Ba 349 ""Natter"" for the first manned vertical takeoff of a rocket in history. In 55 seconds, he traveled a total of 14 km (8.7 miles). The aircraft crashed and he perished violently in this endeavor.
There are a number of unmanned vehicles that flew at supersonic speeds during this period, but they generally do not meet the definition. In 1933, Soviet designers working on ramjet concepts fired phosphorus-powered engines out of artillery guns to get them to operational speeds. It is possible that this produced supersonic performance as high as Mach 2, but this was not due solely to the engine itself. In contrast, the German V-2 ballistic missile routinely broke the sound barrier in flight, for the first time on 3 October 1942. By September 1944, V-2s routinely achieved Mach 4 (1,200 m/s, or 3044 mph) during terminal descent.

Breaking the sound barrier
In 1942, the United Kingdom's Ministry of Aviation began a top-secret project with Miles Aircraft to develop the world's first aircraft capable of breaking the sound barrier. The project resulted in the development of the prototype Miles M.52 turbojet powered aircraft, which was designed to reach 1,000 mph (417 m/s; 1,600 km/h) (over twice the existing speed record) in level flight, and to climb to an altitude of 36,000 ft (11 km) in 1 minute 30 sec.
A huge number of advanced features were incorporated into the resulting M.52 design, many of which hint at a detailed knowledge of supersonic aerodynamics. In particular, the design featured a conical nose and sharp wing leading edges, as it was known that round-nosed projectiles could not be stabilised at supersonic speeds. The design used very thin wings of biconvex section proposed by Jakob Ackeret for low drag. The wing tips were ""clipped"" to keep them clear of the conical shock wave generated by the nose of the aircraft. The fuselage had the minimum cross-section allowable around the centrifugal engine with fuel tanks in a saddle over the top.

Another critical addition was the use of a power-operated stabilator, also known as the all-moving tail or flying tail, a key to supersonic flight control which contrasted with traditional hinged tailplanes (horizontal stabilizers) connected mechanically to the pilots control column. Conventional control surfaces became ineffective at the high subsonic speeds then being achieved by fighters in dives, due to the aerodynamic forces caused by the formation of shockwaves at the hinge and the rearward movement of the centre of pressure, which together could override the control forces that could be applied mechanically by the pilot, hindering recovery from the dive. A major impediment to early transonic flight was control reversal, the phenomenon which caused flight inputs (stick, rudder) to switch direction at high speed; it was the cause of many accidents and near-accidents. An all-flying tail is considered to be a minimum condition of enabling aircraft to break the transonic barrier safely, without losing pilot control. The Miles M.52 was the first instance of this solution, and has since been universally applied.
Initially, the aircraft was to use Frank Whittle's latest engine, the Power Jets W.2/700, which would only reach supersonic speed in a shallow dive. To develop a fully supersonic version of the aircraft, an innovation incorporated was a reheat jetpipe – also known as an afterburner. Extra fuel was to be burned in the tailpipe to avoid overheating the turbine blades, making use of unused oxygen in the exhaust. Finally, the design included another critical element, the use of a shock cone in the nose to slow the incoming air to the subsonic speeds needed by the engine.
Although the project was eventually cancelled, the research was used to construct an unmanned missile that went on to achieve a speed of Mach 1.38 in a successful, controlled transonic and supersonic level test flight; this was a unique achievement at that time which validated the aerodynamics of the M.52.
Meanwhile, test pilots achieved high velocities in the tailless, swept-wing de Havilland DH 108. One of them was Geoffrey de Havilland, Jr. who was killed on 27 September 1946 when his DH 108 broke up at about Mach 0.9. John Derry has been called ""Britain's first supersonic pilot"" because of a dive he made in a DH 108 on 6 September 1948.

Sound barrier officially broken in aircraft
The British Air Ministry signed an agreement with the United States to exchange all its high-speed research, data and designs and Bell Aircraft company was given access to the drawings and research on the M.52, but the U.S. reneged on the agreement and no data was forthcoming in return. Bell's supersonic design was still using a conventional tail and they were battling the problem of control.

They utilized the information to initiate work on the Bell X-1. The final version of the Bell X-1 was very similar in design to the original Miles M.52 version. Also featuring the all-moving tail, the XS-1 was later known as the X-1. It was in the X-1 that Chuck Yeager was credited with being the first person to break the sound barrier in level flight on October 14, 1947, flying at an altitude of 45,000 ft (13.7 km). George Welch made a plausible but officially unverified claim to have broken the sound barrier on 1 October 1947, while flying an XP-86 Sabre. He also claimed to have repeated his supersonic flight on October 14, 1947, 30 minutes before Yeager broke the sound barrier in the Bell X-1. Although evidence from witnesses and instruments strongly imply that Welch achieved supersonic speed, the flights were not properly monitored and are not officially recognized. The XP-86 officially achieved supersonic speed on April 26, 1948.
On 14 October 1947, just under a month after the United States Air Force had been created as a separate service, the tests culminated in the first manned supersonic flight, piloted by Air Force Captain Charles ""Chuck"" Yeager in aircraft #46-062, which he had christened Glamorous Glennis. The rocket-powered aircraft was launched from the bomb bay of a specially modified B-29 and glided to a landing on a runway. XS-1 flight number 50 is the first one where the X-1 recorded supersonic flight, at Mach 1.06 (361 m/s, 1,299 km/h, 807.2 mph) peak speed; however, Yeager and many other personnel believe Flight #49 (also with Yeager piloting), which reached a top recorded speed of Mach 0.997 (339 m/s, 1,221 km/h), may have, in fact, exceeded Mach 1. (The measurements were not accurate to three significant figures and no sonic boom was recorded for that flight.)
As a result of the X-1's initial supersonic flight, the National Aeronautics Association voted its 1948 Collier Trophy to be shared by the three main participants in the program. Honored at the White House by President Harry S. Truman were Larry Bell for Bell Aircraft, Captain Yeager for piloting the flights, and John Stack for the NACA contributions.
Jackie Cochran was the first woman to break the sound barrier on May 18, 1953, in a Canadair Sabre, with Yeager as her wingman.
On August 21, 1961, a Douglas DC-8-43 (registration N9604Z) exceeded Mach one in a controlled dive during a test flight at Edwards Air Force Base. The crew were William Magruder (pilot), Paul Patten (copilot), Joseph Tomich (flight engineer), and Richard H. Edwards (flight test engineer). This is the first and only supersonic flight by a civilian airliner, other than Concorde or the Tu-144.

The sound barrier understood
As the science of high-speed flight became more widely understood, a number of changes led to the eventual understanding that the ""sound barrier"" is easily penetrated, with the right conditions. Among these changes were the introduction of swept wings, the area rule, and engines of ever-increasing performance. By the 1950s, many combat aircraft could routinely break the sound barrier in level flight, although they often suffered from control problems when doing so, such as Mach tuck. Modern aircraft can transit the ""barrier"" without control problems.
By the late 1950s, the issue was so well understood that many companies started investing in the development of supersonic airliners, or SSTs, believing that to be the next ""natural"" step in airliner evolution. However, this has not yet happened. Although the Concorde and the Tupolev Tu-144 entered service in the 1970s, both were later retired without being replaced by similar designs. The last flight of a Concorde in service was in 2003.
Although Concorde and the Tu-144 were the first aircraft to carry commercial passengers at supersonic speeds, they were not the first or only commercial airliners to break the sound barrier. On 21 August 1961, a Douglas DC-8 broke the sound barrier at Mach 1.012 or 1,240 km/h (776.2 mph) while in a controlled dive through 41,088 feet (12,510 m). The purpose of the flight was to collect data on a new leading-edge design for the wing. A China Airlines 747 may have broken the sound barrier in an unplanned descent from 41,000 ft (12,500 m) to 9,500 ft (2,900 m) after an in-flight upset on 19 February 1985. It also reached over 5g.

Breaking the sound barrier in a land vehicle
On January 12, 1948, a Northrop unmanned rocket sled became the first land vehicle to break the sound barrier. At a military test facility at Muroc Air Force Base (now Edwards AFB), California, it reached a peak speed of 1,019 mph (1,640 km/h) before jumping the rails.
On October 15, 1997, in a vehicle designed and built by a team led by Richard Noble, Royal Air Force pilot Andy Green became the first person to break the sound barrier in a land vehicle in compliance with Fédération Internationale de l'Automobile rules. The vehicle, called the ThrustSSC (""Super Sonic Car""), captured the record 50 years and one day after Yeager's first supersonic flight.

Breaking the sound barrier as a human projectile
Felix Baumgartner
In October 2012 Felix Baumgartner, with a team of scientists and sponsor Red Bull, attempted the highest sky-dive on record. The project would see Baumgartner attempt to jump 120,000 ft (36,580 m) from a helium balloon and become the first parachutist to break the sound barrier. The launch was scheduled for October 9, 2012, but was aborted due to adverse weather; subsequently the capsule was launched instead on October 14. Baumgartner's feat also marked the 65th anniversary of U.S. test pilot Chuck Yeager's successful attempt to break the sound barrier in an aircraft.
Baumgartner landed in eastern New Mexico after jumping from a world record 128,100 feet (39,045 m), or 24.26 miles, and broke the sound barrier as he traveled at speeds up to 833.9 mph (1342 km/h or Mach 1.26). In the press conference after his jump, it was announced he was in freefall for 4 minutes, 18 seconds, the second longest freefall after the 1960 jump of Joseph Kittinger for 4 minutes, 36 seconds.

Alan Eustace
In October 2014, Alan Eustace, a senior vice president at Google, broke Baumgartner's record for highest sky-dive and also broke the sound barrier in the process.

Legacy
David Lean directed The Sound Barrier, a fictionalized retelling of the de Havilland DH 108 test flights.

References
Notes
Citations
Bibliography
External links
Fluid Mechanics, a collection of tutorials by Dr. Mark S. Cramer, Ph.D
Breaking the Sound Barrier with an Aircraft by Carl Rod Nave, Ph.D
a video of a Concorde reaching Mach 1 at intersection TESGO taken from below
An interactive Java applet, illustrating the sound barrier.",Category:Airspeed,3
80,81,Acoustic impedance,"Acoustic impedance and specific acoustic impedance are measures of the opposition that a system presents to the acoustic flow resulting of an acoustic pressure applied to the system. The SI unit of acoustic impedance is the pascal second per cubic metre (Pa·s/m3) or the rayl per square metre (rayl/m2), while that of specific acoustic impedance is the pascal second per metre (Pa·s/m) or the rayl. In this article the symbol rayl denotes the MKS rayl. There is a close analogy with electrical impedance, which measures the opposition that a system presents to the electrical flow resulting of an electrical voltage applied to the system.

Mathematical definitions
Acoustic impedance
For a linear time-invariant system, the relationship between the acoustic pressure applied to the system and the resulting acoustic volume flow rate through a surface perpendicular to the direction of that pressure at its point of application is given by

  
    
      
        p
        (
        t
        )
        =
        [
        R
        ?
        Q
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle p(t)=[R*Q](t),}
  
or equivalently by

  
    
      
        Q
        (
        t
        )
        =
        [
        G
        ?
        p
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle Q(t)=[G*p](t),}
  
where
p is the acoustic pressure;
Q is the acoustic volume flow rate;

  
    
      
        ?
      
    
    {\displaystyle *}
   is the convolution operator;
R is the acoustic resistance in the time domain;
G = R ?1 is the acoustic conductance in the time domain (R ?1 is the convolution inverse of R).
Acoustic impedance, denoted Z, is the Laplace transform, or the Fourier transform, or the analytic representation of time domain acoustic resistance:

  
    
      
        Z
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        R
        ]
        (
        s
        )
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              Q
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle Z(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[R](s)={\frac {{\mathcal {L}}[p](s)}{{\mathcal {L}}[Q](s)}},}
  

  
    
      
        Z
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        R
        ]
        (
        ?
        )
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              Q
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle Z(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[R](\omega )={\frac {{\mathcal {F}}[p](\omega )}{{\mathcal {F}}[Q](\omega )}},}
  

  
    
      
        Z
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          R
          
            
              a
            
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                Q
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle Z(t){\stackrel {\mathrm {def} }{{}={}}}R_{\mathrm {a} }(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left(Q^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where

  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
   is the Laplace transform operator;

  
    
      
        
          
            F
          
        
      
    
    {\displaystyle {\mathcal {F}}}
   is the Fourier transform operator;
subscript ""a"" is the analytic representation operator;
Q ?1 is the convolution inverse of Q.
Acoustic resistance, denoted R, and acoustic reactance, denoted X, are the real part and imaginary part of acoustic impedance respectively:

  
    
      
        Z
        (
        s
        )
        =
        R
        (
        s
        )
        +
        i
        X
        (
        s
        )
        ,
      
    
    {\displaystyle Z(s)=R(s)+iX(s),}
  

  
    
      
        Z
        (
        ?
        )
        =
        R
        (
        ?
        )
        +
        i
        X
        (
        ?
        )
        ,
      
    
    {\displaystyle Z(\omega )=R(\omega )+iX(\omega ),}
  

  
    
      
        Z
        (
        t
        )
        =
        R
        (
        t
        )
        +
        i
        X
        (
        t
        )
        ,
      
    
    {\displaystyle Z(t)=R(t)+iX(t),}
  
where
i is the imaginary unit;
in Z(s), R(s) is not the Laplace transform of the time domain acoustic resistance R(t), Z(s) is;
in Z(?), R(?) is not the Fourier transform of the time domain acoustic resistance R(t), Z(?) is;
in Z(t), R(t) is the time domain acoustic resistance and X(t) is the Hilbert transform of the time domain acoustic resistance R(t), according to the definition of the analytic representation.
Inductive acoustic reactance, denoted XL, and capacitive acoustic reactance, denoted XC, are the positive part and negative part of acoustic reactance respectively:

  
    
      
        X
        (
        s
        )
        =
        
          X
          
            L
          
        
        (
        s
        )
        ?
        
          X
          
            C
          
        
        (
        s
        )
        ,
      
    
    {\displaystyle X(s)=X_{L}(s)-X_{C}(s),}
  

  
    
      
        X
        (
        ?
        )
        =
        
          X
          
            L
          
        
        (
        ?
        )
        ?
        
          X
          
            C
          
        
        (
        ?
        )
        ,
      
    
    {\displaystyle X(\omega )=X_{L}(\omega )-X_{C}(\omega ),}
  

  
    
      
        X
        (
        t
        )
        =
        
          X
          
            L
          
        
        (
        t
        )
        ?
        
          X
          
            C
          
        
        (
        t
        )
        .
      
    
    {\displaystyle X(t)=X_{L}(t)-X_{C}(t).}
  
Acoustic admittance, denoted Y, is the Laplace transform, or the Fourier transform, or the analytic representation of time domain acoustic conductance:

  
    
      
        Y
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        G
        ]
        (
        s
        )
        =
        
          
            1
            
              Z
              (
              s
              )
            
          
        
        =
        
          
            
              
                
                  L
                
              
              [
              Q
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle Y(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[G](s)={\frac {1}{Z(s)}}={\frac {{\mathcal {L}}[Q](s)}{{\mathcal {L}}[p](s)}},}
  

  
    
      
        Y
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        G
        ]
        (
        ?
        )
        =
        
          
            1
            
              Z
              (
              ?
              )
            
          
        
        =
        
          
            
              
                
                  F
                
              
              [
              Q
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle Y(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[G](\omega )={\frac {1}{Z(\omega )}}={\frac {{\mathcal {F}}[Q](\omega )}{{\mathcal {F}}[p](\omega )}},}
  

  
    
      
        Y
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          G
          
            
              a
            
          
        
        (
        t
        )
        =
        
          Z
          
            ?
            1
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            Q
            
              
                a
              
            
          
          ?
          
            
              (
              
                p
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle Y(t){\stackrel {\mathrm {def} }{{}={}}}G_{\mathrm {a} }(t)=Z^{-1}(t)={\frac {1}{2}}\!\left[Q_{\mathrm {a} }*\left(p^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where
Z ?1 is the convolution inverse of Z;
p ?1 is the convolution inverse of p.
Acoustic conductance, denoted G, and acoustic susceptance, denoted B, are the real part and imaginary part of acoustic admittance respectively:

  
    
      
        Y
        (
        s
        )
        =
        G
        (
        s
        )
        +
        i
        B
        (
        s
        )
        ,
      
    
    {\displaystyle Y(s)=G(s)+iB(s),}
  

  
    
      
        Y
        (
        ?
        )
        =
        G
        (
        ?
        )
        +
        i
        B
        (
        ?
        )
        ,
      
    
    {\displaystyle Y(\omega )=G(\omega )+iB(\omega ),}
  

  
    
      
        Y
        (
        t
        )
        =
        G
        (
        t
        )
        +
        i
        B
        (
        t
        )
        ,
      
    
    {\displaystyle Y(t)=G(t)+iB(t),}
  
where
in Y(s), G(s) is not the Laplace transform of the time domain acoustic conductance G(t), Y(s) is;
in Y(?), G(?) is not the Fourier transform of the time domain acoustic conductance G(t), Y(?) is;
in Y(t), G(t) is the time domain acoustic conductance and B(t) is the Hilbert transform of the time domain acoustic conductance G(t), according to the definition of the analytic representation.
Acoustic resistance represents the energy transfer of an acoustic wave. The pressure and motion are in phase, so work is done on the medium ahead of the wave.
Acoustic reactance represents, as well, the pressure that is out of phase with the motion and causes no average energy transfer. For example, a closed bulb connected to an organ pipe will have air moving into it and pressure, but they are out of phase so no net energy is transmitted into it. While the pressure rises, air moves in, and while it falls, it moves out, but the average pressure when the air moves in is the same as that when it moves out, so the power flows back and forth but with no time averaged energy transfer. The electrical analogy for this is a capacitor connected across a power line. Current flows through the capacitor but it is out of phase with the voltage, so no net power is transmitted into it.

Specific acoustic impedance
For a linear time-invariant system, the relationship between the acoustic pressure applied to the system and the resulting particle velocity in the direction of that pressure at its point of application is given by

  
    
      
        p
        (
        t
        )
        =
        [
        r
        ?
        v
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle p(t)=[r*v](t),}
  
or equivalently by:

  
    
      
        v
        (
        t
        )
        =
        [
        g
        ?
        p
        ]
        (
        t
        )
        ,
      
    
    {\displaystyle v(t)=[g*p](t),}
  
where
p is the acoustic pressure;
v is the particle velocity;
r is the specific acoustic resistance in the time domain;
g = r ?1 is the specific acoustic conductance in the time domain (r ?1 is the convolution inverse of r).
Specific acoustic impedance, denoted z is the Laplace transform, or the Fourier transform, or the analytic representation of time domain specific acoustic resistance:

  
    
      
        z
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        r
        ]
        (
        s
        )
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              v
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle z(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[r](s)={\frac {{\mathcal {L}}[p](s)}{{\mathcal {L}}[v](s)}},}
  

  
    
      
        z
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        r
        ]
        (
        ?
        )
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              v
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle z(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[r](\omega )={\frac {{\mathcal {F}}[p](\omega )}{{\mathcal {F}}[v](\omega )}},}
  

  
    
      
        z
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          r
          
            
              a
            
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                v
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle z(t){\stackrel {\mathrm {def} }{{}={}}}r_{\mathrm {a} }(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left(v^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where v ?1 is the convolution inverse of v.
Specific acoustic resistance, denoted r, and specific acoustic reactance, denoted x, are the real part and imaginary part of specific acoustic impedance respectively:

  
    
      
        z
        (
        s
        )
        =
        r
        (
        s
        )
        +
        i
        x
        (
        s
        )
        ,
      
    
    {\displaystyle z(s)=r(s)+ix(s),}
  

  
    
      
        z
        (
        ?
        )
        =
        r
        (
        ?
        )
        +
        i
        x
        (
        ?
        )
        ,
      
    
    {\displaystyle z(\omega )=r(\omega )+ix(\omega ),}
  

  
    
      
        z
        (
        t
        )
        =
        r
        (
        t
        )
        +
        i
        x
        (
        t
        )
        ,
      
    
    {\displaystyle z(t)=r(t)+ix(t),}
  
where
in z(s), r(s) is not the Laplace transform of the time domain specific acoustic resistance r(t), z(s) is;
in z(?), r(?) is not the Fourier transform of the time domain specific acoustic resistance r(t), z(?) is;
in z(t), r(t) is the time domain specific acoustic resistance and x(t) is the Hilbert transform of the time domain specific acoustic resistance r(t), according to the definition of the analytic representation.
Specific inductive acoustic reactance, denoted xL, and specific capacitive acoustic reactance, denoted xC, are the positive part and negative part of specific acoustic reactance respectively:

  
    
      
        x
        (
        s
        )
        =
        
          x
          
            L
          
        
        (
        s
        )
        ?
        
          x
          
            C
          
        
        (
        s
        )
        ,
      
    
    {\displaystyle x(s)=x_{L}(s)-x_{C}(s),}
  

  
    
      
        x
        (
        ?
        )
        =
        
          x
          
            L
          
        
        (
        ?
        )
        ?
        
          x
          
            C
          
        
        (
        ?
        )
        ,
      
    
    {\displaystyle x(\omega )=x_{L}(\omega )-x_{C}(\omega ),}
  

  
    
      
        x
        (
        t
        )
        =
        
          x
          
            L
          
        
        (
        t
        )
        ?
        
          x
          
            C
          
        
        (
        t
        )
        .
      
    
    {\displaystyle x(t)=x_{L}(t)-x_{C}(t).}
  
Specific acoustic admittance, denoted y, is the Laplace transform, or the Fourier transform, or the analytic representation of time domain specific acoustic conductance:

  
    
      
        y
        (
        s
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            L
          
        
        [
        g
        ]
        (
        s
        )
        =
        
          
            1
            
              z
              (
              s
              )
            
          
        
        =
        
          
            
              
                
                  L
                
              
              [
              v
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
          
        
        ,
      
    
    {\displaystyle y(s){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {L}}[g](s)={\frac {1}{z(s)}}={\frac {{\mathcal {L}}[v](s)}{{\mathcal {L}}[p](s)}},}
  

  
    
      
        y
        (
        ?
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          
            F
          
        
        [
        g
        ]
        (
        ?
        )
        =
        
          
            1
            
              z
              (
              ?
              )
            
          
        
        =
        
          
            
              
                
                  F
                
              
              [
              v
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
          
        
        ,
      
    
    {\displaystyle y(\omega ){\stackrel {\mathrm {def} }{{}={}}}{\mathcal {F}}[g](\omega )={\frac {1}{z(\omega )}}={\frac {{\mathcal {F}}[v](\omega )}{{\mathcal {F}}[p](\omega )}},}
  

  
    
      
        y
        (
        t
        )
        
          
            
              
                

                
                =
                

                
              
              
                
                  d
                  e
                  f
                
              
            
          
        
        
          g
          
            
              a
            
          
        
        (
        t
        )
        =
        
          z
          
            ?
            1
          
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            v
            
              
                a
              
            
          
          ?
          
            
              (
              
                p
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        ,
      
    
    {\displaystyle y(t){\stackrel {\mathrm {def} }{{}={}}}g_{\mathrm {a} }(t)=z^{-1}(t)={\frac {1}{2}}\!\left[v_{\mathrm {a} }*\left(p^{-1}\right)_{\mathrm {a} }\right]\!(t),}
  
where
z ?1 is the convolution inverse of z;
p ?1 is the convolution inverse of p.
Specific acoustic conductance, denoted g, and specific acoustic susceptance, denoted b, are the real part and imaginary part of specific acoustic admittance respectively:

  
    
      
        y
        (
        s
        )
        =
        g
        (
        s
        )
        +
        i
        b
        (
        s
        )
        ,
      
    
    {\displaystyle y(s)=g(s)+ib(s),}
  

  
    
      
        y
        (
        ?
        )
        =
        g
        (
        ?
        )
        +
        i
        b
        (
        ?
        )
        ,
      
    
    {\displaystyle y(\omega )=g(\omega )+ib(\omega ),}
  

  
    
      
        y
        (
        t
        )
        =
        g
        (
        t
        )
        +
        i
        b
        (
        t
        )
        ,
      
    
    {\displaystyle y(t)=g(t)+ib(t),}
  
where
in y(s), g(s) is not the Laplace transform of the time domain acoustic conductance g(t), y(s) is;
in y(?), g(?) is not the Fourier transform of the time domain acoustic conductance g(t), y(?) is;
in y(t), g(t) is the time domain acoustic conductance and b(t) is the Hilbert transform of the time domain acoustic conductance g(t), according to the definition of the analytic representation.
Specific acoustic impedance z is an intensive property of a particular medium: for instance, the z of air or of water can be specified. Whereas acoustic impedance Z is an extensive property of a particular medium and geometry: for instance, the Z of a particular duct filled with air can be discussed.

Relationship
A one dimensional wave passing through an aperture with area A is now considered. The acoustic volume flow rate Q is the volume of medium passing per second through the aperture. If the acoustic flow moves a distance dx = v dt, then the volume of medium passing through is dV = A dx, so

  
    
      
        Q
        =
        
          
            
              
                d
              
              V
            
            
              
                d
              
              t
            
          
        
        =
        A
        
          
            
              
                d
              
              x
            
            
              
                d
              
              t
            
          
        
        =
        A
        v
        .
      
    
    {\displaystyle Q={\frac {\mathrm {d} V}{\mathrm {d} t}}=A{\frac {\mathrm {d} x}{\mathrm {d} t}}=Av.}
  
Provided that the wave is only one-dimensional, it yields

  
    
      
        Z
        (
        s
        )
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              
                
                  L
                
              
              [
              Q
              ]
              (
              s
              )
            
          
        
        =
        
          
            
              
                
                  L
                
              
              [
              p
              ]
              (
              s
              )
            
            
              A
              
                
                  L
                
              
              [
              v
              ]
              (
              s
              )
            
          
        
        =
        
          
            
              z
              (
              s
              )
            
            A
          
        
        ,
      
    
    {\displaystyle Z(s)={\frac {{\mathcal {L}}[p](s)}{{\mathcal {L}}[Q](s)}}={\frac {{\mathcal {L}}[p](s)}{A{\mathcal {L}}[v](s)}}={\frac {z(s)}{A}},}
  

  
    
      
        Z
        (
        ?
        )
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              
                
                  F
                
              
              [
              Q
              ]
              (
              ?
              )
            
          
        
        =
        
          
            
              
                
                  F
                
              
              [
              p
              ]
              (
              ?
              )
            
            
              A
              
                
                  F
                
              
              [
              v
              ]
              (
              ?
              )
            
          
        
        =
        
          
            
              z
              (
              ?
              )
            
            A
          
        
        ,
      
    
    {\displaystyle Z(\omega )={\frac {{\mathcal {F}}[p](\omega )}{{\mathcal {F}}[Q](\omega )}}={\frac {{\mathcal {F}}[p](\omega )}{A{\mathcal {F}}[v](\omega )}}={\frac {z(\omega )}{A}},}
  

  
    
      
        Z
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                Q
                
                  ?
                  1
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        =
        
          
            1
            2
          
        
        
        
          [
          
            p
            
              
                a
              
            
          
          ?
          
            
              (
              
                
                  
                    v
                    
                      ?
                      1
                    
                  
                  A
                
              
              )
            
            
              
                a
              
            
          
          ]
        
        
        (
        t
        )
        =
        
          
            
              z
              (
              t
              )
            
            A
          
        
        .
      
    
    {\displaystyle Z(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left(Q^{-1}\right)_{\mathrm {a} }\right]\!(t)={\frac {1}{2}}\!\left[p_{\mathrm {a} }*\left({\frac {v^{-1}}{A}}\right)_{\mathrm {a} }\right]\!(t)={\frac {z(t)}{A}}.}

Characteristic acoustic impe",Category:Physical quantities,3
81,82,Category:Sound technology,,Category:Sound,3
82,83,Sound collage,"In music, montage (literally ""putting together"") or sound collage (""gluing together"") is a technique where newly branded sound objects or compositions, including songs, are created from collage, also known as montage. This is often done through the use of sampling, while some playable sound collages were produced by gluing together sectors of different vinyl records. In any case, it may be achieved through the use of previous sound recordings or musical scores. Like its visual cousin, the collage work may have a completely different effect than that of the component parts, even if the original parts are completely recognizable or from only one source.

History
The origin of sound collage can be traced back to the works of Biber's programmatic sonata Battalia (1673) and Mozart's Don Giovanni (1789), and some critics have described certain passages in Mahler symphonies as collage, but the first fully developed collages occur in a few works by Charles Ives, whose piece Central Park in the Dark, composed in 1906, creates the feeling of a walk in the city by layering several distinct melodies and quotations on top of each other. Thus, the use of collage in music actually predates its use in painting by artists like Picasso and Braque, who are generally credited with creating the first collage paintings around 1912.
Earlier traditional forms and procedures such as the quodlibet, medley, potpourri, and centonization differ from collage in that the various elements in them are made to fit smoothly together, whereas in a collage clashes of key, timbre, texture, meter, tempo, or other discrepancies are important in helping to preserve the individuality of the constituent elements and to convey the impression of a heterogeneous assemblage. What made their technique true collage, however, was the juxtaposition of quotations and unrelated melodies, either by layering them or by moving between them in quick succession, as in a film montage sequence.
The first documented instance of sound collage created by electronic means is the piece ""Wochenende"" (in English, ""Weekend""), a collage of words, music and sounds created by film-maker and media artist Walter Ruttmann in 1928. Later, in 1948, Pierre Schaeffer used the techniques of sound collage to create the first piece of musique concrète, ""Étude aux chemins de fer"", which was assembled from recordings of trains. Schaeffer created this piece by recording sounds of trains onto several vinyl records, some of which had lock grooves allowing them to play in a continuous loop. He then set up multiple turntables in his studio, allowing him to trigger and mix together the various train sounds as needed.
Today audio collage may be thought of as Fluxus postmodern and a form of digital art. George Rochberg is an artist well known for his use of collage in pieces including Contra Mortem et Tempus and Symphony No. 3.

Micromontage
Micromontage is the use of montage on the time scale of microsounds. Its primary proponent is composer Horacio Vaggione in works such as Octuor (1982), Thema (1985, Wergo 2026-2), and Schall (1995, Mnémosyne Musique Média LDC 278–1102). The technique may include the extraction and arrangement of sound particles from a sample or the creation and exact placement of each particle to create complex sound patterns or singular particles (transients). It may be accomplished through graphic editing, a script, or automated through a computer program.
Regardless, digital micromontage requires:
creation or compilation of a library of sound files on several different time scales
importation into the library of the editing and mixing program
use of the cursor, script, or algorithm to position each sound at a specific time-point or time-points
editing of the duration, amplitude, and spatial positions of all sounds (possibly done by a script or algorithm)
Granular synthesis incorporates many of the techniques of micromontage, though granular synthesis is inevitably automated while micromontage may be realized directly, point by point. ""It therefore demands unusual patience"" and may be compared to the pointillistic paintings of Georges Seurat.

See also
Musique concrète
Detournement
Mashup
Remix
Sampling (music)
Some Assembly Required (radio program)
WhoSampled

Sources
Further reading
Joline Blais, and Jon Ippolito. At the Edge of Art. London: Thames & Hudson Ltd, 2006.
Buci-Glucksmann, Christine. ""L’art à l’époque virtuel"". In Frontières esthétiques de l’art, Arts 8,. Paris: L’Harmattan, 2004.
Couchot, Edmond. Des Images, du temps et des machines, dans les arts et la communication. [Nîmes]: J. Chambon, 2007. ISBN 2-7427-6940-4.
Forest, Fred. Art et Internet. Paris: Editions Cercle D'Art / Imaginaire Mode d'Emploi, 2008. ISBN 978-2-7022-0864-9.
Liu, Alan. The Laws of Cool: Knowledge, Work, and the Culture of Information. Chicago: University of Chicago Press, 2004.
Lovejoy, Margot. Digital Currents: Art in the Electronic Age. London: Routledge, 2004.
Paul, Christiane. Digital Art. London and New York: Thames & Hudson Ltd, 2003. ISBN 0-500-20367-9.
Popper, Frank. From Technological to Virtual Art. Leonardo (Series). Cambridge: MIT Press, 2007. ISBN 0-262-16230-X.
Taylor, Brandon. Collage. London: Thames & Hudson Ltd, 2006.
Wands, Bruce. Art of the Digital Age. London and New York: Thames & Hudson, 2006. ISBN 0-500-23817-0 (hbk.), ISBN 0-500-28629-9 (pbk.)",Category:Wikipedia articles needing page number citations from March 2010,3
83,84,Rarefaction,"Rarefaction is the reduction of an item's density, the opposite of compression. Like compression, which can travel in waves (sound waves, for instance), rarefaction waves also exist in nature. A common rarefaction wave is the area of low relative pressure following a shock wave (see picture).
Rarefaction waves expand with time (much like sea waves spread out as they reach a beach); in most cases rarefaction waves keep the same overall profile ('shape') at all times throughout the wave's movement: it is a self-similar expansion. Each part of the wave travels at the local speed of sound, in the local medium. This expansion behaviour is in contrast to the behaviour of pressure increases, which gets narrower with time, until they steepen into shock waves.

Physical examples
A natural example of rarefaction occurs in the layers of Earth's atmosphere. Because the atmosphere has mass, most atmospheric matter is nearer to the Earth due to the Earth's gravitation. Therefore, air at higher layers of the atmosphere is less dense, or rarefied, relative to air at lower layers. Thus rarefaction can refer either to a reduction in density over space at a single point of time, or a reduction of density over time for one particular area.
Rarefaction can be easily observed by compressing a spring and releasing it. Instead of seeing compressed loops seeming to move through the spring, spaced-out loops move through it: rarefaction waves.

Rarefaction in manufacturing
Modern construction of guitars is an example of using rarefaction in manufacturing. By forcing the reduction of density (loss of oils and other impurities) in the cellular structure of the soundboard, a rarefied guitar top produces a tonal decompression affecting the sound of the instrument, mimicking aged wood.

See also
Longitudinal wave
P-wave
Prandtl-Meyer expansion fan


== Citations ==",Category:Sound,3
84,85,High fidelity,"High fidelity (often shortened to hi-fi or hifi) reproduction is a term used by home stereo listeners, audiophiles and home audio enthusiasts to refer to high-quality reproduction of sound to distinguish it from the lower quality sound produced by inexpensive audio equipment, or the inferior quality of sound reproduction that can be heard in recordings made until the late 1940s.
Ideally, high-fidelity equipment has inaudible noise and distortion, and a flat (neutral, uncolored) frequency response within the intended frequency range.

History
Bell Laboratories began experimenting with a range of recording techniques in the early 1930s. Performances by Leopold Stokowski and the Philadelphia Orchestra were recorded in 1931 and 1932 using telephone lines between the Academy of Music in Philadelphia and the Bell labs in New Jersey. Some multitrack recordings were made on optical sound film, which led to new advances used primarily by MGM (as early as 1937) and 20th Century-Fox Film Corporation (as early as 1941). RCA Victor began recording performances by several orchestras using optical sound around 1941, resulting in higher-fidelity masters for 78-rpm discs. During the 1930s, Avery Fisher, an amateur violinist, began experimenting with audio design and acoustics. He wanted to make a radio that would sound like he was listening to a live orchestra—that would achieve high fidelity to the original sound. After World War II, Harry F. Olson conducted an experiment whereby test subjects listened to a live orchestra through a hidden variable acoustic filter. The results proved that listeners preferred high fidelity reproduction, once the noise and distortion introduced by early sound equipment was removed.
Beginning in 1948, several innovations created the conditions that made for major improvements of home-audio quality possible:
Reel-to-reel audio tape recording, based on technology taken from Germany after WWII, helped musical artists such as Bing Crosby make and distribute recordings with better fidelity.
The advent of the 33? rpm Long Play (LP) microgroove vinyl record, with lower surface noise and quantitatively specified equalization curves as well as noise-reduction and dynamic range systems. Classical music fans, who were opinion leaders in the audio market, quickly adopted LPs because, unlike with older records, most classical works would fit on a single LP.
FM radio, with wider audio bandwidth and less susceptibility to signal interference and fading than AM radio.
Better amplifier designs, with more attention to frequency response and much higher power output capability, reproducing audio without perceptible distortion.
New loudspeaker designs, including acoustic suspension, developed by Edgar Villchur and Henry Kloss improved bass frequency response.
In the 1950s, audio manufacturers employed the phrase high fidelity as a marketing term to describe records and equipment intended to provide faithful sound reproduction. While some consumers simply interpreted high fidelity as fancy and expensive equipment, many found the difference in quality between ""hi-fi"" and the then standard AM radios and 78 rpm records readily apparent and bought 33? LPs such as RCA's New Orthophonics and London's ffrr (Full Frequency Range Recording, a UK Decca system); and high-fidelity phonographs. Audiophiles paid attention to technical characteristics and bought individual components, such as separate turntables, radio tuners, preamplifiers, power amplifiers and loudspeakers. Some enthusiasts even assembled their own loudspeaker systems. In the 1950s, hi-fi became a generic term for home sound equipment, to some extent displacing phonograph and record player.
In the late 1950s and early 1960s, the development of the Westrex single-groove stereophonic record cutterhead led to the next wave of home-audio improvement, and in common parlance, stereo displaced hi-fi. Records were now played on a stereo. In the world of the audiophile, however, the concept of high fidelity continued to refer to the goal of highly accurate sound reproduction and to the technological resources available for approaching that goal. This period is regarded as the ""Golden Age of Hi-Fi"", when vacuum tube equipment manufacturers of the time produced many models considered endearing by modern audiophiles, and just before solid state (transistorized) equipment was introduced to the market, subsequently replacing tube equipment as the mainstream technology.
A popular type of system for reproducing music beginning in the 1970s was the integrated music centre—which combined a phonograph turntable, AM-FM radio tuner, tape player, preamplifier, and power amplifier in one package, often sold with its own separate, detachable or integrated speakers. These systems advertised their simplicity. The consumer did not have to select and assemble individual components, or be familiar with impedance and power ratings. Purists generally avoid referring to these systems as high fidelity, though some are capable of very good quality sound reproduction. Audiophiles in the 1970s and 1980s preferred to buy each component separately. That way, they could choose models of each component with the specifications that they desired. In the 1980s, a number of audiophile magazines became available, offering reviews of components and articles on how to choose and test speakers, amplifiers and other components.

Listening tests
Listening tests are used by hi-fi manufacturers, audiophile magazines and audio engineering researchers and scientists. If a listening test is done in such a way that the listener who is assessing the sound quality of a component or recording can see the components that are being used for the test (e.g., the same musical piece listened to through a tube power amplifier and a solid state amplifier), then it is possible that the listener's pre-existing biases towards or against certain components or brands could affect their judgment. To respond to this issue, researchers began to use blind tests, in which the researchers can see the components being tested, but not the listeners undergoing the experiments. In a double-blind experiment, neither the listeners nor the researchers know who belongs to the control group and the experimental group, or which type of audio component is being used for which listening sample. Only after all the data has been recorded (and in some cases, analyzed) do the researchers learn which components or recordings were preferred by the listeners. A commonly used variant of this test is the ABX test. A subject is presented with two known samples (sample A, the reference, and sample B, an alternative), and one unknown sample X, for three samples total. X is randomly selected from A and B, and the subject identifies X as being either A or B. Although there is no way to prove that a certain methodology is transparent, a properly conducted double-blind test can prove that a method is not transparent.
Scientific double-blind tests are sometimes used as part of attempts to ascertain whether certain audio components (such as expensive, exotic cables) have any subjectively perceivable effect on sound quality. Data gleaned from these double-blind tests is not accepted by some ""audiophile"" magazines such as Stereophile and The Absolute Sound in their evaluations of audio equipment. John Atkinson, current editor of Stereophile, stated (in a 2005 July editorial named Blind Tests & Bus Stops) that he once purchased a solid-state amplifier, the Quad 405, in 1978 after seeing the results from blind tests, but came to realize months later that ""the magic was gone"" until he replaced it with a tube amp. Robert Harley of The Absolute Sound wrote, in a 2008 editorial (on Issue 183), that: ""...blind listening tests fundamentally distort the listening process and are worthless in determining the audibility of a certain phenomenon.""
Doug Schneider, editor of the online Soundstage network, refuted this position with two editorials in 2009. He stated: ""Blind tests are at the core of the decades' worth of research into loudspeaker design done at Canada's National Research Council (NRC). The NRC researchers knew that for their result to be credible within the scientific community and to have the most meaningful results, they had to eliminate bias, and blind testing was the only way to do so."" Many Canadian companies such as Axiom, Energy, Mirage, Paradigm, PSB and Revel use blind testing extensively in designing their loudspeakers. Audio professional Dr. Sean Olive of Harman International shares this view.

Semblance of realism
Stereophonic sound provided a partial solution to the problem of creating the illusion of live orchestral performers by creating a phantom middle channel when the listener sits exactly in the middle of the two front loudspeakers. When the listener moves slightly to the side, however, this phantom channel disappears or is greatly reduced. An attempt to provide for the reproduction of the reverberation was tried in the 1970s through quadraphonic sound but, again, the technology at that time was insufficient for the task. Consumers did not want to pay the additional costs and space required for the marginal improvements in realism. With the rise in popularity of home theater, however, multi-channel playback systems became affordable, and many consumers were willing to tolerate the six to eight channels required in a home theater. The advances made in signal processors to synthesize an approximation of a good concert hall can now provide a somewhat more realistic illusion of listening in a concert hall.
In addition to spatial realism, the playback of music must be subjectively free from noise, such as hiss or hum, to achieve realism. The compact disc (CD) provides about 90 decibels of dynamic range, which exceeds the 80 dB dynamic range of music as normally perceived in a concert hall. Audio equipment must be able to reproduce frequencies high enough and low enough to be realistic. The human hearing range, for healthy young persons, is 20 Hz to 20,000 Hz.  Most adults can't hear higher than 15 kHz. CDs are capable of reproducing frequencies as low as 10 Hz and as high as 22.05 kHz, making them adequate for reproducing the frequency range that most humans can hear. The equipment must also provide no noticeable distortion of the signal or emphasis or de-emphasis of any frequency in this frequency range.

Modularity
Integrated, mini, or lifestyle systems, also known as music centres or minisystems, contain one or more sources such as a CD player, a tuner, or a cassette deck together with a preamplifier and a power amplifier in one box. Although some high-end manufacturers do produce integrated systems, such products are generally disparaged by audiophiles, who prefer to build a system from separates (or components), often with each item from a different manufacturer specialising in a particular component. This provides the most flexibility for piece-by-piece upgrades and repairs.
For slightly less flexibility in upgrades, a preamplifier and a power amplifier in one box is called an integrated amplifier; with a tuner, it is a receiver. A monophonic power amplifier, which is called a monoblock, is often used for powering a subwoofer. Other modules in the system may include components like cartridges, tonearms, hi-fi turntables, Digital Media Players, digital audio players, DVD players that play a wide variety of discs including CDs, CD recorders, MiniDisc recorders, hi-fi videocassette recorders (VCRs) and reel-to-reel tape recorders. Signal modification equipment can include equalizers and signal processors.
This modularity allows the enthusiast to spend as little or as much as they want on a component that suits their specific needs. In a system built from separates, sometimes a failure on one component still allows partial use of the rest of the system. A repair of an integrated system, though, means complete lack of use of the system. Another advantage of modularity is the ability to spend money on only a few core components at first and then later add additional components to the system. Some of the disadvantages of this approach are increased cost, complexity, and space required for the components.

Modern equipment
In the 2000s, modern hi-fi equipment can include signal sources such as digital audio tape (DAT), digital audio broadcasting (DAB) or HD Radio tuners. Some modern hi-fi equipment can be digitally connected using fibre optic TOSLINK cables, universal serial bus (USB) ports (including one to play digital audio files), or Wi-Fi support. Another modern component is the music server consisting of one or more computer hard drives that hold music in the form of computer files. When the music is stored in an audio file format that is lossless such as FLAC, Monkey's Audio or WMA Lossless, the computer playback of recorded audio can serve as an audiophile-quality source for a hi-fi system.

See also
References
Further reading
Pier Paolo Ferrari, (2016),The Hi-Fi's Golden Age, 1st edition, Bergamo, Italy: Sandit – ISBN 978-88-6928-171-6
Janet Borgerson and Jonathan Schroeder, (2017), Designed for Hi-Fi Living: The Vinyl LP in MIdcentury America. Cambridge, MA: MIT Press – ISBN 9780262036238

External links
HiFi at Curlie (based on DMOZ)
A Dictionary of Home Entertainment Terms",Category:Audio engineering,3
85,86,Soundscape,"The soundscape is the component of the acoustic environment that can be perceived by humans. There is a varied history of the use of soundscape depending on discipline – ranging from urban design to wildlife ecology. An important distinction is to separate soundscape from the broader term acoustic environment. The acoustic environment is the combination of all the acoustic resources within a given area – natural sounds and human-caused sounds – as modified by the environment. The International Organization for Standardization (ISO) standardized these definitions in 2014.(ISO 12913-1:2014)

Historical context
The term soundscape was first noted by Michael Southworth in a 1969 article titled ""The Sonic Environment of Cities,"" published by Environment and Behavior, and fleshed out in more detail eight years later by Canadian composer and naturalist, R. Murray Schafer in his seminal work, ""Tuning of the World."" According to this author there are three main elements of the soundscape:
A soundscape is a sound or combination of sounds that forms or arises from an immersive environment. The study of soundscape is the subject of acoustic ecology or soundscape ecology. The idea of soundscape refers to both the natural acoustic environment, consisting of natural sounds, including animal vocalizations, the collective habitat expression of which is now referred to as the biophony, and, for instance, the sounds of weather and other natural elements, now referred to as the geophony; and environmental sounds created by humans, the anthropophony through a sub-set called controlled sound, such as musical composition, sound design, and language, work, and sounds of mechanical origin resulting from use of industrial technology. Crucially, the term soundscape also includes the listener's perception of sounds heard as an environment: ""how that environment is understood by those living within it""  and therefore mediates their relations. The disruption of these acoustic environments results in noise pollution.
The term ""soundscape"" can also refer to an audio recording or performance of sounds that create the sensation of experiencing a particular acoustic environment, or compositions created using the found sounds of an acoustic environment, either exclusively or in conjunction with musical performances.
Pauline Oliveros, composer of post-World War II electronic art music, defined the term ""soundscape"" as ""All of the waveforms faithfully transmitted to our audio cortex by the ear and its mechanisms"".
Keynote sounds
This is a musical term that identifies the key of a piece, not always audible ... the key might stray from the original, but it will return. The keynote sounds may not always be heard consciously, but they ""outline the character of the people living there"" (Schafer). They are created by nature (geography and climate): wind, water, forests, plains, birds, insects, animals. In many urban areas, traffic has become the keynote sound.
Sound signals
These are foreground sounds, which are listened to consciously; examples would be warning devices, bells, whistles, horns, sirens, etc.
Soundmark
This is derived from the term landmark. A soundmark is a sound which is unique to an area. In his 1993 book, The Soundscape: Our Sonic Environment and the Tuning of the World, Schafer wrote, ""Once a Soundmark has been identified, it deserves to be protected, for soundmarks make the acoustic life of a community unique.""
The elements have been further defined as to essential sources:
Bernie Krause, naturalist and soundscape ecologist, redefined the sources of sound in terms of their three main components: geophony, biophony, and anthropophony.
Geophony
Consisting of the prefix, geo (gr. earth), and phon (gr. sound), this refers to the soundscape sources that are generated by non-biological natural sources such as wind in the trees, water in a stream or waves at the ocean, and earth movement, the first sounds heard on earth by any sound-sentient organism.
Biophony
Consisting of the prefix, bio (gr. life) and the suffix for sound, this term refers to all of the non-human, non-domestic biological soundscape sources of sound.
Anthropophony
Consisting of the prefix, anthro (gr. human), this term refers to all of the sound signatures generated by humans.

In music
In music, soundscape compositions are often a form of electronic music, or electroacoustic music. Composers who use soundscapes include real-time granular synthesis pioneer Barry Truax, Hildegard Westerkamp, and Luc Ferrari, whose Presque rien, numéro 1 (1970) is an early soundscape composition. Soundscape composer Petri Kuljuntausta has created soundscape compositions from the sounds of sky dome and Aurora Borealis and deep sea underwater recordings, and a work entitled ""Charm of Sound"" to be performed at the extreme environment of Saturn's moon Titan. The work landed on the ground of Titan in 2005 after traveling inside the spacecraft Huygens over seven years and four billion kilometres through space.
Irv Teibel's Environments series (1969–79) consisted of 30-minute, uninterrupted environmental soundscapes and synthesized or processed versions of natural sound.
Music soundscapes can also be generated by automated software methods, such as the experimental TAPESTREA application, a framework for sound design and soundscape composition, and others.
The soundscape is often the subject of mimicry in Timbre-centered music such as Tuvan throat singing. The process of Timbral Listening is used to interpret the timbre of the soundscape. This timbre is mimicked and reproduced using the voice or rich harmonic producing instruments.

In United States National Parks
The National Park Service Natural Sounds and Night Skies Division actively protects the soundscapes and acoustic environments in national parks across the country. It is important to distinguish and define certain key terms as used by the National Park Service. Acoustic resources are physical sound sources, including both natural sounds (wind, water, wildlife, vegetation) and cultural and historic sounds (battle reenactments, tribal ceremonies, quiet reverence). The acoustic environment is the combination of all the acoustic resources within a given area – natural sounds and human-caused sounds – as modified by the environment. The acoustic environment includes sound vibrations made by geological processes, biological activity, and even sounds that are inaudible to most humans, such as bat echolocation calls. Soundscape is the component of the acoustic environment that can be perceived and comprehended by the humans. The character and quality of the soundscape influence human perceptions of an area, providing a sense of place that differentiates it from other regions. Noise refers to sound which is unwanted, either because of its effects on humans and wildlife, or its interference with the perception or detection of other sounds. Cultural soundscapes include opportunities for appropriate transmission of cultural and historic sounds that are fundamental components of the purposes and values for which the parks were established.
Sounds recorded in national parks
Yellowstone National Park Sound Library

Soundscapes and the environment
There are two distinct soundscapes, either hi-fi or lo-fi, created by the environment. A hi-fi system possesses a positive signal-to-noise ratio. These settings make it possible for discrete sounds to be heard clearly since there is no background noise to obstruct even the smallest disturbance. A rural landscape offers more hi-fi frequencies than a city because the natural landscape creates an opportunity to hear incidences from nearby and afar. In a lo-fi soundscape, signals are obscured by too many sounds, and perspective is lost within the broad- band of noises. In lo-fi soundscapes everything is very close and compact. A person can only listen to immediate encounters; in most cases even ordinary sounds have to be exuberantly amplified in order to be heard.
All sounds are unique in nature. They occur at one time in one place and can't be replicated. In fact, it is physically impossible for nature to reproduce any phoneme twice in exactly the same manner.

In health care
Soundscapes from a computerized acoustic device with a camera may also offer synthetic vision to the blind, utilizing human echolocation, as is the goal of the seeing with sound project.

Noise pollution
Papers on noise pollution are increasingly taking a holistic, soundscape approach to noise control. Whereas acoustics tends to rely on lab measurements and individual acoustic characteristics of cars and so on, soundscape takes a top-down approach. Drawing on John Cage's ideas of the whole world as composition, soundscape researchers investigate people's attitudes to soundscapes as a whole rather than individual aspects – and look at how the entire environment can be changed to be more pleasing to the ear.
It has been suggested that people's opportunity to access quiet, natural places in urban areas can be enhanced by improving the ecological quality of urban green spaces through targeted planning and design and that in turn has psychological benefits.
Soundscaping as a method to reducing noise pollution incorporates natural elements rather than just man made elements.

See also
References
Further reading
1969 The New Soundscape - R. Murray Schafer
1977 The Tuning of the World - R. Murray Schafer (ISBN 0-394-40966-3)
These 2 works were adapted to become part of the 1993 book, The Soundscape: Our Sonic Environment and the Tuning of the World - R. Murray Schafer (ISBN 0-89281-455-1)

1977 Five village soundscapes (Music of the environment series) - A.R.C. Publications (ISBN 0-88985-005-4)
1978 Handbook for Acoustic Ecology - Barry Truax (ISBN 0-88985-011-9)
1985 Acoustic Communication : Second Edition - Barry Truax & World Soundscape Project (ISBN 1-56750-537-6
1994 Soundscapes: Essays on Vroom and Moo, Eds: Jarviluoma, Helmi - Department of Folk Tradition
2002/2016 Wild Soundscapes: Discovering the Voice of the Natural World - Bernie Krause (Yale University Press, ISBN 0300218192) - book & QR link to audio
2002 Linking Soundscape Composition and Acoustic Ecology - Hildegard Westerkamp:
2003 Site Soundscapes: Landscape architecture in the light of sound - Sonotope Design Strategies, Per Hedfors (Diss.: ISSN 1401-6249 ISBN 91-576-6425-0 [1] Swedish University of Agricultural Sciences. Diss. summary: ISBN 978-3-639-09413-8
2004?""Voicescapes: The (en)chanting voice & its performance soundscapes"" in Soundscape: The Journal of Acoustic Ecology Vol.5 No.2 - Henry Johnson 26-29 ISSN 1607-3304
2004 The Auditory Culture Reader (Sensory Formations) - Michael Bull (ISBN 1-85973-618-1)
2005?""Acoustic Ecology Considered as a Connotation: Semiotic, Post-Colonial and Educational Views of Soundscape"" in Soundscape: The Journal of Acoustic Ecology Vol.6 No.2 - Tadahiko Imada 13-17 (ISSN 1607-3304)
2006 Qualitative Judgements of Urban Soundscapes: Questionning Questionnaires and Semantic Scales - Raimbault, Manon, Acta Acustica united with Acustica 92(6), 929–937
""Soundscapes / Paesaggi sonori"". lo Squaderno (10). December 2008. ISSN 1973-9141. Retrieved 2009-03-14. 
2006, ""Gebiete, Schichten und Klanglandschaften in den Alpen. Zum Gebrauch einiger historischer Begriffe aus der Musikethnologie"", Marcello Sorce Keller, in T. Nussbaumer (ed.), Volksmusik in den Alpen: Interkulturelle Horizonte und Crossovers, Salzburg, Verlag Mueller-Speiser, 2006, pp. 9–18.
2006 The West Meets the East in Acoustic Ecology (Tadahiko Imada Kozo Hiramatsu et al. Eds), Japanese Association for Sound Ecology & Hirosaki University International Music Centre ISBN 4-9903332-1-7
2008 ""Soundscape, postcolonial and music education: Experiencing the earliest grain of the body and music"" - Tadahiko Imada in Music Education Policy and Implementation: International Perspectives (Chi Cheung Leung, Lai Chi Rita Yip and Tadahiko Imada Eds, Hirosaki University Press) ISBN 978-4-902774-39-9
2009 A Little Sound Education - R. Murray Schafer & Tadahiko Imada (Shunjusha, Tokyo) ISBN 978-4-393-93539-2
2012 The Great Animal Orchestra: Finding the Origins of Music in the World's Wild Places, Bernie Krause, Little Brown New York, ISBN 978-0-316-08687-5
2015 Voices of the Wild: Animal Songs, Human Din, and the Call to Save Natural Soundscapes - Bernie Krause (Yale University Press, ISBN 978-0-300-20631-9) - book & links to audio examples

External links
World Forum for Acoustic Ecology (WFAE)
Soundscape: The Journal of Acoustic Ecology, published by WFAE
How Sound Affects Us (8:18)—TED talk by Julian Treasure
Mailman, Joshua B. ""Seven Metaphors for (Music) Listening: DRAMaTIC"" in Journal of Sonic Studies v.2.
Napolisoundscape Urban Space Research Web archive of the audio mapping of the city of Naples",Category:Sound,3
86,87,Palinacousis,"Palinacousis is an auditory form of perseveration — continuing to hear a sound after the physical noise has disappeared. The condition is often associated with lesions of the temporal lobe.

See also
Earworm


== References ==",Category:Rare diseases,3
87,88,Sound studies,"Sound Studies is an interdisciplinary field that to date has focused largely on the emergence of the concept of ""sound"" in Western modernity, with an emphasis on the development of sound reproduction technologies. The field first emerged in venues like the journal Social Studies of Science by scholars working in Science and Technology Studies and Communication Studies; it has however greatly expanded and now includes a broad array of scholars working in music, anthropology, sound art, deaf studies, architecture, and many other fields besides. Important studies have focused on the idea of a ""soundscape,"" architectural acoustics, nature sounds, the history of aurality in Western philosophy and nineteenth-century Colombia, Islamic approaches to listening, the voice, studies of deafness, loudness, and related topics. A foundational text is Jonathan Sterne's 2003 book ""The Audible Past,"" though the field has retroactively taken as foundational two texts from 1977, Jacques Attali's ""Noise"" and R. Murray Schafer's ""The Soundscape.""
Initial work in the field was criticized for focusing mainly on white male inventors in Euro-America. Consequently the field is currently in a period of expansion, with important texts coming out in recent years on sound, listening, and hearing as they relate to blackness, gender, and colonialism.

Hearing and Listening
Two significant categories to what we hear and pay attention to are natural and technological sounds. According to R. Murray Schaeffer (through a survey of quotes in the literature), the proportion of nature sounds heard and noticed among European authors has decreased over the past two centuries from 43% to 20%, but not for North America, where it has stayed around 50%. Additionally, the proportion of technological sounds mentioned in literature has stayed around 35% for Europe, but decreased in North America. While technological increases have not been sonically noticed, the decrease in silence has been noticed, from 19% to 9%.
For the idea of listening, objects can be considered auditorily as compared to visually. The objects that are able to be experienced by sight and by sound can be thought of in a venn diagram, with mute and visible objects in the vision category, with aural and invisible objects in the sound category, and aural and visible objects in the overlapping category. Objects that do not fall into a specific category can be considered beyond the horizons of sound and sight. The common denominator for aural objects is movement.
Three modes of listening have been recognized; causal listening, semantic listening, and reduced listening. Causal listening, the most common, consists of listening in order to gather ideas about its source. Sound in this case is informational and can be used to recognize voices, determine distance, or understand differences between humans and machines. Semantic listening is when a sound is not only heard but also processed. When a sound is given meaning and context, as seen in speech and fluent dialogue. Reduced listening focuses on the traits of the sound itself regardless of cause and meaning.

Spaces, Sites and Scapes
Sound is heard through space. But this defining of sound and space is further nuanced by their interdependent existence, creation, and dissolution. This idea of the acoustic environment and its social inextricability has become a source of interest within the field of sound studies. Critical to this contemporary discussion of the symbiotic social space and sonic space is R. Murray Schafer’s concept of the soundscape. Schafer uses the term soundscape to describe ""a total appreciation of the sonic environment,"" and, through soundscape studies, attempts to more holistically understand ""the relationship between man and the sounds of his environment and what happens when those sounds change,"". In understanding the environment as events being heard, the soundscape is indicative of the social conditions and characteristics that create it. In industrialized cities, the soundscape is industrial noises, in a rainforest the soundscape is the sound of nature, and in an empty space the soundscape is silence. Moreover, the soundscape is argued to foretell future societal trends. The soundscape is not just representative of the environment which surrounds it but it makes up its very essence. The soundscape is the environment on a wavelength that is auditory rather than tactile or visible, but very much as real.
Schaffer’s concept of the soundscape has become a hallmark of sound studies and is referenced, built upon, and criticized by writers from a wide breadth of disciplines and perspectives. Common themes explored through the analysis of the soundscape are the conflict between nature and industry, the impact of technology on sound production and consumption, the issue of cultural sound values and the evolution of acoustics, and the power dynamics of silence and noise.

Transduce and Record
Our perception of a recorded sound’s authenticity has been greatly impacted by the commercial influence of capitalism. Even the dead now profit from recordings they’ve made, making music more timeless than ever before. Bringing the past into the present generates a sense of familiarity which compels the public to engage in new forms of listening.
In a Memorex commercial involving Ella Fitzgerald and Chuck Mangione, Fitzgerald is unable to discern the difference between a live performance and a recording of Mangione playing the trumpet. This presents a scene to viewers which sells cassette tapes as ideal objects of high-fidelity, auditory preservation. What was once an autonomic experience of memory which integrated visual and auditory stimuli (live music) has become a consumable item which popularizes and commodifies sonic memory explicitly.
Part of this shift in the dynamics of recorded sound has to do with a desire for noise reduction. This desire is representative of a mode of recording referred to by scholar James Lastra as ""telephonic:"" a mode in which sound is regarded as having hierarchically important qualities, with clarity and intelligibility being the most important aspects. This contrasts with phonographic recording, which generates a ""point of audition"" from which a sense of space can be derived, sacrificing quality for uniqueness and fidelity. This technique is often used in movies to demonstrate how a character hears something (such as muffled voices through a closed door). Through various forms of media, recorded music affects our perceptions and consumptive practices more often than we realize.

See also
Audiophile

References
Further reading
R. Murray Schafer (1977), The Tuning of the World, (considered as the first contribution in sound studies.)
R. Murray Schafer (1994), The soundscape. In The Soundscape: Our Sonic Environment and the Tuning of the World. Rochester, Vermont: Destiny Books. pp. 3–12
Michael Doucet (1983), ""Space, Sound, Culture, and Politics: Radio Broadcasting in Southern Ontario"". Canadian Geographer / Le Géographe canadienVolume 27, Issue 2, pages 109–127, June 1983, [1]
Jacques Attali (1985), Noise: The Political Economy of Music
John Potts (1997), ""Is There a Sound Culture?"", Convergence: The International Journal of Research into New Media Technologies, December 1997, vol. 3 no. 4, pp. 10–14
Trevor Pinch and Frank Trocco (2002), Analog Days
Thompson, Emily (2002), The Soundscape of Modernity: Architectural Acoustics and the Culture of Listening in America 1900-1930. Cambridge: MIT Press. pp. 1–12
Jonathan Sterne (2003), The Audible Past
Georgina Born (1995), Rationalizing Culture
Georgina Born (ed.) (2013), Music, Sound and Space: Transformations of Public and Private Experience
Peter Szendy (2007), Listen, A History of Our Ears (the original French version, Ecoute, une histoire de nos oreilles, was published in 2001)
Michele Hilmes (2005), ""Is There a Field Called Sound Culture Studies? And Does It Matter?"", American Quarterly, Volume 57, Number 1, March 2005, pp. 249–259, [2]
Holger Schulze & Christoph Wulf (2007), Klanganthropologie
Holger Schulze (2008), Sound Studies
special issue on ""The Politics of Recorded Sound"" by Social Text 102 (2010), edited by Gustavus Stadler.
Veit Erlmann (2010), Reason and Resonance
Trevor Pinch & Karin Bijsterveld (2011), Oxford Handbook of Sound Studies
Florence Feiereisen & Alexandra Merley Hill (2011), Germany in the Loud Twentieth Century
Kate Crawford (2009) ""Following You: Disciplines of Listening in Social Media"". Continuum: Journal of Media and Cultural Studies Volume 23, Issue 4, pp. 525–535
Shuhei Hosokawa (1984), ""The Walkman Effect"", Popular Music 4:165-80
James Lastra (2000), ""Fidelity Versus Intelligibility"" pp. 138–43. New York: Columbia University Press
Kodwo Eshun (1999). Operating System for the Redesign of Sonic Reality. London: Quartet Books. 
Goodman, Steve (2010) ""The Ontology of Vibrational Force"" Sonic Warfare: Sound, Affect and the Ecology of Fear Cambridge: MIT Press. pp 81-84
Don Ihde (1974). The Auditory Dimension. In Listening and Voice: A Phenomenology of Sound. Athens: Ohio University Press. Pp. 49-55
John Picker (2003). Victorian Soundscapes. New York: Oxford University Press. pp. 41–52. 
Michael Bull (2008) Sound Moves : iPod Culture and Urban Experience. London: Routledge. pp 39–49.

External links
European Sound Studies Association
A syllabus from a graduate seminar on Sound Studies taught by Jonathan Sterne in the fall of 2006.
Weird Vibrations, a sound studies blog.
Sounding Out!, a sound studies blog
Anthropology of Sound, a sound studies blog
Master of Arts: Sound Studies, study Sound Studies at the University of Arts Berlin
Sound Studies Lab, a research project on auditory culture at the Humboldt-University of Berlin",Category:Musicology,3
88,89,FindSounds,"FindSounds is a website run by the Comparisonics Corporation. It searches an index of over 1,000,000 sounds on the internet, with 100,000 users and 1,000,000 searches each month. The index mainly consists of sound effects and musical instrument samples. Results are in AIFF, AU and WAV formats, in both mono and stereo.
The site offers the FindSounds Palette, a program which also searches the FindSounds index.
The website has been shown on television: TechTV; in newspapers: The New York Times; in magazines: Electronic Musician, Mix, Online, PC World, Popular Science Video Systems, Yahoo! Internet Life; and on countless websites, such as Yahoo's Pick of the Day, and USAToday.com.
The New York Times wrote:

Although professional sound and video producers often use this site, it's a hoot for any visitor with an audio player. You can search for a sound by typing in a phrase like ""footsteps on gravel"" or browse the Sound Types section for several hundred categorized examples (""burp"" yielded 143 sonic belches).

See also
List of search engines

References
External links
FindSounds.com",Category:Sound,3
89,90,History of broadcasting,"The first broadcasting of a radio transmission consisted of Morse code (or wireless telegraphy) was made from a temporary station set up by Guglielmo Marconi in 1895. This followed on from pioneering work in the field by Alessandro Volta, André-Marie Ampère, Georg Ohm, James Clerk Maxwell and Heinrich Hertz. The broadcasting of music and talk via radio started experimentally around 1905-1906, and commercially around 1920 to 1923. VHF (very high frequency) stations started 30 to 35 years later.
In the early days, radio stations broadcast on the long wave, medium wave and short wave bands, and later on VHF (very high frequency) and UHF (ultra high frequency). However, in the United Kingdom, Hungary, France and some other places, from as early as 1890 there was already a system whereby news, music, live theatre, music hall, fiction readings, religious broadcasts, etc., were available in private homes [and other places] via the conventional telephone line, with subscribers being supplied with a number of special, personalised headsets. In Britain this system was known as Electrophone, and was available as early as 1895 or 1899 [sources vary] and up until 1926. In Hungary, it was called Telefon Hírmondó [1893-1920s], and in France, Théâtrophone [1890-1932]). The Wikipedia Telefon Hírmondó page includes a 1907 program guide which looks remarkably similar to the types of schedules used by many broadcasting stations some 20 or 30 years later.
By the 1950s, virtually every country had a broadcasting system, typically one owned and operated by the government. Alternative modes included commercial radio, as in the United States; or a dual system with both state sponsored and commercial stations, introduced in Australia as early as 1924, with Canada following in 1932. Today, most countries have evolved into a dual system, including the UK. By 1955, practically every family in North America and Western Europe, as well as Japan, had a radio. A dramatic change came in the 1960s with the introduction of small inexpensive portable transistor radio, the greatly expanded ownership and usage. Access became practically universal across the world.

Early broadcasting
Australia
The History of broadcasting in Australia has been shaped for over a century by the problem of communication across long distances, coupled with a strong base in a wealthy society with a deep taste for aural communications. Australia developed its own system, through its own engineers, manufacturers, retailers, newspapers, entertainment services, and news agencies. The government set up the first radio system, and business interests marginalized the hobbyists and amateurs. The Labor Party was especially interested in radio because it allowed them to bypass the newspapers, which were mostly controlled by the opposition. Both parties agreed on the need for a national system, and in 1932 set up the Australian Broadcasting Commission, as a government agency that was largely separate from political interference. The first commercial broadcasters, originally known as ""B"" class stations, were on the air as early as 1925. The number of stations (commercial and national) remained relatively dormant throughout World War II and in the post-war era.

Formative years
Australian radio hams can be traced to the early 1900s. The 1905 Wireless Telegraphy Act whilst acknowledging the existence of wireless telgraphy, brought all broadcasting matters in Australia under the control of the Federal Government. In 1906, the first official Morse code transmission in Australia was by the Marconi Company between Queenscliff, Victoria and Devonport, Tasmania.

Experiments with broadcasting music
The first broadcast of music was made during a demonstration on 13 August 1919 by Ernest Fisk (later Sir Ernest) of AWA – Amalgamated Wireless (Australasia). A number of amateurs commenced broadcasting music in 1920 and 1921. Many other amateurs soon followed. 2CM was run by Charles MacLuran who started the station in 1921 with regular Sunday evening broadcasts from the Wentworth Hotel, Sydney. 2CM is often regarded as Australia's first, regular, non-official station.

Sealed set system
It was not until November 1923 when the government finally gave its approval for a number of officially recognised medium wave stations. All stations operated under a unique Sealed Set system under which each set was sealed to the frequency of one station. Part of the price of the set went to the government via the Postmaster-General's Department (PMG), with money also going to the broadcaster. Apart from extremely limited advertising, this was the broadcasters' only source of income. From the outset problems with the system came to the fore. Many young people built their own sets, which could receive all the stations. The sealed set system was devised by broadcasting pioneer Ernest Fisk of AWA – Amalgamated Wireless (Australasia).

Categories in Australia from 1924
As quickly as July 1924, the Sealed Set system was declared to be unsuccessful and it was replaced by a system of A Class and B Class stations. There were one or two A Class stations in each major market and these were paid for by a listener's licence fee imposed on all listeners-in. The five former sealed set stations became A Class stations, and they were soon joined by stations in other State capitals.

From 1929, all A Class stations received all their programs from the one source, the Australian Broadcasting Company which was made up of the following shareholders: Greater Union Theatres (a movie theatre chain), Fuller's Theatres (a live theatre chain) and J. Albert & Sons (music publishers and retailers). A number of B Class stations were also licensed. These did not receive any government monies and were expected to derive their income from advertising, sponsorship, or other sources. Within a few years B Class stations were being referred to as ""commercial stations"".
Amateur broadcasters continued to operate in the long wave and short wave bands.
A national service, the Australian Broadcasting Commission, was formed in July 1932, when the Australian Broadcasting Company's contract expired. The Corporation took over the assets of all A Class stations. It still exists as the Australian Broadcasting Corporation. The Australian Broadcasting Co changed its name to the Commonwealth Broadcasting Company and later the Australian Radio Network. It soon purchased Sydney commercial station 2UW and now has an Australia-wide network of commercial stations.

Types of programs
As with most countries, most Australian stations originally broadcast music interspersed with such things as talks, coverage of sporting events, church broadcasts, weather, news and time signals of various types. Virtually all stations also had programs of interest to women, and children's sessions. From the outset, A Class stations' peak-hour evening programs often consisted of live broadcasts from various theatres, i.e. dramas, operas, musicals, variety shows, vaudeville, etc. The first dramas especially written for radio were transmitted in the mid-1920s. By the 1930s, the ABC was transmitting a number of British programs sourced from the BBC, and commercial stations were receiving a number of US programs, particularly dramas. However, in the 1940s, war-time restrictions made it difficult to access overseas programs and, therefore, the amount of Australian dramatic material increased. As well as using original ideas and scripts, there were a number of local versions of overseas programs. Initially, much of the music broadcast in Australia was from live studio concerts. However, the amount of gramophone (and piano roll) music soon increased dramatically, particularly on commercial stations.
In the late 1930s, the number of big production variety shows multiplied significantly, particularly on the two major commercial networks, Macquarie and Major. After World War II the independent Colgate-Palmolive radio production unit was formed. It poached most major radio stars from the various stations. Until the 1950s, the popular image of the whole family seated around a set in the living room was the most accepted way of listening to radio. Therefore, most stations had to be all things to all people, and specialised programming was not really thought about at this stage (it did not come in until the late 1950s). Because of this, programming on most stations was pretty much the same.

Early experiments with television
As early as 1929, two Melbourne commercial radio stations, 3UZ and 3DB were conducting experimental mechanical television broadcasts – these were conducted in the early hours of the morning, after the stations had officially closed down. In 1934 Dr Val McDowall at amateur station 4CM Brisbane conducted experiments in electronic television.

Mobile stations
Two of Australia's most unusual medium wave stations were mobile stations 2XT and 3YB. They both operated in eras prior to the universal establishment of rural radio stations. 2XT was designed and operated by AWA within the State of New South Wales, from a NSW Railways train, between November 1925 and December 1927. 2XT, which stood for experimental train, visited over 100 rural centres. Engineers would set up a transmitting aerial and the station would then begin broadcasting. This led to the further sales of AWA products. 3YB provided a similar service in rural Victoria between October 1931 and November 1935. Initially, the station operated from a Ford car and a Ford truck, but from 17 October 1932 they operated from a converted 1899 former Royal Train carriage. Whilst the engineers were setting up the station's 50-watt transmitter in the town being visited, salesmen would sign up advertisers for the fortnight that 3YB would broadcast from that region. The station was on the air from 6.00 and 10.00 pm daily, and its 1,000-record library was divided into set four-hour programs, one for each of 14 days. In other words, the music broadcast from each town was identical. The station was operated by Vic Dinenny, but named after announcer Jack Young from Ballarat. On 18 January 1936, Dinenny set up 3YB Warnambool, followed on 18 May 1937 by 3UL Warragul.
The merchant vessel MV Kanimbla is believed to be the world's only ship built with an inbuilt broadcasting station. The Kanimlba was constructed in Northern Ireland in 1936 and was primarily designed for McIlwraith McEachern Limited to ply passengers between Cairns, Queensland and Fremantle, Western Australia. The broadcasting station was constructed and operated by AWA and was initially given the ham radio callsign VK9MI but was later 9MI. (At this time, the ""9"" in the callsign was aberrationary [see ""Call Signs, above].) The station made an experimental broadcast before leaving Northern Ireland, and a number of such broadcasts at sea, on the way to Australia. 9MI's first official broadcast in April 1939 was made from the Great Australian Bight. The station broadcast on short wave, usually a couple of times per week, but many of its programs were relayed to commercial medium wave stations that were also owned by AWA. The 9MI manager and announcer (and probably the only member of staff) was Eileen Foley. 9MI ceased broadcasting at the commencement of World War II in September 1939. The Kanimbla was commissioned as a Royal Navy (later Royal Australian Navy) vessel with the name HMS/HMAS Kanimbla. It had an extremely prominent and successful war-time career.

Canada
The history of broadcasting in Canada begins as early as 1919 with the first experimental broadcast programs in Montreal. Canadians were swept up in the radio craze and built crystal sets to listen to American stations while The Marconi Wireless Telegraph Company of Canada offered its first commercially produced radio-broadcast receiver (Model ""C"") in 1921, followed by its ""Marconiphone"" Model I in 1923. Main themes in the history include the development of the engineering technology; the construction of stations across the country and the building of networks; the widespread purchase and use of radio and television sets by the general public; debates regarding state versus private ownership of stations; financing of the broadcasts media through the government, license fees, and advertising; the changing content of the programming; the impact of the programming on Canadian identity; the media's influence on shaping audience responses to music, sports and politics; the role of the Québec government; Francophone versus Anglophone cultural tastes; the role of other ethnic groups and First Nations; fears of American cultural imperialism via the airwaves; and the impact of the Internet and smartphones on traditional broadcasting media.
Radio signals carried long distances, and a number of American stations could easily be received in parts of Canada. The first Canadian station was CFCF, originally an experimental station from the Marconi Company in Montreal. Civilian use of Wireless Telegraphy had been forbidden in Canada for the duration of World War I. The Marconi Wireless Telegraph Company of Canada was the only one to retain the right to continue radio experiments for military use. This proved instrumental in giving the company a lead in developing an experimental radio broadcasting station immediately after the war. The first radio broadcast in Canada was accomplished by The Marconi Wireless Telegraph Company of Canada in Montreal on December 1, 1919 under the call sign XWA (for ""Experimental Wireless Apparatus"") from its Williams Street factory. The station began regular programming on May 20, 1920 and its call letters were changed to CFCF on November 4, 1920. In Toronto, the first radio station was operated by the Toronto Star newspaper. Station CKCE began in April 1922. and were so well received that the Star pushed forward with its own studios and transmitting facilities, returning to the air as CFCA in late June 1922. In Montreal, another newspaper, La Presse, put its own station, CKAC on the air in late September 1922. Because there were governmental limitations on radio frequencies back then, CKAC and CFCF alternated—one would broadcast one night, and the other would broadcast the night after that. For a time, CKAC was broadcasting some programs in French, and some in English: in 1924, for example, the station rebroadcast fifteen Boston Bruins hockey games from station WBZ in Boston. Meanwhile, in other Canadian provinces, 1922 was also the year for their first stations, including CJCE in Vancouver, and CQCA (which soon became CHCQ) in Calgary.
As radio grew in popularity during the mid-1920s, a problem arose: the U.S. stations dominated the airwaves and with a limited number of frequencies available for broadcasters to use, it was the American stations that seemed to get most of them. This was despite an agreement with the US Department of Commerce (which supervised broadcasting in the years prior to the Federal Radio Commission) that a certain number of frequencies were reserved exclusively for Canadian signals. But if a US station wanted one of those frequencies, the Department of Commerce seemed unwilling to stop it, much to the frustration of Canadian owners who wanted to put stations on the air. The Canadian government and the US government began negotiations in late 1926, in hopes of finding a satisfactory solution. Meanwhile, in 1928, Canada got its first network, operated by the Canadian National Railways. CNR had already made itself known in radio since 1923, thanks in large part to the leadership of CNR's president, Sir Henry Thornton. The company began equipping its trains with radio receivers, and allowed passengers to hear radio stations from Canada and the US. In 1924, CN began building its own stations, and by 1928, it was able to create a network. In 1932, the Canadian Radio Broadcasting Commission was formed, and in 1936, the Canadian Broadcasting Corporation, the country's national radio service, made its debut.

Cuba
There was interest in radio almost from broadcasting's earliest days. Due to the proximity of Cuba to the U.S. state of Florida, some Cubans would try to listen to the American stations whose signals reached the island. But there was no radio station in Cuba until 1922. The arrival of the first radio station, PWX, was greeted with enthusiasm. PWX, owned by the Cuban Telephone Company, was located in Havana. It was a joint venture with the International Telephone and Telegraph Company of New York. PWX debuted on the air on October 10, 1922. PWX broadcast programs in both English and Spanish, and its signal was easily received at night in a number of American cities. Another early station in Cuba was owned by Frank Jones, an American amateur radio operator and Chief Engineer of the Tuinucu Sugar Company. The station used amateur call letters, and went on the air as 6KW. In late 1928, PWX began using the call letters CMC. Its slogan was ""If you hear 'La Paloma,' you are in tune with CMC."" As with many other countries, interest in radio expanded, and by 1932, Cuba had more than thirty stations, spread out in cities all over the island.

France
Radio Paris began operations in 1922, followed by Radio Toulouse and Radio Lyon. Before 1940, 14 commercial and 12 public sector radio stations were in operation. The government exerted tight control over radio broadcasting. Political debate was not encouraged. In the 1932 election campaign, for example, the opposition was allowed one broadcast while the incumbent made numerous campaign broadcasts. Radio was a potentially powerful new medium, but France was quite laggard in consumer ownership of radio sets, With 5 million radio receivers in 1937, compared to over 8 million and both Britain and Germany, and 26 million in the United States. The government imposed very strict controls on news dissemination. After 1938, stations were allowed only three brief daily bulletins, of seven minutes each, to cover all the day's news. The Prime Minister's office closely supervised the news items that were to be broadcast. As war approached, Frenchmen learned little or nothing about it from the radio. The government thought that policy wise, because it wanted no interference in its policies. The unexpected result, however, was the Frenchman were puzzled and uncertain great crises erupted in 1938-39, and their morale and support for government policies was much weaker than in Britain.

Germany
The first radio station in Germany went on the air in Berlin in late 1923, using the call letters ""LP."" Before 1933, German radio broadcasting was Conducted by 10 regional broadcasting monopolies, each of which had a government representative on its board. The Post Office Provided overall supervision. A listening fee of 2 Reichsmark per receiver paid most costs, and radio station frequencies were limited, which even restricted the number of amateur radio operators. Immediately following Hitler's assumption of power in 1933, Joseph Goebbels became head of the Ministry for Propaganda and Public Enlightenment and took full control of broadcasting. Non-Nazis were removed from broadcasting and editorial positions. Jews were fired from all positions.
Germany was easily served by a number of European mediumwave stations, including the BBC, but the Nazis made it illegal for Germans to listen to foreign broadcasts. During the war, German stations broadcast not only war propaganda and entertainment for German forces dispersed through Europe, as well as air raid alerts. There was heavy use of short wave for ""Germany Calling"" programmes directed at Britain and Allied forces around the world. Goebbels Also set up numerous Nazi stations that pretended to be from the Allied world. Germany experimented with television broadcasting, using a 180-line raster system beginning before 1935. German propaganda claimed the system was superior to the British mechanical scanning system.

Japan
The first radio station in Japan was JOAK, which opened in Tokyo in March 1925. It was founded by Masajiro Kotamura, an inventor and engineer. It was unique in that at least one of its announcers was a woman, Akiko Midorikawa. JOAK was followed soon after by JOBK in Osaka and JOCK in Nagoya. The National Broadcasting Service, today known as NHK (Nippon Hoso Kyokai), began in August 1926. All stations were supported by licensing fees: in 1926, for example, people wishing to receive a permit to own a radio set paid a fee of one yen a month to the government. Programming on Japanese stations of the 1920s included music, news, language instruction (lessons were offered in English, French and German) and educations talks. These early stations broadcast on average about eight hours of programs a day.

Mexico
Amateur radio was very popular in Mexico; while most of the hams were male, notably Constantino de Tarnava, acknowledged in some sources as Mexico's first amateur radio operator, one of the early ham radio operators was female—Maria Dolores Estrada. But commercial radio was difficult to achieve, due to a federal regulation forbidding any broadcasts that were not for the benefit of the Mexican government. Still, in November 1923, CYL in Mexico City went on the air, featuring music (both folk songs and popular dance concerts), religious services, and news. CYL used as its slogans ""El Universal"" and ""La Casa del Radio"", and it won over the government, by giving political candidates the opportunity to use the station to campaign. Its signal was so powerful that it was even received in Canada sometimes. Pressure from listeners and potential station owners also contributed to the government relenting and allowing more stations to go on the air. In 1931, the ""C"" call letters were all changed to ""X"" call letters (XE being reserved for broadcasting), and by 1932, Mexico had nearly forty radio stations, ten of which were in Mexico City.

Philippines
Interest in amateur radio was noted in the Philippines in the early 1920s. There were radio stations operating in the Philippines, including one owned by American businessman named Henry Hermann, as early as 1922, according to some sources; not much documentation about that period of time exists. In the autumn of 1927, KZRM in Manila, owned by the Radio Corporation of the Philippines, went on the air. The Radio Corporation of the Philippines was a subsidiary of American company RCA (Radio Corporation of America). By 1932, the island had three radio stations: KRZC in Cebu, as well as KZIB (owned by a department store) and KZFM, the government-owned station in Manila. Of the stations listed by Pierre Key, KZFM was the strongest, with 50,000 watts. Two radio networks were ultimately created: one, the Manila Broadcasting Company, began as a single station, KZRH in Manila, in July 1939, and after World War II, in 1946, the station's owners began to develop their network by buying other radio properties. As for the Philippine Broadcasting Company, it too began with one station (KZFM), and received its new name in mid-1946, after the Philippines became an independent country. At the end of 1946, the new network had six stations. Both KZRH and KZFM also affiliated with American networks; the stations wanted to have access to certain popular American programs, and the American networks wanted to sell products in the Philippines.

Sri Lanka
Sri Lanka has the oldest radio station in Asia (world's second oldest). The station was known as Radio Ceylon. It developed into one of the finest broadcasting institutions in the world. It is now known as the Sri Lanka Broadcasting Corporation. Sri Lanka created broadcasting history in Asia when broadcasting was started in Ceylon by the Telegraph Department in 1923 on an experimental footing, just three years after the inauguration of broadcasting in Europe. Gramophone music was broadcast from a tiny room in the Central Telegraph Office with the aid of a small transmitter built by the Telegraph Department engineers from the radio equipment of a captured German submarine. This broadcasting experiment was successful; barely three years later, on December 16, 1925, a regular broadcasting service came to be instituted. Edward Harper who came to Ceylon as Chief Engineer of the Telegraph Office in 1921, was the first person to actively promote broadcasting in Ceylon. Sri Lanka occupies an important place in the history of broadcasting with broadcasting services inaugurated just three years after the launch of the BBC in the United Kingdom. Edward Harper launched the first experimental broadcast as well as founding the Ceylon Wireless Club, together with British and Ceylonese radio enthusiasts on the island. Edward Harper has been dubbed ' the Father of Broadcasting in Ceylon,' because of his pioneering efforts, his skill and his determination to succeed. Edward Harper and his fellow Ceylonese radio enthusiasts, made it happen.

United Kingdom
The first experimental music broadcasts, from Marconi's factory in Chelmsford, began in 1920. Two years later, in October 1922, a consortium of radio manufacturers formed the British Broadcasting Company (BBC); they allowed some sponsored programs, although they were not what we would today consider a fully commercial station. Meanwhile, the first radio stations in England were experimental station 2MT, located near Chelmsford, and station 2LO in London: both were operated by the Marconi Company. By late 1923, there were six stations broadcasting regularly in the United Kingdom: London's 2LO, Manchester's 2ZY, and stations in Birmingham, Cardiff, Newcastle, and Glasgow. As for the consortium of radio manufacturers, it dissolved in 1926, when its license expired; it then became the British Broadcasting Corporation, a non-commercial organization. Its governors are appointed by the British government, but they do not answer to it. Lord Reith took a formative role in developing the BBC, especially in radio. Working as its first manager and Director-General, he promoted the philosophy of public service broadcasting, firmly grounded in the moral benefits of education and of uplifting entertainment, eschewing commercial influence and maintaining a maximum of independence from political control.
Commercial stations such as Radio Normandie and Radio Luxembourg broadcast into the UK from other European countries. This provided a very popular alternative to the rather austere BBC. These stations were closed during the War, and only Radio Luxembourg returned afterward. BBC television broadcasts in Britain began on November 2, 1936, and continued until wartime conditions closed the service in 1939.

United States
Reginald Fessenden did ground-breaking experiments with voice and music by 1906. Charles ""Doc"" Herrold of San Jose, California sent out broadcasts as early as April 1909 from his Herrold School electronics institute in downtown San Jose, using the identification San Jose Calling, and then a variety of different call signs as the Department of Commerce began to regulate radio. He was on the air daily for nearly a decade when the World War interrupted operations.

Pioneer radio station 2XG, also known as the ""Highbridge station"", was an experimental station located in New York City and licensed to the DeForest Radio Telephone and Telegraph Company. It was the first station to use a vacuum tube transmitter to make radio broadcasts on a regular schedule. From 1912 to 1917 Charles Herrold made regular broadcasts, but used an arc transmitter. He switched to a vacuum tube transmitter when he restarted broadcasting activities in 1921. Herrold coined the terms broadcasting and narrowcasting,. Herrold claimed the invention of broadcasting to a wide audience, through the use of antennas designed to radiate signals in all directions. David Sarnoff has been considered by many as ""the prescient prophet of broadcasting who predicted the medium's rise in 1915"", referring to his radio music box concept.

A few organizations were allowed to keep working on radio during the war. Westinghouse was the most well-known of these. Frank Conrad, a Westinghouse engineer, had been making transmissions from 8XK since 1916 that included music programming. A team at the University of Wisconsin–Madison headed by Professor Earle M. Terry was also on the air. They operated 9XM, originally licensed by Professor Edward Bennett in 1914, and experimented with voice broadcasts starting in 1917.

1920s
By 1919, after the war, radio pioneers across the country resumed transmissions. The early stations gained new call signs. Many early stations were started by newspapers worried radio might replace their newspapers. 8XK became KDKA in 1920. KDKA received the first federal license and began broadcasting on November 2, 1920. Madison Avenue early on recognized the importance of radio as a new advertising medium. Advertising provided the major funding for most stations. The United States never had a licensing fee for set users. The National Broadcasting Company began regular broadcasting in 1926, with telephone links between New York and other Eastern cities. NBC became the dominant radio network, splitting into Red and Blue networks. The Columbia Broadcasting System began in 1927 under the guidance of William S. Paley.
Radio in education began as early as April 1922, when Medford Hillside's WGI Radio broadcast the first of an ongoing series of educational lectures from Tufts College professors. These lectures were described by the press as a sort of ""wireless college."" Soon, other colleges across the U.S. began adding radio broadcasting courses to their curricula; some, like the University of Iowa, even provided what today would be known as distance-learning credits. Curry College, first in Boston and then in Milton, Massachusetts, introduced one of the nation's first broadcasting majors in 1932 when the college teamed up with WLOE in Boston to have students broadcast programs. This success led to numerous radio courses in the curriculum which has taught thousands of radio broadcasters from the 1930s to today.
In 1934, several independent stations formed the Mutual Broadcasting System to exchange syndicated programming, including The Lone Ranger and Amos 'n' Andy. Prior to 1927, U.S. radio was supervised by the Department of Commerce. Then, the Radio Act of 1927 created the Federal Radio Commission (FRC); in 1934, this agency became known as the Federal Communications Commission (FCC). A Federal Communications Commission decision in 1939 required NBC to divest itself of its Blue Network. That decision was sustained by the Supreme Court in a 1943 decision, National Broadcasting Co. v. United States, which established the framework that the ""scarcity"" of radio-frequency meant that broadcasting was subject to greater regulation than other media. This Blue Network network became the American Broadcasting Company (ABC). Around 1946, ABC, NBC, and CBS began regular television broadcasts. Another TV network, the DuMont Television Network, was founded earlier, but was disbanded in 1956; later in 1986 the surviving DuMont independent stations formed the nucleus of the new Fox Broadcasting Company.

1950s and 1960s
Australia
Norman Banks was one of Melbourne's (and Australia's) most prominent broadcasters at 3KZ (1930-1952) and 3AW (1952-1978). He is remembered for founding Carols by Candlelight, as a pioneer football commentator, and for hosting both musical and interview programs. In later years he was one of Melbourne's first and most prominent talk back hosts. At the commencement of his career, Banks was known for his double entredes and risque remarks; as a talk back host he was outspoken in his conservative views, especially regarding the White Australia policy and Apartheid. In 1978 his 47-year career in radio was hailed as the longest in world history. Not including the early television experiments (see above), mainstream television transmission commenced in Sydney and Melbourne in the latter part of 1956, that is, in time for the 1956 Melbourne Olympic Games in November/December 1956. It was then phased in to other capital cities, and then into rural markets. Many forms entertainment, particularly drama and variety, were considered more suited to television than radio, and many such programs were gradually deleted from radio schedules.

The transistor radio first appeared on the market in 1954. In particular, it made portable radios even more transportable. All sets quicklly became smaller, cheaper and more convenient. The aim of radio manufacturers became a radio in every room, in the car, and in the pocket. The upshot of these two changes was that stations started to specialise and conc",Category:History of telecommunications in Australia,3
90,91,Infrasound,"Infrasound, sometimes referred to as low-frequency sound, is sound that is lower in frequency than 20 Hz or cycles per second, the ""normal"" limit of human hearing. Hearing becomes gradually less sensitive as frequency decreases, so for humans to perceive infrasound, the sound pressure must be sufficiently high. The ear is the primary organ for sensing infrasound, but at higher intensities it is possible to feel infrasound vibrations in various parts of the body.
The study of such sound waves is sometimes referred to as infrasonics, covering sounds beneath 20 Hz down to 0.1 Hz and rarely to 0.001 Hz. People use this frequency range for monitoring earthquakes, charting rock and petroleum formations below the earth, and also in ballistocardiography and seismocardiography to study the mechanics of the heart.
Infrasound is characterized by an ability to cover long distances and get around obstacles with little dissipation. In music, acoustic waveguide methods, such as a large pipe organ or, for reproduction, exotic loudspeaker designs such as transmission line, rotary woofer, or traditional subwoofer designs can produce low-frequency sounds, including near-infrasound. Subwoofers designed to produce infrasound are capable of sound reproduction an octave or more below that of most commercially available subwoofers, and are often about 10 times the size.

Definition
Infrasound is defined by the American National Standards Institute as ""sound at frequencies less than 20 Hz.""

History and study
The Allies of World War I first used infrasound to locate artillery. One of the pioneers in infrasonic research was French scientist Vladimir Gavreau. His interest in infrasonic waves first came about in his laboratory during the 1960s, when he and his laboratory assistants experienced shaking laboratory equipment and pain in the eardrums, but his microphones did not detect audible sound. He concluded it was infrasound caused by a large fan and duct system, and soon got to work preparing tests in the laboratories. One of his experiments was an infrasonic whistle, an oversized organ pipe.

Sources
Infrasound can result from both natural and man-made sources:

Natural events: infrasonic sound sometimes results naturally from severe weather, surf, lee waves, avalanches, earthquakes, volcanoes, bolides, waterfalls, calving of icebergs, aurorae, meteors, lightning and upper-atmospheric lightning. Nonlinear ocean wave interactions in ocean storms produce pervasive infrasound vibrations around 0.2 Hz, known as microbaroms. According to the Infrasonics Program at NOAA, infrasonic arrays can be used to locate avalanches in the Rocky Mountains, and to detect tornadoes on the high plains several minutes before they touch down.

Animal communication: whales, elephants, hippopotamuses, rhinoceros, giraffes, okapi, and alligators are known to use infrasound to communicate over distances—up to hundreds of miles in the case of whales. In particular, the Sumatran Rhinoceros has been shown to produce sounds with frequencies as low as 3 Hz which have similarities with the song of the humpback whale. The roar of the tiger contains infrasound of 18 Hz and lower, and the purr of felines is reported to cover a range of 20 to 50 Hz. It has also been suggested that migrating birds use naturally generated infrasound, from sources such as turbulent airflow over mountain ranges, as a navigational aid. Infrasound also may be used for long-distance communication, especially well documented in baleen whales (see Whale vocalization), and African elephants. The frequency of baleen whale sounds can range from 10 Hz to 31 kHz, and that of elephant calls from 15 Hz to 35 Hz. Both can be extremely loud (around 117 dB), allowing communication for many kilometres, with a possible maximum range of around 10 km (6 mi) for elephants, and potentially hundreds or thousands of kilometers for some whales. Elephants also produce infrasound waves that travel through solid ground and are sensed by other herds using their feet, although they may be separated by hundreds of kilometres. These calls may be used to coordinate the movement of herds and allow mating elephants to find each other.

Human singers: some vocalists, including Tim Storms, can produce notes in the infrasound range.

Human created sources: infrasound can be generated by human processes such as sonic booms and explosions (both chemical and nuclear), or by machinery such as diesel engines, wind turbines and specially designed mechanical transducers (industrial vibration tables). Certain specialized loudspeaker designs are also able to reproduce extremely low frequencies; these include large-scale rotary woofer models of subwoofer loudspeaker, as well as large horn loaded, bass reflex, sealed and transmission line loudspeakers.

Animal reactions
Animals have been known to perceive the infrasonic waves going through the earth by natural disasters and can use these as an early warning. A recent example of this is the 2004 Indian Ocean earthquake and tsunami. Animals were reported to flee the area hours before the actual tsunami hit the shores of Asia. It is not known for sure that this is the cause; some have suggested that it may have been the influence of electromagnetic waves, and not of infrasonic waves, that prompted these animals to flee.
Research in 2013 by Jon Hagstrum of the US Geological Survey suggests that homing pigeons use low-frequency infrasound to navigate.

Human reactions
20 Hz is considered the normal low-frequency limit of human hearing. When pure sine waves are reproduced under ideal conditions and at very high volume, a human listener will be able to identify tones as low as 12 Hz. Below 10 Hz it is possible to perceive the single cycles of the sound, along with a sensation of pressure at the eardrums.
From about 1000 Hz, the dynamic range of the auditory system decreases with decreasing frequency. This compression is observable in the equal-loudness-level contours, and it implies that even a slight increase in level can change the perceived loudness from barely audible to loud. Combined with the natural spread in thresholds within a population, its effect may be that a very low-frequency sound which is inaudible to some people may be loud to others.
One study has suggested that infrasound may cause feelings of awe or fear in humans. It has also been suggested that since it is not consciously perceived, it may make people feel vaguely that odd or supernatural events are taking place. Engineer Vic Tandy provided such an explanation in his investigations in the 1980s. Tandy, while working in his laboratory, started to feel uneasy and as if a supernatural presence was with him. Later, he could attribute these feelings to a broken metal fan that was causing noises of a frequency that triggered them. The noise could not be perceived by the human ear, but Tandy's body reacted to the 19Hz sounds.
A scientist working at Sydney University's Auditory Neuroscience Laboratory reports growing evidence that infrasound may affect some people's nervous system by stimulating the vestibular system, and this has shown in animal models an effect similar to sea sickness.
In 2006 research about the impact of sound emissions from wind turbines on nearby population, perceived infrasound has been associated to effects such as annoyance or fatigue, depending on its intensity, with little evidence supporting physiological effects of infrasound below the human perception threshold. Later studies, however, have linked inaudible infrasound to effects such as fullness, pressure or tinnitus, and acknowledged the possibility that it could disturb sleep. Other studies have also suggested associations between noise levels in turbines and self-reported sleep disturbances in the nearby population, while adding that the contribution of infrasound to this effect is still not fully understood.
In a study at Ibaraki University in Japan, researchers said EEG tests showed that the infrasound produced by wind turbines was “considered to be an annoyance to the technicians who work close to a modern large-scale wind turbine.”

Infrasonic 17 Hz tone experiment
On 31 May 2003 a group of UK researchers held a mass experiment, where they exposed some 700 people to music laced with soft 17 Hz sine waves played at a level described as ""near the edge of hearing"", produced by an extra-long-stroke subwoofer mounted two-thirds of the way from the end of a seven-meter-long plastic sewer pipe. The experimental concert (entitled Infrasonic) took place in the Purcell Room over the course of two performances, each consisting of four musical pieces. Two of the pieces in each concert had 17 Hz tones played underneath.
In the second concert, the pieces that were to carry a 17 Hz undertone were swapped so that test results would not focus on any specific musical piece. The participants were not told which pieces included the low-level 17 Hz near-infrasonic tone. The presence of the tone resulted in a significant number (22%) of respondents reporting feeling uneasy or sorrowful, getting chills down the spine or nervous feelings of revulsion or fear.
In presenting the evidence to the British Association for the Advancement of Science, Professor Richard Wiseman said ""These results suggest that low frequency sound can cause people to have unusual experiences even though they cannot consciously detect infrasound. Some scientists have suggested that this level of sound may be present at some allegedly haunted sites and so cause people to have odd sensations that they attribute to a ghost—our findings support these ideas.""

Suggested relationship to ghost sightings
Psychologist Richard Wiseman of the University of Hertfordshire suggests that the odd sensations that people attribute to ghosts may be caused by infrasonic vibrations. Vic Tandy, experimental officer and part-time lecturer in the school of international studies and law at Coventry University, along with Dr. Tony Lawrence of the University's psychology department, wrote in 1998 a paper called ""Ghosts in the Machine"" for the Journal of the Society for Psychical Research. Their research suggested that an infrasonic signal of 19 Hz might be responsible for some ghost sightings. Tandy was working late one night alone in a supposedly haunted laboratory at Warwick, when he felt very anxious and could detect a grey blob out of the corner of his eye. When Tandy turned to face the grey blob, there was nothing.
The following day, Tandy was working on his fencing foil, with the handle held in a vice. Although there was nothing touching it, the blade started to vibrate wildly. Further investigation led Tandy to discover that the extractor fan in the lab was emitting a frequency of 18.98 Hz, very close to the resonant frequency of the eye given as 18 Hz by NASA. This, Tandy conjectured, was why he had seen a ghostly figure—it was, he believed, an optical illusion caused by his eyeballs resonating. The room was exactly half a wavelength in length, and the desk was in the centre, thus causing a standing wave which caused the vibration of the foil.
Tandy investigated this phenomenon further and wrote a paper entitled The Ghost in the Machine. He carried out a number of investigations at various sites believed to be haunted, including the basement of the Tourist Information Bureau next to Coventry Cathedral and Edinburgh Castle.

Infrasound for nuclear detonation detection
Infrasound is one of several techniques used to identify if a nuclear detonation has occurred. A network of 60 infrasound stations, in addition to seismic and hydroacoustic stations, comprise the International Monitoring System (IMS) that is tasked with monitoring compliance with the Comprehensive Nuclear Test-Ban Treaty (CTBT). IMS Infrasound stations consist of eight microbarometer sensors and space filters arranged in an array covering an area of approximately 1 to 9 km^2. The space filters used are radiating pipes with inlet ports along their length, designed to average out pressure variations like wind turbulence for more precise measurements. The microbarometers used are designed to monitor frequencies below approximately 20 hertz. Sound waves below 20 hertz have longer wavelengths and are not easily absorbed, allowing for detection across large distances.
Infrasound wavelengths can be generated artificially through detonations and other human activity, or naturally from earthquakes, severe weather, lightning, and other sources. Like forensic seismology, algorithms and other filter techniques are required to analyze gathered data and characterize events to determine if a nuclear detonation has actually occurred. Data is transmitted from each station via secure communication links for further analysis. A digital signature is also embedded in the data sent from each station to verify if the data is authentic.

Detection and measurement
NASA Langley has designed and developed an infrasonic detection system that can be used to make useful infrasound measurements at a location where it was not possible previously. The system comprises an electret condenser microphone PCB Model 377M06, having a 3-inch membrane diameter, and a small, compact windscreen. Electret-based technology offers the lowest possible background noise, because Johnson noise generated in the supporting electronics (preamplifier) is minimized.
The microphone features a high membrane compliance with a large backchamber volume, a prepolarized backplane and a high impedance preamplifier located inside the backchamber. The windscreen, based on the high transmission coefficient of infrasound through matter, is made of a material having a low acoustic impedance and has a sufficiently thick wall to ensure structural stability. Close-cell polyurethane foam has been found to serve the purpose well. In the proposed test, test parameters will be sensitivity, background noise, signal fidelity (harmonic distortion), and temporal stability.
The microphone design differs from that of a conventional audio system in that the peculiar features of infrasound are taken into account. First, infrasound propagates over vast distances through the Earth's atmosphere as a result of very low atmospheric absorption and of refractive ducting that enables propagation by way of multiple bounces between the Earth's surface and the stratosphere. A second property that has received little attention is the great penetration capability of infrasound through solid matter – a property utilized in the design and fabrication of the system windscreens.
Thus the system fulfills several instrumentation requirements advantageous to the application of acoustics: (1) a low-frequency microphone with especially low background noise, which enables detection of low-level signals within a low-frequency passband; (2) a small, compact windscreen that permits (3) rapid deployment of a microphone array in the field. The system also features a data acquisition system that permits real time detection, bearing, and signature of a low-frequency source.
The Comprehensive Nuclear-Test-Ban Treaty Organization Preparatory Commission uses infrasound as one of its monitoring technologies, along with seismic, hydroacoustic, and atmospheric radionuclide monitoring. The loudest infrasound recorded to date by the monitoring system was generated by the 2013 Chelyabinsk meteor.

Notes
See also
Bioacoustics
Brown note
Clear-air turbulence
Feraliminal Lycanthropizer
Demonstrated infrasonic weapon
Helmholtz resonance
Microbarom
The Hum
Ultrasound
Contrabass tuba
Subcontrabass tuba

References
External links
Inframatics, an international infrasound monitoring organization
NOAA Infrasonics Program
US Army Space and Missile Defense Command Monitoring Research Program
Los Alamos Infrasound Monitoring Laboratory
Infrasonic and Acoustic-Gravity Waves Generated by the Mount Pinatubo Eruption of 15 June 1991, Makoto Tahira, Masahiro Nomura, Yosihiro Sawada and Kosuke Kamo
Sub-surface windscreen for the measurement of outdoor infrasound Qamar A. Shams, Cecil G. Burkett and Toby Comeaux NASA Langley Research Center, Allan J. Zuckerwar Analytical Services and Material, and George R. Weistroffer Virginia Commonwealth University",Category:CS1 maint: Multiple names: authors list,3
91,92,Robinson–Dadson curves,"The Robinson–Dadson curves are one of many sets of equal-loudness contours for the human ear, determined experimentally by D. W. Robinson and R. S. Dadson, and reported in a paper entitled ""A re-determination of the equal-loudness relations for pure tones"" in Br. J. Appl. Phys. 7, 166-181 (1956).
Until recently, it was common to see the term 'Fletcher–Munson' used to refer to equal-loudness contours generally, even though the re-determination carried out by Robinson and Dadson in 1956, became the basis for an ISO standard ISO 226 which was only revised recently.
It is now better to use the term 'Equal-loudness contours' as the generic term, especially as a recent survey by ISO redefined the curves in a new standard, ISO 226 :2003.
According to the ISO report, the Robinson-Dadson results were the odd one out, differing more from the current standard than did the Fletcher–Munson curves. It comments that it is fortunate that the 40-Phon Fletcher-Munson curve on which the A-weighting standard was based turns out to have been in good agreement with modern determinations.
The article also comments on the large differences apparent in the low-frequency region, which remain unexplained. Possible explanations are:
The equipment used was not properly calibrated.
The criteria used for judging equal loudness (which is tricky) differed.
Different races actually vary greatly in this respect (possible, and most recent determinations were by the Japanese).
Subjects were not properly rested for days in advance, or were exposed to loud noise in travelling to the tests which tensed the tensor timpani and stapedius muscles controlling low-frequency mechanical coupling.

See also
dB(A)
CCIR (ITU) 468 Noise Weighting

External links
ISO Standard
Fletcher–Munson is not Robinson–Dadson
Full Revision of International Standards for Equal-Loudness Level Contours (ISO 226)
Hearing curves and on-line hearing test
Equal-loudness contours by Robinson and Dadson",Category:Audio engineering,3
92,93,String vibration,"A vibration in a string is a wave. Resonance causes a vibrating string to produce a sound with constant frequency, i.e. constant pitch. If the length or tension of the string is correctly adjusted, the sound produced is a musical note. Vibrating strings are the basis of string instruments such as guitars, cellos, and pianos.

Wave
The velocity of propagation of a wave in a string (
  
    
      
        v
      
    
    {\displaystyle v}
  ) is proportional to the square root of the force of tension of the string (
  
    
      
        T
      
    
    {\displaystyle T}
  ) and inversely proportional to the square root of the linear density (
  
    
      
        ?
      
    
    {\displaystyle \mu }
  ) of the string:

  
    
      
        v
        =
        
          
            
              T
              ?
            
          
        
        .
      
    
    {\displaystyle v={\sqrt {T \over \mu }}.}
  
This relationship was discovered by Vincenzo Galilei in the late 1500s.

Derivation [1]
Let 
  
    
      
        ?
        x
      
    
    {\displaystyle \Delta x}
   be the length of a piece of string, 
  
    
      
        m
      
    
    {\displaystyle m}
   its mass, and 
  
    
      
        ?
      
    
    {\displaystyle \mu }
   its linear density. If the horizontal component of tension in the string is a constant, 
  
    
      
        T
      
    
    {\displaystyle T}
  , then the tension acting on each side of the string segment is given by

  
    
      
        
          T
          
            1
            x
          
        
        =
        
          T
          
            1
          
        
        cos
        ?
        (
        ?
        )
        ?
        T
        .
      
    
    {\displaystyle T_{1x}=T_{1}\cos(\alpha )\approx T.}
  

  
    
      
        
          T
          
            2
            x
          
        
        =
        
          T
          
            2
          
        
        cos
        ?
        (
        ?
        )
        ?
        T
        .
      
    
    {\displaystyle T_{2x}=T_{2}\cos(\beta )\approx T.}
  
If both angles are small, then the tensions on either side are equal and the net horizontal force is zero. From Newton's second law for the vertical component, the mass of this piece times its acceleration, 
  
    
      
        a
      
    
    {\displaystyle a}
  , will be equal to the net force on the piece:

  
    
      
        ?
        
          F
          
            y
          
        
        =
        
          T
          
            1
            y
          
        
        ?
        
          T
          
            2
            y
          
        
        =
        ?
        
          T
          
            2
          
        
        sin
        ?
        (
        ?
        )
        +
        
          T
          
            1
          
        
        sin
        ?
        (
        ?
        )
        =
        ?
        m
        a
        ?
        ?
        ?
        x
        
          
            
              
                ?
                
                  2
                
              
              y
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle \Sigma F_{y}=T_{1y}-T_{2y}=-T_{2}\sin(\beta )+T_{1}\sin(\alpha )=\Delta ma\approx \mu \Delta x{\frac {\partial ^{2}y}{\partial t^{2}}}.}
  
Dividing this expression by 
  
    
      
        T
      
    
    {\displaystyle T}
   and substituting the first and second equations obtains

  
    
      
        
          
            
              ?
              ?
              x
            
            T
          
        
        
          
            
              
                ?
                
                  2
                
              
              y
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        =
        ?
        
          
            
              
                T
                
                  2
                
              
              sin
              ?
              (
              ?
              )
            
            
              
                T
                
                  2
                
              
              cos
              ?
              (
              ?
              )
            
          
        
        +
        
          
            
              
                T
                
                  1
                
              
              sin
              ?
              (
              ?
              )
            
            
              
                T
                
                  1
                
              
              cos
              ?
              (
              ?
              )
            
          
        
        =
        ?
        tan
        ?
        (
        ?
        )
        +
        tan
        ?
        (
        ?
        )
      
    
    {\displaystyle {\frac {\mu \Delta x}{T}}{\frac {\partial ^{2}y}{\partial t^{2}}}=-{\frac {T_{2}\sin(\beta )}{T_{2}\cos(\beta )}}+{\frac {T_{1}\sin(\alpha )}{T_{1}\cos(\alpha )}}=-\tan(\beta )+\tan(\alpha )}
  
The tangents of the angles at the ends of the string piece are equal to the slopes at the ends, with an additional minus sign due to the definition of alpha and beta. Using this fact and rearranging provides

  
    
      
        
          
            1
            
              ?
              x
            
          
        
        
          (
          
            
              
                
                
                  
                    
                      ?
                      y
                    
                    
                      ?
                      x
                    
                  
                
                |
              
              
                x
                +
                ?
                x
              
            
            ?
            
              
                
                
                  
                    
                      ?
                      y
                    
                    
                      ?
                      x
                    
                  
                
                |
              
              
                x
              
            
          
          )
        
        =
        
          
            ?
            T
          
        
        
          
            
              
                ?
                
                  2
                
              
              y
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\frac {1}{\Delta x}}\left(\left.{\frac {\partial y}{\partial x}}\right|^{x+\Delta x}-\left.{\frac {\partial y}{\partial x}}\right|^{x}\right)={\frac {\mu }{T}}{\frac {\partial ^{2}y}{\partial t^{2}}}}
  
In the limit that 
  
    
      
        ?
        x
      
    
    {\displaystyle \Delta x}
   approaches zero, the left hand side is the definition of the second derivative of 
  
    
      
        y
      
    
    {\displaystyle y}
  :

  
    
      
        
          
            
              
                ?
                
                  2
                
              
              y
            
            
              ?
              
                x
                
                  2
                
              
            
          
        
        =
        
          
            ?
            T
          
        
        
          
            
              
                ?
                
                  2
                
              
              y
            
            
              ?
              
                t
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\frac {\partial ^{2}y}{\partial x^{2}}}={\frac {\mu }{T}}{\frac {\partial ^{2}y}{\partial t^{2}}}.}
  
This is the wave equation for 
  
    
      
        y
        (
        x
        ,
        t
        )
      
    
    {\displaystyle y(x,t)}
  , and the coefficient of the second time derivative term is equal to 
  
    
      
        
          v
          
            ?
            2
          
        
      
    
    {\displaystyle v^{-2}}
  ; thus

  
    
      
        v
        =
        
          
            
              T
              ?
            
          
        
        ,
      
    
    {\displaystyle v={\sqrt {T \over \mu }},}
  
where 
  
    
      
        v
      
    
    {\displaystyle v}
   is the speed of propagation of the wave in the string. (See the article on the wave equation for more about this). However, this derivation is only valid for vibrations of small amplitude; for those of large amplitude, 
  
    
      
        ?
        x
      
    
    {\displaystyle \Delta x}
   is not a good approximation for the length of the string piece, the horizontal component of tension is not necessarily constant, and the horizontal tensions are not well approximated by 
  
    
      
        T
      
    
    {\displaystyle T}
  .

Frequency of the wave
Once the speed of propagation is known, the frequency of the sound produced by the string can be calculated. The speed of propagation of a wave is equal to the wavelength 
  
    
      
        ?
      
    
    {\displaystyle \lambda }
   divided by the period 
  
    
      
        ?
      
    
    {\displaystyle \tau }
  , or multiplied by the frequency 
  
    
      
        f
      
    
    {\displaystyle f}
  :

  
    
      
        v
        =
        
          
            ?
            ?
          
        
        =
        ?
        f
        .
      
    
    {\displaystyle v={\frac {\lambda }{\tau }}=\lambda f.}
  
If the length of the string is 
  
    
      
        L
      
    
    {\displaystyle L}
  , the fundamental harmonic is the one produced by the vibration whose nodes are the two ends of the string, so 
  
    
      
        L
      
    
    {\displaystyle L}
   is half of the wavelength of the fundamental harmonic. Hence one obtains Mersenne's laws:

  
    
      
        f
        =
        
          
            v
            
              2
              L
            
          
        
        =
        
          
            1
            
              2
              L
            
          
        
        
          
            
              T
              ?
            
          
        
      
    
    {\displaystyle f={\frac {v}{2L}}={1 \over 2L}{\sqrt {T \over \mu }}}
  
where 
  
    
      
        T
      
    
    {\displaystyle T}
   is the tension (in Newtons), 
  
    
      
        ?
      
    
    {\displaystyle \mu }
   is the linear density (that is, the mass per unit length), and 
  
    
      
        L
      
    
    {\displaystyle L}
   is the length of the vibrating part of the string. Therefore:
the shorter the string, the higher the frequency of the fundamental
the higher the tension, the higher the frequency of the fundamental
the lighter the string, the higher the frequency of the fundamental
Moreover, if we take the nth harmonic as having a wavelength given by 
  
    
      
        
          ?
          
            n
          
        
        =
        2
        L
        
          /
        
        n
      
    
    {\displaystyle \lambda _{n}=2L/n}
  , then we easily get an expression for the frequency of the nth harmonic:

  
    
      
        
          f
          
            n
          
        
        =
        
          
            
              n
              v
            
            
              2
              L
            
          
        
      
    
    {\displaystyle f_{n}={\frac {nv}{2L}}}
  
And for a string under a tension T with density 
  
    
      
        ?
      
    
    {\displaystyle \mu }
  , then

  
    
      
        
          f
          
            n
          
        
        =
        
          
            n
            
              2
              L
            
          
        
        
          
            
              T
              ?
            
          
        
      
    
    {\displaystyle f_{n}={\frac {n}{2L}}{\sqrt {\frac {T}{\mu }}}}

Observing string vibrations
One can see the waveforms on a vibrating string if the frequency is low enough and the vibrating string is held in front of a CRT screen such as one of a television or a computer (not of an analog oscilloscope). This effect is called the stroboscopic effect, and the rate at which the string seems to vibrate is the difference between the frequency of the string and the refresh rate of the screen. The same can happen with a fluorescent lamp, at a rate that is the difference between the frequency of the string and the frequency of the alternating current. (If the refresh rate of the screen equals the frequency of the string or an integer multiple thereof, the string will appear still but deformed.) In daylight and other non-oscillating light sources, this effect does not occur and the string appears still but thicker, and lighter or blurred, due to persistence of vision.
A similar but more controllable effect can be obtained using a stroboscope. This device allows matching the frequency of the xenon flash lamp to the frequency of vibration of the string. In a dark room, this clearly shows the waveform. Otherwise, one can use bending or, perhaps more easily, by adjusting the machine heads, to obtain the same, or a multiple, of the AC frequency to achieve the same effect. For example, in the case of a guitar, the 6th (lowest pitched) string pressed to the third fret gives a G at 97.999 Hz. A slight adjustment can alter it to 100 Hz, exactly one octave above the alternating current frequency in Europe and most countries in Africa and Asia, 50 Hz. In most countries of the Americas—where the AC frequency is 60 Hz—altering A# on the fifth string, first fret from 116.54 Hz to 120 Hz produces a similar effect.

See also
Fretted instruments
Musical acoustics
Vibrations of a circular drum
Melde's experiment
3rd bridge (harmonic resonance based on equal string divisions)
String resonance
Reflection phase change

References
Molteno, T. C. A.; N. B. Tufillaro (September 2004). ""An experimental investigation into the dynamics of a string"". American Journal of Physics. 72 (9): 1157–1169. Bibcode:2004AmJPh..72.1157M. doi:10.1119/1.1764557. 
Tufillaro, N. B. (1989). ""Nonlinear and chaotic string vibrations"". American Journal of Physics. 57 (5): 408. Bibcode:1989AmJPh..57..408T. doi:10.1119/1.16011.

External links
Java simulation of waves on a string
Physics of a harpsichord string
A friendly explanation of standing waves and fundamental frequency
""The Vibrating String"" by Alain Goriely and Mark Robertson-Tessi, The Wolfram Demonstrations Project.",Category:Sound,3
93,94,Sound localization in owls,"Most owls are nocturnal or crepuscular birds of prey. Because they hunt at night, they must rely on non-visual senses. Experiments by Roger Payne have shown that owls are sensitive to the sounds made by their prey, not the heat or the smell. In fact, the sound cues are both necessary and sufficient for localization of mice from a distant location where they are perched. For this to work, the owls must be able to accurately localize both the azimuth and the elevation of the sound source.

ITD and ILD
Owls must be able to determine the necessary angle of descent, i.e. the elevation, in addition to azimuth (horizontal angle to the sound). This bi-coordinate sound localization is accomplished through two binaural cues: the interaural time difference (ITD) and the interaural level difference (ILD), also known as the interaural intensity difference (IID). The ability in owls is unusual; in ground-bound mammals such as mice, ITD and ILD are not utilized in the same manner. In these mammals, ITDs tend to be utilized for localization of lower frequency sounds, while ILDs tend to be used for higher frequency sounds.
ITD occurs whenever the distance from the source of sound to the two ears is different, resulting in differences in the arrival times of the sound at the two ears. When the sound source is directly in front of the owl, there is no ITD, i.e. the ITD is zero. In sound localization, ITDs are used as cues for location in the azimuth. ITD changes systematically with azimuth. Sounds to the right arrive first at the right ear; sounds to the left arrive first at the left ear.
In mammals there is a level difference in sounds at the two ears caused by the sound-shadowing effect of the head. But in many species of owls, level differences arise primarily for sounds that are shifted above or below the elevation of the horizontal plane. This is due to the asymmetry in placement of the ear openings in the owl's head, such that sounds from below the owl reach the left ear first and sounds from above reach the right ear first. IID is a measure of the difference in the level of the sound as it reaches each ear. In many owls, IIDs for high-frequency sounds (higher than 4 or 5 kHz) are the principal cues for locating sound elevation.

Parallel processing pathways in the brain
The axons of the auditory nerve originate from the hair cells of the cochlea in the inner ear. Different sound frequencies are encoded by different fibers of the auditory nerve, arranged along the length of the auditory nerve, but codes for the timing and level of the sound are not segregated within the auditory nerve. Instead, the ITD is encoded by phase locking, i.e. firing at or near a particular phase angle of the sinusoidal stimulus sound wave, and the IID is encoded by spike rate. Both parameters are carried by each fiber of the auditory nerve.
The fibers of the auditory nerve innervate both cochlear nuclei in the brainstem, the cochlear nucleus magnocellularis (mammalian anteroventral cochlear nucleus) and the cochlear nucleus angularis (see figure; mammalian posteroventral and dorsal cochlear nuclei). The neurons of the nucleus magnocellularis phase-lock, but are fairly insensitive to variations in sound pressure, while the neurons of the nucleus angularis phase-lock poorly, if at all, but are sensitive to variations in sound pressure. These two nuclei are the starting points of two separate but parallel pathways to the inferior colliculus: the pathway from nucleus magnocellularis processes ITDs, and the pathway from nucleus angularis processes IID.

In the time pathway, the nucleus laminaris (mammalian medial superior olive) is the first site of binaural convergence. It is here that ITD is detected and encoded using neuronal delay lines and coincidence detection, as in the Jeffress model; when phase-locked impulses coming from the left and right ears coincide at a laminaris neuron, the cell fires most strongly. Thus, the nucleus laminaris acts as a delay-line coincidence detector, converting distance traveled to time delay and generating a map of interaural time difference. Neurons from the nucleus laminaris project to the core of the central nucleus of the inferior colliculus and to the anterior lateral lemniscal nucleus.
In the sound level pathway, the posterior lateral lemniscal nucleus (mammalian lateral superior olive) is the site of binaural convergence and where IID is processed. Stimulation of the contralateral ear inhibits and that of the ipsilateral ear excites the neurons of the nuclei in each brain hemisphere independently. The degree of excitation and inhibition depends on sound pressure, and the difference between the strength of the inhibitory input and that of the excitatory input determines the rate at which neurons of the lemniscal nucleus fire. Thus the response of these neurons is a function of the difference in sound pressure between the two ears.
The time and sound-pressure pathways converge at the lateral shell of the central nucleus of the inferior colliculus. The lateral shell projects to the external nucleus, where each space-specific neuron responds to acoustic stimuli only if the sound originates from a restricted area in space, i.e. the receptive field of that neuron. These neurons respond exclusively to binaural signals containing the same ITD and IID that would be created by a sound source located in the neuron’s receptive field. Thus their receptive fields arise from the neurons’ tuning to particular combinations of ITD and IID, simultaneously in a narrow range. These space-specific neurons can thus form a map of auditory space in which the positions of receptive fields in space are isomorphically projected onto the anatomical sites of the neurons.

Significance of asymmetrical ears for localization of elevation
The ears of many species of owls are asymmetrical. For example, in barn owls (Tyto alba), the placement of the two ear flaps (operculi) lying directly in front of the ear canal opening is different for each ear. This asymmetry is such that the center of the left ear flap is slightly above a horizontal line passing through the eyes and directed downward, while the center of the right ear flap is slightly below the line and directed upward. In two other species of owls with asymmetrical ears, the saw-whet owl and the long-eared owl, the asymmetry is achieved by different means: in saw whets, the skull is asymmetrical; in the long-eared owl, the skin structures lying near the ear form asymmetrical entrances to the ear canals, which is achieved by a horizontal membrane. Thus, ear asymmetry seems to have evolved on at least three different occasions among owls. Because owls depend on their sense of hearing for hunting, this convergent evolution in owl ears suggests that asymmetry is important for sound localization in the owl.
Ear asymmetry allows for sound originating from below the eye level to sound louder in the left ear, while sound originating from above the eye level to sound louder in the right ear. Asymmetrical ear placement also causes IID for high frequencies (between 4 kHz and 8 kHz) to vary systematically with elevation, converting IID into a map of elevation. Thus, it is essential for an owl to have the ability to hear high frequencies. Many birds have the neurophysiological machinery to process both ITD and IID, but because they have small heads and low frequency sensitivity, they use both parameters only for localization in the azimuth. Through evolution, the ability to hear frequencies higher than 3 kHz, the highest frequency of owl flight noise, enabled owls to exploit elevational IIDs, produced by small ear asymmetries that arose by chance, and began the evolution of more elaborate forms of ear asymmetry.
Another demonstration of the importance of ear asymmetry in owls is that, in experiments, owls with symmetrical ears, such as the screech owl (Otus asio) and the great horned owl (Bubo virginianus), could not be trained to locate prey in total darkness, whereas owls with asymmetrical ears could be trained.


== References ==",Category:Sound,3
94,95,Soundscape ecology,"The term soundscape ecology, first appeared in the Handbook for Acoustic Ecology, Barry Truax Ed., in 1978. It focuses on the study of the effects of the acoustic environment on the physical and behavioral characteristics of those organisms living within it. It has occasionally been used, sometimes interchangeably, with the term, acoustic ecology. Soundscape ecologists also study the relationships between the three basic sources of sound that comprise the soundscape: those generated by organisms are referred to as the biophony; those from non-biological natural categories are classified as the geophony, and those produced by humans, the anthropophony. Increasingly soundscapes are dominated by a sub-set of anthropophony (sometimes referred to in older, more archaic terminology as ""anthropogenic noise""), or technophony, the overwhelming presence of electro-mechanical noise. This sub-class of noise pollution or disturbance may produce a negative effect on a wide range of organisms. Variations in soundscapes as a result of natural phenomena and/or human endeavor may have wide-ranging ecological effects as many organisms have evolved to respond to acoustic cues that emanate primarily from undisturbed habitats. Soundscape ecologists use recording devices, audio tools, and elements of traditional ecological and acoustic analyses to study soundscape structure. Soundscape ecology has deepened current understandings of ecological issues and established profound visceral connections to ecological data. The preservation of natural soundscapes is now a recognized conservation goal.

Description
Soundscape ecology is the bio- and geo-acoustic branch of ecology that studies acoustic signatures from whatever source within a landscape (the soundscape). The soundscape of a given region can be viewed as the sum of three separate sound sources: Geophony is the first sound heard on earth. Non-biological in nature, it consists of the effect of wind in trees or grasses, water flowing in a stream, waves at an ocean or lake shoreline, and movement of the earth. Biophony is a term introduced by soundscape ecologist, Bernie Krause, who in 1998, first began to express the soundscape in terms of its acoustic sources. The biophony refers to the collective acoustic signatures generated by all sound-producing organisms in a given habitat at a given moment. It includes vocalizations that are used for conspecific communication in some cases. Anthropophony is another term introduced by Bernie Krause along with colleague, Stuart Gage. It represents human sources from heavily populated urban regions usually contains information that was intentionally produced for communication with a sound receiver. The expression in various combinations of these acoustic features across space and time generate unique soundscapes.
Soundscape ecologists seek to investigate the structure of soundscapes, explain how they are generated, and study how organisms interrelate acoustically. A number of hypotheses have been proposed to explain the structure of soundscapes, particularly elements of biophony. For instance, an ecological theory known as the acoustic adaptation hypothesis predicts that acoustic signals of animals are altered in different physical environments in order to maximize their propagation through the habitat. In addition, acoustic signals from organisms may be under selective pressure to minimize their frequency (pitch) overlap with other auditory features of the environment. This acoustic niche hypothesis is analogous to the classical ecological concept of niche partitioning. It suggests that acoustic signals in the environment should display frequency partitioning as a result of selection acting to maximize the effectiveness of intraspecific communication for different species. Observations of frequency differentiation among insects, birds, and anurans support the acoustic niche hypothesis. Organisms may also partition their vocalization frequencies to avoid overlap with pervasive geophonic sounds. For example, territorial communication in some frog species takes place partially in the high frequency ultrasonic spectrum. This communication method represents an evolutionary adaptation to the frogs' riparian habitat where running water produces constant low frequency sound. Invasive species that introduce new sounds into soundscapes can disrupt acoustic niche partitioning in native communities, a process known as biophonic invasion. Although adaptation to acoustic niches may explain the frequency structure of soundscapes, spatial variation in sound is likely to be generated by environmental gradients in altitude, latitude, or habitat disturbance. These gradients may alter the relative contributions of biophony, geophony, and anthrophony to the soundscape. For example, when compared with unaltered habitats, regions with high levels of urban land-use are likely to have increased levels of anthrophony and decreased physical and organismal sound sources. Soundscapes typically exhibit temporal patterns, with daily and seasonal cycles being particularly prominent. These patterns are often generated by the communities of organisms that contribute to biophony. For example, birds chorus heavily at dawn and dusk while anurans call primarily at night; the timing of these vocalization events may have evolved to minimize temporal overlap with other elements of the soundscape.

Contributions from other fields
As an academic discipline, soundscape ecology shares some characteristics with other fields of inquiry but is also distinct from them in significant ways. For instance, acoustic ecology is also concerned with the study of multiple sound sources. However, acoustic ecology, which derives from the founding work of R. Murray Schafer and Barry Truax, primarily focuses on human perception of soundscapes. Soundscape ecology seeks a broader perspective by considering soundscape effects on communities of living organisms, human and Other, and the potential interactions between sounds in the environment. Compared to soundscape ecology, the discipline of bioacoustics tends to have a narrower interest in individual species’ physiological and behavioral mechanisms of auditory communication. Soundscape ecology also borrows heavily from some concepts in landscape ecology, which focuses on ecological patterns and processes occurring over multiple spatial scales. Landscapes may directly influence soundscapes as some organisms use physical features of their habitat to alter their vocalizations. For example, baboons and other animals exploit specific habitats to generate echoes of the sounds they produce.
The function and importance of sound in the environment may not be fully appreciated unless one adopts an organismal perspective on sound perception, and, in this way, soundscape ecology is also informed by sensory ecology. Sensory ecology focuses on understanding the sensory systems of organisms and the biological function of information obtained from these systems. In many cases, humans must acknowledge that sensory modalities and information used by other organisms may not be obvious from an anthropocentric viewpoint. This perspective has already highlighted many instances where organisms rely heavily on sound cues generated within their natural environments to perform important biological functions. For example, a broad range of crustaceans are known to respond to biophony generated around coral reefs. Species that must settle on reefs to complete their developmental cycle are attracted to reef noise while pelagic and nocturnal crustaceans are repelled by the same acoustic signal, presumably as a mechanism to avoid predation (predator densities are high in reef habitats). Similarly, juvenile fish may use biophony as a navigational cue to locate their natal reefs. Other species’ movement patterns are influenced by geophony, as in the case of the reed frog which is known to disperse away from the sound of fire. In addition, a variety of bird and mammal species use auditory cues, such as movement noise, in order to locate prey. Disturbances created by periods of environmental noise may also be exploited by some animals while foraging. For example, insects that prey on spiders concentrate foraging activities during episodes of environmental noise to avoid detection by their prey. These examples demonstrate that many organisms are highly capable of extracting information from soundscapes.

Methods
Acoustic information describing the environment is the primary data required in soundscape ecology studies. Technological advances have provided improved methods for the collection of such data. Automated recording systems allow for temporally replicated samples of soundscapes to be gathered with relative ease. Data collected from such equipment can be extracted to generate a visual representation of the soundscape in the form of a spectrogram. Spectrograms provide information on a number of sound properties that may be subject to quantitative analysis. The vertical axis of a spectrogram indicates the frequency of a sound while the horizontal axis displays the time scale over which sounds were recorded. In addition, spectrograms display the amplitude of sound, a measure of sound intensity. Ecological indices traditionally used with species-level data, such as diversity and evenness, have been adapted for use with acoustic metrics. These measures provide a method of comparing soundscapes across time or space. For example, automated recording devices have been used to gather acoustic data in different landscapes across yearlong time scales, and diversity metrics were employed to evaluate daily and seasonal fluctuations in soundscapes across sites. Spatial patterns of sound may also be studied using tools familiar to landscape ecologists such as geographic information systems (GIS). Finally, recorded samples of the soundscape can provide proxy measures for biodiversity inventories in cases where other sampling methods are impractical or inefficient. These techniques may be especially important for the study of rare or elusive species that are especially difficult to monitor in other ways.

Insights from soundscape ecology: anthropophony
Although soundscape ecology has only recently been defined as an independent academic discipline (it was first described in 2011 and formalized at the first meeting of the International Society of Ecoacoustics, held in Paris in 2014), many earlier ecological investigations have incorporated elements of soundscape ecology theory. For instance, a large body of work has focused on documenting the effects of anthropophony on wildlife. Anthropophony (the uncontrolled version, is often used synonymously with noise pollution) can emanate from a variety of sources, including transportation networks or industry, and may represent a pervasive disturbance to natural systems even in seemingly remote regions such as national parks. A major effect of noise is the masking of organismal acoustic signals that contain information. Against a noisy background, organisms may have trouble perceiving sounds that are important for intraspecific communication, foraging, predator recognition, or a variety of other ecological functions. In this way, anthropogenic noise may represent a soundscape interaction wherein increased anthropophony interferes with biophonic processes. The negative effects of anthropogenic noise impact a wide variety of taxa including fish, amphibians, birds, and mammals. In addition to interfering with ecologically important sounds, anthropophony can also directly affect the biological systems of organisms. Noise exposure, which may be perceived as a threat, can lead to physiological changes. For example, noise can increase levels of stress hormones, impair cognition, reduce immune function, and induce DNA damage. Although much of the research on anthropogenic noise has focused on behavioral and population-level responses to noise disturbance, these molecular and cellular systems may prove promising areas for future work.

Anthropophony and birds
Birds have been used as study organisms in much of the research concerning wildlife responses to anthropogenic noise, and the resulting literature documents many effects that are relevant to other taxa affected by anthropophony. Birds may be particularly sensitive to noise pollution given that they rely heavily on acoustic signals for intraspecific communication. Indeed, a wide range of studies demonstrate that birds use altered songs in noisy environments. Research on great tits in an urban environment revealed that male birds inhabiting noisy territories tended to use higher frequency sounds in their songs. Presumably these higher-pitched songs allow male birds to be heard above anthropogenic noise, which tends to have high energy in the lower frequency range thereby masking sounds in that spectra. A follow-up study of multiple populations confirmed that great tits in urban areas sing with an increased minimum frequency relative to forest-dwelling birds. In addition, this study suggests that noisy urban habitats host birds that use shorter songs but repeat them more rapidly. In contrast to frequency modulations, birds may simply increase the amplitude (loudness) of their songs to decrease masking in environments with elevated noise. Experimental work and field observations show that these song alterations may be the result of behavioral plasticity rather than evolutionary adaptations to noise (i.e., birds actively change their song repertoire depending on the acoustic conditions they experience). In fact, avian vocal adjustments to anthropogenic noise are unlikely to be the products of evolutionary change simply because high noise levels are a relatively recent selection pressure. However, not all bird species adjust their songs to improve communication in noisy environments, which may limit their ability to occupy habitats subject to anthropogenic noise. In some species, individual birds establish a relatively rigid vocal repertoire when they are young, and these sorts of developmental constraints may limit their ability to make vocal adjustments later in life. Thus, species that do not or cannot modify their songs may be particularly sensitive to habitat degradation as a result of noise pollution.

Even among birds that are able to alter their songs to be better heard in environments inundated with anthropophony, these behavioral changes may have important fitness consequences. In the great tit, for example, there is a tradeoff between signal strength and signal detection that depends on song frequency. Male birds that include more low frequency sounds in their song repertoire experience better sexual fidelity from their mates which results in increased reproductive success. However, low frequency sounds tend to be masked when anthropogenic noise is present, and high frequency songs are more effective at eliciting female responses under these conditions. Birds may therefore experience competing selective pressures in habitats with high levels of anthropogenic noise: pressure to call more at lower frequencies in order to improve signal strength and secure good mates versus opposing pressure to sing at higher frequencies in order to ensure that calls are detected against a background of anthrophony. In addition, use of certain vocalizations, including high amplitude sounds that reduce masking in noisy environments, may impose energetic costs that reduce fitness. Because of the reproductive trade-offs and other stresses they impose on some birds, noisy habitats may represent ecological traps, habitats in which individuals have reduced fitness yet are colonized at rates greater than or equal to other habitats.
Anthropophony may ultimately have population- or community-level impacts on avian fauna. One study focusing on community composition found that habitats exposed to anthropophony hosted fewer bird species than regions without noise, but both areas had similar numbers of nests. In fact, nests in noisy habitats had higher survival than those laid in control habitats, presumably because noisy environments hosted fewer western scrub jays which are major nest predators of other birds. Thus, anthropophony can have negative effects on local species diversity, but the species capable of coping with noise disturbance may actually benefit from the exclusion of negative species interactions in those areas. Other experiments suggest that noise pollution has the potential to affect avian mating systems by altering the strength of pair bonds. When exposed to high amplitude environmental noise in a laboratory setting, zebra finches, a monogamous species, show a decreased preference for their mated partners. Similarly, male reed buntings in quiet environments are more likely to be part of a mated pair than males in noisy locations. Such effects may ultimately result in reduced reproductive output of birds subject to high levels of environmental noise.

Soundscape conservation
The discipline of conservation biology has traditionally been concerned with the preservation of biodiversity and the habitats that organisms are dependent upon. However, soundscape ecology encourages biologists to consider natural soundscapes as resources worthy of conservation efforts. Soundscapes that come from relatively untrammeled habitats have value for wildlife as demonstrated by the numerous negative effects of anthropogenic noise on various species. Organisms that use acoustic cues generated by their prey may be particularly impacted by human-altered soundscapes. In this situation, the (unintentional) senders of the acoustic signals will have no incentive to compensate for masking imposed by anthropogenic sound. In addition, natural soundscapes can have benefits for human wellbeing and may help generate a distinct sense of place, connecting people to the environment and providing unique aesthetic experiences. Because of the various values inherent in natural soundscapes, they may be considered ecosystem services that are provisioned by intact, functioning ecosystems. Targets for soundscape conservation may include soundscapes necessary for the persistence of threatened wildlife, soundscapes that are themselves being severely altered by anthrophony, and soundscapes that represent unique places or cultural values. Some governments and management agencies have begun to consider preservation of natural soundscapes as an environmental priority. In the United States, the National Park Service's Natural Sounds and Night Skies Division is working to protect natural and cultural soundscapes.

See also
Acoustic ecology
Bioacoustics
Conservation biology
Ecology
Niche hypothesis
Noise pollution
Sensory ecology

References
External links
Science and Humanities related to Soundscape Ecology at Wild Sanctuary
Biophonic study related to California drought
California Drought has Distinct Sound
Soundscape ecology at the Purdue University Human-Environment Modeling and Analysis Laboratory
The New York Times Magazine article on soundscape research with soundscape recordings from Denali National Park
National Science Foundation article on soundscape ecology
National Public Radio story on soundscape ecology with associated audio recordings
BLDGBLOG article on soundscape ecology with comments on the related field of acoustic ecology
ScienceDaily article on soundscape ecology
U.S. National Park Service's Natural Sounds and Night Skies Division
Soundscape: The Journal of Acoustic Ecology, published by the World Forum for Acoustic Ecology
Leonardo Soundscape and Acoustic Ecology Bibliography, compiled by Maksymilian Kapela?ski",Category:Ecological techniques,3
95,96,Sonic boom,"A sonic boom is the sound associated with the shock waves created by an object traveling through the air faster than the speed of sound. Sonic booms generate significant amounts of sound energy, sounding much like an explosion to the human ear. The crack of a supersonic bullet passing overhead or the crack of a bullwhip are examples of a sonic boom in miniature.
Contrary to popular belief, a sonic boom does not occur only at the moment an object crosses the speed of sound; and neither is it heard in all directions emanating from the speeding object. Rather the boom is a continuous effect that occurs while the object is travelling at supersonic speeds. But it only affects observers that are positioned at a some point that intersects an imaginary geometrical cone behind the object. As the object moves, this imaginary cone also moves behind it and when the cone passes over the observer, they will briefly experience the boom.

Causes
When an aircraft passes through the air it creates a series of pressure waves in front of it and behind it, similar to the bow and stern waves created by a boat. These waves travel at the speed of sound and, as the speed of the object increases, the waves are forced together, or compressed, because they cannot get out of the way of each other. Eventually they merge into a single shock wave, which travels at the speed of sound, a critical speed known as Mach 1, and is approximately 1,235 km/h (767 mph) at sea level and 20 °C (68 °F).
In smooth flight, the shock wave starts at the nose of the aircraft and ends at the tail. Because the different radial directions around the aircraft's direction of travel are equivalent (given the ""smooth flight"" condition), the shock wave forms a Mach cone, similar to a vapour cone, with the aircraft at its tip. The half-angle between direction of flight and the shock wave 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
   is given by:

  
    
      
        sin
        ?
        (
        ?
        )
        =
        
          
            
              v
              
                sound
              
            
            
              v
              
                object
              
            
          
        
      
    
    {\displaystyle \sin(\alpha )={\frac {v_{\text{sound}}}{v_{\text{object}}}}}
  ,
where 
  
    
      
        
          
            
              v
              
                sound
              
            
            
              v
              
                object
              
            
          
        
      
    
    {\displaystyle {\frac {v_{\text{sound}}}{v_{\text{object}}}}}
   is the inverse 
  
    
      
        
          
            (
          
        
        
          
            1
            
              M
              a
            
          
        
        
          
            )
          
        
      
    
    {\displaystyle {\Big (}{\frac {1}{Ma}}{\Big )}}
   of the plane's Mach number (
  
    
      
        M
        a
        =
        
          
            
              v
              
                object
              
            
            
              v
              
                sound
              
            
          
        
      
    
    {\displaystyle Ma={\frac {v_{\text{object}}}{v_{\text{sound}}}}}
  ). Thus the faster the plane travels, the finer and more pointed the cone is.
There is a rise in pressure at the nose, decreasing steadily to a negative pressure at the tail, followed by a sudden return to normal pressure after the object passes. This ""overpressure profile"" is known as an N-wave because of its shape. The ""boom"" is experienced when there is a sudden change in pressure; therefore, an N-wave causes two booms – one when the initial pressure-rise reaches an observer, and another when the pressure returns to normal. This leads to a distinctive ""double boom"" from a supersonic aircraft. When the aircraft is maneuvering, the pressure distribution changes into different forms, with a characteristic U-wave shape.
Since the boom is being generated continually as long as the aircraft is supersonic, it fills out a narrow path on the ground following the aircraft's flight path, a bit like an unrolling red carpet, and hence known as the boom carpet. Its width depends on the altitude of the aircraft. The distance from the point on the ground where the boom is heard to the aircraft depends on its altitude and the angle 
  
    
      
        ?
      
    
    {\displaystyle \alpha }
  .
For today's supersonic aircraft in normal operating conditions, the peak overpressure varies from less than 50 to 500 Pa (1 to 10 psf (pound per square foot)) for an N-wave boom. Peak overpressures for U-waves are amplified two to five times the N-wave, but this amplified overpressure impacts only a very small area when compared to the area exposed to the rest of the sonic boom. The strongest sonic boom ever recorded was 7,000 Pa (144 psf) and it did not cause injury to the researchers who were exposed to it. The boom was produced by an F-4 flying just above the speed of sound at an altitude of 100 feet (30 m). In recent tests, the maximum boom measured during more realistic flight conditions was 1,010 Pa (21 psf). There is a probability that some damage — shattered glass, for example — will result from a sonic boom. Buildings in good condition should suffer no damage by pressures of 530 Pa (11 psf) or less. And, typically, community exposure to sonic boom is below 100 Pa (2 psf). Ground motion resulting from sonic boom is rare and is well below structural damage thresholds accepted by the U.S. Bureau of Mines and other agencies.
The power, or volume, of the shock wave depends on the quantity of air that is being accelerated, and thus the size and shape of the aircraft. As the aircraft increases speed the shock cone gets tighter around the craft and becomes weaker to the point that at very high speeds and altitudes no boom is heard. The ""length"" of the boom from front to back depends on the length of the aircraft to a power of 3/2. Longer aircraft therefore ""spread out"" their booms more than smaller ones, which leads to a less powerful boom.
Several smaller shock waves can and usually do form at other points on the aircraft, primarily at any convex points, or curves, the leading wing edge, and especially the inlet to engines. These secondary shockwaves are caused by the air being forced to turn around these convex points, which generates a shock wave in supersonic flow.
The later shock waves are somewhat faster than the first one, travel faster and add to the main shockwave at some distance away from the aircraft to create a much more defined N-wave shape. This maximizes both the magnitude and the ""rise time"" of the shock which makes the boom seem louder. On most aircraft designs the characteristic distance is about 40,000 feet (12,000 m), meaning that below this altitude the sonic boom will be ""softer"". However, the drag at this altitude or below makes supersonic travel particularly inefficient, which poses a serious problem.

Measurement and examples
The pressure from sonic booms caused by aircraft often are a few pounds per square foot. A vehicle flying at greater altitude will generate lower pressures on the ground, because the shock wave reduces in intensity as it spreads out away from the vehicle, but the sonic booms are less affected by vehicle speed.

Abatement
In the late 1950s when supersonic transport (SST) designs were being actively pursued, it was thought that although the boom would be very large, the problems could be avoided by flying higher. This assumption was proven false when the North American B-70 Valkyrie started flying, and it was found that the boom was a problem even at 70,000 feet (21,000 m). It was during these tests that the N-wave was first characterized.
Richard Seebass and his colleague Albert George at Cornell University studied the problem extensively and eventually defined a ""figure of merit"" (FM) to characterize the sonic boom levels of different aircraft. FM is a function of the aircraft weight and the aircraft length. The lower this value, the less boom the aircraft generates, with figures of about 1 or lower being considered acceptable. Using this calculation, they found FMs of about 1.4 for Concorde and 1.9 for the Boeing 2707. This eventually doomed most SST projects as public resentment mixed with politics eventually resulted in laws that made any such aircraft impractical (flying supersonically only over water for instance). Another way to express this is wing span. The fuselage of even a large supersonic aircraft is very sleek and with enough angle of attack and wing span the plane can fly so high that the boom by the fuselage is not important. The larger the wing span, the greater the downwards impulse which can be applied to the air, the greater the boom felt. A smaller wing span favors small aeroplane designs like business jets.
Seebass and George also worked on the problem from a different angle, trying to spread out the N-wave laterally and temporally (longitudinally), by producing a strong and downwards-focused (SR-71 Blackbird, Boeing X-43) shock at a sharp, but wide angle nosecone, which will travel at slightly supersonic speed (bow shock), and using a swept back flying wing or an oblique flying wing to smooth out this shock along the direction of flight (the tail of the shock travels at sonic speed). To adapt this principle to existing planes, which generate a shock at their nose cone and an even stronger one at their wing leading edge, the fuselage below the wing is shaped according to the area rule. Ideally this would raise the characteristic altitude from 40,000 feet (12,000 m) to 60,000 feet (from 12,000 m to 18,000 m), which is where most SST aircraft were expected to fly.

This remained untested for decades, until DARPA started the Quiet Supersonic Platform project and funded the Shaped Sonic Boom Demonstration (SSBD) aircraft to test it. SSBD used an F-5 Freedom Fighter. The F-5E was modified with a highly refined shape which lengthened the nose to that of the F-5F model. The fairing extended from the nose all the way back to the inlets on the underside of the aircraft. The SSBD was tested over a two-year period culminating in 21 flights and was an extensive study on sonic boom characteristics. After measuring the 1,300 recordings, some taken inside the shock wave by a chase plane, the SSBD demonstrated a reduction in boom by about one-third. Although one-third is not a huge reduction, it could have reduced Concorde's boom to an acceptable level; one below the FM = 1 limit stated above, for instance.
As a follow-on to SSBD, in 2006 a NASA-Gulfstream Aerospace team tested the Quiet Spike on NASA-Dryden's F-15B aircraft 836. The Quiet Spike is a telescoping boom fitted to the nose of an aircraft specifically designed to weaken the strength of the shock waves forming on the nose of the aircraft at supersonic speeds. Over 50 test flights were performed. Several flights included probing of the shockwaves by a second F-15B, NASA's Intelligent Flight Control System testbed, aircraft 837.
There are theoretical designs that do not appear to create sonic booms at all, such as the Busemann's Biplane. However, creating a shockwave is inescapable if they generate aerodynamic lift.

Perception, noise and other concerns
The sound of a sonic boom depends largely on the distance between the observer and the aircraft shape producing the sonic boom. A sonic boom is usually heard as a deep double ""boom"" as the aircraft is usually some distance away. However, as those who have witnessed landings of space shuttles have heard, when the aircraft is nearby the sonic boom is a sharper ""bang"" or ""crack"". The sound is much like that of mortar bombs, commonly used in firework displays. It is a common misconception that only one boom is generated during the subsonic to supersonic transition; rather, the boom is continuous along the boom carpet for the entire supersonic flight. As a former Concorde pilot puts it, ""You don't actually hear anything on board. All we see is the pressure wave moving down the aeroplane - it gives an indication on the instruments. And that's what we see around Mach 1. But we don't hear the sonic boom or anything like that. That's rather like the wake of a ship - it's behind us."".
In 1964, NASA and the Federal Aviation Administration began the Oklahoma City sonic boom tests, which caused eight sonic booms per day over a period of six months. Valuable data was gathered from the experiment, but 15,000 complaints were generated and ultimately entangled the government in a class action lawsuit, which it lost on appeal in 1969.
Sonic booms were also a nuisance in North Cornwall and North Devon as these areas were underneath the flight path of Concorde. Windows would rattle and in some cases the ""torching"" (pointing underneath roof slates) would be dislodged with the vibration.
There has been recent work in this area, notably under DARPA's Quiet Supersonic Platform studies. Research by acoustics experts under this program began looking more closely at the composition of sonic booms, including the frequency content. Several characteristics of the traditional sonic boom ""N"" wave can influence how loud and irritating it can be perceived by listeners on the ground. Even strong N-waves such as those generated by Concorde or military aircraft can be far less objectionable if the rise time of the overpressure is sufficiently long. A new metric has emerged, known as perceived loudness, measured in PLdB. This takes into account the frequency content, rise time, etc. A well-known example is the snapping of one's fingers in which the ""perceived"" sound is nothing more than an annoyance.
The energy range of sonic boom is concentrated in the 0.1–100 hertz frequency range that is considerably below that of subsonic aircraft, gunfire and most industrial noise. Duration of sonic boom is brief; less than a second, 100 milliseconds (0.1 second) for most fighter-sized aircraft and 500 milliseconds for the space shuttle or Concorde jetliner. The intensity and width of a sonic boom path depends on the physical characteristics of the aircraft and how it is operated. In general, the greater an aircraft's altitude, the lower the overpressure on the ground. Greater altitude also increases the boom's lateral spread, exposing a wider area to the boom. Overpressures in the sonic boom impact area, however, will not be uniform. Boom intensity is greatest directly under the flight path, progressively weakening with greater horizontal distance away from the aircraft flight track. Ground width of the boom exposure area is approximately 1 statute mile (1.6 km) for each 1,000 feet (300 m) of altitude (the width is about five times the altitude); that is, an aircraft flying supersonic at 30,000 feet (9,100 m) will create a lateral boom spread of about 30 miles (48 km). For steady supersonic flight, the boom is described as a carpet boom since it moves with the aircraft as it maintains supersonic speed and altitude. Some manoeuvers, diving, acceleration or turning, can cause focusing of the boom. Other manoeuvers, such as deceleration and climbing, can reduce the strength of the shock. In some instances weather conditions can distort sonic booms.
Depending on the aircraft's altitude, sonic booms reach the ground two to 60 seconds after flyover. However, not all booms are heard at ground level. The speed of sound at any altitude is a function of air temperature. A decrease or increase in temperature results in a corresponding decrease or increase in sound speed. Under standard atmospheric conditions, air temperature decreases with increased altitude. For example, when sea-level temperature is 59 degrees Fahrenheit (15 °C), the temperature at 30,000 feet (9,100 m) drops to minus 49 degrees Fahrenheit (?45 °C). This temperature gradient helps bend the sound waves upward. Therefore, for a boom to reach the ground, the aircraft speed relative to the ground must be greater than the speed of sound at the ground. For example, the speed of sound at 30,000 feet (9,100 m) is about 670 miles per hour (1,080 km/h), but an aircraft must travel at least 750 miles per hour (1,210 km/h) (Mach 1.12, where Mach 1 equals the speed of sound) for a boom to be heard on the ground.
The composition of the atmosphere is also a factor. Temperature variations, humidity, atmospheric pollution, and winds can all have an effect on how a sonic boom is perceived on the ground. Even the ground itself can influence the sound of a sonic boom. Hard surfaces such as concrete, pavement, and large buildings can cause reflections which may amplify the sound of a sonic boom. Similarly grassy fields and lots of foliage can help attenuate the strength of the overpressure of a sonic boom.
Currently there are no industry accepted standards for the acceptability of a sonic boom. Until such metrics can be established, either through further study or supersonic overflight testing, it is doubtful that legislation will be enacted to remove the current prohibition on supersonic overflight in place in several countries, including the United States.

Health impact
Some studies claim to show that sonic booms from U.S. Navy testing in Vieques, Puerto Rico, increased the incidence of vibroacoustic disease, a thickening of heart tissue. However, other scientists dispute the claims. Common health effects of noise would be applicable for sonic booms as well.

Bullwhip
The cracking sound a bullwhip makes when properly wielded is, in fact, a small sonic boom. The end of the whip, known as the ""cracker"", moves faster than the speed of sound, thus creating a sonic boom. The whip is probably the first human invention to break the sound barrier.
A bullwhip tapers down from the handle section to the cracker. The cracker has much less mass than the handle section. When the whip is sharply swung, the energy is transferred down the length of the tapering whip. Goriely and McMillen showed that the physical explanation is complex, involving the way that a loop travels down a tapered filament under tension.

See also
Cherenkov radiation
Hypersonic
Supershear earthquake
Ground vibration boom

References
External links
""Audio Recording of SR-71 Blackbird Sonic Booms - YouTube"". Retrieved February 12, 2015. 
Boston Globe profile of Spike Aerospace planned S-521 supersonic jet",Category:All articles with unsourced statements,3
96,97,Sound generator,,Category:Articles needing additional references from September 2009,3
97,98,Alignment level,"The alignment level in an audio signal chain or on an audio recording is a defined anchor point that represents a reasonable or typical level. It does not represent a particular sound level or signal level or digital representation, but it can be defined as corresponding to particular levels in each of these domains.
For example, alignment level is commonly 0 dBu (Equal to 0.775 Volts RMS) in broadcast chains and in professional audio what is commonly known as ""0VU"", which is +4dBu (Equal to 1.227 Volts RMS) in places where the signal exists as analogue voltage. Under normal situations the ""0VU"" reference allowed for a headroom of 18dB or more above the reference level without significant distortion. This is largely due to the use of slow responding VU meters in almost all analog professional audio equipment which, by their design, and by specification responded to an average level, not peak levels. It most commonly is at ?18 dB FS (18 dB below full scale digital) on digital recordings for programme exchange, in accordance with EBU recommendations. Digital equipment must use peak reading metering systems to avoid severe digital distortion caused by the signal going beyond 'full scale' or maximum digital levels. 24-bit original or master recordings commonly have alignment level at ?24 dB FS to allow extra headroom, which can then be reduced to match the available headroom of the final medium by audio level compression. FM broadcasts usually have only 9 dB of headroom as recommended by the EBU, but digital broadcasts, which could operate with 18dB of headroom, given their low noise floor even in difficult reception areas, currently operate in a state of confusion, with some transmitting at maximum level while others operate at much lower level even though they carry material that has been compressed for compatibility with the lower dynamic range of FM transmissions.

Alignment level as used in the EBU
The EBU uses the term ""alignment level"" not for levelling any real audio signals. In the EBU documents ""alignment level"" just defines -18 dBFS as the level of the Alignment Signal (1 kHz sinus tone generator resp. 997 Hz in the digital domain).

The reason for alignment level
Using alignment level rather than maximum permitted level as the reference point allows more sensible headroom management throughout the audio chain, so that quality is only sacrificed through compression as late as possible.
Loudness wars have caused a general fall in audio quality, initially on radio stations and more recently on CDs. As radio stations competed for attention and to raise the listener scores their ad revenue is based on, they used audio compression to give their sound more impact. They used level compressors, and in particular multi-band compressors that compress different frequencies independently. Such compressors usually incorporate fast acting limiters to eliminate brief peaks, since brief peaks, though they may not contribute much to perceived loudness, limit the modulation level that can be applied to FM transmissions in particular, if serious clipping and distortion are to be avoided. Digital broadcasting has changed all this: stations are no longer found by tuning across the band, so the loudest stations no longer stand out. Low noise level is also guaranteed regardless of signal level, so that it is no longer necessary to fully modulate to ensure acceptable clarity in poor reception areas. Many professionals feel that the more widespread adoption and understanding of alignment level throughout the audio industry could help bring modulation levels down, leaving headroom to cope with brief peaks, and using a different form of level compression that reduces dynamic range on programmes where this is considered desirable, but does not remove the brief peaks which add 'sparkle' and contribute to clearer sound. CDs in particular have suffered a loss of quality since they were introduced through the widespread use of fast limiting, which, given their very low noise level is quite unnecessary.
Digital audio players such as the iPod, demonstrate the need for a common alignment level. While tracks taken from recent CDs sound loud enough, many older recordings (such as Pink Floyd albums which notably allowed lots of headroom for stunning dynamic range and rarely reach peak digital level) are far too quiet, even at full volume setting. Older audio systems typically incorporated 12dB of 'overvolume', meaning that it was possible to turn up the loudness on a quiet recording to make maximum use of amplifier output even if peak level was never reached on the recording. Modern devices, however, tend to produce maximum output at full volume only on recordings that reach full-scale digital level. If extra gain is added, then playing a modern CD after listening to a well recorded older one is likely to deafen, requiring the volume control to be turned down by a huge amount. Again, the adoption of a common alignment level (early CDs allowed around 18dB of headroom by common consent) would make sense, improving quality and usability and ending the loudness war.

Making compression a listening option
The incorporation of (switchable) level compression in domestic music systems and car in-car systems would allow higher quality on systems capable of wide dynamic range and in situations that allowed realistic reproduction. Such compression systems have been suggested and tried from time to time, but are not in widespread use — a 'chicken and egg' problem since producers feel they must make programmes and recordings that sound good in car with high ambient noise or on cheap low-power music systems. In the UK, some DAB receivers do incorporate a menu setting for automatic loudness compensation which adds extra gain on BBC Radio 3 and BBC Radio 4, to allow for the fact that these programmes adopt lower levels than, for example, the pop station Radio 1. Some television receivers also have a menu setting for loudness normalisation, aimed at helping to reduce excessive loudness on advertisements. However, there is no common agreement to reduce compression and limiting and leave these tasks to the receiver.

See also
Audio normalization
Full scale
Nominal level
Programme level
Transmission-level point

External links
EBU Recommendation R128 - Loudness normalisation and permitted maximum level of audio levels (2010)
EBU Recommendation R68-2000
EBU Recommendation R117-2006 (against loudness war)
AES Convention Paper 5538 On Levelling and Loudness Problems at Broadcast Studios
EBU Tech 3282-E on EBU RDAT Tape Levels
EBU R89-1997 on CD-R levels
Distortion to the People — TC Electronics
EBU Loudness Group",Category:All articles lacking sources,3
98,99,Sound unit,,Category:Sound,3
99,100,Brown note,"The brown note is a hypothetical infrasonic frequency that would cause humans to lose control of their bowels due to resonance. Attempts to demonstrate the existence of a ""brown note"" using sound waves transmitted through air have failed.
The name is a metonym for the common colour of human feces. Frequencies supposedly involved are between 5 and 9 Hz, which is below the lower frequency limit of human hearing. High power sound waves below 20 Hz are felt in the body, not heard by the ear as sound.

Physiological effects of low frequency vibration
Jürgen Altmann of the Dortmund University of Technology, an expert on sonic weapons, has said that there is no reliable evidence for nausea and vomiting caused by infrasound.
High volume levels at concerts from subwoofer arrays have been cited as causing lung collapse in individuals who are very close to the subwoofers, especially for smokers who are particularly tall and thin.
In September 2009, London student Tom Reid died of sudden arrhythmic death syndrome (SADS) after complaining that ""loud bass notes"" were ""getting to his heart"". The inquest recorded a verdict of natural causes, although some experts commented that the bass could have acted as a trigger.
Air is a very inefficient medium for transferring low frequency vibration from a transducer to the human body. Mechanical connection of the vibration source to the human body, however, provides a potentially dangerous combination. The U.S. space program, worried about the harmful effects of rocket flight on astronauts, ordered vibration tests that used cockpit seats mounted on vibration tables to transfer ""brown note"" and other frequencies directly to the human subjects. Very high power levels of 160 dB were achieved at frequencies of 2–3 Hz. Test frequencies ranged from 0.5 Hz to 40 Hz. Test subjects suffered motor ataxia, nausea, visual disturbance, degraded task performance and difficulties in communication. These tests are assumed by researchers to be the nucleus of the current urban myth.
The report ""A Review of Published Research on Low Frequency Noise and its Effects"" contains a long list of research about exposure to high-level infrasound among humans and animals. For instance, in 1972, Borredon exposed 42 young men to tones at 7.5 Hz at 130 dB for 50 minutes. This exposure caused no adverse effects other than reported drowsiness and a slight blood pressure increase. In 1975, Slarve and Johnson exposed four male subjects to infrasound at frequencies from 1 to 20 Hz, for eight minutes at a time, at levels up to 144 dB SPL. There was no evidence of any detrimental effect other than middle ear discomfort. Tests of high-intensity infrasound on animals resulted in measurable changes, such as cell changes and ruptured blood vessel walls.
In February 2005 the television show MythBusters used twelve Meyer Sound 700-HP subwoofers—a model and quantity that has been employed for major rock concerts. Normal operating frequency range of the selected subwoofer model was 28 Hz to 150 Hz but the 12 enclosures at MythBusters had been specially modified for deeper bass extension. Roger Schwenke and John Meyer directed the Meyer Sound team in devising a special test rig that would produce very high sound levels at infrasonic frequencies. The subwoofers' tuning ports were blocked and their input cards were altered. The modified cabinets were positioned in an open ring configuration: four stacks of three subwoofers each. Test signals were generated by a SIM 3 audio analyzer, with its software modified to produce infrasonic tones. A Brüel & Kjær sound level analyzer, fed with an attenuated signal from a model 4189 measurement microphone, displayed and recorded sound pressure levels. The experimenters on the show tried a series of frequencies as low as 5 Hz, attaining a level of 120 decibels of sound pressure at 9 Hz and up to 153 dB at frequencies above 20 Hz, but the rumored physiological effects did not materialize. The test subjects all reported some physical anxiety and shortness of breath, even a small amount of nausea, but this was dismissed by the experimenters, noting that sound at that frequency and intensity moves air rapidly in and out of one's lungs. The show declared the brown note myth ""busted.""

In fiction
In the comic book series Transmetropolitan, the main character, Spider Jerusalem, wields a ""Bowel Disruptor"" that operates using ultrasonic waves, with varying settings of intensity.
In the season 3 episode ""World Wide Recorder Concert"" (episode 17) of the animated show South Park in 2000, the brown note myth is featured prominently; the boys rewrite the music for a worldwide recorder concert to include the brown note, with the result that everyone in the world defecates.
In the season 4 episode ""The Curse of Shiva"" (episode 13) of The League, Taco (Jon Lajoie) has found a use for the thousands of discarded vuvuzela instruments left over from the 2010 FIFA World Cup hosted by South Africa. The instruments were tinkered with to produce the brown note with hopes that it could then be sold to the military as a weapon.
In the season 1 episode ""Angel Boy"" (episode 4) of the Adult Swim show Tim and Eric's Bedtime Stories, the character Scotty sings a note that causes attendees at a birthday party to have intense gastrointestinal distress and defecate uncontrollably.
In the season 6 episode ""Edie's Wedding"" (episode 4) of Archer, Doctor Krieger builds an ultrasonic weapon which hypothetically produces the brown note.

See also
Acoustic resonance
Feraliminal Lycanthropizer
The Mosquito, a commercial device that deters loitering by emitting sound with a very high frequency
The Republic XF-84H, an experimental aircraft that produced enough noise to cause headaches, nausea and seizures among its ground crew


== References ==",Category:Sound,3
100,101,PSPLab,"The Perceptual Signal Processing Lab, or PSPLab, is an audio research lab of National Chiao Tung University. It is located in Hsinchu, Taiwan, and focuses on researching better perceptual signal processing techniques, particularly in regard to DSP, Perception, and Software.
Current areas of research in PSPLab include:
Multi Channel Audio Compression
MP3 codec, MPEG-2/4 AAC codec
Psychoacoustic model, Bit Allocation, Filterbank
Low-delay AAC codec
Objective Quality Measurement
MPEG Surround

Multi Channel Audio Effect Processing
Room Reverberation
Room Acoustics

Real-time DSP Programming/Optimization
Variant Platform Optimization
Fixed-Point DSP Programming

External links
Perceptual Signal Processing Lab homepage",Category:Sound,3
101,102,Recording consciousness,"Bennett (1980, p. 114) describes the development of recording consciousness, the consequence of ""a society which is literally wired for sound"" in which, according to Middleton (1990, p. 88) ""this consciousness defines the social reality of popular music."" ""Acoustic instruments and unamplified, 'pure'-tone singing can now not be heard except as contrasts to more recent kinds of sounds, just as live performances are inevitably 'checked' against memories of recordings,"" and ""live performances have to try to approximate the sounds which inhabit this consciousness.""
""Similarly, musicians learn to play, and learn specific songs, from records, and so 'recording consciousness' helps to explain the ubiquity of non-literate composition methods: 'sheet music is just for people who can't hear' (musician quoted in Bennett 1980, p. 139) The structure of this consciousness has been produced by various elements, among them experience of editing techniques, reverberation and echo, use of equalization to alter timbre, high decibel levels, both in general and in particular parts of the texture (notably, strong bass-lines), and, most interestingly, the 'polyvocality' created by multi-mike or multi-channel recording. Mixing different 'earpoints' produces a 'way of hearing [that] is an acoustic expectation for anyone who listens to contemporary recordings. It cannot be achieved without the aid of electronic devices. It has never before existed on earth' (ibid, p. 119)."" (Middleton 1990, p. 88)

See also
Recording studio as musical instrument

References
Bennett, H. Stith, On Becoming a Rock Musician, Amherst : University of Massachusetts Press, 1980. ISBN 0-87023-311-4
Middleton, Richard (1990/2002). Studying Popular Music. Philadelphia: Open University Press. ISBN 0-335-15275-9.

Further reading
Milner, Greg, ""Perfecting Sound Forever: An Aural History of Recorded Music"", Faber & Faber; 1 edition (June 9, 2009) ISBN 978-0-571-21165-4. Cf. p. 14 on H. Stith Bennett and ""recording consciousness"".",Category:Popular music,3
102,103,Category:Hearing,,Category:Sound,3
103,104,AES11,,Category:All articles lacking sources,3
104,105,Monaural,"Monaural or monophonic sound reproduction (often shortened to mono) is intended to be heard as if it were a single channel of sound perceived as coming from one position (unlike stereo, which uses two channels to convey the impression of sound coming from different places from left, middle, and right). In mono, only one loudspeaker is necessary, but, when played through multiple loudspeakers or headphones, identical signals are fed through each of the wires into each speaker, resulting in the perception of a one-channel sound, which ""images"" in one sonic space between the speakers (provided that the speakers are set up in a proper symmetrical critical-listening placement). Monaural recordings, like stereo, customarily use multiple microphones, fed into multiple channels on a recording console, but each channel is ""panned"" to be in the center. In the final stage, the various center-panned signal paths are usually mixed down to two identical tracks, which because they are identical, are perceived upon playback as representing a single unified signal in a single place in the soundstage. In some cases the multitrack source is mixed down to a one track tape becoming one signal. In the mastering stage, particularly in the days of mono records, the one-track or two-track mono master tape was then transferred to a one-track lathe intended to be used in the pressing of a monophonic record. However, today monaural recordings are usually mastered to be played on stereo and multi-track formats, yet retain their center-panned mono soundstage characteristics when played back.
Monaural sound has been replaced by stereo sound in most entertainment applications. However, it remains the standard for radiotelephone communications, telephone networks, and audio induction loops for use with hearing aids. A few FM radio stations, particularly talk radio shows, choose to broadcast in monaural, as a monaural signal has a slight advantage in signal strength and bandwidth over a stereophonic signal of the same power.

History
While some experiments were made with stereophonic recording and reproduction from the early days of the phonograph in the late-19th century, monaural was the rule for almost all audio recording until the second half of the 20th century.
Monaural sound is normal on:
Phonograph cylinders
Gramophone records made before 1958, such as those made for playing at 78 rpm and earlier ?16 2?3, ?33 1?3 and 45 rpm microgroove records
Some FM radio stations that broadcast spoken word only or talk radio content in order to maximize their coverage area (eg. CBC Radio One stations on the FM dial)
Subcarrier signals for FM radio, which carry leased content such as background music for businesses or a radio reading service for the blind
Background music services such as Seeburg 1000, satellite broadcasts by Muzak as well as some public address systems intended to be used with such services
Incompatible standards exist for:
Later records (monophonic records – which almost disappeared in the United States by the end of 1967 – could be played with a stereo cartridge)
Reel-to-reel audio tape recording (depending on track alignment)
Compatible monaural and stereophonic standards exist for:
MiniDisc
Compact audio cassette
FM (and in rare circumstances AM radio broadcasting)
VCR formats
TV
digital audio file on many computers in many formats (WAV, MP3, etc.)
No native monaural standards exist for:
8-track tape
Compact disc
In those formats, the mono-source material is presented as two identical channels, thus being technically stereo.
At various times artists have preferred to work in mono, either in recognition of the technical limitations of the equipment of the era or because of simple preference (this can be seen as analogous to filmmakers working in black and white) – such as John Mellencamp's 2010 album No Better Than This, recorded in mono to emulate the mid-20th century blues and folk records. Some early recordings such as The Beatles' first four albums (Please Please Me, With the Beatles, A Hard Day's Night, Beatles for Sale) were re-released in the CD era as monophonic in recognition of the fact that the source tapes for the earliest recordings were two-track, with vocals on one track and instruments on the other (even though this was only true on the first two albums, while the latter two had been recorded on four-track). This was actually intended to provide flexibility in producing a final mono mix, not to provide a stereo recording, although because of demand this was done anyway, and the early material was available on vinyl in both mono and stereo formats. In the 1960s and 1970s, it was common in the pop world for stereophonic versions of mono tracks to be generated electronically using filtering techniques to attempt to pick out various instruments and vocals; but these were often considered unsatisfactory, owing to the artifacts of the conversion process.
Many of Stanley Kubrick's and Woody Allen's movies were recorded in mono because of director preferences.
Monaural LP records were eventually phased out and no longer manufactured after the early 1970s, with a few exceptions. For example, Decca UK had a few double issues until the end of 1970 – the last one being Tom Jones's ""I Who Have Nothing""; in Brazil records were released in both mono and stereo as late as 1972. During the 1960s it was common for albums to be released as both mono and stereo LPs, occasionally with slight differences between the two (again, detailed information of The Beatles' recordings provides a good example of the differences). This was because many people owned mono record players that were incapable of playing stereo records, as well as the prevalence of AM radio. Because of the limited quantities pressed and alternative mixes of several tracks, the monaural versions of these albums are often valued more highly than their stereo LP counterparts in record-collecting circles today.
On 9 September 2009, The Beatles re-released a remastered box set of their mono output spanning the Please Please Me album to The Beatles (commonly called ""The White Album""). The set, simply called The Beatles in Mono, also includes a two-disc summary of the mono singles, B-sides and EP tracks released throughout their career. Also included were five tracks originally mixed for an unissued mono Yellow Submarine EP. Bob Dylan followed suit on 19 October 2010 with The Original Mono Recordings, a box set featuring the mono releases from Bob Dylan (1962) to John Wesley Harding (1967). On 21 November 2011, The Kinks' mono recordings were issued as The Kinks in Mono box set, featuring the releases of the band's albums from Kinks (1964) to Arthur (1969), with three additional CDs of non-album tracks that appeared as singles or EP tracks. When the initial run of the box set sold out, no more were pressed, unlike the Beatles and Dylan sets.

Compatibility between mono and stereo sound
Sometimes mono sound or monaural can simply refer to a merged pair of stereo channels - also known as ""collapsed stereo"" or ""folded-down stereo"". Over time some devices have used mono sound amplification circuitry with two or more speakers since it can cut the cost of the hardware. Some consumer electronics with stereo RCA outputs have a microswitch in the red RCA output (i.e., the right stereo channel) that disables merging of stereo sound into the white (left stereo channel) RCA output. Common devices with this are VCRs, DVD/Blu-ray players, information appliances, set-top boxes, and the like. Video game consoles sometimes have male RCA ends of cables with a proprietary multi-A/V plug on the other end, which prevents automatic stereo merging unless adapters are used.
Disadvantages of merged stereo involve phase cancellations that may have the effect of muffling the final sound output. If channels are merged after being sent through a power amplifier but before being connected to a loudspeaker, it places more stress on the loudspeaker. It has usually been the practice in recording studios to make separate mixes for mono recordings (rather than folded-down stereo-to-mono), so that the final mono master will avoid the pitfalls of collapsed stereo. In video games, merging stereo to mono sound prevents player from discerning what direction distant SFX are coming from, and reverse stereo has a similar setback too. Having an array of loudspeakers connected to their own amplifier outputs can mitigate issues with the electrical load for a single loudspeaker coil and allow the listener to perceive an ""image"" of sound in the free space between the speakers.

Mirrored mono
Mirrored mono sound is the opposite of merged stereo, since it can be a case where media with mono sound that stereo playback devices automatically mirror it with are played on both channels of the receiver. It can also mean having a mono input mixed down to stereo amplification circuitry, or a mono system with a headphone output compatible with stereo headphones. An example of an application where both merged stereo and mirror mono apply is when a compact audio cassette respectively plays back ""summed"" stereo channels on a mono reading head, and when a compact cassette recorded with mono sound is played back with a stereo tape head.
Other instances of ""mirrored mono"" also include using the right stereo channel in lieu of a ""left"" one (or vice versa) where both channels are wired to mirror only one.

Both
Instances of both ""merged stereo"" and ""mirrored mono"" can occur when the stereo channels are merged to a mono system with stereo headphone compatibility or when a mono system has ""twin speakers"" (or ""pseudo-stereo"").
Other applications that involve mirrored mono with merged stereo occur when MONO is available as an internal feature of a device that can toggle between STEREO and MONO, for instance many FM radios are capable of toggling between MONO and STEREO in a way where stereo can both MERGE into mono, and then MIRROR between both stereo speakers. This tactic can also be used on other devices, of which computer software and some video games will have a feature that will allow STEREO or MONO for the soundtrack, in which sometimes this can facilitate MERGING stereo internally to spare one from using a Y adapter with LEFT and RIGHT RCA plugs when using mono equipment, such as guitar amplifiers.

Native stereo equipment with mono-only features
Some TV/VCR combo decks on the market had stereo TV functionality with ""twin speakers"", whereas the VCR feature was only mono, which is typical of ""consumer-grade VCRs"" from decades ago. Some of these devices even had front RCA inputs for composite video (yellow), and mono audio (white) in which many of these devices didn't even have a right-channel RCA plug (red) even if it was just for ""merging"" stereo into mono for mono soundtracks to be recorded onto videotapes. This is odd since one would think that a ""right channel"" would be included for A/V in on a TV which had MTS stereo TV sound on its tuner.
Some stereo receivers will also include mono microphone inputs.

See also
Stereophonic sound
Binaural recording",Category:All articles lacking sources,3
105,106,Textsound journal,"textsound journal (textsound) is an audio online literary magazine that publishes experimental poetry and sound.

History
textsound began in 2008 as a bi-annual publication under the editorial direction of Anya Cobler, Adam Fagin, Anna Vitale, and Laura Wetherington.

Selected contributors
Jaap Blonk
Anne-James Chaton
Paul DeMarinis
Linh Dinh
Kenneth Goldsmith
Rick Moody
Thylias Moss
Alice Notley
Alva Noto
Leslie Scalapino
Anne Tardos
Edwin Torres
Anne Waldman

Events
On April 5, 2008, the textsound editorial collective organized a celebration in Ypsilanti, Michigan for the journal's launch featuring Barrett Watten, Joel Levise, Christine Hume, James Marks, and Viki.
In the fall of 2008, the textsound collective teamed-up with Megan Levad and Adam Boehmer to curate the Work-In-Progress Reading Series at the Crazy Wisdom Bookstore in Ann Arbor, Michigan. Performers included Vievee Francis, Jill Darling, Onna Solomon, Sandy Tolbert, Aaron McCollough, Adam Boehmer, Michael Shilling, David Karczynski, T Hetzel, Katie Hartsock, Meghann Rotary, Anna Prushnikaya, and Stephanie Rowden.

See also
List of literary magazines

External links
textsound's website
Self-Consuming Artifacts … towards an unquiet metaphysics A blog posting by American poet Aaron McCollough on Barrett Watten & textsound, initiated by the 2008 crisis in ""Tibet"" (& ""Tibet"" being the title of a Watten poem)
2008 Work-In-Progress Reading Series Blog",Category:Literature websites,3
106,107,Bore (wind instruments),"In music, the bore of a wind instrument (including woodwind and brass) is its interior chamber. This defines a flow path through which air travels, which is set into vibration to produce sounds. The shape of the bore has a strong influence on the instrument's timbre.

Bore shapes
The cone and the cylinder are the two idealized shapes used to describe the bores of wind instruments. These shapes affect the prominence of harmonics associated with the timbre of the instrument.

Cylindrical bore
The diameter of a cylindrical bore remains constant along its length. The acoustic behavior depends on whether the instrument is stopped (closed at one end and open at the other), or open (at both ends). For an open pipe, the wavelength produced by the first normal mode (the fundamental note) is approximately twice the length of the pipe. The wavelength produced by the second normal mode is half that, that is, the length of the pipe, so its pitch is an octave higher; thus an open cylindrical bore instrument overblows at the octave. This corresponds to the second harmonic, and generally the harmonic spectrum of an open cylindrical bore instrument is strong in both even and odd harmonics. For a stopped pipe, the wavelength produced by the first normal mode is approximately four times the length of the pipe. The wavelength produced by the second normal mode is one third that, i.e. the 4/3 length of the pipe, so its pitch is a twelfth higher; a stopped cylindrical bore instrument overblows at the twelfth. This corresponds to the third harmonic; generally the harmonic spectrum of a stopped cylindrical bore instrument, particularly in its bottom register, is strong in the odd harmonics only.
Instruments having a cylindrical, or mostly cylindrical, bore include:
Chalumeau
Clarinet
Cornamuse
Crumhorn
Flute (Boehm system — open)
Kortholt
Rackett (renaissance)
Sudrophone
Trumpet

Conical bore
The diameter of a conical bore varies linearly with distance from the end of the instrument. A complete conical bore would begin at zero diameter—the cone's vertex. However, actual instrument bores approximate a frustum of a cone. The wavelength produced by the first normal mode is approximately twice the length of the cone measured from the vertex. The wavelength produced by the second normal mode is approximately equal to the length of the cone, so its pitch is an octave higher. Therefore, a conical bore instrument, like one with an open cylindrical bore, overblows at the octave and generally has a harmonic spectrum strong in both even and odd harmonics.
Instruments having a conical, or approximately conical, bore include:
Alphorn
Bassoon
Conch shell
Cornet
Dulcian
Euphonium
Flugelhorn
Flute (pre-Boehm)
Oboe
Rackett (baroque)
Rauschpfeife
Saxhorn
Saxophone
Shawm
Tuba
Uilleann pipes

Woodwinds
Sections of the bores of woodwind instruments deviate from a true cone or a cylinder. For example, although oboes and oboes d'amore are similarly pitched, they have differently shaped terminal bells. Accordingly, the voice of the oboe is described as ""piercing"" as compared to the more ""full"" voice of the oboe d'amore.
Although the bore shape of woodwind instruments generally determines their timbre, the instruments' exterior geometry typically has little effect on their voice. In addition, the exterior shape of woodwind instruments may not overtly match the shape of their bores. For example, while oboes and clarinets may outwardly appear similar, oboes have a conical bore while clarinets have a cylindrical bore.
The bore of a modern recorder has a ""reversed"" taper, being wider at the head and narrower at the foot of the instrument.

Brasses
Brass instruments also are sometimes categorized as conical or cylindrical, though most in fact have cylindrical sections between a conical section (the mouthpiece taper or leadpipe) and a non-conical, non-cylindrical flaring section (the bell). Benade gives the following typical proportions:
These proportions vary as valves or slides are operated; the above numbers are for instruments with the valves open or the slide fully in. Therefore the normal mode frequencies of brass instruments do not correspond to integer multiples of the first mode. However, players of brasses (in contrast to woodwinds) are able to ""lip"" notes up or down substantially, and to make use of certain privileged frequencies in addition to those of the normal modes, to obtain in-tune notes.

See also
Acoustic resonance

Notes
References
Nederveen, Cornelis Johannes, Acoustical aspects of woodwind instruments. Amsterdam, Frits Knuf, 1969.
for waveform and harmonic characteristics, clarinet, and a conical, cylindrical comparison.
The previous author refers to: ""The conical bore in musical acoustics,"" by R. D. Ayers, L. J. Eliason, and D. Mahgerefteh, American Journal of Physics, Vol 53, No. 6, pgs 528-537, (1985).
A short description with waveforms of the bassoon. Also a general discussion of acoustics (with calculations and waveforms) in wind instruments [1] Jan. 18, 2011.",Category:Sound,3
107,108,Summing localization,"Summing localization occurs when two or more coherent sound waves arrive within a limited time interval and only one sound sensation is perceived. If the time variations are smaller than 1 ms, the time and level variations of all sound sources contribute to the direction of the perceived sound. The resulting auditory event is called a phantom source.
Summing localization is the basis of stereophony.

References
Jens Blauert ""Spatial Hearing"", The MIT Press; Rev Sub edition (October 2, 1996)
On the Localisation in the superimposed Soundfield by Günther Theile",Category:Orphaned articles from February 2009,3
108,109,Particle displacement,"Particle displacement or displacement amplitude is a measurement of distance of the movement of a particle from its equilibrium position in a medium as it transmits a second wave. The SI unit of particle displacement is the metre (m). In most cases this is a longitudinal wave of pressure (such as sound), but it can also be a transverse wave, such as the vibration of a taut string. In the case of a sound wave travelling through air, the particle displacement is evident in the oscillations of air molecules with, and against, the direction in which the sound wave is travelling.
A particle of the medium undergoes displacement according to the particle velocity of the sound wave traveling through the medium, while the sound wave itself moves at the speed of sound, equal to 343 m/s in air at 20 °C.

Mathematical definition
Particle displacement, denoted ?, is given by

  
    
      
        
          ?
        
        =
        
          ?
          
            t
          
        
        
          v
        
        
        
          d
        
        t
      
    
    {\displaystyle \mathbf {\delta } =\int _{t}\mathbf {v} \,\mathrm {d} t}
  
where v is the particle velocity.

Progressive sine waves
The particle displacement of a progressive sine wave is given by

  
    
      
        ?
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            ?
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle \delta (\mathbf {r} ,\,t)=\delta \cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}),}
  
where
? is the amplitude of the particle displacement;

  
    
      
        
          ?
          
            ?
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{\delta ,0}}
   is the phase shift of the particle displacement;
k is the angular wavevector;
? is the angular frequency.
It follows that the particle velocity and the sound pressure along the direction of propagation of the sound wave x are given by

  
    
      
        v
        (
        
          r
        
        ,
        
        t
        )
        =
        
          
            
              ?
              ?
            
            
              ?
              t
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        v
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            v
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle v(\mathbf {r} ,\,t)={\frac {\partial \delta }{\partial t}}(\mathbf {r} ,\,t)=\omega \delta \cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=v\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{v,0}),}
  

  
    
      
        p
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        ?
        
          c
          
            2
          
        
        
          
            
              ?
              ?
            
            
              ?
              x
            
          
        
        (
        
          r
        
        ,
        
        t
        )
        =
        ?
        
          c
          
            2
          
        
        
          k
          
            x
          
        
        ?
        cos
        
        
          (
          
            
              k
            
            ?
            
              r
            
            ?
            ?
            t
            +
            
              ?
              
                ?
                ,
                0
              
            
            +
            
              
                ?
                2
              
            
          
          )
        
        =
        p
        cos
        ?
        (
        
          k
        
        ?
        
          r
        
        ?
        ?
        t
        +
        
          ?
          
            p
            ,
            0
          
        
        )
        ,
      
    
    {\displaystyle p(\mathbf {r} ,\,t)=-\rho c^{2}{\frac {\partial \delta }{\partial x}}(\mathbf {r} ,\,t)=\rho c^{2}k_{x}\delta \cos \!\left(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{\delta ,0}+{\frac {\pi }{2}}\right)=p\cos(\mathbf {k} \cdot \mathbf {r} -\omega t+\varphi _{p,0}),}
  
where
v is the amplitude of the particle velocity;

  
    
      
        
          ?
          
            v
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}}
   is the phase shift of the particle velocity;
p is the amplitude of the acoustic pressure;

  
    
      
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{p,0}}
   is the phase shift of the acoustic pressure.
Taking the Laplace transforms of v and p with respect to time yields

  
    
      
        
          
            
              v
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        v
        
          
            
              s
              cos
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  v
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        ,
      
    
    {\displaystyle {\hat {v}}(\mathbf {r} ,\,s)=v{\frac {s\cos \varphi _{v,0}-\omega \sin \varphi _{v,0}}{s^{2}+\omega ^{2}}},}
  

  
    
      
        
          
            
              p
              ^
            
          
        
        (
        
          r
        
        ,
        
        s
        )
        =
        p
        
          
            
              s
              cos
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
              ?
              ?
              sin
              ?
              
                ?
                
                  p
                  ,
                  0
                
              
            
            
              
                s
                
                  2
                
              
              +
              
                ?
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle {\hat {p}}(\mathbf {r} ,\,s)=p{\frac {s\cos \varphi _{p,0}-\omega \sin \varphi _{p,0}}{s^{2}+\omega ^{2}}}.}
  
Since 
  
    
      
        
          ?
          
            v
            ,
            0
          
        
        =
        
          ?
          
            p
            ,
            0
          
        
      
    
    {\displaystyle \varphi _{v,0}=\varphi _{p,0}}
  , the amplitude of the specific acoustic impedance is given by

  
    
      
        z
        (
        
          r
        
        ,
        
        s
        )
        =
        
          |
        
        z
        (
        
          r
        
        ,
        
        s
        )
        
          |
        
        =
        
          |
          
            
              
                
                  
                    
                      p
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
              
                
                  
                    
                      v
                      ^
                    
                  
                
                (
                
                  r
                
                ,
                
                s
                )
              
            
          
          |
        
        =
        
          
            p
            v
          
        
        =
        
          
            
              ?
              
                c
                
                  2
                
              
              
                k
                
                  x
                
              
            
            ?
          
        
        .
      
    
    {\displaystyle z(\mathbf {r} ,\,s)=|z(\mathbf {r} ,\,s)|=\left|{\frac {{\hat {p}}(\mathbf {r} ,\,s)}{{\hat {v}}(\mathbf {r} ,\,s)}}\right|={\frac {p}{v}}={\frac {\rho c^{2}k_{x}}{\omega }}.}
  
Consequently, the amplitude of the particle displacement is related to those of the particle velocity and the sound pressure by

  
    
      
        ?
        =
        
          
            v
            ?
          
        
        ,
      
    
    {\displaystyle \delta ={\frac {v}{\omega }},}
  

  
    
      
        ?
        =
        
          
            p
            
              ?
              z
              (
              
                r
              
              ,
              
              s
              )
            
          
        
        .
      
    
    {\displaystyle \delta ={\frac {p}{\omega z(\mathbf {r} ,\,s)}}.}

See also
Sound
Sound particle
Particle velocity
Particle acceleration

References and notes
Related Reading:
Wood, Robert Williams (1914). Physical optics. New York: The Macmillan Company. 
Strong, John Donovan & Hayward, Roger (January 2004). Concepts of Classical Optics. Dover Publications. ISBN 978-0-486-43262-5. 
Barron, Randall F. (January 2003). Industrial noise control and acoustics. NYC, New York: CRC Press. pp. 79, 82, 83, 87. ISBN 978-0-8247-0701-9.

External links
Acoustic Particle-Image Velocimetry. Development and Applications
Ohm's Law as Acoustic Equivalent. Calculations
Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave",Category:Physical quantities,3
109,110,Parametric array,"In the field of acoustics, a parametric array is a nonlinear transduction mechanism that generates narrow, nearly side lobe-free beams of low frequency sound, through the mixing and interaction of high frequency sound waves, effectively overcoming the diffraction limit (a kind of spatial 'uncertainty principle') associated with linear acoustics. The main side lobe-free beam of low frequency sound is created as a result of nonlinear mixing of two high frequency sound beams at their difference frequency. Parametric arrays can be formed in water, air, and earth materials/rock.

History
Priority for discovery and explanation of the parametric array owes to Peter J. Westervelt, winner of the Lord Rayleigh Medal (currently Professor Emeritus at Brown University), although important experimental work was contemporaneously underway in the former Soviet Union.
According to Muir [16, p. 554] and Albers [17], the concept for the parametric array occurred to Dr. Westervelt while he was stationed at the London, England, branch office of the Office of Naval Research in 1951.
According to Albers [17], he (Westervelt) there first observed an accidental generation of low frequency sound in air by Captain H.J. Round (British pioneer of the superheterodyne receiver) via the parametric array mechanism.
The phenomenon of the parametric array, seen first experimentally by Westervelt in the 1950s, was later explained theoretically in 1960, at a meeting of the Acoustical Society of America. A few years after this, a full paper [2] was published as an extension of Westervelt's classic work on the nonlinear Scattering of Sound by Sound, as described in [8,6,12].

Foundations
The foundation for Westervelt's theory of sound generation and scattering in nonlinear acoustic media owes to an application of Lighthill's equation (see Aeroacoustics) for fluid particle motion.
The application of Lighthill’s theory to the nonlinear acoustic realm yields the Westervelt–Lighthill Equation (WLE). Solutions to this equation have been developed using Green's functions [4,5] and Parabolic Equation (PE) Methods, most notably via the Kokhlov–Zablotskaya–Kuznetzov (KZK) equation.
An alternate mathematical formalism using Fourier operator methods in wavenumber space, was also developed by Westervelt, and generalized in [1] for solving the WLE in a most general manner. The solution method is formulated in Fourier (wavenumber) space in a representation related to the beam patterns of the primary fields generated by linear sources in the medium. This formalism has been applied not only to parametric arrays [15], but also to other nonlinear acoustic effects, such as the absorption of sound by sound and to the equilibrium distribution of sound intensity spectra in cavities [18].

Applications
Practical applications are numerous and include:
underwater sound
sonar
depth sounding
sub-bottom profiling
non-destructive testing
and 'see through walls' sensing
remote ocean sensing

medical ultrasound
and tomography [6]
underground seismic prospecting
active noise control
and directional high-fidelity commercial audio systems (Sound from ultrasound)
Parametric receiving arrays can also be formed for directional reception. In 2005, Elwood Norris won the $500,000 MIT-Lemelson Prize for his application of the parametric array to commercial high-fidelity loudspeakers.

References
Further reading
[1] H.C. Woodsum and P.J. Westervelt, ""A General Theory for the Scattering of Sound by Sound"", Journal of Sound and Vibration (1981), 76(2), 179-186.
[2] Peter J. Westervelt, ""Parametric Acoustic Array"", Journal of the Acoustical Society of America, Vol. 35, No. 4 (535-537), 1963
[4] Mark B. Moffett and Robert H. Mellen, ""Model for Parametric Sources"", J. Acoust. Soc. Am. Vol. 61, No. 2, Feb. 1977
[5] Mark B. Moffett and Robert H. Mellen, ""On Parametric Source Aperture Factors"", J. Acoust. Soc. Am. Vol. 60, No. 3, Sept. 1976
[6] Ronald A. Roy and Junru Wu, ""An Experimental Investigation of the Interaction of Two Non-Collinear Beams of Sound"", Proceedings of the 13th International Symposium on Nonlinear Acoustics, H. Hobaek, Editor, Elsevier Science Ltd., London (1993)
[7] Harvey C. Woodsum, ""Analytical and Numerical Solutions to the 'General Theory for the Scattering of Sound by Sound”, J. Acoust. Soc. Am. Vol. 95, No. 5, Part 2 (2PA14), June, 1994 (Program of the 134th Meeting of the Acoustical Society of America, Cambridge Massachusetts)
[8] Robert T. Beyer, Nonlinear Acoustics, 1st Edition (1974),. Published by the Naval Sea Systems Command.
[9] H.O. Berktay and D.J. Leahy, Journal of the Acoustical Society of America, 55, p. 539 (1974)
[10] M.J. Lighthill, ""On Sound Generated Aerodynamically”, Proc. R. Soc. Lond. A211, 564-687 (1952)
[11] M.J. Lighhill, “On Sound Generated Aerodynamically”, Proc. R. Soc. Lond. A222, 1-32 (1954)
[12] J.S. Bellin and R. T. Beyer, “Scattering of Sound by Sound”, J. Acoust. Soc. Am. 32, 339-341 (1960)
[13] M.J. Lighthill, Math. Revs. 19, 915 (1958)
[14] H.C. Woodsum, Bull. Of Am. Phys. Soc., Fall 1980; “A Boundary Condition Operator for Nonlinear Acoustics”
[15] H.C. Woodsum, Proc. 17th International Conference on Nonlinear Acoustics, AIP Press (NY), 2006; "" Comparison of Nonlinear Acoustic Experiments with a Formal Theory for the Scattering of Sound by Sound"", paper TuAM201.
[16] T.G. Muir, Office of Naval Research Special Report - ""Science, Technology and the Modern Navy, Thirtieth Anniversary (1946-1976), Paper ONR-37, ""Nonlinear Acoustics: A new Dimension in Underwater Sound"", published by the Department of the Navy (1976)
[17] V.M. Albers,""Underwater Sound, Benchmark Papers in Acoustics, p.415; Dowden, Hutchinson and Ross, Inc., Stroudsburg, PA (1972)
[18] M. Cabot and Seth Putterman, ""Renormalized Classical Non-linear Hydrodynamics, Quantum Mode Coupling and Quantum Theory of Interacting Phonons"", Physics Letters Vol. 83A, No. 3, 18 May 1981, pp. 91–94 (North Holland Publishing Company-Amsterdam)
[19] Nonlinear Parameter Imaging Computed Tomography by Parametric Acoustic Array Y. Nakagawa; M. Nakagawa; M. Yoneyama; M. Kikuchi IEEE 1984 Ultrasonics Symposium Volume, Issue, 1984 Page(s):673–676",Category:Sound,3
110,111,Macrosonics,"Macrosonics is the use of high amplitude sound waves for industrial applications. Applications include gas compression, cleaning of surfaces, plastic and metal welding, metal forming, machining, and chemical processing.

See also
Megasonic cleaning


== References ==",Category:Industry stubs,3
111,112,Real-time analyzer,"A real-time analyzer (RTA) is a professional audio device that measures and displays the frequency spectrum of an audio signal; a spectrum analyzer that works in real time. An RTA can range from a small PDA-sized device to a rack-mounted hardware unit to software running on a laptop. It works by measuring and displaying sound input, often from an integrated microphone or with a signal from a PA system. Basic RTAs show three measurements per octave at 3 or 6 dB increments; sophisticated software solutions can show 24 or more measurements per octave as well as 0.1 dB resolution.

Types
There are generally two types of RTAs:
RTAs employing analog signal processing, and
RTAs employing digital signal processing (DSP).
The main difference between the two types is that the analog RTAs use a series of hardwired, analog bandpass filters to break the signal into frequency bands prior to measuring it. Digital RTAs use digital sampling technology and microprocessor based digital signal processing to perform necessary calculations, such as Fast Fourier Transforms, to perform the measurements and thus do not need analog hardware filters to isolate each frequency band. The digital approach to signal analysis generally yields much higher accuracy and resolution and thus most RTAs currently in production use digital signal processing technology. Digital signal processing is more cost effective.

Professional use
RTAs are often used by sound engineers and by acousticians installing audio systems in all kinds of listening spaces: Venues, home theatres, cars etc. The parameters that can be measured are the spectral aspects of sound reproduction caused by effects like resonances and constructive and destructive interference, but not imaging and spatial aspects. In professional audio many systems incorporate an RTA along with a device that also performs equalization. While measuring pink noise or other test tones, such a controller can level out the frequency response by employing a set of adjustments in the appropriate frequency areas according to the system's interaction with the venue's size, shape and construction materials, among other things.

See also
Architectural acoustics
Real-time computing
Spectrum analyzer
Pink Noise
Auditory scene analysis incl. 3D-sound perception, localisation
Audio signal processing
Sound localization
Source separation
Timbre


== References ==",Category:Sound,3
112,113,Speed of sound,"The speed of sound is the distance travelled per unit time by a sound wave as it propagates through an elastic medium. In dry air at 0 °C (32 °F), the speed of sound is 331.2 metres per second (1,087 ft/s; 1,192 km/h; 741 mph; 644 kn). At 20 °C (68 °F), the speed of sound is 343 metres per second (1,125 ft/s; 1,235 km/h; 767 mph; 667 kn), or a kilometre in 2.91 s or a mile in 4.69 s.
The speed of sound in an ideal gas depends only on its temperature and composition. The speed has a weak dependence on frequency and pressure in ordinary air, deviating slightly from ideal behavior.
In common everyday speech, speed of sound refers to the speed of sound waves in air. However, the speed of sound varies from substance to substance: sound travels most slowly in gases; it travels faster in liquids; and faster still in solids. For example, (as noted above), sound travels at 343 m/s in air; it travels at 1,484 m/s in water (4.3 times as fast as in air); and at 5,120 m/s in iron (about 15 times as fast as in air). In an exceptionally stiff material such as diamond, sound travels at 12,000 metres per second (26,843 mph); (about 35 times as fast as in air) which is around the maximum speed that sound will travel under normal conditions.
Sound waves in solids are composed of compression waves (just as in gases and liquids), and a different type of sound wave called a shear wave, which occurs only in solids. Shear waves in solids usually travel at different speeds, as exhibited in seismology. The speed of compression waves in solids is determined by the medium's compressibility, shear modulus and density. The speed of shear waves is determined only by the solid material's shear modulus and density.
In fluid dynamics, the speed of sound in a fluid medium (gas or liquid) is used as a relative measure for the speed of an object moving through the medium. The ratio of the speed of an object to the speed of sound in the fluid is called the object's Mach number. Objects moving at speeds greater than Mach1 are said to be traveling at supersonic speeds.

History
Sir Isaac Newton computed the speed of sound in air as 979 feet per second (298 m/s), which is too low by about 15%,. Newton's analysis was good save for neglecting the (then unknown) effect of rapidly-fluctuating temperature in a sound wave (in modern terms, sound wave compression and expansion of air is an adiabatic process, not an isothermal process). This error was later rectified by Laplace.
During the 17th century, there were several attempts to measure the speed of sound accurately, including attempts by Marin Mersenne in 1630 (1,380 Parisian feet per second), Pierre Gassendi in 1635 (1,473 Parisian feet per second) and Robert Boyle (1,125 Parisian feet per second).
In 1709, the Reverend William Derham, Rector of Upminster, published a more accurate measure of the speed of sound, at 1,072 Parisian feet per second. Derham used a telescope from the tower of the church of St Laurence, Upminster to observe the flash of a distant shotgun being fired, and then measured the time until he heard the gunshot with a half second pendulum. Measurements were made of gunshots from a number of local landmarks, including North Ockendon church. The distance was known by triangulation, and thus the speed that the sound had travelled was calculated.

Basic concepts
The transmission of sound can be illustrated by using a model consisting of an array of spherical objects interconnected by springs.
In real material terms, the spheres represent the material's molecules and the springs represent the bonds between them. Sound passes through the system by compressing and expanding the springs, transmitting the acoustic energy to neighboring spheres. This helps transmit the energy in-turn to the neighboring sphere's springs (bonds), and so on.
The speed of sound through the model depends on the stiffness/rigidity of the springs, and the mass of the spheres. As long as the spacing of the spheres remains constant, stiffer springs/bonds transmit energy quicker, while larger spheres transmit the energy slower. Effects like dispersion and reflection can also be understood using this model.
In a real material, the stiffness of the springs is known as the ""elastic modulus"", and the mass corresponds to the material density. Given that all other things being equal (ceteris paribus), sound will travel slower in spongy materials, and faster in stiffer ones. For instance, sound will travel 1.59 times faster in nickel than in bronze, due to the greater stiffness of nickel at about the same density. Similarly, sound travels about 1.41 times faster in light hydrogen (protium) gas than in heavy hydrogen (deuterium) gas, since deuterium has similar properties but twice the density. At the same time, ""compression-type"" sound will travel faster in solids than in liquids, and faster in liquids than in gases, because the solids are more difficult to compress than liquids, while liquids in turn are more difficult to compress than gases.
Some textbooks mistakenly state that the speed of sound increases with increasing density. This is usually illustrated by presenting data for three materials, such as air, water and steel, which also have vastly different levels compressibilities which more than make up for the density differences. An illustrative example of the two effects is that sound travels only 4.3 times faster in water than air, despite enormous differences in compressibility of the two media. The reason is that the larger density of water, which works to slow sound in water relative to air, nearly makes up for the compressibility differences in the two media.
A practical example can be observed in Edinburgh, when the ""One o' Clock Gun"" is fired at the eastern end of Edinburgh Castle. Standing at the base of the western end of the Castle Rock, the sound of the Gun can be heard through the rock, slightly before it arrives by the air route, partly delayed by the slightly longer route. It is particularly effective if a multi-gun salute such as for ""The Queen's Birthday"" is being fired.

Compression and shear waves
In a gas or liquid, sound consists of compression waves. In solids, waves propagate as two different types. A longitudinal wave is associated with compression and decompression in the direction of travel, and is the same process in gases and liquids, with an analogous compression-type wave in solids. Only compression waves are supported in gases and liquids. An additional type of wave, the transverse wave, also called a shear wave, occurs only in solids because only solids support elastic deformations. It is due to elastic deformation of the medium perpendicular to the direction of wave travel; the direction of shear-deformation is called the ""polarization"" of this type of wave. In general, transverse waves occur as a pair of orthogonal polarizations.
These different waves (compression waves and the different polarizations of shear waves) may have different speeds at the same frequency. Therefore, they arrive at an observer at different times, an extreme example being an earthquake, where sharp compression waves arrive first, and rocking transverse waves seconds later.
The speed of a compression wave in fluid is determined by the medium's compressibility and density. In solids, the compression waves are analogous to those in fluids, depending on compressibility and density, but with the additional factor of shear modulus which affects compression waves due to off-axis elastic energies which are able to influence effective tension and relaxation in a compression. The speed of shear waves, which can occur only in solids, is determined simply by the solid material's shear modulus and density.

Equations
The speed of sound in mathematical notation is conventionally represented by c, from the Latin celeritas meaning ""velocity"".
In general, the speed of sound c is given by the Newton–Laplace equation:

  
    
      
        c
        =
        
          
            
              
                K
                
                  s
                
              
              ?
            
          
        
        ,
      
    
    {\displaystyle c={\sqrt {\frac {K_{s}}{\rho }}},}
  
where
Ks is a coefficient of stiffness, the isentropic bulk modulus (or the modulus of bulk elasticity for gases);
? is the density.
Thus the speed of sound increases with the stiffness (the resistance of an elastic body to deformation by an applied force) of the material, and decreases with increase in density. For ideal gases the bulk modulus K is simply the gas pressure multiplied by the dimensionless adiabatic index, which is about 1.4 for air under normal conditions of pressure and temperature.
For general equations of state, if classical mechanics is used, the speed of sound c is given by

  
    
      
        c
        =
        
          
            
              
                (
                
                  
                    
                      ?
                      p
                    
                    
                      ?
                      ?
                    
                  
                
                )
              
              
                s
              
            
          
        
        ,
      
    
    {\displaystyle c={\sqrt {\left({\frac {\partial p}{\partial \rho }}\right)_{s}}},}
  
where
p is the pressure;
? is the density and the derivative is taken isentropically, that is, at constant entropy s.
If relativistic effects are important, the speed of sound is calculated from the relativistic Euler equations.
In a non-dispersive medium, the speed of sound is independent of sound frequency, so the speeds of energy transport and sound propagation are the same for all frequencies. Air, a mixture of oxygen and nitrogen, constitutes a non-dispersive medium. However, air does contain a small amount of CO2 which is a dispersive medium, and causes dispersion to air at ultrasonic frequencies (> 28 kHz).
In a dispersive medium, the speed of sound is a function of sound frequency, through the dispersion relation. Each frequency component propagates at its own speed, called the phase velocity, while the energy of the disturbance propagates at the group velocity. The same phenomenon occurs with light waves; see optical dispersion for a description.

Dependence on the properties of the medium
The speed of sound is variable and depends on the properties of the substance through which the wave is travelling. In solids, the speed of transverse (or shear) waves depends on the shear deformation under shear stress (called the shear modulus), and the density of the medium. Longitudinal (or compression) waves in solids depend on the same two factors with the addition of a dependence on compressibility.
In fluids, only the medium's compressibility and density are the important factors, since fluids do not transmit shear stresses. In heterogeneous fluids, such as a liquid filled with gas bubbles, the density of the liquid and the compressibility of the gas affect the speed of sound in an additive manner, as demonstrated in the hot chocolate effect.
In gases, adiabatic compressibility is directly related to pressure through the heat capacity ratio (adiabatic index), while pressure and density are inversely related to the temperature and molecular weight, thus making only the completely independent properties of temperature and molecular structure important (heat capacity ratio may be determined by temperature and molecular structure, but simple molecular weight is not sufficient to determine it).
In low molecular weight gases such as helium, sound propagates faster as compared to heavier gases such as xenon. For monatomic gases, the speed of sound is about 75% of the mean speed that the atoms move in that gas.
For a given ideal gas the molecular composition is fixed, and thus the speed of sound depends only on its temperature. At a constant temperature, the gas pressure has no effect on the speed of sound, since the density will increase, and since pressure and density (also proportional to pressure) have equal but opposite effects on the speed of sound, and the two contributions cancel out exactly. In a similar way, compression waves in solids depend both on compressibility and density—just as in liquids—but in gases the density contributes to the compressibility in such a way that some part of each attribute factors out, leaving only a dependence on temperature, molecular weight, and heat capacity ratio which can be independently derived from temperature and molecular composition (see derivations below). Thus, for a single given gas (assuming the molecular weight does not change) and over a small temperature range (for which the heat capacity is relatively constant), the speed of sound becomes dependent on only the temperature of the gas.
In non-ideal gas behavior regimen, for which the van der Waals gas equation would be used, the proportionality is not exact, and there is a slight dependence of sound velocity on the gas pressure.
Humidity has a small but measurable effect on the speed of sound (causing it to increase by about 0.1%–0.6%), because oxygen and nitrogen molecules of the air are replaced by lighter molecules of water. This is a simple mixing effect.

Altitude variation and implications for atmospheric acoustics
In the Earth's atmosphere, the chief factor affecting the speed of sound is the temperature. For a given ideal gas with constant heat capacity and composition, the speed of sound is dependent solely upon temperature; see Details below. In such an ideal case, the effects of decreased density and decreased pressure of altitude cancel each other out, save for the residual effect of temperature.
Since temperature (and thus the speed of sound) decreases with increasing altitude up to 11 km, sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. The decrease of the speed of sound with height is referred to as a negative sound speed gradient.
However, there are variations in this trend above 11 km. In particular, in the stratosphere above about 20 km, the speed of sound increases with height, due to an increase in temperature from heating within the ozone layer. This produces a positive speed of sound gradient in this region. Still another region of positive gradient occurs at very high altitudes, in the aptly-named thermosphere above 90 km.

Practical formula for dry air
The approximate speed of sound in dry (0% humidity) air, in meters per second, at temperatures near 0 °C, can be calculated from

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        (
        331.3
        +
        0.606
        ?
        ?
        )
         
         
         
        
          m
          
            /
          
          s
        
        ,
      
    
    {\displaystyle c_{\mathrm {air} }=(331.3+0.606\cdot \vartheta )~~~\mathrm {m/s} ,}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \vartheta }
   is the temperature in degrees Celsius (°C).
This equation is derived from the first two terms of the Taylor expansion of the following more accurate equation:

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        331.3
         
        
          
            1
            +
            
              
                ?
                273.15
              
            
          
        
         
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=331.3~{\sqrt {1+{\frac {\vartheta }{273.15}}}}~~~~\mathrm {m/s} .}
  
Dividing the first part, and multiplying the second part, on the right hand side, by ?273.15 gives the exactly equivalent form

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        20.05
         
        
          
            ?
            +
            273.15
          
        
         
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=20.05~{\sqrt {\vartheta +273.15}}~~~~\mathrm {m/s} .}
  
which can also be written as

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        20.05
         
        
          
            T
            
              /
            
            K
          
        
         
         
         
         
        
          m
          
            /
          
          s
        
      
    
    {\displaystyle c_{\mathrm {air} }=20.05~{\sqrt {T/K}}~~~~\mathrm {m/s} }
  
where T denotes the thermodynamic temperature.
The value of 331.3 m/s, which represents the speed at 0 °C (or 273.15 K), is based on theoretical (and some measured) values of the heat capacity ratio, ?, as well as on the fact that at 1 atm real air is very well described by the ideal gas approximation. Commonly found values for the speed of sound at 0 °C may vary from 331.2 to 331.6 due to the assumptions made when it is calculated. If ideal gas ? is assumed to be 7/5 = 1.4 exactly, the 0 °C speed is calculated (see section below) to be 331.3 m/s, the coefficient used above.
This equation is correct to a much wider temperature range, but still depends on the approximation of heat capacity ratio being independent of temperature, and for this reason will fail, particularly at higher temperatures. It gives good predictions in relatively dry, cold, low pressure conditions, such as the Earth's stratosphere. The equation fails at extremely low pressures and short wavelengths, due to dependence on the assumption that the wavelength of the sound in the gas is much longer than the average mean free path between gas molecule collisions. A derivation of these equations will be given in the following section.
A graph comparing results of the two equations is at right, using the slightly different value of 331.5 m/s for the speed of sound at 0 °C.

Details
Speed of sound in ideal gases and air
For an ideal gas, K (the bulk modulus in equations above, equivalent to C, the coefficient of stiffness in solids) is given by

  
    
      
        K
        =
        ?
        ?
        p
        ,
      
    
    {\displaystyle K=\gamma \cdot p,}
  
thus, from the Newton–Laplace equation above, the speed of sound in an ideal gas is given by

  
    
      
        c
        =
        
          
            ?
            ?
            
              
                p
                ?
              
            
          
        
        ,
      
    
    {\displaystyle c={\sqrt {\gamma \cdot {p \over \rho }}},}
  
where
? is the adiabatic index also known as the isentropic expansion factor. It is the ratio of specific heats of a gas at a constant-pressure to a gas at a constant-volume(
  
    
      
        
          C
          
            p
          
        
        
          /
        
        
          C
          
            v
          
        
      
    
    {\displaystyle C_{p}/C_{v}}
  ), and arises because a classical sound wave induces an adiabatic compression, in which the heat of the compression does not have enough time to escape the pressure pulse, and thus contributes to the pressure induced by the compression;
p is the pressure;
? is the density.
Using the ideal gas law to replace p with nRT/V, and replacing ? with nM/V, the equation for an ideal gas becomes

  
    
      
        
          c
          
            
              i
              d
              e
              a
              l
            
          
        
        =
        
          
            ?
            ?
            
              
                p
                ?
              
            
          
        
        =
        
          
            
              
                ?
                ?
                R
                ?
                T
              
              M
            
          
        
        =
        
          
            
              
                ?
                ?
                k
                ?
                T
              
              m
            
          
        
        ,
      
    
    {\displaystyle c_{\mathrm {ideal} }={\sqrt {\gamma \cdot {p \over \rho }}}={\sqrt {\gamma \cdot R\cdot T \over M}}={\sqrt {\gamma \cdot k\cdot T \over m}},}
  
where
cideal is the speed of sound in an ideal gas;
R (approximately 8.314,5 J · mol?1 · K?1) is the molar gas constant(universal gas constant);
k is the Boltzmann constant;
? (gamma) is the adiabatic index. At room temperature, where thermal energy is fully partitioned into rotation (rotations are fully excited) but quantum effects prevent excitation of vibrational modes, the value is 7/5 = 1.400 for diatomic molecules, according to kinetic theory. Gamma is actually experimentally measured over a range from 1.399,1 to 1.403 at 0 °C, for air. Gamma is exactly 5/3 = 1.6667 for monatomic gases such as noble gases;
T is the absolute temperature;
M is the molar mass of the gas. The mean molar mass for dry air is about 0.028,964,5 kg/mol;
n is the number of moles;
m is the mass of a single molecule.
This equation applies only when the sound wave is a small perturbation on the ambient condition, and the certain other noted conditions are fulfilled, as noted below. Calculated values for cair have been found to vary slightly from experimentally determined values.
Newton famously considered the speed of sound before most of the development of thermodynamics and so incorrectly used isothermal calculations instead of adiabatic. His result was missing the factor of ? but was otherwise correct.
Numerical substitution of the above values gives the ideal gas approximation of sound velocity for gases, which is accurate at relatively low gas pressures and densities (for air, this includes standard Earth sea-level conditions). Also, for diatomic gases the use of ? = 1.400,0 requires that the gas exists in a temperature range high enough that rotational heat capacity is fully excited (i.e., molecular rotation is fully used as a heat energy ""partition"" or reservoir); but at the same time the temperature must be low enough that molecular vibrational modes contribute no heat capacity (i.e., insignificant heat goes into vibration, as all vibrational quantum modes above the minimum-energy-mode, have energies too high to be populated by a significant number of molecules at this temperature). For air, these conditions are fulfilled at room temperature, and also temperatures considerably below room temperature (see tables below). See the section on gases in specific heat capacity for a more complete discussion of this phenomenon.
For air, we use a simplified symbol

  
    
      
        
          R
          
            ?
          
        
        =
        R
        
          /
        
        
          M
          
            
              a
              i
              r
            
          
        
        .
      
    
    {\displaystyle R_{*}=R/M_{\mathrm {air} }.}
  
Additionally, if temperatures in degrees Celsius(°C) are to be used to calculate air speed in the region near 273 kelvin, then Celsius temperature ? = T ? 273.15 may be used. Then

  
    
      
        
          c
          
            
              i
              d
              e
              a
              l
            
          
        
        =
        
          
            ?
            ?
            
              R
              
                ?
              
            
            ?
            T
          
        
        =
        
          
            ?
            ?
            
              R
              
                ?
              
            
            ?
            (
            ?
            +
            273.15
            )
          
        
        ,
      
    
    {\displaystyle c_{\mathrm {ideal} }={\sqrt {\gamma \cdot R_{*}\cdot T}}={\sqrt {\gamma \cdot R_{*}\cdot (\vartheta +273.15)}},}
  

  
    
      
        
          c
          
            
              i
              d
              e
              a
              l
            
          
        
        =
        
          
            ?
            ?
            
              R
              
                ?
              
            
            ?
            273.15
          
        
        ?
        
          
            1
            +
            
              
                ?
                273.15
              
            
          
        
        .
      
    
    {\displaystyle c_{\mathrm {ideal} }={\sqrt {\gamma \cdot R_{*}\cdot 273.15}}\cdot {\sqrt {1+{\frac {\vartheta }{273.15}}}}.}
  
For dry air, where ? (theta) is the temperature in degrees Celsius(°C).
Making the following numerical substitutions,

  
    
      
        R
        =
        8.314
        
        510
         
        
          J
          
            /
          
          (
          m
          o
          l
          ?
          K
          )
        
      
    
    {\displaystyle R=8.314\,510~\mathrm {J/(mol\cdot K)} }
  
is the molar gas constant in J/mole/Kelvin, and

  
    
      
        
          M
          
            
              a
              i
              r
            
          
        
        =
        0.028
        
        964
        
        5
         
        
          k
          g
          
            /
          
          m
          o
          l
        
      
    
    {\displaystyle M_{\mathrm {air} }=0.028\,964\,5~\mathrm {kg/mol} }
  
is the mean molar mass of air, in kg; and using the ideal diatomic gas value of ? = 1.4000.
Then

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        331.3
         
         
        
          
            1
            +
            
              
                
                  
                    ?
                    
                      ?
                    
                  
                  
                    C
                  
                
                
                  
                    273.15
                    
                      ?
                    
                  
                  
                    C
                  
                
              
            
          
        
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=331.3~~{\sqrt {1+{\frac {\vartheta ^{\circ }\mathrm {C} }{273.15^{\circ }\mathrm {C} }}}}~~~\mathrm {m/s} .}
  
Using the first two terms of the Taylor expansion:

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        331.3
         
        (
        1
        +
        
          
            
              
                ?
                
                  ?
                
              
              
                C
              
            
            
              2
              ?
              
                273.15
                
                  ?
                
              
              
                C
              
            
          
        
        )
         
         
         
        
          m
          
            /
          
          s
        
        ,
      
    
    {\displaystyle c_{\mathrm {air} }=331.3~(1+{\frac {\vartheta ^{\circ }\mathrm {C} }{2\cdot 273.15^{\circ }\mathrm {C} }})~~~\mathrm {m/s} ,}
  

  
    
      
        
          c
          
            
              a
              i
              r
            
          
        
        =
        (
        331.3
        +
        
          0.606
          
            ?
          
        
        
          
            C
          
          
            ?
            1
          
        
        ?
        ?
        )
         
         
         
        
          m
          
            /
          
          s
        
        .
      
    
    {\displaystyle c_{\mathrm {air} }=(331.3+0.606^{\circ }\mathrm {C} ^{-1}\cdot \vartheta )~~~\mathrm {m/s} .}
  
The derivation includes the first two equations given in the ""Practical formula for dry air"" section above.

Effects due to wind shear
The speed of sound varies with temperature. Since temperature and sound velocity normally decrease with increasing altitude, sound is refracted upward, away from listeners on the ground, creating an acoustic shadow at some distance from the source. Wind shear of 4 m/(s · km) can produce refraction equal to a typical temperature lapse rate of 7.5 °C/km. Higher values of wind gradient will refract sound downward toward the surface in the downwind direction, eliminating the acoustic shadow on the downwind side. This will increase the audibility of sounds downwind. This downwind refraction effect occurs because there is a wind gradient; the sound is not being carried along by the wind.
For sound propagation, the exponential variation of wind speed with height can be defined as follows:

  
    
      
        U
        (
        h
        )
        =
        U
        (
        0
        )
        
          h
          
            ?
          
        
        ,
      
    
    {\displaystyle U(h)=U(0)h^{\zeta },}
  

  
    
      
        
          
            
              
                d
              
              U
            
            
              
                d
              
              H
            
          
        
        (
        h
        )
        =
        ?
        
          
            
              U
              (
              h
              )
            
            h
          
        
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} U}{\mathrm {d} H}}(h)=\zeta {\frac {U(h)}{h}},}
  
where
U(h) is the speed of the wind at height h;
? is the exponential coefficient based on ground surface roughness, typically between 0.08 and 0.52;
dU/dH(h) is the expected wind gradient at height h.
In the 1862 American Civil War Battle of Iuka, an acoustic shadow, believed to have been enhanced by a northeast wind, kept two divisions of Union soldiers out of the battle, because they could not hear the sounds of battle only 10 km (six miles) downwind.

Tables
In the standard atmosphere:
T0 is 273.15 K (= 0 °C = 32 °F), giving a theoretical value of 331.3 m/s (= 1086.9 ft/s = 1193 km/h = 741.1 mph = 644.0 kn). Values ranging from 331.3-331.6 may be found in reference literature, however;
T20 is 293.15 K (= 20 °C = 68 °F), giving a value of 343.2 m/s (= 1126.0 ft/s = 1236 km/h = 767.8 mph = 667.2 kn);
T25 is 298.15 K (= 25 °C = 77 °F), giving a value of 346.1 m/s (= 1135.6 ft/s = 1246 km/h = 774.3 mph = 672.8 kn).
In fact, assuming an ideal gas, the speed of sound c depends on temperature only, not on the pressure or density (since these change in lockstep for a given temperature and cancel out). Air is almost an ideal gas. The temperature of the air varies with altitude, giving the following variations in the speed of sound using the standard atmosphere—actual conditions may vary.
Given normal atmospheric conditions, the temperature, and thus speed of sound, varies with altitude:

Effect of frequency and gas composition
General physical considerations
The medium in which a sound wave is travelling does not always respond adiabatically, and as a result the speed of sound can vary with frequency.
The limitations of the concept of speed of sound due to extreme attenuation are also of concern. The attenuation which exists at sea level for high frequencies applies to successively lower frequencies as atmospheric pressure decreases, or as the mean free path increases. For this reason, the concept of speed of sound (except for frequencies approaching zero) progressively loses its range of applicability at high altitudes. The standard equations for the speed of sound apply with reasonable accuracy only to sit",Category:Physical quantities,3
113,114,Atmospheric diffraction,"Atmospheric diffraction is manifested in the following principal ways:
Optical atmospheric diffraction
Radio wave diffraction is the scattering of radio frequency or lower frequencies from the Earth's ionosphere, resulting in the ability to achieve greater distance radio broadcasting.
Sound wave diffraction is the bending of sound waves, as the sound travels around edges of geometric objects. This produces the effect of being able to hear even when the source is blocked by a solid object. The sound waves bend appreciably around the solid object.
However, if the object has a diameter greater than the acoustic wavelength, a 'sound shadow' is cast behind the object where the sound is inaudible. (Note: some sound may be propagated through the object depending on material).

Optical atmospheric diffraction
When light travels through thin clouds made up of nearly uniform sized water or aerosol droplets or ice crystals, diffraction or bending of light occurs as the light is diffracted by the edges of the particles. This degree of bending of light depends on the frequency (color) of light and the size of the particles. The result is a pattern of rings, which seem to emanate from the Sun, the Moon, a planet, or another astronomical object. The most distinct part of this pattern is a central, nearly white disk. This resembles an atmospheric Airy disc but is not actually an Airy disk. It is distinct from rainbows and halos, which are mainly caused by refraction.

The left photo shows a diffraction ring around the rising Sun caused by a veil of aerosol. This effect dramatically disappeared when the Sun rose high enough until the pattern was no longer visible on the Earth's surface. This phenomenon is sometimes called the corona effect, not to be confused with the solar corona.
On the right is a 1/10-second exposure showing an overexposed full moon. The Moon is seen through thin vaporous clouds, which glow with a bright disk surrounded by an illuminated red ring. A longer exposure would show more faint colors beyond the outside red ring.
Another form of atmospheric diffraction or bending of light occurs when light moves through fine layers of particulate dust trapped primarily in the middle layers of the troposphere. This effect differs from water based atmospheric diffraction because the dust material is opaque whereas water allows light to pass through it. This has the effect of tinting the light the color of the dust particles. This tinting can vary from red to yellow depending on geographical location. the other primary difference is that dust based diffraction acts as a magnifier instead of creating a distinct halo. This occurs because the opaque matter does not share the lensing properties of water. The effect is to make an object visibly larger while being more indistinct as the dust distorts the image. This effect varies largely based on the amount and type of dust in the atmosphere.

Radio wave propagation in the ionosphere
The ionosphere is a layer of partially ionized gases high above the majority of the Earth's atmosphere; these gases are ionized by cosmic rays originating on the sun. When radio waves travel into this zone, which commences about 80 kilometers above the earth, they experience diffraction in a manner similar to the visible light phenomenon described above. In this case some of the electromagnetic energy is bent in a large arc, such that it can return to the Earth's surface at a very distant point (on the order of hundreds of kilometers from the broadcast source. More remarkably some of this radio wave energy bounces off the Earth's surface and reaches the ionosphere for a second time, at a distance even farther away than the first time. Consequently, a high powered transmitter can effectively broadcast over 1000 kilometers by using multiple ""skips"" off of the ionosphere. And, at times of favorable atmospheric conditions good ""skip"" occurs, then even a low power transmitter can be heard halfway around th world. This often occurs for ""novice"" radio amateurs ""hams"" who are limited by law to transmitters with no more than 65 watts. The Kon-Tiki expedition communicated regularly with a 6 watt transmitter from the middle of the Pacific. For more details see the ""communications"" part of the ""Kon-Tiki expedition"" entry in Wikipedia.
An exotic variant of this radio wave propagation has been examined to show that, theoretically, the ionospheric bounce could be greatly exaggerated if a high powered spherical acoustical wave were created in the ionosphere from a source on earth.

Acoustical diffraction near the Earth's surface
In the case of sound waves travelling near the Earth's surface, the waves are diffracted or bent as they traverse by a geometric edge, such as a wall or building. This phenomenon leads to a very important practical effect: that we can hear ""around corners"". Because of the frequencies involved considerable amount of the sound energy (on the order of ten percent) actually travels into this would be sound ""shadow zone"". Visible light exhibits a similar effect, but, due to its much higher frequency, only a minute amount of light energy travels around a corner.
A useful branch of acoustics dealing with the design of noise barriers examines this acoustical diffraction phenomenon in quantitative detail to calculate the optimum height and placement of a soundwall or berm adjacent to a highway.
This phenomenon is also inherent in calculating the sound levels from aircraft noise, so that an accurate determination of topographic features may be understood. In that way one can produce sound level isopleths, or contour maps, which faithfully depict outcomes over variable terrain.

Bibliography
See also
Atmospheric refraction
Noise barrier

External links
Explanation and image gallery - Atmospheric Optics by Les Cowley",Category:Diffraction,3
114,115,Line level,"Line level is the specified strength of an audio signal used to transmit analog sound between audio components such as CD and DVD players, television sets, audio amplifiers, and mixing consoles.
As opposed to line level, there are weaker audio signals, such as those from microphones and instrument pickups, and stronger signals, such as those used to drive headphones and loudspeakers. The ""strength"" of these various signals does not necessarily refer to the output voltage of the source device; it also depends on its output impedance and output power capability.
Consumer electronic devices concerned with audio (for example sound cards) often have a connector labeled line in and/or line out. Line out provides an audio signal output and line in receives a signal input. The line in/out connections on consumer-oriented audio equipment are typically unbalanced, with a 3.5 mm (0.14 inch, but commonly called ""eighth inch"") 3-conductor TRS minijack connector providing ground, left channel, and right channel, or stereo RCA jacks. Professional equipment commonly uses balanced connections on 6.35 mm (1/4 inch) TRS phone jacks or XLR connectors. Professional equipment may also use unbalanced connections with (1/4 inch) TS phone jacks.

Nominal levels
A line level describes a line's nominal signal level as a ratio, expressed in decibels, against a standard reference voltage. The nominal level and the reference voltage against which it is expressed depend on the line level being used. While the nominal levels themselves vary, only two reference voltages are common: decibel volts (dBV) for consumer applications, and decibels unloaded (dBu) for professional applications.
The decibel volt reference voltage is 1 VRMS = 0 dBV. The decibel unloaded reference voltage, 0 dBu, is the AC voltage required to produce 1 mW of power across a 600 ? impedance (approximately 0.7746 VRMS). This awkward unit is a holdover from the early telephone standards, which used 600 ? sources and loads, and measured dissipated power in decibel-milliwatts (dBm). Modern audio equipment does not use 600 ? matched loads, hence dBm unloaded (dBu).
The most common nominal level for consumer audio equipment is ?10 dBV, and the most common nominal level for professional equipment is +4 dBu (by convention, decibel values are written with an explicit sign symbol).
Expressed in absolute terms, a signal at ?10 dBV is equivalent to a sine wave signal with a peak amplitude (VPK) of approximately 0.447 volts, or any general signal at 0.316 volts root mean square (VRMS). A signal at +4 dBu is equivalent to a sine wave signal with a peak amplitude of approximately 1.736 volts, or any general signal at approximately 1.228 VRMS.
Peak-to-peak (sometimes abbreviated as p-p) amplitude (VPP) refers to the total voltage swing of a signal, which is double the peak amplitude of the signal. For instance, a signal with a peak amplitude of ±0.5 V has a p-p amplitude of 1.0 V.
The line level signal is an alternating current signal without a DC offset, meaning that its voltage varies with respect to signal ground from the peak amplitude (for example +1.5 V) to the equivalent negative voltage (?1.5 V).

Impedances
As cables between line output and line input are generally extremely short compared to the audio signal wavelength in the cable, transmission line effects can be disregarded and impedance matching need not be used. Instead, line level circuits use the impedance bridging principle, in which a low impedance output drives a high impedance input. A typical line out connection has an output impedance from 100 to 600 ?, with lower values being more common in newer equipment. Line inputs present a much higher impedance, typically 10 k? or more.
The two impedances form a voltage divider with a shunt element that is large relative to the size of the series element, which ensures that little of the signal is shunted to ground and that current requirements are minimized. Most of the voltage asserted by the output appears across the input impedance and almost none of the voltage is dropped across the output. The line input acts similarly to a high impedance voltmeter or oscilloscope input, measuring the voltage asserted by the output while drawing minimal current (and hence minimal power) from the source. The high impedance of the line in circuit does not load down the output of the source device.
These are voltage signals (as opposed to current signals) and it is the signal information (voltage) that is desired, not power to drive a transducer, such as a speaker or antenna. The actual information that is exchanged between the devices is the variance in voltage; it is this alternating voltage signal that conveys the information, making the current irrelevant.

Line out
Line-out symbol. PC Guide color      lime green.
Line outputs usually present a source impedance of from 100 to 600 ohms. The voltage can reach 2 volts peak-to-peak with levels referenced to ?10 dBV (300 mV) at 10 k?. The frequency response of most modern equipment is advertised as at least 20 Hz to 20 kHz, which corresponds to the range of human hearing. Line outputs are intended to drive a load impedance of 10,000 ohms; with only a few volts, this requires only minimal current.

Connecting other devices
Connecting a low-impedance load such as a loudspeaker (usually 4 to 8 ?) to a line out will essentially short circuit the output circuit. Such loads are around 1/1000 the impedance a line out is designed to drive, so the line out is usually not designed to source the current that would be drawn by a 4 to 8 ohm load at normal line out signal voltages. The result will be very weak sound from the speaker and possibly a damaged line out circuit.
Headphone outputs and line outputs are sometimes confused. Different make and model headphones have widely varying impedances, from as little as 20 ? to a few hundred ohms; the lowest of these will have results similar to a speaker, while the highest may work acceptably if the line out impedance is low enough and the headphones are sensitive enough.
Conversely, a headphone output generally has a source impedance of only a few ohms (to provide a bridging connection with 32 ohm headphones) and will easily drive a line input.
For similar reasons, ""wye""-cables (or ""Y-splitters"") should not be used to combine two line out signals into a single line in. Each line output would be driving the other line output as well as the intended input, again resulting in a much heavier load than designed for. This will result in signal loss and possibly even damage. An active mixer, using for example op-amps, should be used instead. A large resistor in series with each output can be used to safely mix them together, but must be appropriately designed for the load impedance and cable length.

Line in
Line-in symbol. PC Guide color      light blue.
It is intended by designers that the line out of one device be connected to the line input of another. Line inputs are designed to accept voltage levels in the range provided by line outputs. Impedances, on the other hand, are deliberately not matched from output to input. The impedance of a line input is typically around 10 k?. When driven by a line output's usual low impedance of 100 to 600 ohms, this forms a ""bridging"" connection in which most of the voltage generated by the source (the output) is dropped across the load (the input), and minimal current flows due to the load's relatively high impedance.
Although line inputs have a high impedance compared to that of line outputs, they should not be confused with so-called ""Hi-Z"" inputs (Z being the symbol for impedance) which have an impedance of 47 k? to over 1 M?. These ""Hi-Z"" or ""instrument"" inputs generally have higher gain than a line input. They are designed to be used with, for example, electric guitar pickups and ""direct injection"" boxes. Some of these sources can provide only minimal voltage and current and the high impedance input is designed to not load them excessively.

Line level in traditional signal paths
Acoustic sounds (such as voices or musical instruments) are often recorded with transducers (microphones and pickups) that produce weak electrical signals. These signals must be amplified to line level, where they are more easily manipulated by other devices such as mixing consoles and tape recorders. Such amplification is performed by a device known as a preamplifier or ""preamp"", which boosts the signal to line level. After manipulation at line level, signals are then typically sent to a power amplifier, where they are amplified to levels that can drive headphones or loudspeakers. These convert the signals back into sounds that can be heard through the air.
Most phonograph cartridges also have a low output level and require a preamp; typically, a home stereo integrated amplifier or receiver will have a special phono input. This input passes the signal through a phono preamp, which applies RIAA equalization to the signal as well as boosting it to line level.

See also
Nominal level
Alignment level
Microphone
Preamplifier
Amplifier
Soundcard

References
External links
Conversion of dBu to volts, dBV to volts, and volts to dBu, and dBV
Conversion of voltage V to dB, dBu, dBV, and dBm
The Decibel",Category:Audio engineering,3
115,116,Computational auditory scene analysis,"Computational auditory scene analysis (CASA) is the study of auditory scene analysis by computational means. In essence, CASA systems are ""machine listening"" systems that aim to separate mixtures of sound sources in the same way that human listeners do. CASA differs from the field of blind signal separation in that it is (at least to some extent) based on the mechanisms of the human auditory system, and thus uses no more than two microphone recordings of an acoustic environment. It is related to the cocktail party problem.

Principles
Since CASA serves to model functionality parts of the auditory system, it is necessary to view parts of the biological auditory system in terms of known physical models. Consisting of three areas, the outer, middle and inner ear, the auditory periphery acts as a complex transducer that converts sound vibrations into action potentials in the auditory nerve. The outer ear consists of the external ear, ear canal and the ear drum. The outer ear, like an acoustic funnel, helps locating the sound source. The ear canal acts as a resonant tube (like an organ pipe) to amplify frequencies between 2-5.5 kHz with a maximum amplification of about 11dB occurring around 4 kHz. As the organ of hearing, the cochlea consists of two membranes, Reissner’s and the basilar membrane. The basilar membrane moves to audio stimuli through the specific stimulus frequency matches the resonant frequency of a particular region of the basilar membrane. The movement the basilar membrane displaces the inner hair cells in one direction, which encodes a half-wave rectified signal of action potentials in the spiral ganglion cells. The axons of these cells make up the auditory nerve, encoding the rectified stimulus. The auditory nerve responses select certain frequencies, similar to the basilar membrane. For lower frequencies, the fibers exhibit “phase locking”. Neurons in higher auditory pathway centers are tuned to specific stimuli features, such as periodicity, sound intensity, amplitude and frequency modulation. There are also neuroanatomical associations of ASA through the posterior cortical areas, including the posterior superior temporal lobes and the posterior cingulate. Studies have found that impairments in ASA and segregation and grouping operations are affected in patients with Alzheimer's disease.

System Architecture
Cochleagram
As the first stage of CASA processing, the cochleagram creates a time-frequency representation of the input signal. By mimicking the components of the outer and middle ear, the signal is broken up into different frequencies that are naturally selected by the cochlea and hair cells. Because of the frequency selectivity of the basilar membrane, a filter bank is used to model the membrane, with each filter associated with a specific point on the basilar membrane.
Since the hair cells produce spike patterns, each filter of the model should also produce a similar spike in the impulse response. The use of a gammatone filter provides an impulse response as the product of a gamma function and a tone. The output of the gammatone filter can be regarded as a measurement of the basilar membrane displacement. Most CASA systems represent the firing rate in the auditory nerve rather than a spike-based. To obtain this, the filter bank outputs are half-wave rectified followed by a square root. (Other models, such as automatic gain controllers have been implemented). The half-rectified wave is similar to the displacement model of the hair cells. Additional models of the hair cells include the Meddis hair cell model which pairs with the gammatone filter bank, by modeling the hair cell transduction. Based on the assumption that there are three reservoirs of transmitter substance within each hair cell, and the transmitters are released in proportion to the degree of displacement to the basilar membrane, the release is equated with the probability of a spike generated in the nerve fiber. This model replicates many of the nerve responses in the CASA systems such as rectification, compression, spontaneous firing, and adaptation.

Correlogram
Important model of pitch perception by unifying 2 schools of pitch theory:
Place theories (emphasizing the role of resolved harmonics)
Temporal theories (emphasizing the role of unresolved harmonics)
The correlogram is generally computed in the time domain by autocorrelating the simulated auditory nerve firing activity to the output of each filter channel. By pooling the autocorrelation across frequency, the position of peaks in the summary correlogram corresponds to the perceived pitch.

Cross-Correlogram
Because the ears receive audio signals at different times, the sound source can be determined by using the delays retrieved from the two ears. By cross-correlating the delays from the left and right channels (of the model), the coincided peaks can be categorized as the same localized sound, despite their temporal location in the input signal. The use of interaural cross-correlation mechanism has been supported through physiological studies, paralleling the arrangement of neurons in the auditory midbrain.

Time-Frequency Masks
To segregate the sound source, CASA systems mask the cochleagram. This mask, sometimes a Wiener filter, weighs the target source regions and suppresses the rest. The physiological motivation behind the mask results from the auditory perception where sound is rendered inaudible by a louder sound.

Resynthesis
A resynthesis pathway reconstructs an audio signal from a group of segments. Achieved by inverting the cochleagram, high quality resynthesized speech signals can be obtained.

Applications
Monaural CASA
Monaural sound separation first began with separating voices based on frequency. There were many early developments based on segmenting different speech signals through frequency. Other models followed on this process, by the addition of adaption through state space models, batch processing, and prediction-driven architecture. The use of CASA has improved the robustness of ASR and speech separation systems.

Binaural CASA
Since CASA is modeling human auditory pathways, binaural CASA systems better the human model by providing sound localization, auditory grouping and robustness to reverberation by including 2 spatially separated microphones. With methods similar to cross-correlation, systems are able to extract the target signal from both input microphones.

Neural CASA Models
Since the biological auditory system is deeply connected with the actions of neurons, CASA systems also incorporated neural models within the design. Two different models provide the basis for this area. Malsburg and Schneider proposed a neural network model with oscillators to represent features of different streams (synchronized and desynchronized). Wang also presented a model using a network of excitatory units with a global inhibitor with delay lines to represent the auditory scene within the time-frequency.

Analysis of Musical Audio Signals
Typical approaches in CASA systems starts with segmenting sound-sources into individual constituents, in its attempts to mimic the physical auditory system. However, there is evidence that the brain does not necessarily process audio input separately, but rather as a mixture. Instead of breaking the audio signal down to individual constituents, the input is broken down of by higher level descriptors, such as chords, bass and melody, beat structure, and chorus and phrase repetitions. These descriptors run into difficulties in real-world scenarios, with monaural and binaural signals. Also, the estimation of these descriptors is highly dependent on the cultural influence of the musical input. For example, within Western music, the melody and bass influences the identity of the piece, with the core formed by the melody. By distinguishing the frequency responses of melody and bass, a fundamental frequency can be estimated and filtered for distinction. Chord detection can be implemented through pattern recognition, by extracting low-level features describing harmonic content. The techniques utilized in music scene analysis can also be applied to speech recognition, and other environmental sounds. Future bodies of work include a top-down integration of audio signal processing, such as a real-time beat-tracking system and expanding out of the signal processing realm with the incorporation of auditory psychology and physiology.

Neural Perceptual Modeling
While many models consider the audio signal as a complex combination of different frequencies, modeling the auditory system can also require consideration for the neural components. By taking a holistic process, where a stream (of feature-based sounds) correspond to neuronal activity distributed in many brain areas, the perception of the sound could be mapped and modeled. Two different solutions have been proposed to the binding of the audio perception and the area in the brain. Hierarchical coding models many cells to encode all possible combinations of features and objects in the auditory scene. Temporal or oscillatory correlation addressing the binding problem by focusing on the synchrony and desynchrony between neural oscillations to encode the state of binding among the auditory features. These two solutions are very similar to the debacle between place coding and temporal coding. While drawing from modeling neural components, another phenomenon of ASA comes into play with CASA systems: the extent of modeling neural mechanisms. The studies of CASA systems have involved modeling some known mechanisms, such as the bandpass nature of cochlear filtering and random auditory nerve firing patterns, however, these models may not lead to finding new mechanisms, but rather give an understanding of purpose to the known mechanisms.

See also
auditory scene analysis
blind signal separation
cocktail party problem
machine vision
speech recognition

Further reading
D. F. Rosenthal and H. G. Okuno (1998) Computational auditory scene analysis. Mahwah, NJ: Lawrence Erlbaum


== References ==",Category:Sound,3
116,117,Teatr Tworzenia,"Teatr Tworzenia, or The Theater of Creation, Polish avant-garde theater, founded by Jaros?aw Pijarowski. According to the author's definition it is: ""Uninstitutionalized form of realization of creative ideas, consisting on intuitive-improvisational thematic activities; Have on the goal of integration and promotion of creative thought (word, music, theater, paintings) in audio-visual forms. The Theater of Creation does not have a permanent place, which is related to the idea -Live Act - live creation according to a previously accepted scenario, for the audience with no time, space or space constraints, more than once with the audience.

History of Teatr Tworzenia (The Theater of Creation)
In the years 2006-2010 - the productions of The Theater of Creation were based on the basis of monographs and performative activities of Pijarowski (Scream Nudity, Di Logos Moon Di, Frozen in Monitoring) Warsaw, London and Prague. The turning point has become monumental a theatrical-musical spectacle entitled ""Sound Castle"" from the series:""Castles of Sound""; Realized at Golub-Dobrzy? Castle in 2011. Participants included: Jorgos Skolias - initiator ""The Sound of Peace"", drummer S?awomir Ciesielski, Boguslaw Raatz, Timothy Sanford, visual artist: Xavier Bayle, Lukasz Wody?ski and others. Another great form was the first Polish avant-garde Oratorio ""Terrarium"" realized with Józef Skrzek  in Saint Andrew Bobola's Church in Bydgoszcz. In 2013, in connection with the 80th birthday of Krzysztof Penderecki. The musical and theater project entitled:The dream Off Penderecki. In 2014 the first spectacle from the series ""Czasoprzestrze? - Live Forever "" - based on J. Pijarowski's play: ""Space-time"", describing cases of the precursor life of Polish magic - Pan Twardowski. On the way back from the premiere of the show as the final show of the Bydgoszcz Festival ""Ster on Bydgoszcz"" there was a tragic accident, which was the suspension of the performance of The Theater of Creations.

Artistic program
Pijarowski is the author of almost all texts by The Theater of Creation. Music is a result of creator and artist activities cooperating.

Stage and performance forms (selection)
„Frozen in monitoring” – performance (cycle),
„Zamek D?wi?ku” (Castle of Sound) – concert, show and movie,
„Terrarium” – concert, show and record,
„Gate 2012/2013” – radio broadcasting,
„Album Rodzi Inny” – concert, performance, show,
„Misty Mountain High” – recording session,
„Fukushima LowTuDed” – concert - performance
„Martwa Natura – Live” – concert - charity show,
„The dream Off Penderecki” – record, concert,
„Czasoprzestrze? – Live Forever” – concert, performance, show,
„Breakfast in the Exorcist’s brain” – recording session, show.

Permanent artistic cooperation
Music:
Keyboard instruments, Organ (music)s – Józef Skrzek, W?adys?aw Komendarek
Chordophone – Bogus?aw Raatz, Waldemar Knade
Wind instrument,trumpet – Jakub Marsza?ek
Percussion instruments – S?awomir Ciesielski, Marcin Jahr
Harmonica – Micha? Kielak
Voc – Jorgos Skolias, Eurazja Srzednicka, Marek Piekarczyk
Choir – Via Musica
Conductors – S?awomir ?obaczewski, Bogdan ?ywek

Artists collaborating occasionally
actors: Derek Jacobi, Daniel Olbrychski, Mariusz Benoit, Adam Ferency
musicians: Andrzej Nowak, Damian Pietrasik, Micha? Milczarek, Marcelina Sankowska, Paulina Heyer, Ma?gorzata Sanford, Robert Bielak, Tim Sanford, Marek i Krzysztof Kroschel, Ryszard Lubieniecki, Maciej Myga, Andrzej Przybielski, Krzysztof Toczko, Miko?aj Toczko, Misha Ogorodov, M.P. Szumski, Tomasz Osiecki, Andrzej Borzym, Micha? Straszewski, Rados?aw Zaworski, Glass Duo
artists: W?adys?aw Wa??ga, Leszek Goldyszewicz, Jacek Kami?ski, ?ukasz Wody?ski, Xavier Bayle, Marek Ronowski, Alina Bloch, Ma?gorzata Grydniewska
others: Robert Bernatowicz, Zdzis?aw Paj?k, Andrzej Gawro?ski, Iwona Wasilewska

Discography
Albums
Terrarium – Live in Bydgoszcz
The dream Off Penderecki
Katharsis (A Small Victory) (2017)(Brain Active Records)

Varia
OFF - ?ycie bez dotacji – music documentation of Teatr Tworzenia (compositions:8, 9, 10, 18, 19).

Illustrative formations of The Theater of Creation
The Hidden Dimensions of M. Ronowski Paintings – Bydgoszcz, Hotel City (2014)
The Paintings Exhibition of the W?adys?aw Wa??ga – „Ogrody Wyobra?ni”, Kujawsko-Pomorskie Centrum Kultury w Bydgoszczy (2015)
Interdisciplinary Exhibition of the Abakanowicz /Pijarowski - The Art Dimensions (Prologue - Warsaw)(2016)

Bibliography
OFF - ?ycie bez dotacji – The book about live and musical actions of The Theatre of Creations – (2006–2015).

References
External links
http://www.teatrtworzenia.art.pl
http://www.terrarium.art.pl
http://www.lesartes.cba.pl
http://www.progrock.org.pl",Category:Modern art,3
117,118,Schizophonia,"Schizophonia is a term coined by R. Murray Schafer to describe the splitting of an original sound and its electroacoustic reproduction. This concept comes from the invention of electroacoustic equipment for the transmission of sound, which meant that any sound could be recorded and sent anywhere around the world. Originally, that was not possible, as every sound was an original and could only be heard once. Schizophonia is the separation of this native sound and the recording of it.

In popular culture
Mike Batt released an album in 1977 entitled Schizophonia
Rinôçérôse released an album in 2005 entitled Schizophonia
A number of albums are entitled the related term Schizophonic

See also
Soundscape
Sound culture
Acoustic ecology
Schismogenesis
Acousmatic sound


== Notes ==",Category:Musicology,3
118,119,Sound,"In physics, sound is a vibration that typically propagates as an audible wave of pressure, through a transmission medium such as a gas, liquid or solid.
In human physiology and psychology, sound is the reception of such waves and their perception by the brain. Humans can hear sound waves with frequencies between about 20 Hz and 20 kHz. Sound above 20 kHz is ultrasound and below 20 Hz is infrasound. Animals have different hearing ranges.

Acoustics
Acoustics is the interdisciplinary science that deals with the study of mechanical waves in gases, liquids, and solids including vibration, sound, ultrasound, and infrasound. A scientist who works in the field of acoustics is an acoustician, while someone working in the field of acoustical engineering may be called an acoustical engineer. An audio engineer, on the other hand, is concerned with the recording, manipulation, mixing, and reproduction of sound.
Applications of acoustics are found in almost all aspects of modern society, subdisciplines include aeroacoustics, audio signal processing, architectural acoustics, bioacoustics, electro-acoustics, environmental noise, musical acoustics, noise control, psychoacoustics, speech, ultrasound, underwater acoustics, and vibration.

Definition
Sound is defined as ""(a) Oscillation in pressure, stress, particle displacement, particle velocity, etc., propagated in a medium with internal forces (e.g., elastic or viscous), or the superposition of such propagated oscillation. (b) Auditory sensation evoked by the oscillation described in (a). "" Sound can be viewed as a wave motion in air or other elastic media. In this case, sound is a stimulus. Sound can also be viewed as an excitation of the hearing mechanism that results in the perception of sound. In this case, sound is a sensation.

Physics of sound
Sound can propagate through a medium such as air, water and solids as longitudinal waves and also as a transverse wave in solids (see Longitudinal and transverse waves, below). The sound waves are generated by a sound source, such as the vibrating diaphragm of a stereo speaker. The sound source creates vibrations in the surrounding medium. As the source continues to vibrate the medium, the vibrations propagate away from the source at the speed of sound, thus forming the sound wave. At a fixed distance from the source, the pressure, velocity, and displacement of the medium vary in time. At an instant in time, the pressure, velocity, and displacement vary in space. Note that the particles of the medium do not travel with the sound wave. This is intuitively obvious for a solid, and the same is true for liquids and gases (that is, the vibrations of particles in the gas or liquid transport the vibrations, while the average position of the particles over time does not change). During propagation, waves can be reflected, refracted, or attenuated by the medium.
The behavior of sound propagation is generally affected by three things:
A complex relationship between the density and pressure of the medium. This relationship, affected by temperature, determines the speed of sound within the medium.
Motion of the medium itself. If the medium is moving, this movement may increase or decrease the absolute speed of the sound wave depending on the direction of the movement. For example, sound moving through wind will have its speed of propagation increased by the speed of the wind if the sound and wind are moving in the same direction. If the sound and wind are moving in opposite directions, the speed of the sound wave will be decreased by the speed of the wind.
The viscosity of the medium. Medium viscosity determines the rate at which sound is attenuated. For many media, such as air or water, attenuation due to viscosity is negligible.
When sound is moving through a medium that does not have constant physical properties, it may be refracted (either dispersed or focused).

The mechanical vibrations that can be interpreted as sound can travel through all forms of matter: gases, liquids, solids, and plasmas. The matter that supports the sound is called the medium. Sound cannot travel through a vacuum.

Longitudinal and transverse waves
Sound is transmitted through gases, plasma, and liquids as longitudinal waves, also called compression waves. It requires a medium to propagate. Through solids, however, it can be transmitted as both longitudinal waves and transverse waves. Longitudinal sound waves are waves of alternating pressure deviations from the equilibrium pressure, causing local regions of compression and rarefaction, while transverse waves (in solids) are waves of alternating shear stress at right angle to the direction of propagation.
Sound waves may be ""viewed"" using parabolic mirrors and objects that produce sound.
The energy carried by an oscillating sound wave converts back and forth between the potential energy of the extra compression (in case of longitudinal waves) or lateral displacement strain (in case of transverse waves) of the matter, and the kinetic energy of the displacement velocity of particles of the medium.

Sound wave properties and characteristics
Although there are many complexities relating to the transmission of sounds, at the point of reception (i.e. the ears), sound is readily dividable into two simple elements: pressure and time. These fundamental elements form the basis of all sound waves. They can be used to describe, in absolute terms, every sound we hear.
However, in order to understand the sound more fully, a complex wave such as this is usually separated into its component parts, which are a combination of various sound wave frequencies (and noise).
Sound waves are often simplified to a description in terms of sinusoidal plane waves, which are characterized by these generic properties:
Frequency, or its inverse, wavelength
Amplitude, sound pressure or Intensity
Speed of sound
Direction
Sound that is perceptible by humans has frequencies from about 20 Hz to 20,000 Hz. In air at standard temperature and pressure, the corresponding wavelengths of sound waves range from 17 m to 17 mm. Sometimes speed and direction are combined as a velocity vector; wave number and direction are combined as a wave vector.
Transverse waves, also known as shear waves, have the additional property, polarization, and are not a characteristic of sound waves.

Speed of sound
The speed of sound depends on the medium the waves pass through, and is a fundamental property of the material. The first significant effort towards measurement of the speed of sound was made by Isaac Newton. He believed the speed of sound in a particular substance was equal to the square root of the pressure acting on it divided by its density:

  
    
      
        c
        =
        
          
            
              p
              ?
            
          
        
        
      
    
    {\displaystyle c={\sqrt {p \over \rho }}\,}
  
This was later proven wrong when found to incorrectly derive the speed. The French mathematician Laplace corrected the formula by deducing that the phenomenon of sound travelling is not isothermal, as believed by Newton, but adiabatic. He added another factor to the equation—gamma—and multiplied 
  
    
      
        
          
            ?
          
        
        
      
    
    {\displaystyle {\sqrt {\gamma }}\,}
   by 
  
    
      
        
          
            
              p
              ?
            
          
        
        
      
    
    {\displaystyle {\sqrt {p \over \rho }}\,}
  , thus coming up with the equation 
  
    
      
        c
        =
        
          
            ?
            ?
            
              
                p
                ?
              
            
          
        
        
      
    
    {\displaystyle c={\sqrt {\gamma \cdot {p \over \rho }}}\,}
  . Since 
  
    
      
        K
        =
        ?
        ?
        p
        
      
    
    {\displaystyle K=\gamma \cdot p\,}
  , the final equation came up to be 
  
    
      
        c
        =
        
          
            
              K
              ?
            
          
        
        
      
    
    {\displaystyle c={\sqrt {\frac {K}{\rho }}}\,}
  , which is also known as the Newton-Laplace equation. In this equation, K = elastic bulk modulus, c = velocity of sound, and 
  
    
      
        
          ?
        
      
    
    {\displaystyle {\rho }}
   = density. Thus, the speed of sound is proportional to the square root of the ratio of the bulk modulus of the medium to its density.
Those physical properties and the speed of sound change with ambient conditions. For example, the speed of sound in gases depends on temperature. In 20 °C (68 °F) air at sea level, the speed of sound is approximately 343 m/s (1,230 km/h; 767 mph) using the formula ""v = (331 + 0.6 T) m/s"". In fresh water, also at 20 °C, the speed of sound is approximately 1,482 m/s (5,335 km/h; 3,315 mph). In steel, the speed of sound is about 5,960 m/s (21,460 km/h; 13,330 mph). The speed of sound is also slightly sensitive, being subject to a second-order anharmonic effect, to the sound amplitude, which means there are non-linear propagation effects, such as the production of harmonics and mixed tones not present in the original sound (see parametric array).

Perception of sound
A distinct use of the term sound from its use in physics is that in physiology and psychology, where the term refers to the subject of perception by the brain. The field of psychoacoustics is dedicated to such studies. Historically the word ""sound"" referred exclusively to an effect in the mind. Webster's 1947 dictionary defined sound as: ""that which is heard; the effect which is produced by the vibration of a body affecting the ear."" This meant (at least in 1947) the correct response to the question: ""if a tree falls in the forest with no one to hear it fall, does it make a sound?"" was ""no"". However, owing to contemporary usage, definitions of sound as a physical effect are prevalent in most dictionaries. Consequently, the answer to the same question is now ""yes, a tree falling in the forest with no one to hear it fall does make a sound"".
The physical reception of sound in any hearing organism is limited to a range of frequencies. Humans normally hear sound frequencies between approximately 20 Hz and 20,000 Hz (20 kHz),:382 The upper limit decreases with age. Sometimes sound refers to only those vibrations with frequencies that are within the hearing range for humans or sometimes it relates to a particular animal. Other species have different ranges of hearing. For example, dogs can perceive vibrations higher than 20 kHz.
As a signal perceived by one of the major senses, sound is used by many species for detecting danger, navigation, predation, and communication. Earth's atmosphere, water, and virtually any physical phenomenon, such as fire, rain, wind, surf, or earthquake, produces (and is characterized by) its unique sounds. Many species, such as frogs, birds, marine and terrestrial mammals, have also developed special organs to produce sound. In some species, these produce song and speech. Furthermore, humans have developed culture and technology (such as music, telephone and radio) that allows them to generate, record, transmit, and broadcast sound.
Noise is a term often used to refer to an unwanted sound. In science and engineering, noise is an undesirable component that obscures a wanted signal. However, in sound perception it can often be used to identify the source of a sound and is an important component of timbre perception (see above).
Soundscape is the component of the acoustic environment that can be perceived by humans. The acoustic environment is the combination of all sounds (whether audible to humans or not) within a given area as modified by the environment and understood by people, in context of the surrounding environment.
There are six experimentally separable ways in which sound waves are analysed. They are: pitch, duration, loudness, timbre, sonic texture and spatial location.

Pitch
Pitch is perceived as how ""low"" or ""high"" a sound is and represents the cyclic, repetitive nature of the vibrations that make up sound. For simple sounds, pitch relates to the frequency of the slowest vibration in the sound (called the fundamental harmonic). In the case of complex sounds, pitch perception can vary. Sometimes individuals identify different pitches for the same sound, based on their personal experience of particular sound patterns. Selection of a particular pitch is determined by pre-conscious examination of vibrations, including their frequencies and the balance between them. Specific attention is given to recognising potential harmonics. Every sound is placed on a pitch continuum from low to high. For example: white noise (random noise spread evenly across all frequencies) sounds higher in pitch than pink noise (random noise spread evenly across octaves) as white noise has more high frequency content. Figure 1 shows an example of pitch recognition. During the listening process, each sound is analysed for a repeating pattern (See Figure 1: orange arrows) and the results forwarded to the auditory cortex as a single pitch of a certain height (octave) and chroma (note name).

Duration
Duration is perceived as how ""long"" or ""short"" a sound is and relates to onset and offset signals created by nerve responses to sounds. The duration of a sound usually lasts from the time the sound is first noticed until the sound is identified as having changed or ceased. Sometimes this is not directly related to the physical duration of a sound. For example; in a noisy environment, gapped sounds (sounds that stop and start) can sound as if they are continuous because the offset messages are missed owing to disruptions from noises in the same general bandwidth. This can be of great benefit in understanding distorted messages such as radio signals that suffer from interference, as (owing to this effect) the message is heard as if it was continuous. Figure 2 gives an example of duration identification. When a new sound is noticed (see Figure 2, Green arrows), a sound onset message is sent to the auditory cortex. When the repeating pattern is missed, a sound offset messages is sent.

Loudness
Loudness is perceived as how ""loud"" or ""soft"" a sound is and relates to the totalled number of auditory nerve stimulations over short cyclic time periods, most likely over the duration of theta wave cycles. This means that at short durations, a very short sound can sound softer than a longer sound even though they are presented at the same intensity level. Past around 200 ms this is no longer the case and the duration of the sound no longer affects the apparent loudness of the sound. Figure 3 gives an impression of how loudness information is summed over a period of about 200 ms before being sent to the auditory cortex. Louder signals create a greater 'push' on the Basilar membrane and thus stimulate more nerves, creating a stronger loudness signal. A more complex signal also creates more nerve firings and so sounds louder (for the same wave amplitude) than a simpler sound, such as a sine wave.

Timbre
Timbre is perceived as the quality of different sounds (e.g. the thud of a fallen rock, the whir of a drill, the tone of a musical instrument or the quality of a voice) and represents the pre-conscious allocation of a sonic identity to a sound (e.g. “it’s an oboe!""). This identity is based on information gained from frequency transients, noisiness, unsteadiness, perceived pitch and the spread and intensity of overtones in the sound over an extended time frame. The way a sound changes over time (see figure 4) provides most of the information for timbre identification. Even though a small section of the wave form from each instrument looks very similar (see the expanded sections indicated by the orange arrows in figure 4), differences in changes over time between the clarinet and the piano are evident in both loudness and harmonic content. Less noticeable are the different noises heard, such as air hisses for the clarinet and hammer strikes for the piano.

Sonic texture
Sonic texture relates to the number of sound sources and the interaction between them. The word 'texture', in this context, relates to the cognitive separation of auditory objects. In music, texture is often referred to as the difference between unison, polyphony and homophony, but it can also relate (for example) to a busy cafe; a sound which might be referred to as 'cacophony'. However texture refers to more than this. The texture of an orchestral piece is very different to the texture of a brass quintet because of the different numbers of players. The texture of a market place is very different to a school hall because of the differences in the various sound sources.

Spatial location
Spatial location (see: Sound localization) represents the cognitive placement of a sound in an environmental context; including the placement of a sound on both the horizontal and vertical plane, the distance from the sound source and the characteristics of the sonic environment. In a thick texture, it is possible to identify multiple sound sources using a combination of spatial location and timbre identification. It is the main reason why we can pick the sound of an oboe in an orchestra and the words of a single person at a cocktail party.

Sound pressure level
Sound pressure is the difference, in a given medium, between average local pressure and the pressure in the sound wave. A square of this difference (i.e., a square of the deviation from the equilibrium pressure) is usually averaged over time and/or space, and a square root of this average provides a root mean square (RMS) value. For example, 1 Pa RMS sound pressure (94 dBSPL) in atmospheric air implies that the actual pressure in the sound wave oscillates between (1 atm 
  
    
      
        ?
        
          
            2
          
        
      
    
    {\displaystyle -{\sqrt {2}}}
   Pa) and (1 atm 
  
    
      
        +
        
          
            2
          
        
      
    
    {\displaystyle +{\sqrt {2}}}
   Pa), that is between 101323.6 and 101326.4 Pa. As the human ear can detect sounds with a wide range of amplitudes, sound pressure is often measured as a level on a logarithmic decibel scale. The sound pressure level (SPL) or Lp is defined as

  
    
      
        
          L
          
            
              p
            
          
        
        =
        10
        
        
          log
          
            10
          
        
        ?
        
          (
          
            
              
                
                  p
                
                
                  2
                
              
              
                
                  
                    p
                    
                      
                        r
                        e
                        f
                      
                    
                  
                
                
                  2
                
              
            
          
          )
        
        =
        20
        
        
          log
          
            10
          
        
        ?
        
          (
          
            
              p
              
                p
                
                  
                    r
                    e
                    f
                  
                
              
            
          
          )
        
        
          
             dB
          
        
        
      
    
    {\displaystyle L_{\mathrm {p} }=10\,\log _{10}\left({\frac {{p}^{2}}{{p_{\mathrm {ref} }}^{2}}}\right)=20\,\log _{10}\left({\frac {p}{p_{\mathrm {ref} }}}\right){\mbox{ dB}}\,}
  
where p is the root-mean-square sound pressure and 
  
    
      
        
          p
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle p_{\mathrm {ref} }}
   is a reference sound pressure. Commonly used reference sound pressures, defined in the standard ANSI S1.1-1994, are 20 µPa in air and 1 µPa in water. Without a specified reference sound pressure, a value expressed in decibels cannot represent a sound pressure level.
Since the human ear does not have a flat spectral response, sound pressures are often frequency weighted so that the measured level matches perceived levels more closely. The International Electrotechnical Commission (IEC) has defined several weighting schemes. A-weighting attempts to match the response of the human ear to noise and A-weighted sound pressure levels are labeled dBA. C-weighting is used to measure peak levels.

Ultrasound
Ultrasound is sound waves with frequencies higher than the upper audible limit of human hearing. Ultrasound is no different from 'normal' (audible) sound in its physical properties, except in that humans cannot hear it. Ultrasound devices operate with frequencies from 20 kHz up to several gigahertz.
Ultrasound is commonly used for medical diagnostics such as sonograms.

See also
References
External links
Sounds Amazing; a KS3/4 learning resource for sound and waves
HyperPhysics: Sound and Hearing
Introduction to the Physics of Sound
Hearing curves and on-line hearing test
Audio for the 21st Century
Conversion of sound units and levels
Sound calculations
Audio Check: a free collection of audio tests and test tones playable on-line
More Sounds Amazing; a sixth-form learning resource about sound waves",Category:Sound,3
119,120,Node (physics),"A node is a point along a standing wave where the wave has minimum amplitude. For instance, in a vibrating guitar string, the ends of the string are nodes. By changing the position of the end node through frets, the guitarist changes the effective length of the vibrating string and thereby the note played. The opposite of a node is an anti-node, a point where the amplitude of the standing wave is a maximum. These occur midway between the nodes.

Explanation
Standing waves result when two sinusoidal wave trains of the same frequency are moving in opposite directions in the same space and interfere with each other. They occur when waves are reflected at a boundary, such as sound waves reflected from a wall or electromagnetic waves reflected from the end of a transmission line, and particularly when waves are confined in a resonator at resonance, bouncing back and forth between two boundaries, such as in an organ pipe or guitar string.
In a standing wave the nodes are a series of locations at equally spaced intervals where the wave amplitude (motion) is zero (see animation above). At these points the two waves add with opposite phase and cancel each other out. They occur at intervals of half a wavelength (?/2). Midway between each pair of nodes are locations where the amplitude is maximum. These are called the antinodes. At these points the two waves add with the same phase and reinforce each other.
In cases where the two opposite wave trains are not the same amplitude, they do not cancel perfectly, so the amplitude of the standing wave at the nodes is not zero but merely a minimum. This occurs when the reflection at the boundary is imperfect. This is indicated by a finite standing wave ratio (SWR), the ratio of the amplitude of the wave at the antinode to the amplitude at the node.
In resonance of a two dimensional surface or membrane, such as a drumhead or vibrating metal plate, the nodes become nodal lines, lines on the surface where the surface is motionless, dividing the surface into separate regions vibrating with opposite phase. These can be made visible by sprinkling sand on the surface, and the intricate patterns of lines resulting are called Chladni figures.
In transmission lines a voltage node is a current antinode, and a voltage antinode is a current node.
Nodes are the points of zero displacement, not the points where two constituent waves intersect.

Boundary conditions
Where the nodes occur in relation to the boundary reflecting the waves depends on the end conditions or boundary conditions. Although there are many types of end conditions, the ends of resonators are usually one of two types that cause total reflection:
Fixed boundary: Examples of this type of boundary are the attachment point of a guitar string, the closed end of an open pipe like an organ pipe or a woodwind pipe, the periphery of a drumhead, a transmission line with the end short circuited, or the mirrors at the ends of a laser cavity. In this type, the amplitude of the wave is forced to zero at the boundary, so there is a node at the boundary, and the other nodes occur at multiples of half a wavelength from it:

0,  ?/2,  ?,  3?/2,  2?, ...

Free boundary: Examples of this type are an open-ended organ or woodwind pipe, the ends of the vibrating resonator bars in a xylophone, glockenspiel or tuning fork, the ends of an antenna, or a transmission line with an open end. In this type the derivative (slope) of the wave's amplitude (in sound waves the pressure, in electromagnetic waves the current) is forced to zero at the boundary. So there is an amplitude maximum (antinode) at the boundary, the first node occurs a quarter wavelength from the end, and the other nodes are at half wavelength intervals from there:

?/4,  3?/4,  5?/4,  7?/4, ...

Examples
Sound
A sound wave consists of alternating cycles of compression and expansion of the wave medium. During compression, the molecules of the medium are forced together, resulting in the increased pressure and density. During expansion the molecules are forced apart, resulting in the decreased pressure and density.
The number of nodes in a specified length is directly proportional to the frequency of the wave.
Occasionally on a guitar, violin, or other stringed instrument, nodes are used to create harmonics. When the finger is placed on top of the string at a certain point, but does not push the string all the way down to the fretboard, a third node is created (in addition to the bridge and nut) and a harmonic is sounded. During normal play when the frets are used, the harmonics are always present, although they are quieter. With the artificial node method, the overtone is louder and the fundamental tone is quieter. If the finger is placed at the midpoint of the string, the first overtone is heard, which is an octave above the fundamental note which would be played, had the harmonic not been sounded. When two additional nodes divide the string into thirds, this creates an octave and a perfect fifth (twelfth). When three additional nodes divide the string into quarters, this creates a double octave. When four additional nodes divide the string into fifths, this creates a double-octave and a major third (17th). The octave, major third and perfect fifth are the three notes present in a major chord.
The characteristic sound that allows the listener to identify a particular instrument is largely due to the relative magnitude of the harmonics created by the instrument.

Chemistry
In chemistry, quantum mechanical waves, or ""orbitals"", are used to describe the wave-like properties of electrons. Many of these quantum waves have nodes and antinodes as well. The number and position of these nodes and antinodes give rise to many of the properties of an atom or covalent bond. Atomic orbitals are classified according to the number of radial and angular nodes, while molecular orbitals are classified according to bonding character. Molecular orbitals with an antinode between nuclei are very stable, and are known as ""bonding orbitals"" which strengthen the bond. In contrast, molecular orbitals with a node between nuclei will not be stable due to electrostatic repulsion and are known as ""anti-bonding orbitals"" which weaken the bond. Another such quantum mechanical concept is the particle in a box where the number of nodes of the wavefunction can help determine the quantum energy state—zero nodes corresponds to the ground state, one node corresponds to the 1st excited state, etc. In general, If one arranges the eigenstates in the order of increasing energies, 
  
    
      
        
          ?
          
            1
          
        
        ,
        
          ?
          
            2
          
        
        ,
        
          ?
          
            3
          
        
        ,
        .
        .
        .
      
    
    {\displaystyle \epsilon _{1},\epsilon _{2},\epsilon _{3},...}
  , the eigenfunctions likewise fall in the order of increasing number of nodes; the nth eigenfunction has n?1 nodes, between each of which the following eigenfunctions have at least one node.


== References ==",Category:Musical tuning,3
120,121,Multichannel television sound,"Multichannel television sound, better known as MTS (often still as BTSC, for the Broadcast Television Systems Committee that created it), is the method of encoding three additional channels of audio into an analog NTSC-format audio carrier.

History
Multichannel Television Sound was adopted by the Federal Communications Commission as the U.S. standard for stereo television transmission in 1984. Initial work on design and testing of a stereophonic audio system began in 1975 when Telesonics approached Chicago public television station WTTW. WTTW was producing a music show titled Soundstage at that time, and was simulcasting the stereo audio mix on local FM stations. Telesonics and WTTW formed a working relationship and began developing the system which was similar to FM stereo modulation. Twelve WTTW studio and transmitter engineers added the needed broadcast experience to the relationship. The Telesonics system was tested and refined using the WTTW transmitter facilities on the Sears Tower. In 1979, WTTW had installed a stereo Grass Valley master control switcher, and had added a second audio channel to the microwave STL (Studio Transmitter Link). By that time, WTTW engineers had further developed stereo audio on videotape recorders in their plant, using split audio track heads manufactured to their specifications, outboard record electronics, and Dolby noise reduction that allowed Soundstage to be recorded and electronically edited. In addition, an Ampex MM1100, 24-track audio recorder was also used for music production and mixing. PBS member stations who wished to deliver Soundstage in stereo were provided with a four-track (left, right, vertical drive, and time code) audio tape that could be synced with the video machines in those cities.
During the FCC approval process, several manufacturers applied to the FCC for consideration. Most notably the Electronic Industries Alliance (EIA) and Japanese EIA asked to be included in order to represent their members in the testing and specification phases of the approval process. WTTW engineers helped set standards for frequency response, separation, and other uses of the spectrum. They also provided program source material used for the testing and maintained the broadcast chain. A 3M 24-track audio recorder was used to allow the selection of 12 different stereo programs for testing. The Matsushita Quasar TV manufacturing plant and laboratory, just west of Chicago, was used as the source for all testing of the competing systems. Following the testing, several questions were raised about the validity of some of the tests, and a second round of testing began.
WTTW installed a Broadcast Electronics prototype stereo modulator in October 1983, and began full-time broadcasting in stereo at that time using the Telesonics system prior to FCC rule-making on the BTSC system. Following EIA and FCC recommendations, the BE modulator was modified to meet BTSC specifications, and by August 1984 was in full-time use on WTTW.
Sporadic network transmission of stereo audio began on NBC on July 26, 1984, with The Tonight Show Starring Johnny Carson, although at the time only the network's New York City flagship station, WNBC, had stereo broadcast capability; regular stereo transmission of NBC programs began during early 1985. ABC and CBS followed suit in 1986 and 1987, respectively. FOX was the last network to join around 1990, with the four networks having their entire prime-time schedules in stereo by late 1994 (The WB and UPN launched the following season with their entire line-ups in stereo). One of the first television receiving systems to include BTSC capability was the RCA Dimensia, released in 1984.
From the mid-1980s to the mid-1990s and also in the 2000s, the networks would display the disclaimer ""In stereo (where available)"" at the beginning of stereo programming.

Adopted in
It has also been adopted by
United States for NTSC
Canada for NTSC
Mexico for NTSC
Chile for NTSC
Brazil for PAL-M
Taiwan for NTSC (historic, switched to DVB-T)
Argentina for PAL-N
Philippines for NTSC

How MTS works
The first channel is the stereo difference (left minus right), used to add stereophonic sound to the existing monophonic (the left plus right stereo sum) audio track.
In other words, the normal mono television audio consists of L+R information. A second signal (MTS) rides on top of this mono carrier wave. This MTS signal consists of L minus R.
When the two audio channels are added together, or summed (L+R plus L-R), the left channel is derived.
When the second audio channel is subtracted from the first by a phase reversal (L+R minus L-R), the right channel is derived.

MTS real world performance
In ideal circumstances MTS Stereo is about 1.5 db better in performance than standard VHF FM stereo.
Usually with MTS, as with VHF FM stereo, a certain amount of crosstalk is encountered, limiting stereo separation.
The stereo information is dbx-encoded to increase the signal-to-noise ratio (at low levels), to aid in noise reduction.
Typical S/N (signal to noise) is better than -50 dB.
The original specifications called for a brick wall elliptical filter in each of the audio channels prior to encoding. The cutoff frequency of this filter was 15 kHz to prevent any horizontal sync crosstalk from being encoded. Manufacturers of modulators, however, used lower cutoff frequencies as they saw fit. Typically, they chose 14 kHz although some used filters as low as 12.5 kHz. The elliptical filter was chosen for having the greatest bandwidth with the lowest phase distortion at the cutoff frequency. The filter used during EIA testing had a characteristic that was -60dB at 15.5kHz. As transformer audio coupling was common at that time, the lower frequency limit was set to 50 Hz although modulators without transformer inputs were flat down to at least 20Hz.
Typical separation was better than 35 dB. However, level matching was essential to achieve this specification. Left and Right audio levels needed to be matched within less than 0.1 dB to achieve the separation stated.
Maintaining the phase stability of the horizontal sync is essential to good audio in the decode process. During transmission, the phase of the horizontal sync could vary with picture and chroma levels. ICPM (Incidental Carrier Phase Modulation), a measure of transmitted phase stability, needs to be less than 4.5% for best audio sub channel decoding. This was more of a problem with UHF transmitters of the day. Multi-cavity klystron amplifiers of that time typically had an ICPM above 7%.

MTS licensing
Because of the use of dbx companding, every TV device that decoded MTS originally required the payment of royalties, first to dbx, Inc., then to THAT Corporation which was spun off from dbx in 1989 and acquired its MTS patents in 1994; however, those patents expired worldwide in 2004. Though THAT now owns some patents related to digital implementations of MTS, a letter from THAT to the National Telecommunications and Information Administration in 2007 confirms that no license is required from THAT for all analog and some digital implementations of MTS.

How MTS audio channels are used
The second audio program (SAP) also is part of the standard, providing another language, a video description service like DVS, or a completely separate service like a campus radio station or weatheradio. This sub-carrier is at 5x horizontal sync and is also dBx encoded.
A third PRO (professional) channel is provided for internal use by the station, and may handle audio or data. The PRO channel is normally used with electronic news gathering during news broadcasts to talk to the remote location (such as a reporter on-location), which can then talk back through the remote link to the TV station. Specialized receivers for the PRO channel are generally only sold to broadcast professionals. This sub-carrier is at 6.5x horizontal sync.
MTS signals are indicated to the television receiver by adding a 15.734 kHz pilot tone to the signal. The MTS pilot is locked or derived from the horizontal sync signal used to lock the video display. Variations in phase or frequency of the horizontal sync are therefore transferred to the audio. UHF transmitters in use in 1984 generally had significant phase errors introduced in this signal making the transmission of stereo audio on UHF stations of that time nearly impossible. Later refinements in UHF transmitters minimized these effects and allowed stereo transmission for those stations.
Most FM broadcast receivers are capable of receiving the audio portion of NTSC Channel 6 at 87.75 MHz, but only in monaural. Because the pilot tone frequency at 15.734 kHz is different from that of the ordinary FM band (19 kHz), such radios cannot decode MTS.

MTS and the DTV transition in the United States
As a component of the NTSC standard, MTS is no longer being used in U.S. full-power television broadcasting after the June 12, 2009 DTV transition in the United States. It remains in use in LPTV and in analogue cable television. All coupon-eligible converter boxes (CECBs) are required to output stereo sound via RCA connectors, but MTS is merely optional for the RF modulator that every CECB contains. NTIA has stated that MTS was made optional for cost reasons; this may have been due to a belief that MTS still required royalty payments to THAT Corporation, which is no longer true except for some digital implementations.
THAT has created consumer pages on the DTV transition and how it affects MTS. The site describes the situation by stating that most consumers with CECBs will end up with monaural TV sound, since RF-only connections are common and MTS is optional (and rare) for CECBs.

See also
NICAM
Zweikanalton
Secondary Audio Program
EIAJ MTS

References

7. Alan Skierkiewicz,""Stereo one year later"", Television Broadcast, November 1985, p76",Category:Articles with unsourced statements from December 2016,3
121,122,Sound symbolism,"In linguistics, sound symbolism, phonesthesia or phonosemantics is the idea that vocal sounds or phonemes carry meaning in and of themselves.

Origin
In the 18th century, Mikhail Lomonosov propagated a theory that words containing certain sounds should bear certain meanings; for instance, the front vowel sounds E, I, YU should be used when depicting tender subjects and those with back vowel sounds O, U, Y when describing things that may cause fear (""like anger, envy, pain, and sorrow"").
However, it is Ferdinand de Saussure (1857–1913) who is considered to be the founder of modern 'scientific' linguistics. Central to what de Saussure says about words are two related statements: First, he says that ""the sign is arbitrary"". He considers the words that we use to indicate things and concepts could be any words — they are essentially just a consensus agreed upon by the speakers of a language and have no discernible pattern or relationship to the thing. Second, he says that, because words are arbitrary, they have meaning only in relation to other words. A dog is a dog because it is not a cat or a mouse or a horse, etc. These ideas have permeated the study of words since the 19th century.

Types
Margaret Magnus is the author of a comprehensive book designed to explain phonosemantics to the lay reader: Gods in the Word. This work describes three types of sound symbolism using a model first proposed by Wilhelm von Humboldt (see below):

Onomatopoeia
This is the least significant type of symbolism. It is simply imitative of sounds or suggests something that makes a sound. Some examples are crash, bang, whoosh.

Clustering
Words that share a sound sometimes have something in common. If we take, for example, words that have no prefix or suffix and group them according to meaning, some of them will fall into a number of categories. So we find that there is a group of words beginning with /b/ that are about barriers, bulges and bursting, and some other group of /b/ words that are about being banged, beaten, battered, bruised, blistered and bashed. This proportion is, according to Magnus, above the average for other letters.
Another hypothesis states that if a word begins with a particular phoneme, then there is likely to be a number of other words starting with that phoneme that refer to the same thing. An example given by Magnus is if the basic word for 'house' in a given language starts with a /h/, then by clustering, disproportionately many words containing /h/ can be expected to concern housing: hut, home, hovel, habitat...
Clustering is language dependent, although closely related languages will have similar clustering relationships.

Iconism
Iconism, according to Magnus, becomes apparent when comparing words which have the same sort of referent. One way is to look at a group of words that all refer to the same thing and that differ only in their sound, such as 'stamp', 'stomp', 'tamp', 'tromp', 'tramp', and 'step'. An /m/ before the /p/ in some words makes the action more forceful; compare 'stamp' with 'step' or 'tamp' with 'tap'. According to Magnus, the /r/ sets the word in motion, especially after a /t/ so a 'tamp' is in one place, but a 'tramp' goes for a walk. The /p/ in all those words would be what emphasizes the individual steps.
Magnus suggests that this kind of iconism is universal across languages.

Phenomimes and psychomimes
Some languages possess a category of words midway between onomatopoeia and usual words. Whereas onomatopoeia refers to the use of words to imitate actual sounds, there are languages known for having a special class of words that ""imitate"" soundless states or events, called phenomimes (when they describe external phenomena) and psychomimes (when they describe psychological states). On a scale that orders all words according to the correlation between their meaning and their sound, with the sound-imitating words like meow and whack at one end, and with the conventional words like water and blue at the other end, the phenomimes and the psychomimes would be somewhere in the middle. In the case of the Japanese language, for example, such words are learned in early childhood and are considerably more effective than usual words in conveying feelings and states of mind or in describing states, motions, and transformations. They are not found, however, only in children's vocabulary, but widely used in daily conversation among adults and even in more formal writing. Like Japanese, the Korean language also has a relatively high proportion of phenomimes and psychomimes.

History of phonosemantics
Several ancient traditions exist which talk about an archetypal relationship between sounds and ideas. Some of these are discussed below, but there are others as well. If we include a link between letters and ideas then the list includes the Viking Runes, the Hebrew Kabbalah, the Arab Abjad, etc.. References of this kind are very common in The Upanishads, The Nag Hammadi Library, the Celtic Book of Taliesin, as well as early Christian works, the Shinto Kototama, and Shingon Buddhism.

Old Chinese
Sinologist Axel Schuessler asserts that in Old Chinese, ""Occasionally, certain meanings are associated with certain sounds."" Concerning initials, he suggests that words with meanings such as ""dark, black, covered"" etc. tend begin with *m-, while those indicating ""soft, subtle, flexible"" begin with *n-. Taking a broader perspective, he also notes that ""Roots and stems meaning 'round, turn, return' have an initial *w- not only in Chinese, but generally in the languages of the area.""
As for finals in Old Chinese, Schuessler points out, ""Words that signify movement with an abrupt endpoint often end in *-k,"" and ""Words with the meaning 'shutting, closing' [...] tend to end in final *-p."" He also notes an overlap between the significations of initial *m- and final *-m: ""Words that imply 'keeping in a closed mouth' tend to end in a final *-m"".

Plato and the Cratylus Dialogue
In Cratylus, Plato has Socrates commenting on the origins and correctness of various names and words. When Hermogenes asks if he can provide another hypothesis on how signs come into being (his own is simply 'convention'), Socrates initially suggests that they fit their referents in virtue of the sounds they are made of:
""Now the letter rho, as I was saying, appeared to the imposer of names an excellent instrument for the expression of motion; and he frequently uses the letter for this purpose: for example, in the actual words rein and roe he represents motion by rho; also in the words tromos (trembling), trachus (rugged); and again, in words such as krouein (strike), thrauein (crush), ereikein (bruise), thruptein (break), kermatixein (crumble), rumbein (whirl): of all these sorts of movements he generally finds an expression in the letter R, because, as I imagine, he had observed that the tongue was most agitated and least at rest in the pronunciation of this letter, which he therefore used in order to express motion"" - Cratylus.
(note this is an open source translation available at Internet Classics Archive)
However, faced by an overwhelming number of counterexamples given by Hermogenes, Socrates has to admit that ""my first notions of original names are truly wild and ridiculous"".

Upanishads
The Upanishads contain a lot of material about sound symbolism, for instance:
""The mute consonants represent the earth, the sibilants the sky, the vowels heaven. The mute consonants represent fire, the sibilants air, the vowels the sun… The mute consonants represent the eye, the sibilants the ear, the vowels the mind"". Aitareya Aranyaka III.2.6.2.

Shingon Buddhism
K?kai, the founder of Shingon, wrote his Sound, word, reality in the 9th century which relates all sounds to the voice of the Dharmakaya Buddha.

Early Western phonosemantics
The idea of phonosemantics was sporadically discussed during the Middle Ages and the Renaissance. In 1690, Locke wrote against the idea in an essay called ""An Essay on Human Understanding"". His argument was that if there were any connection between sounds and ideas, then we would all be speaking the same language, but this is an over-generalisation. Leibniz's book New Essays on Human Understanding published in 1765 contains a point by point critique of Locke's essay. Leibniz picks up on the generalization used by Locke and adopts a less rigid approach: clearly there is no perfect correspondence between words and things, but neither is the relationship completely arbitrary, although he seems vague about what that relationship might be.

Modern phonosemantics
In 1836 Wilhelm von Humboldt published Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwicklung des Menschengeschlechts. It is here that he establishes the three kinds of relationship between sounds and ideas as discussed above under Types of Sound Symbolism. Below is a sample of researchers in the field of phonosemantics.
Otto Jespersen suggests that: ""Sound symbolism, we may say, makes some words more fit to survive."" Dwight Bolinger of Harvard University was the primary proponent of phonosemantics through the late 1940s and the 1950s. In 1949, he published The Sign is Not Arbitrary. He concluded that morphemes cannot be defined as the minimal meaning-bearing units, in part because linguistic meaning is so ill-defined, and in part because there are obvious situations in which smaller units are meaning-bearing.
Ivan Fónagy (1963) correlates phonemes with metaphors. For example, nasal and velarized vowels are quite generally considered ‘dark’, front vowels as ‘fine’ and ‘high’. Unvoiced stops have been considered ‘thin’ by European linguists, whereas the fricatives were labelled ‘raw’ and ‘hairy’ by the Greeks.
Hans Marchand provided the first extensive list of English phonesthemes. He wrote, for example, that ""/l/ at the end of a word symbolizes prolongation, continuation"" or ""nasals at the end of a word express continuous vibrating sounds.""
Gérard Genette published the only full length history of phonosemantics, Mimologics (1976). In 450 pages, Genette details the evolution of the linguistic iconism among linguists and poets, in syntax, morphology and phonology.
Linguist Keith McCune demonstrated in his doctoral thesis that virtually every word in the Indonesian language has an iconic (phonosemantic) component. His two-volume doctoral thesis ""The Internal Structure of Indonesian Roots"" was completed at the University of Michigan in 1983 and published in Jakarta in 1985.

Relationship with neuroscience
In the 2003 BBC Reith Lectures, Vilayanur S. Ramachandran outlined his research into the links between brain structure and function. In the fourth lecture of the series he describes the phenomena of synesthesia in which people experience, for example, sounds in terms of colors, or sounds in terms of tastes. In one type of synesthesia, people see numbers, letters of the alphabet, or even musical notes as having a distinct color. Ramachandran proposes a model for how language might have evolved. The theory may explain how humans create metaphors and how sounds can be metaphors for images – why for example sounds can be described as ""bright"" or ""dull"". In explaining how language might have evolved from cross activation of adjacent areas in the brain, Ramachandran notes four crucial factors, not all related to language, but which combined might have resulted in the emergence of language. Two of these four processes are of particular interest here.
Synesthetic cross modal abstraction: i.e. we recognize properties that sounds and images have in common and abstract them to store them independently. The sounds and shapes of the objects have characteristics in common that can be abstracted; for example, a ""sharp"", ""cutting"" quality of a word, and the shape it describes. Ramachandran calls this the 'Bouba/kiki effect', based on the results of an experiment with two abstract shapes, one blob-like and the other spiky, that asked people to relate the nonsense words bouba and kiki to them. The effect is real and observable, repeatable across linguistic groups, and evident even in the description of the experiment (with the bouba shape usually described using similar-sounding words like bulbous or blobby while the kiki shape is prickly or spiky).
Built in preexisting cross activation. Ramachandran points out that areas of the brain which appear to be involved in the mix-ups in synesthesia are adjacent to each other physically, and that cross-wiring, or cross activation, could explain synesthesia and our ability to make metaphors. He notes that the areas that control the muscles around the mouth are also adjacent to the visual centers, and suggests that certain words appear to make our mouth imitate the thing we are describing. Examples of this might be words like ""teeny weeny"", ""diminutive"" to describe small things; ""large"" or ""enormous"" to describe big things.
More recently, research on ideasthesia indicated that Kiki and Bouba have an entire semantic-like network of associated cross-modal experiences.

Relationship with poetry
The sound of words is important in the field of poetry, and rhetoric more generally. Tools such as euphony, alliteration, and rhyme all depend on the speaker or writer confidently choosing the best-sounding word.
John Michell's book Euphonics: A Poet's Dictionary of Enchantments collects lists of words of similar meaning and similar sounds. For example, the entry for ""gl-"" contains words for shiny things: glisten, gleam, glint, glare, glam, glimmer, glaze, glass, glitz, gloss, glory, glow, and glitter. Likewise, in German, nouns starting with ""kno-"" and ""knö-"" are mostly small and round: Knoblauch ""garlic"", Knöchel ""ankle"", Knödel ""dumpling"", Knolle ""tuber"", Knopf ""button"", Knorren ""knot (in a tree)"", Knospe ""bud (of a plant)"", Knoten ""knot (in string or rope)"".

Use in commerce
Phonesthesia is used in commerce for the names of products and even companies themselves. According to linguist Steven Pinker, one particularly ""egregious example"" was when cigarette maker Philip Morris rebranded to Altria. The name ""Altria"" is claimed to come from the Latin word for ""high"" but Pinker sees the change as an attempt to ""switch its image from bad people who sell addictive carcinogens to a place or state marked by altruism and other lofty values"". The brand names of many pharmaceuticals are common examples.

See also
Ideasthesia
Ideophone
Imitation of natural sounds in various cultures
Japanese sound symbolism
Onomatopoeia
Origin of speech
Phono-semantic matching
Phonestheme
Phonaesthetics
Sign (linguistics)

References
Sources
Jakobson, Roman; Waugh, Linda R. (2002). The Sound Shape of Language. Walter de Gruyter. ISBN 978-3-11-017285-0. 
Magnus, Margaret (2010). Gods in the Word: Archetypes in the Consonants. CreateSpace. ISBN 978-1-4538-2444-3. 
Mitchell, John (2006). Euphonics: A Poet's Dictionary of Enchantments. Wooden Books. ISBN 978-1-904263-43-2. 
Schuessler, Axel (2007). ABC Etymological Dictionary of Old Chinese. Honolulu: University of Hawaii Press. ISBN 978-0-8248-2975-9.

Further reading
Hinton, L., J. Nichols and J. J. Ohala (eds), 1994. Sound Symbolism. Cambridge: Cambridge University Press.

External links
Margaret Magnus's Phonosemantics Website
Meaningfulness of Sounds
phonosemantics.com Website
The Influence of Phonesthesia on the English Language - an alternative approach to phonosemantics.
Cratylus Dialogue
Reference and symbol in Plato's Cratylus and kuukai's Shojijissogi Philosophy East and West Vol. 32:4 (October 1982)
BBC Reith Lectures 2003
especially ""Lecture 4: Purple Numbers and Sharp Cheese""

An example of phonosemantics in advertising
Oral Metaphor Construct Asa M. Stepak
Induced generic sound symbolism for the blind - ""seeing with sound""
Conversational Grunts
Eating the Wind a satirical but illustrative example of sound symbolism and iconicity using airstream mechanisms.
""Phonological Clusters of Semantically Similar Words""
comparison of dirty words in Chinese and English
La Caverna Sonora (spanish) Miguel Ángel Mendo
The lost words of Life and Death",Category:Pages using citations with accessdate and no URL,3
122,123,Experimental music,"Experimental music is a general label for any music that pushes existing boundaries and genre definitions (Anon. & n.d.(c)). Experimental compositional practice is defined broadly by exploratory sensibilites radically opposed to, and questioning of, institutionalized compositional, performing, and aesthetic conventions in music (Sun 2013). Elements of experimental music include indeterminate music, in which the composer introduces the elements of chance or unpredictability with regard to either the composition or its performance. Artists may also approach a hybrid of disparate styles or incorporate unorthodox and unique elements (Anon. & n.d.(c)).
The practice became prominent in the mid-20th century, particularly in Europe and North America. John Cage was one of the earliest composers to use the term and one of experimental music's primary innovators, utilizing indeterminacy techniques and seeking unknown outcomes. In France, as early as 1953, Pierre Schaeffer had begun using the term ""musique expérimentale"" to describe compositional activities that incorporated tape music, musique concrète, and elektronische Musik. Also, in America, a quite distinct sense of the term was used in the late 1950s to describe computer-controlled composition associated with composers such as Lejaren Hiller. Harry Partch as well as Ivor Darreg worked with other tuning scales based on the physical laws for harmonic music. For this music they both developed a group of experimental musical instruments. Musique concrète (French; literally, ""concrete music""), is a form of electroacoustic music that utilises acousmatic sound as a compositional resource. Free improvisation or free music is improvised music without any rules beyond the taste or inclination of the musician(s) involved; in many cases the musicians make an active effort to avoid ""clichés"", i.e., overt references to recognizable musical conventions or genres.

Definitions and usage
Origins
The Groupe de Recherches de Musique Concrète (GRMC), under the leadership of Pierre Schaeffer, organized the First International Decade of Experimental Music between 8 and 18 June 1953. This appears to have been an attempt by Schaeffer to reverse the assimilation of musique concrète into the German elektronische Musik, and instead tried to subsume musique concrète, elektronische Musik, tape music, and world music under the rubric ""musique experimentale"" (Palombini 1993, 18). Publication of Schaeffer's manifesto (Schaeffer 1957) was delayed by four years, by which time Schaeffer was favoring the term ""recherche musicale"" (music research), though he never wholly abandoned ""musique expérimentale"" (Palombini 1993a, 19; Palombini 1993b, 557).
John Cage was also using the term as early as 1955. According to Cage's definition, ""an experimental action is one the outcome of which is not foreseen"" (Cage 1961, 39), and he was specifically interested in completed works that performed an unpredictable action (Mauceri 1997, 197). In Germany, the publication of Cage's article was anticipated by several months in a lecture delivered by Wolfgang Edward Rebner at the Darmstädter Ferienkurse on 13 August 1954, titled “Amerikanische Experimentalmusik"". Rebner's lecture extended the concept back in time to include Charles Ives, Edgard Varèse, and Henry Cowell, as well as Cage, due to their focus on sound as such rather than compositional method (Rebner 1997).

Alternate classifications
Composer and critic Michael Nyman starts from Cage's definition (Nyman 1974, 1), and develops the term ""experimental"" also to describe the work of other American composers (Christian Wolff, Earle Brown, Meredith Monk, Malcolm Goldstein, Morton Feldman, Terry Riley, La Monte Young, Philip Glass, John Cale, Steve Reich, etc.), as well as composers such as Gavin Bryars, Toshi Ichiyanagi, Cornelius Cardew, John Tilbury, Frederic Rzewski, and Keith Rowe (Nyman 1974, 78–81, 93–115). Nyman opposes experimental music to the European avant-garde of the time (Boulez, Kagel, Xenakis, Birtwistle, Berio, Stockhausen, and Bussotti), for whom ""The identity of a composition is of paramount importance"" (Nyman 1974, 2 and 9). The word ""experimental"" in the former cases ""is apt, providing it is understood not as descriptive of an act to be later judged in terms of success or failure, but simply as of an act the outcome of which is unknown"" (Cage 1961).
David Cope also distinguishes between experimental and avant garde, describing experimental music as that ""which represents a refusal to accept the status quo"" (Cope 1997, 222). David Nicholls, too, makes this distinction, saying that ""...very generally, avant-garde music can be viewed as occupying an extreme position within the tradition, while experimental music lies outside it"" (Nicholls 1998, 318).
Warren Burt cautions that, as ""a combination of leading-edge techniques and a certain exploratory attitude"", experimental music requires a broad and inclusive definition, ""a series of ands, if you will"", encompassing such areas as ""Cageian influences and work with low technology and improvisation and sound poetry and linguistics and new instrument building and multimedia and music theatre and work with high technology and community music, among others, when these activities are done with the aim of finding those musics 'we don't like, yet,' [citing Herbert Brün] in a 'problem-seeking environment' [citing Chris Mann]” (Burt 1991, 5).
Benjamin Piekut argues that this ""consensus view of experimentalism"" is based on an a priori ""grouping"", rather than asking the question ""How have these composers been collected together in the first place, that they can now be the subject of a description?"" That is, ""for the most part, experimental music studies describes  [sic] a category without really explaining it"" (Piekut 2008, 2–5). He finds laudable exceptions in the work of David Nicholls and, especially, Amy Beal (Piekut 2008, 5), and concludes from their work that ""The fundamental ontological shift that marks experimentalism as an achievement is that from representationalism to performativity"", so that ""an explanation of experimentalism that already assumes the category it purports to explain is an exercise in metaphysics, not ontology"" (Piekut 2008, 7).
Leonard B. Meyer, on the other hand, includes under ""experimental music"" composers rejected by Nyman, such as Berio, Boulez and Stockhausen, as well as the techniques of ""total serialism"" (Meyer 1994, 106–107 and 266), holding that ""there is no single, or even pre-eminent, experimental music, but rather a plethora of different methods and kinds"" (Meyer 1994, 237).

Abortive critical term
In the 1950s, the term ""experimental"" was often applied by conservative music critics—along with a number of other words, such as ""engineers art"", ""musical splitting of the atom"", ""alchemist's kitchen"", ""atonal"", and ""serial""—as a deprecating jargon term, which must be regarded as ""abortive concepts"", since they did not ""grasp a subject"" (Metzger 1959, 21). This was an attempt to marginalize, and thereby dismiss various kinds of music that did not conform to established conventions (Mauceri 1997, 189). In 1955, Pierre Boulez identified it as a ""new definition that makes it possible to restrict to a laboratory, which is tolerated but subject to inspection, all attempts to corrupt musical morals. Once they have set limits to the danger, the good ostriches go to sleep again and wake only to stamp their feet with rage when they are obliged to accept the bitter fact of the periodical ravages caused by experiment."" He concludes, ""There is no such thing as experimental music … but there is a very real distinction between sterility and invention"" (Boulez 1986, 430 and 431). Starting in the 1960s, ""experimental music"" began to be used in America for almost the opposite purpose, in an attempt to establish an historical category to help legitimize a loosely identified group of radically innovative, ""outsider"" composers. Whatever success this might have had in academe, this attempt to construct a genre was as abortive as the meaningless namecalling noted by Metzger, since by the ""genre's"" own definition the work it includes is ""radically different and highly individualistic"" (Mauceri 1997, 190). It is therefore not a genre, but an open category, ""because any attempt to classify a phenomenon as unclassifiable and (often) elusive as experimental music must be partial"" (Nyman 1974, 5). Furthermore, the characteristic indeterminacy in performance ""guarantees that two versions of the same piece will have virtually no perceptible musical 'facts' in common"" (Nyman 1974, 9).

Computer composition
In the late 1950s, Lejaren Hiller and L. M. Isaacson used the term in connection with computer-controlled composition, in the scientific sense of ""experiment"" (Hiller and Isaacson 1959): making predictions for new compositions based on established musical technique (Mauceri 1997, 194–95). The term ""experimental music"" was used contemporaneously for electronic music, particularly in the early musique concrète work of Schaeffer and Henry in France (Vignal 2003, 298). There is a considerable overlap between Downtown music and what is more generally called experimental music, especially as that term was defined at length by Nyman in his book Experimental Music: Cage and Beyond (1974, second edition 1999).

History
Influential antecedents
A number of early 20th-century American composers, seen as precedents to and influences on John Cage, are sometimes referred to as the ""American Experimental School"". These include Charles Ives, Charles and Ruth Crawford Seeger, Henry Cowell, Carl Ruggles, and John Becker (Nicholls 1990; Rebner 1997).

New York School
Artists: John Cage, Earle Brown, Christian Wolff, Morton Feldman, David Tudor, Related: Merce Cunningham

Musique concrète
Musique concrète (French; literally, ""concrete music""), is a form of electroacoustic music that utilises acousmatic sound as a compositional resource. The compositional material is not restricted to the inclusion of sonorities derived from musical instruments or voices, nor to elements traditionally thought of as ""musical"" (melody, harmony, rhythm, metre and so on). The theoretical underpinnings of the aesthetic were developed by Pierre Schaeffer, beginning in the late 1940s.

Fluxus
Fluxus was an artistic movement started in the 1960s, characterized by an increased theatricality and the use of mixed media. Another known musical aspect appearing in the Fluxus movement was the use of Primal Scream at performances, derived from the primal therapy. Yoko Ono used this technique of expression (Bateman n.d.).

Minimalism
Transethnicism
The term ""experimental"" has sometimes been applied to the mixture of recognizable music genres, especially those identified with specific ethnic groups, as found for example in the music of Laurie Anderson, Chou Wen-chung, Steve Reich, Kevin Volans, Martin Scherzinger, Michael Blake, and Rüdiger Meyer (Blake 1999; Jaffe 1983; Lubet 1999).

Free improvisation
Free improvisation or free music is improvised music without any rules beyond the taste or inclination of the musician(s) involved; in many cases the musicians make an active effort to avoid overt references to recognizable musical genres. The term is somewhat paradoxical, since it can be considered both as a technique (employed by any musician who wishes to disregard rigid genres and forms) and as a recognizable genre in its own right.

Influence
The Residents started in the seventies as an idiosyncratic musical group mixing all kinds of artistic genres like pop music, electronic music, experimental music with movies, comic books and performance art (Ankeny n.d.). Rhys Chatham and Glenn Branca composed multi guitar compositions in the late 1970s. Chatham worked for some time with LaMonte Young and afterwards mixed the experimental musical ideas with punk rock in his piece Guitar Trio. Lydia Lunch started incorporating spoken word with punk rock and Mars explored new sliding guitar techniques. Arto Lindsay neglected to use any kind of musical practise or theory to develop an idiosyncratic atonal playing technique. DNA and James Chance are other famous no wave artists. Chance later on moved more up to Free improvisation. The No Wave movement was closely related to transgressive art and, just like Fluxus, often mixed performance art with music. It is alternatively seen, however, as an avant-garde offshoot of 1970s punk, and a genre related to experimental rock (Anon. & n.d.(b)).

References
Ankeny, Jason. n.d. ""The Residents: Biography"". Allmusic.com.
Anon. n.d.(a). ""Explore Music ... Explore by ... /Avant-Garde//Experimental: Genre"". Allmusic.com.
Anon. n.d.(b). ""Explore Music ... Explore by ... /Pop/Rock/Punk/New Wave/No Wave"". Allmusic.com.
Anon. n.d.(c). ""Avant-Garde » Modern Composition » Experimental"". AllMusic
Bateman, Shahbeila. n.d. ""Biography of Yoko Ono"". Website of Hugh McCarney, Communication Department, Western Connecticut University. (Accessed 15 February 2009)
Beal, Amy C. 2006. New Music, New Allies: American Experimental Music in West Germany from the Zero Hour to Reunification. Berkeley: University of California Press. ISBN 0-520-24755-8.
Blake, Michael. 1999. ""The Emergence of a South African Experimental Aesthetic"". In Proceedings of the 25th Annual Congress of the Musicological Society of Southern Africa, edited by Izak J. Grové. Pretoria: Musicological Society of Southern Africa.
Boulez, Pierre. 1986. ""Experiment, Ostriches, and Music"", in his Orientations: Collected Writings, translated by Martin Cooper, 430–31. Cambridge: Harvard University Press. ISBN 0-674-64375-5 Originally published as ""Expérience, autriches et musique"". Nouvelle Revue Française, no. 36 (December 1955): 1, 174–76.
Burt, Warren. 1991. ""Australian Experimental Music 1963–1990"". Leonardo Music Journal 1, no. 1:5–10.
Cage, John. 1961. Silence: Lectures and Writings. Middletown, Connecticut: Wesleyan University Press. Unaltered reprints: Weslyan University press, 1966 (pbk), 1967 (cloth), 1973 (pbk [""First Wesleyan paperback edition""], 1975 (unknown binding); Cambridge, Mass: MIT Press, 1966, 1967, 1969, 1970, 1971; London: Calder & Boyars, 1968, 1971, 1973 ISBN 0-7145-0526-9 (cloth) ISBN 0-7145-1043-2 (pbk). London: Marion Boyars, 1986, 1999 ISBN 0-7145-1043-2 (pbk); [n.p.]: Reprint Services Corporation, 1988 (cloth) ISBN 99911-780-1-5 [In particular the essays ""Experimental Music"", pp. 7–12, and ""Experimental Music: Doctrine"", pp. 13–17.]
Cope, David. 1997. Techniques of the Contemporary Composer. New York, New York: Schirmer Books. ISBN 0-02-864737-8.
Cox, Christoph. 2004. Audio Culture. Continuum International Publishing Group..
Crumsho, Michael. 2008. ""Dusted Reviews: Neptune—Gong Lake"". Dusted Magazine (February 19).
Hasse, Gretchen. 2007. ""Heavy Metal Is Just That for the Heavy-Lifting Jason Sanford of Neptune"". Gearwire.com (5 December).
Hiller, Lejaren, and L. M. Isaacson. 1959. Experimental Music: Composition with an Electronic Computer. New York: McGraw-Hill Book Company.
Jaffe, Lee David. 1983. ""The Last Days of the Avant Garde; or How to Tell Your Glass from Your Eno"". Drexel Library Quarterly 19, no. 1 (Winter): 105–22.
Lubet, Alex. 1999. ""Indeterminate Origins: A Cultural theory of American Experimental Music"". In Perspectives on American music since 1950, edited by , James R. Heintze. New York, NY: General Music Publishing Co. ISBN 0-8153-2144-9* Masters, Marc. 2007. No Wave. London: Black Dog Publishing. ISBN 978-1-906155-02-5.
Mauceri, Frank X. 1997. ""From Experimental Music to Musical Experiment"". Perspectives of New Music 35, no. 1 (Winter): 187-204.
Metzger, Heinz-Klaus. 1959. ""Abortive Concepts in the Theory and Criticism of Music"", translated by Leo Black. Die Reihe 5: ""Reports, Analysis"" (English edition): 21–29).
Meyer, Leonard B. 1994. Music, the Arts, and Ideas: Patterns and Predictions in Twentieth-Century Culture. Second edition. Chicago: University of Chicago Press. ISBN 0-226-52143-5.
Nicholls, David. 1990. American Experimental Music, 1890–1940. New York, NY: Cambridge University Press. ISBN 0-521-34578-2.
Nicholls, David. 1998. ""Avant-garde and Experimental Music"". In Cambridge History of American Music. Cambridge and New York: Cambridge University Press. ISBN 0-521-45429-8.
Nyman, Michael. 1974. Experimental Music: Cage and Beyond. London: Studio Vista ISBN 0-289-70182-1. New York: Schirmer Books. ISBN 0-02-871200-5. Second edition, Cambridge and New York: Cambridge University Press, 1999. ISBN 0-521-65297-9.
Palombini, Carlos. 1993a. ""Machine Songs V: Pierre Schaeffer: From Research into Noises to Experimental Music"". Computer Music Journal, 17, No. 3 (Autumn): 14–19.
Palombini, Carlos. 1993b. ""Pierre Schaeffer, 1953: Towards an Experimental Music"". Music and Letters 74, no. 4 (November): 542–57.
Parkin, Chris. 2008. ""Micachu: Interview"". Time Out London (February 26).
Piekut, Benjamin. 2008. ""Testing, Testing …: New York Experimentalism 1964"". Ph.D. diss. New York: Columbia University.
Rebner, Wolfgang Edward. 1997. ""Amerikanische Experimentalmusik"". In Im Zenit der Moderne: Geschichte und Dokumentation in vier Bänden—Die Internationalen Ferienkurse für Neue Musik Darmstadt, 1946-1966. Rombach Wissenschaften: Reihe Musicae 2, 4 vols., edited by Gianmario Borio and Hermann Danuser, 3:178–89. Freiburg im Breisgau: Rombach.
Schaeffer, Pierre. 1957. ""Vers une musique experimentale"". La revue musicale no. 236 (Vers une musique experimentale), edited by Pierre Schaeffer, 18–23. Paris: Richard-Masse.
Sun, Cecilia. 2013. ""Experimental Music"". 2013. The Grove Dictionary of American Music, second edition, edited by Charles Hiroshi Garrett. New York: Oxford University Press.
Vignal, Marc (ed.). 2003. ""Expérimentale (musique)"". In Dictionnaire de la Musique. Paris: Larousse. ISBN 2-03-511354-7.

Further reading
Ballantine, Christopher. 1977. ""Towards an Aesthetic of Experimental Music"". The Musical Quarterly 63, no. 2 (April): 224–46.
Benitez, Joaquim M. 1978. ""Avant-Garde or Experimental? Classifying Contemporary Music"". International Review of the Aesthetics and Sociology of Music 9, no. 1 (June): 53–77.
Broyles, Michael. 2004. Mavericks and Other Traditions in American Music. New Haven: Yale University Press.
Cameron, Catherine. 1996. Dialectics in the Arts: The Rise of Experimentalism in American Music. Westport, Conn.: Praeger.
Ensemble Modern. 1995. ""Was ist experimentelles Musiktheater? Mitglieder des 'Ensemble Modern' befragen Hans Zender"". Positionen: Beiträge zur Neuen Musik 22 (February): 17–20.
Bailey, Derek. 1980. ""Musical Improvisation: Its Nature and Practice in Music"". Englewood Cliffs, N.J.: Prentice-Hall; Ashbourne: Moorland. ISBN 0-13-607044-2. Second edition, London: British Library National Sound Archive, 1992. ISBN 0-7123-0506-8
Experimental Musical Instruments. 1985–1999. A periodical (no longer published) devoted to experimental music and instruments.
Gligo, Nikša. 1989. ""Die musikalische Avantgarde als ahistorische Utopie: Die gescheiterten Implikationen der experimentellen Musik"". Acta Musicologica 61, no. 2 (May–August): 217–37.
Grant, Morag Josephine. 2003. ""Experimental Music Semiotics"". International Review of the Aesthetics and Sociology of Music 34, no. 2 (December): 173–91.
Henius, Carla. 1977. ""Musikalisches Experimentiertheater. Kommentare aus der Praxis"". Melos/Neue Zeitschrift für Musik 3, no. 6:489–92.
Henius, Carla. 1994. ""Experimentelles Musiktheater seit 1946"". Bayerische Akademie der Schönen Künste: Jahrbuch 8:131-54.
Holmes, Thomas B. 2008. Electronic and Experimental Music: Pioneers in Technology and Composition. Third edition. London and New York: Routledge. ISBN 978-0-415-95781-6 (hbk.) ISBN 978-0-415-95782-3 (pbk.)
Lucier, Alvin. 2002. ""An einem hellen Tag: Avantgarde und Experiment"", trans. Gisela Gronemeyer. MusikTexte: Zeitschrift für Neue Musik, no. 92 (February), pp. 13–14.
Lucier, Alvin. 2012. Music 109: Notes on Experimental Music. Middletown, Connecticut: Wesleyan University Press. ISBN 9780819572974 (cloth); ISBN 9780819572981 (ebook).
Piekut, Benjamin. 2011. Experimentalism Otherwise: The New York Avant-Garde and its Limits. Berkeley: University of California Press. ISBN 978-0-520-26851-7.
Saunders, James. 2009. The Ashgate Research Companion to Experimental Music. Aldershot, Hants, and Burlington, VT: Ashgate. ISBN 978-0-7546-6282-2
Schnebel, Dieter. 2001. ""Experimentelles Musiktheater"". In Das Musiktheater: Exempel der Kunst, edited by Otto Kolleritsch, 14–24. Vienna: Universal Edition. ISBN 3-7024-0263-2
Shultis, Christopher. 1998. Silencing the Sounded Self: John Cage and the American Experimental Tradition. Boston: Northeastern University Press. ISBN 1-55553-377-9
Smith Brindle, Reginald. 1987. The New Music: The Avant-Garde Since 1945, second edition. Oxford and New York: Oxford University Press. ISBN 0-19-315471-4 (cloth) ISBN 0-19-315468-4 (pbk.)
Sutherland, Roger, 1994. New Perspectives in Music. London: Sun Tavern Fields. ISBN 0-9517012-6-6

External links
Experimental music at Curlie (based on DMOZ)",Category:Articles with dead external links from August 2014,3
123,124,Sonotrode,"In ultrasonic machining, welding and mixing, a sonotrode is a tool that creates ultrasonic vibrations and applies this vibrational energy to a gas, liquid, solid or tissue.
A sonotrode usually consists of a stack of piezoelectric transducers attached to a tapering metal rod. The end of the rod is applied to the working material. An alternating current oscillating at ultrasonic frequency is applied by a separate power supply unit to the piezoelectric transducers. The current causes them to expand and contract. The frequency of the current is chosen to be the resonant frequency of the tool, so the entire sonotrode acts as a half-wavelength resonator, vibrating lengthwise with standing waves at its resonant frequency. The standard frequencies used with ultrasonic sonotrodes range from 20 kHz to 70 kHz. The amplitude of the vibration is small, about 13 to 130 micrometres.
Sonotrodes are made of titanium, aluminium or steel, with or without heat treatment (carbide). The shape of the sonotrode (round, square, with teeth, profiled ...), depends on the quantity of vibratory energy and a physical constraint for a specific application. Its shape must be optimized for the particular application.
Sonotrodes of small diameter are sometimes called probes.
For an ultrasonic welding or cutting application, the sonotrode gives energy directly to the welding contact area, with little diffraction. This is particularly helpful when vibrations (wave propagation) could damage surrounding electronic components.

References
See also
Horn analyzer
Ultrasonic horn",Category:Sound,3
124,125,Category:Audible medical signs,,Category:Sound,3
125,126,Headroom (audio signal processing),"In digital and analog audio, headroom refers to the amount by which the signal-handling capabilities of an audio system exceed a designated nominal level. Headroom can be thought of as a safety zone allowing transient audio peaks to exceed the nominal level without damaging the system or the audio signal, e.g., via clipping. Standards bodies differ in their recommendations for nominal level and headroom.

Digital audio
In digital audio, headroom is defined as the amount by which digital full scale (FS) exceeds the nominal level in decibels (dB). The European Broadcasting Union (EBU) specifies several nominal levels and resulting headroom for different applications.

Analog audio
In analog audio, headroom can mean low-level signal capabilities as well as the amount of extra power reserve available within the amplifiers that drive the loudspeakers.

Alignment level
Alignment level is an anchor point 9 dB below the nominal level, a reference level that exists throughout the system or broadcast chain, though it may imply different voltage levels at different points in the analog chain. Typically, nominal (not alignment) level is 0 dB, corresponding to an analog sine wave of voltage of 1.23 volts RMS (+4 dBu or 3.47 volts peak to peak). In the digital realm, alignment level is ?18 dBFS.
AL = analog level
SPL = sound pressure level

See also
Audio quality measurement
Noise measurement
Programme levels
Rumble measurement
ITU-R 468 noise weighting
A-weighting
Weighting filter
Equal-loudness contour
Fletcher-Munson curves
Loudness war

References
Further reading
BS.1726 ""Signal level of digital audio accompanying television in international programme exchange"" (2005)
BS.1864 ""Operational practices for loudness in the international exchange of digital television programmes"" (2010)
BS.1770-3 ""Algorithms to measure audio programme loudness and true-peak audio level"" (2012)

External links
EBU Recommendation R68-2000
AES Preprint 4828 - Levels in Digital Audio Broadcasting by Neil Gilchrist (not free)
EBU Recommendation R117-2006 (against loudness war)
AES Convention Paper 5538 On Levelling and Loudness Problems at Broadcast Studios
EBU Tech 3282-E on EBU RDAT Tape Levels
AES17-1998 (r2004): AES standard method for digital audio engineering -- Measurement of digital audio equipment",Category:Audio engineering,3
126,127,Humming,"–

A hum is a sound made by producing a wordless tone with the mouth opened or closed, forcing the sound to emerge from the nose. To hum is to produce such a sound, often with a melody.
A hum has a particular timbre (or sound quality), usually a monotone or with slightly varying tones. There are other similar sounds not produced by human singing that are also called hums, such as a sound produced by machinery in operation or by an insect in flight. The hummingbird was named for the sound that bird makes in flight which sounds like a hum.

Mechanics
A 'hum' or 'humming' by humans is created by the resonance of air in various parts of passages in the head and throat, in the act of breathing. The 'hum' that a hummingbird creates is also created by resonance: in this case by the passage of air against wings in the actions of flying, especially of hovering.

Humming in human evolution
Joseph Jordania suggested that humming could have played an important role in the early human (hominid) evolution as contact calls. Many social animals produce seemingly haphazard and indistinct sounds (like chicken cluck) when they are going about their everyday business (foraging, feeding). These sounds have two functions: (1) to let group members know that they are among kin and there is no danger, and (2) in case of the appearance of any signs of danger (suspicious sounds, movements in a forest), the animal that notices danger first, stops moving, stops producing sounds, remains silent and looks in the direction of the danger sign. Other animals quickly follow suit and very soon all the group is silent and is scanning the environment for the possible danger. Charles Darwin was the first to notice this phenomenon on the example of the wild horses and the cattle. Joseph Jordania suggested that for humans, as for many social animals, silence can be a sign of danger, and that's why gentle humming and musical sounds relax humans (see the use of gentle music in music therapy, lullabies).

Music
Humming is often used in music of genres, from classical to jazz to R&B.
Another form of music derived from basic humming is the humwhistle. The folk art, also known as ""whistle-hum,"" produces a high pitch and low pitch simultaneously. The two-tone sound is related to field holler, overtone singing, and yodeling.

See also
Kazoo
The Hum – an apparently widespread phenomenon involving a low-frequency hum of unknown origin, inaudible to most people
Mains hum – an electric or electromagnetic phenomenon that causes a low frequency (50 or 60 Hz) audible signal
In 17th century England, ""humming"" was a form of public approval in social and political or judicial proceedings.

References
External links
 The dictionary definition of hum at Wiktionary",Category:Singing techniques,3
127,128,International Sound Communication,"International Sound Communication (frequently abbreviated as I.S.C.) was a series of compilation cassettes, compiled and distributed as a mail art project by Andi Xport from Peterborough, England, in the mid-1980s. Fifteen volumes were issued, and it was one of the largest and most versatile series of its kind.
The series was intended to provide an outlet for any kind of music from any country in the world. Most artists who appeared were not signed to a record label, but had released their music privately on cassettes sold via mail, and these were often the source of the material that appeared on the compilations. All volumes came with a list of contact addresses, with the exception of artists from ""Iron Curtain"" or Soviet Union countries whose addresses were not published, to protect them from government persecution, as the ""importation"" of Western culture and influences, and communication with artists outside the Soviet Union, without government approval were generally illegal. In those cases, a contact address for an associate outside of the Soviet Union was provided. As stated on the inserts, ""All bands and individuals featured on I.S.C. comps have more music available, so get in touch now!"" A slogan, ""Communicate to Create"" often appeared.
In addition to compiling the series, Andi Xport recorded under the name Man's Hate, which was also the name of his cassette label which distributed I.S.C. (plus 5 cassette albums by Man's Hate). Xport was also a member of APF Brigade, and The Peace & Freedom Band. Xport claims that more than 3,000 tapes were sent to him, and many had to be stored under his bed due to space limitations.
Released on the same date as volume 13, an extra volume appeared as The Noise Collective which, although presented as an artist name, was actually another compilation project. All tracks were collaborations between two or more artists, most of whom had appeared previously on I.S.C. The collaborations were accomplished by having the artists send each other unfinished recordings through the mail. The tracks were edited to overlap and segue, forming a continuous suite on each side of the tape.
Volumes 1 to 8 were C-60 (60 minute) tapes, with a cover price of ?1.00. The remaining volumes, including The Noise Collective, were C-90 with a cover price of ?1.50. Volumes 1 to 9 used a fold-out insert (shown at the right), while volume 10 had a cardboard insert separate from the track list and contacts sheet. Starting with volume 11 (and including The Noise Collective), the outer cover was a cardboard sleeve wrapped around the plastic jewel case.

Track listings
I.S.C. No. 1 (on front)International Sound Communication (on spine)
not dated, issued 1984 or 1985
60 minutes
Comment on cover: ""Music from Belgium, England, Japan, Norway, Scotland, U.S.A. & Wales. All the hits of the 80s on one great cassette!""
I.S.C. 2 (on front)International Sound Communication 2 (on spine)
16 March 1985
60 minutes
Comment on cover: ""Ape shit maaan!""
I.S.C. 3 (on front)International Sound Communiqué 3 (on spine)
22 April 1985
60 minutes
I.S.C. 4
Track list not available
List of artists (from advertisement): The Klinik; Unknownmix; Bloody Hypocrites; F/i; The Dreg; Unovidual; Electro Hippies; D.V.A. Minuta Mrznje; Les Bouseaux Pychedeliques; The Affairs; If, Bwana; Anathema; Synthetic Productions; Miasma; Pseudo Code; The Submensa's; M.A.L.; Narzisse
List of countries: Belgium; England; France; Northern Ireland; Switzerland; USA; West Germany; Yugoslavia
International Sound Communication 5 (on front)I.S.C. 5 (on spine)
27 July 1985
60 minutes
I.S.C. 6
10 October 1985
60 minutes
I.S.C. 7
Track list not available
List of artists (from advertisement): Alan Cornelius; Post War; Mystery Hearsay; Last Rites; Headcleaners; Famlende Forsok; Paranoid Visions; N.B.N.; Opera Multisteel; The Apostles; Face in the Crowd; Fever Heroes; Celestial Orgy; In ' 8; Compulsion Brothers; C'llaaps
List of countries: England; France; Ireland; Japan; Norway; Scotland; USA; Wales
International Sound Communication Compilation Volume Number Eight
1 January 1986
60 minutes
International Sound Communication Volume Number Nine
22 February 1986
90 minutes
Comment on cover: ""It's like an electric chair in your living room!""
I.S.C. 10
6 April 1986
90 minutes
Comment on cover: ""Stop pushing, there's enough copy's for everyone""
International Sound Communication Eleven
26 July 1986
90 minutes
Comment on cover: ""What are YOU doing to protect yourself from: (Misinformation... Half-truths... Mind Rot...) MEDIA BURN, the Nation's leading mental crippler - Nothing? Then Turn On, Tune In, And Drop Out With I.S.C.""
International Sound Communication Twelve
18 October 1986
90 minutes
I S C Thirteen
21 March 1987
90 minutes
Comment on cover: ""Unlucky for some""
The Noise CollectiveHello! Hello! Can Anyone Hear Me?
21 March 1987
90 minutes
Each track is a collaboration between two or more artists.
International Sound Communication Compilation Volume Number Fourteen
not dated, issued 1987
90 minutes
Comment on cover: ""Load up with I.S.C., get a copy now!""
I.S.C. 15
Track list not available

See also
Cassette culture


== References ==",Category:1980s compilation albums,3
128,129,Category:Supersonic transports,,Category:Emerging technologies,3
129,130,Category:Fictional characters who can manipulate sound,,Category:Fictional characters by physics or reality manipulation,3
130,131,Silence,"Silence is the lack of audible sound or presence of sounds of very low intensity. By analogy, the word silence can also refer to any absence of communication or hearing, including in media other than speech and music. Silence is also used as total communication, in reference to nonverbal communication and spiritual connection. Silence also refers to no sounds uttered by anybody in a room or area. Silence is an important factor in many cultural spectacles, as in rituals.
In discourse analysis, speakers use brief absences of speech to mark the boundaries of prosodic units. Silence in speech can be due to hesitation, stutters, self-correction—or a deliberate slowing of speech to clarify or aid the processing of ideas. These are short silences. Longer pauses in language occur in interactive roles, reactive tokens, or turn-taking.
According to cultural norms, silence can be positive or negative. For example, in a Christian Methodist faith organization, silence and reflection during the sermons might be appreciated by the congregation, while in a Southern Baptist church, silence might mean disagreement with what is being said, or perhaps disconnectedness from the congregated community.

In art, entertainment, and media
Film
The independent documentary film, In Pursuit of Silence (2016), portrays the spiritual and physical benefits of silence, and the price paid individually and collectively for a noisy world. It is narrated by authors Dr. Helen Lees (Silence in Schools), Pico Iyer (The Art of Stillness), Susan Cain (Quiet), Maggie Ross (Silence: A User’s Guide), and George Prochnik (In Pursuit of Silence).

Music
Music inherently depends on silence, in some form or another, to distinguish other periods of sound and allow dynamics, melodies, and rhythms to have greater impact. For example, most music scores feature rests denoting periods of silence. In addition, silence in music can be seen as a time for contemplation to reflect on the piece. The audience feels the effects of the previous notes and can reflect on that moment intentionally. Silence does not hinder musical excellence but can enhance the sounds of instruments and vocals within the piece.
In his book Sound and Silence (1970), the composer John Paynter says, ""the dramatic effect of silence has long been appreciated by composers."" He gives as an example, ""the general pause in the middle of the chorus ‘Have lightnings and thunders …’ in Bach’s St. Matthew Passion"":

After the pause, the music continues to the words: ""Open up the fiery bottomless pit, O hell!"" The silence is intended to communicate a momentary sensation of terror, of staring into unfathomable darkness. Another example of a dramatic silence comes in the ""rest full of tension"" at the climactic ending of the Hallelujah Chorus in Handel’s Messiah:

Musical silences may also convey humour. Haydn’s Quartet in E flat, Op. 33 was nicknamed ""The Joke"", because of the comic timing of the pauses at the end of the last movement :

Taruskin (2010, p. 552) says, ""whenever this ending is performed, it takes the audience an extra second or so to recover its wits and realize that the piece is indeed over. The result is an inevitable giggle—the same giggle that overtakes a prestidigitator’s audience when it realizes that it has been ‘had’."" Barry Cooper (2011, p.38)  writes extensively of Beethoven’s many uses of silence for contemplation, for dramatic effect and especially for driving the rhythmic impetus of the music. He cites the start of the second movement of the Ninth Symphony, where the silences contribute to a powerful sense of propulsion :

""The rhythm of bar 1 is incomplete and demands a note at the beginning of bar 2. The substitution of such a note by a whole-bar rest therefore gives the effect of a suppressed sound, as if one were about to speak but then refrains at the last moment. The 'suppressed sound' is then repeated in bar 4, and 'developed' (by being doubled) in bars 7 and 8."" Grove (1898, p. 355) writes of the ""strange irregularity of rhythm in the sixth bar"" of this movement. Much has been said about the harmony of the opening to Richard Wagner’s opera Tristan und Isolde, which Taruskin (2010, p.540) calls ""perhaps the most famous, surely the most commented-on, single phrase of music ever written."" However, Wagner’s strategic use of silences between phrases intensifies the troubled ambiguity of the music: ""The chord that fills the ensuing silence in the listener’s inner ear… is the unstated – indeed never to be stated, and ultimately needless to be stated – tonic of that key."" 

Some of the most effective musical silences are very short, lasting barely a fraction of a second. In the spirited and energetic finale of his Symphony No. 2, Brahms uses silences at several points to powerfully disrupt the rhythmic momentum that has been building. (See also syncopation.)

During the 20th century, composers explored further the expressive potential of silence in their music. The contemplative concluding bars of Anton Webern’s Symphony (1928) and Stravinsky’s Les Noces (The Wedding, 1923) make telling and atmospheric use of pauses. Eric Walter White (1947, p.74) describes the ending of Les Noces as follows: ""As the voices cease singing, pools of silence come flooding in between the measured strokes of the bell chord, and the music dies away in a miraculously fresh and radiant close."" John Paynter (1970, p. 24) vividly conveys how silence contributes to the titanic impact of the third section of Messiaen’s orchestral work Et exspecto resurrectionem mortuorum (1964): ""Woodwinds jump, growl and shriek. Silence. Eight solemn bell strokes echo and die. Again silence. Suddenly the brasses blare, and out of the trombones’ awesome processional grows a steady roar … the big gongs the tam-tam beaten in a long and powerful resonance, shattering and echoing across mountains and along valleys. This is music of the high hills, music for vast spaces: ‘The hour is coming when the dead will hear the voice of the Son of God’. We can feel the awe and the majesty of the High Alps and the great churches. The instrumental sounds are vast the silences are deep. The words of St John are alive in the music, and through these sounds Messiaen reveals himself and his vision.""
An extreme example from 1952 is 4?33?, an experimental musical work by avant-garde composer John Cage, incorporating ambient sounds not foreseeable by the composer. Though first performed on the piano, the piece was composed for any instrument or instruments and is structured in three movements. The length of each movement is not fixed by the composer, but the total length of the combination of three movements is. The score instructs the performer(s) to remain silent throughout the piece.
There are telling examples of the use of silence in jazz. A frequently used effect, known as Stop-time, places silences at moments where listeners or dancers might expect a strong beat, contributing to the syncopation. Scott Joplin's Rag-Time Dance(1902) features stop-time silences.

Early recordings of the Rag Time Dance follow Joplin’s instructions as follows: “To get the desired effect of ‘Stop Time’, the pianist will please stamp the heel of one foot heavily upon the floor.” However, later recordings disregard this direction – the regular beat is implied rather than stated and the silences are more palpable. Keith Swanwick (1979, p.70) is enchanted by the “playfulness and humour” engendered by the stop-time effects in Jelly Roll Morton's solo piano recording of The Crave (1939): “If we listen to this, tapping or clicking along with the beat, we shalt find ourselves surprised by two patches of silence near the end. The beat goes on but the sound stops. The effect is something like being thrown forward when a car stops suddenly. It is the biggest surprise in an engaging piece of music full of little deviations (syncopations) from the repeated beat.” Other examples include the closing bars of Louis Armstrong's recording of Struttin' with Some Barbecue (1928) and the hair's-breadth pause at the end of pianist Bill Evans' solo on Miles Davis' recording of On Green Dolphin Street (1959). Duke Ellington's ""Madness in Great Ones"", from his Shakespearean Suite Such Sweet Thunder (1957) conveys the feigned madness of Prince Hamlet through abrupt and unpredictable pauses that interrupt the flow of the music. The reggae band Black Slate had a hit in 1980 with the song Amigo. The instrumental introduction features sudden silences before the voice enters.

In danger
Joseph Jordania suggested that in social animals (including humans) silence can be a sign of danger. Many social animals produce seemingly haphazard sounds which are known as contact calls. These are a mixture of various sounds, accompanying the group's everyday business (for example, foraging, feeding), and they are used to maintain audio contact with the members of the group. Some social animal species communicate the signal of potential danger by stopping contact calls and freezing, without the use of alarm calls, through silence. Charles Darwin wrote about this in relation with wild horse and cattle. Joseph Jordania suggested that human humming could have been a contact method that early humans used to avoid silence. According to his suggestion, humans find prolonged silence distressing (suggesting danger to them). This may help explain why lone humans in relative sonic isolation feel a sense of comfort from humming, whistling, talking to themselves, or having the TV/radio on.

In debate
Argumentative silence is the rhetorical practice of saying nothing when an opponent in a debate expects something to be said. Poorly executed, it can be offensive, like refusing to answer a direct question. However, well-timed silence can throw an opponent off and give the debater the upper hand.
An argument from silence (Latin: argumentum ex silentio) is an argument based on the assumption that someone's silence on a matter suggests (an informal fallacy) that person's ignorance of the matter. In general, ex silentio refers to the claim that the absence of something demonstrates the proof of a proposition.

In law
The right to silence is a legal protection enjoyed by people undergoing police interrogation or trial in certain countries. The law is either explicit or recognized in many legal systems.

In spirituality
""Silence"" in spirituality is often a metaphor for inner stillness. A silent mind, freed from the onslaught of thoughts and thought patterns, is both a goal and an important step in spiritual development. Such ""inner silence"" is not about the absence of sound; instead, it is understood to bring one in contact with the divine, the ultimate reality, or one's own true self, one's divine nature. Many religious traditions imply the importance of being quiet and still in mind and spirit for transformative and integral spiritual growth to occur. In Christianity, there is the silence of contemplative prayer such as centering prayer and Christian meditation; in Islam, there are the wisdom writings of the Sufis who insist on the importance of finding silence within. In Buddhism, the descriptions of silence and allowing the mind to become silent are implied as a feature of spiritual enlightenment. In Hinduism, including the teachings of Advaita Vedanta and the many paths of yoga, teachers insist on the importance of silence, Mauna, for inner growth. Perkey Avot, the Jewish Sages guide for living, states that, ""Tradition is a safety fence to Torah, tithing a safety fence to wealth, vows a safety fence for abstinence; a safety fence for wisdom ... is silence."" In some traditions of Quakerism, communal silence is the usual context of worship meetings, in patient expectancy for the divine to speak in the heart and mind. Eckhart Tolle says that silence can be seen either as the absence of noise, or as the space in which sound exists, just as inner stillness can be seen as the absence of thought, or the space in which thoughts are perceived.

Commemorative silence
A common way to remember a tragic incident and to remember the victims or casualties of such an event is a commemorative moment of silence.

See also
Anechoic chamber
Awkward silence
Background noise
Blue Code of Silence
List of silent musical compositions
Murke's Collected Silences
Muteness
Noble Silence
Omertà
Radio silence
Retreat
Silencer (disambiguation)
Silent film
Silent letter
Spiral of silence
Shunning

References
External links
 Quotations related to Silence at Wikiquote
 The dictionary definition of taciturn at Wiktionary
 Media related to Silence at Wikimedia Commons",Category:Pages with citations lacking titles,3
131,132,Weighting curve,"A Weighting curve is a graph of a set of factors, that are used to 'weight' measured values of a variable according to their importance in relation to some outcome. An important example is frequency weighting in sound level measurement where a specific set of weighting curves known as A, B, C and D weighting as defined in IEC 61672 are used. Unweighted measurements of sound pressure do not correspond to perceived loudness because the human ear is less sensitive at low and high frequencies, with the effect more pronounced at lower sound levels. The four curves are applied to the measured sound level, for example by the use of a weighting filter in a sound level meter, to arrive at readings of loudness in Phons or in decibels (dB) above the threshold of hearing. (see A-weighting).

Weighting curves in Electronic Engineering, Audio and Broadcasting
Although A-weighting with a slow rms detector, as commonly used in sound level meters is frequently used when measuring noise in audio circuits, a different weighting curve, ITU-R 468 weighting uses a psophometric weighting curve and a quasi-peak detector. This method, formerly known as CCIR weighting, is preferred by the telecommunications industry, broadcasters, and some equipment manufacturers as it reflects more accurately the audibility of pops and short bursts of random noise as opposed to pure tones. Psophometric weighting is used in telephony and telecommunications where narrow-band circuits are common. Hearing weighting curves are also used for sound in water.

Other applications of weighting
Acoustics is by no means the only subject which finds use for weighting curves however, and they are widely used in deriving measures of effect for sun exposure, gamma radiation exposure, and many other things. In the measurement of gamma rays or other ionising radiation, a radiation monitor or dosimeter will commonly use a filter to attenuate those energy levels or wavelengths that cause the least damage to the human body, while letting through those that do the most damage, so that any source of radiation may be measured in terms of its true danger rather than just its 'strength'. The sievert is a unit of weighted radiation dose for ionising radiation, which supersedes the older unit the REM (roentgen equivalent man).
Weighting is also applied to the measurement of sunlight when assessing the risk of skin damage through sunburn, since different wavelengths have different biological effects. Common examples are the SPF of sunscreen, and the UV index.
Another use of weighting is in television, where the red, green and blue components of the signal are weighted according to their perceived brightness. This ensures compatibility with black and white receivers, and also benefits noise performance and allows separation into meaningful luminance and chrominance signals for transmission.

See also
Weighting
Weighting filter
A-weighting


== References ==",Category:Audio engineering,3
132,133,Category:Stereophonic sound,,Category:Audio engineering,3
133,134,Music,"Music is an art form and cultural activity whose medium is sound organized in time. The common elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics (loudness and softness), and the sonic qualities of timbre and texture (which are sometimes termed the ""color"" of a musical sound). Different styles or types of music may emphasize, de-emphasize or omit some of these elements. Music is performed with a vast range of instruments and vocal techniques ranging from singing to rapping; there are solely instrumental pieces, solely vocal pieces (such as songs without instrumental accompaniment) and pieces that combine singing and instruments. The word derives from Greek ??????? (mousike; ""art of the Muses""). See glossary of musical terminology.
In its most general form, the activities describing music as an art form or cultural activity include the creation of works of music (songs, tunes, symphonies, and so on), the criticism of music, the study of the history of music, and the aesthetic examination of music. Ancient Greek and Indian philosophers defined music as tones ordered horizontally as melodies and vertically as harmonies. Common sayings such as ""the harmony of the spheres"" and ""it is music to my ears"" point to the notion that music is often ordered and pleasant to listen to. However, 20th-century composer John Cage thought that any sound can be music, saying, for example, ""There is no noise, only sound.""
The creation, performance, significance, and even the definition of music vary according to culture and social context. Indeed, throughout history, some new forms or styles of music have been criticized as ""not being music"", including Beethoven's Grosse Fuge string quartet in 1825, early jazz in the beginning of the 1900s and hardcore punk in the 1980s. There are many types of music, including popular music, traditional music, art music, music written for religious ceremonies and work songs such as chanteys. Music ranges from strictly organized compositions–such as Classical music symphonies from the 1700s and 1800s, through to spontaneously played improvisational music such as jazz, and avant-garde styles of chance-based contemporary music from the 20th and 21st centuries.
Music can be divided into genres (e.g., country music) and genres can be further divided into subgenres (e.g., country blues and pop country are two of the many country subgenres), although the dividing lines and relationships between music genres are often subtle, sometimes open to personal interpretation, and occasionally controversial. For example, it can be hard to draw the line between some early 1980s hard rock and heavy metal. Within the arts, music may be classified as a performing art, a fine art or as an auditory art. Music may be played or sung and heard live at a rock concert or orchestra performance, heard live as part of a dramatic work (a music theater show or opera), or it may be recorded and listened to on a radio, MP3 player, CD player, smartphone or as film score or TV show.
In many cultures, music is an important part of people's way of life, as it plays a key role in religious rituals, rite of passage ceremonies (e.g., graduation and marriage), social activities (e.g., dancing) and cultural activities ranging from amateur karaoke singing to playing in an amateur funk band or singing in a community choir. People may make music as a hobby, like a teen playing cello in a youth orchestra, or work as a professional musician or singer. The music industry includes the individuals who create new songs and musical pieces (such as songwriters and composers), individuals who perform music (which include orchestra, jazz band and rock band musicians, singers and conductors), individuals who record music (music producers and sound engineers), individuals who organize concert tours, and individuals who sell recordings and sheet music and scores to customers.

Etymology
The word derives from Greek ??????? (mousike; ""art of the Muses""). In Greek mythology, the nine Muses were the goddesses who inspired literature, science, and the arts and who were the source of the knowledge embodied in the poetry, song-lyrics, and myths in the Greek culture. According to the Online Etymological Dictionary, the term ""music"" is derived from ""mid-13c., musike, from Old French musique (12c.) and directly from Latin musica ""the art of music,"" also including poetry (also [the] source of Spanish musica, Italian musica, Old High German mosica, German Musik, Dutch muziek, Danish musik)."" This is derived from the ""...Greek mousike (techne) ""(art) of the Muses,"" from fem. of mousikos ""pertaining to the Muses,"" from Mousa ""Muse"" (see muse (n.)). Modern spelling [dates] from [the] 1630s. In classical Greece, [the term ""music"" refers to] any art in which the Muses presided, but especially music and lyric poetry.""

As a form of art or entertainment
Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. When music was only available through sheet music scores, such as during the Classical and Romantic eras, music lovers would buy the sheet music of their favourite pieces and songs so that they could perform them at home on the piano. With the advent of sound recording, records of popular songs, rather than sheet music became the dominant way that music lovers would enjoy their favourite songs. With the advent of home tape recorders in the 1980s and digital music in the 1990s, music lovers could make tapes or playlists of their favourite songs and take them with them on a portable cassette player or MP3 player. Some music lovers create mix tapes of their favorite songs, which serve as a ""self-portrait, a gesture of friendship, prescription for an ideal party... [and] an environment consisting solely of what is most ardently loved.""
Amateur musicians can compose or perform music for their own pleasure, and derive their income elsewhere. Professional musicians are employed by a range of institutions and organisations, including armed forces (in marching bands, concert bands and popular music groups), churches and synagogues, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers or session musicians, seeking contracts and engagements in a variety of settings. There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles such as community concert bands and community orchestras.
A distinction is often made between music performed for a live audience and music that is performed in a studio so that it can be recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is also recorded and distributed. Live concert recordings are popular in both classical music and in popular music forms such as rock, where illegally taped live concerts are prized by music lovers. In the jam band scene, live, improvised jam sessions are preferred to studio recordings.

Composition
""Composition"" is the act or practice of creating a song, an instrumental music piece, a work with both singing and instruments, or another type of music. In many cultures, including Western classical music, the act of composing also includes the creation of music notation, such as a sheet music ""score"", which is then performed by the composer or by other singers or musicians. In popular music and traditional music, the act of composing, which is typically called songwriting, may involve the creation of a basic outline of the song, called the lead sheet, which sets out the melody, lyrics and chord progression. In classical music, the composer typically orchestrates his or her own compositions, but in musical theatre and in pop music, songwriters may hire an arranger to do the orchestration. In some cases, a songwriter may not use notation at all, and instead compose the song in her mind and then play or record it from memory. In jazz and popular music, notable recordings by influential performers are given the weight that written scores play in classical music.
Even when music is notated relatively precisely, as in classical music, there are many decisions that a performer has to make, because notation does not specify all of the elements of music precisely. The process of deciding how to perform music that has been previously composed and notated is termed ""interpretation"". Different performers' interpretations of the same work of music can vary widely, in terms of the tempos that are chosen and the playing or singing style or phrasing of the melodies. Composers and songwriters who present their own music are interpreting their songs, just as much as those who perform the music of others. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, whereas interpretation is generally used to mean the individual choices of a performer.
Although a musical composition often uses musical notation and has a single author, this is not always the case. A work of music can have multiple composers, which often occurs in popular music when a band collaborates to write a song, or in musical theatre, when one person writes the melodies, a second person writes the lyrics, and a third person orchestrates the songs. In some styles of music, such as the blues, a composer/songwriter may create, perform and record new songs or pieces without ever writing them down in music notation. A piece of music can also be composed with words, images, or computer programs that explain or notate how the singer or musician should create musical sounds. Examples range from avant-garde music that uses graphic notation, to text compositions such as Aus den sieben Tagen, to computer programs that select sounds for musical pieces. Music that makes heavy use of randomness and chance is called aleatoric music, and is associated with contemporary composers active in the 20th century, such as John Cage, Morton Feldman, and Witold Lutos?awski. A more commonly known example of chance-based music is the sound of wind chimes jingling in a breeze.
The study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include the creation of popular music and traditional music songs and instrumental pieces as well as spontaneously improvised works like those of free jazz performers and African percussionists such as Ewe drummers.

Notation
In the 2000s, music notation typically means the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music, such as the notes of a melody, are notated. Music notation also often provides instructions on how to perform the music. For example, the sheet music for a song may state that the song is a ""slow blues"" or a ""fast swing"", which indicates the tempo and the genre. To read music notation, a person must have an understanding of music theory, harmony and the performance practice associated with a particular song or piece's genre.
Written notation varies with style and period of music. In the 2000s, notated music is produced as sheet music or, for individuals with computer scorewriter programs, as an image on a computer screen. In ancient times, music notation was put onto stone or clay tablets. To perform music from notation, a singer or instrumentalist requires an understanding of the rhythmic and pitch elements embodied in the symbols and the performance practice that is associated with a piece of music or a genre. In genres requiring musical improvisation, the performer often plays from music where only the chord changes and form of the song are written, requiring the performer to have a great understanding of the music's structure, harmony and the styles of a particular genre (e.g., jazz or country music).
In Western art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Fake books are also used in jazz; they may consist of lead sheets or simply chord charts, which permit rhythm section members to improvise an accompaniment part to jazz songs. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz ""big bands."" In popular music, guitarists and electric bass players often read music notated in tablature (often abbreviated as ""tab""), which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tabulature was also used in the Baroque era to notate music for the lute, a stringed, fretted instrument.

Improvisation
Musical improvisation is the creation of spontaneous music, often within (or based on) a pre-existing harmonic framework or chord progression. Improvisation is the act of instantaneous composition by performers, where compositional techniques are employed with or without preparation. Improvisation is a major part of some types of music, such as blues, jazz, and jazz fusion, in which instrumental performers improvise solos, melody lines and accompaniment parts. In the Western art music tradition, improvisation was an important skill during the Baroque era and during the Classical era. In the Baroque era, performers improvised ornaments and basso continuo keyboard players improvised chord voicings based on figured bass notation. In the Classical era, solo performers and singers improvised virtuoso cadenzas during concerts. However, in the 20th and early 21st century, as ""common practice"" Western art music performance became institutionalized in symphony orchestras, opera houses and ballets, improvisation has played a smaller role. At the same time, some modern composers have increasingly included improvisation in their creative work. In Indian classical music, improvisation is a core component and an essential criterion of performances.

Theory
Music theory encompasses the nature and mechanics of music. It often involves identifying patterns that govern composers' techniques and examining the language and notation of music. In a grand sense, music theory distills and analyzes the parameters or elements of music – rhythm, harmony (harmonic function), melody, structure, form, and texture. Broadly, music theory may include any statement, belief, or conception of or about music. People who study these properties are known as music theorists. Some have applied acoustics, human physiology, and psychology to the explanation of how and why music is perceived.

Elements
Music has many different fundamentals or elements. Depending on the definition of ""element"" being used, these can include: pitch, beat or pulse, tempo, rhythm, melody, harmony, texture, style, allocation of voices, timbre or color, dynamics, expression, articulation, form and structure. The elements of music feature prominently in the music curriculums of Australia, UK and USA. All three curriculums identify pitch, dynamics, timbre and texture as elements, but the other identified elements of music are far from universally agreed. Below is a list of the three official versions of the ""elements of music"":
Australia: pitch, timbre, texture, dynamics and expression, rhythm, form and structure.
UK: pitch, timbre, texture, dynamics, duration, tempo, structure.
USA: pitch, timbre, texture, dynamics, rhythm, form, harmony, style/articulation.
In relation to the UK curriculum, in 2013 the term: ""appropriate musical notations"" was added to their list of elements and the title of the list was changed from the ""elements of music"" to the ""inter-related dimensions of music"". The inter-related dimensions of music are listed as: pitch, duration, dynamics, tempo, timbre, texture, structure and appropriate musical notations.
The phrase ""the elements of music"" is used in a number of different contexts. The two most common contexts can be differentiated by describing them as the ""rudimentary elements of music"" and the ""perceptual elements of music"".

Rudimentary elements
In the 1800s, the phrases ""the elements of music"" and ""the rudiments of music"" were used interchangeably. The elements described in these documents refer to aspects of music that are needed in order to become a musician, Recent writers such as Estrella  seem to be using the phrase ""elements of music"" in a similar manner. A definition which most accurately reflects this usage is: ""the rudimentary principles of an art, science, etc.: the elements of grammar."" The UK's curriculum switch to the ""inter-related dimensions of music"" seems to be a move back to using the rudimentary elements of music.

Perceptual elements
Since the emergence of the study of psychoacoustics in the 1930s, most lists of elements of music have related more to how we hear music than how we learn to play it or study it. C.E. Seashore, in his book Psychology of Music, identified four ""psychological attributes of sound"". These were: ""pitch, loudness, time, and timbre"" (p. 3). He did not call them the ""elements of music"" but referred to them as ""elemental components"" (p. 2). Nonetheless these elemental components link precisely with four of the most common musical elements: ""Pitch"" and ""timbre"" match exactly, ""loudness"" links with dynamics and ""time"" links with the time-based elements of rhythm, duration and tempo. This usage of the phrase ""the elements of music"" links more closely with Webster's New 20th Century Dictionary definition of an element as: ""a substance which cannot be divided into a simpler form by known methods"" and educational institutions' lists of elements generally align with this definition as well.
Although writers of lists of ""rudimentary elements of music"" can vary their lists depending on their personal (or institutional) priorities, the perceptual elements of music should consist of an established (or proven) list of discrete elements which can be independently manipulated to achieve an intended musical effect. It seems at this stage that there is still research to be done in this area.

Analysis of styles
Some styles of music place an emphasis on certain of these fundamentals, while others place less emphasis on certain elements. To give one example, while Bebop-era jazz makes use of very complex chords, including altered dominants and challenging chord progressions, with chords changing two or more times per bar and keys changing several times in a tune, funk places most of its emphasis on rhythm and groove, with entire songs based around a vamp on a single chord. While Romantic era classical music from the mid- to late-1800s makes great use of dramatic changes of dynamics, from whispering pianissimo sections to thunderous fortissimo sections, some entire Baroque dance suites for harpsichord from the early 1700s may use a single dynamic. To give another example, while some art music pieces, such as symphonies are very long, some pop songs are just a few minutes long.

Description of elements
Pitch and melody
Pitch is an aspect of a sound that we can hear, reflecting whether one musical sound, note or tone is ""higher"" or ""lower"" than another musical sound, note or tone. We can talk about the highness or lowness of pitch in the more general sense, such as the way a listener hears a piercingly high piccolo note or whistling tone as higher in pitch than a deep thump of a bass drum. We also talk about pitch in the precise sense associated with musical melodies, basslines and chords. Precise pitch can only be determined in sounds that have a frequency that is clear and stable enough to distinguish from noise. For example, it is much easier for listeners to discern the pitch of a single note played on a piano than to try to discern the pitch of a crash cymbal that is struck.

A melody (also called a ""tune"") is a series of pitches (notes) sounding in succession (one after the other), often in a rising and falling pattern. The notes of a melody are typically created using pitch systems such as scales or modes. Melodies also often contain notes from the chords used in the song. The melodies in simple folk songs and traditional songs may use only the notes of a single scale, the scale associated with the tonic note or key of a given song. For example, a folk song in the key of C (also referred to as C major) may have a melody that uses only the notes of the C major scale (the individual notes C, D, E, F, G, A, B and C; these are the ""white notes"" on a piano keyboard. On the other hand, Bebop-era jazz from the 1940s and contemporary music from the 20th and 21st centuries may use melodies with many chromatic notes (i.e., notes in addition to the notes of the major scale; on a piano, a chromatic scale would include all the notes on the keyboard, including the ""white notes"" and ""black notes"" and unusual scales, such as the whole tone scale (a whole tone scale in the key of C would contain the notes C, D, E, F?, G? and A?). A low, deep musical line played by bass instruments such as double bass, electric bass or tuba is called a bassline.

Harmony and chords
Harmony refers to the ""vertical"" sounds of pitches in music, which means pitches that are played or sung together at the same time to create a chord. Usually this means the notes are played at the same time, although harmony may also be implied by a melody that outlines a harmonic structure (i.e., by using melody notes that are played one after the other, outlining the notes of a chord). In music written using the system of major-minor tonality (""keys""), which includes most classical music written from 1600 to 1900 and most Western pop, rock and traditional music, the key of a piece determines the scale used, which centres around the ""home note"" or tonic of the key. Simple classical pieces and many pop and traditional music songs are written so that all the music is in a single key. More complex Classical, pop and traditional music songs and pieces may have two keys (and in some cases three or more keys). Classical music from the Romantic era (written from about 1820–1900) often contains multiple keys, as does jazz, especially Bebop jazz from the 1940s, in which the key or ""home note"" of a song may change every four bars or even every two bars.

Rhythm
Rhythm is the arrangement of sounds and silences in time. Meter animates time in regular pulse groupings, called measures or bars, which in Western classical, popular and traditional music often group notes in sets of two (e.g., 2/4 time), three (e.g., 3/4 time, also known as Waltz time, or 3/8 time), or four (e.g., 4/4 time). Meters are made easier to hear because songs and pieces often (but not always) place an emphasis on the first beat of each grouping. Notable exceptions exist, such as the backbeat used in much Western pop and rock, in which a song that uses a measure that consists of four beats (called 4/4 time or common time) will have accents on beats two and four, which are typically performed by the drummer on the snare drum, a loud and distinctive-sounding percussion instrument. In pop and rock, the rhythm parts of a song are played by the rhythm section, which includes chord-playing instruments (e.g., electric guitar, acoustic guitar, piano, or other keyboard instruments), a bass instrument (typically electric bass or for some styles such as jazz and bluegrass, double bass) and a drum kit player.

Texture
Musical texture is the overall sound of a piece of music or song. The texture of a piece or sing is determined by how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall nature of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices (see common types below). For example, a thick texture contains many 'layers' of instruments. One of these layers could be a string section, or another brass. The thickness also is affected by the amount and the richness of the instruments. Texture is commonly described according to the number of and relationship between parts or lines of music:
monophony: a single melody (or ""tune"") with neither instrumental accompaniment nor a harmony part. A mother singing a lullaby to her baby would be an example.
heterophony: two or more instruments or singers playing/singing the same melody, but with each performer slightly varying the rhythm or speed of the melody or adding different ornaments to the melody. Two bluegrass fiddlers playing the same traditional fiddle tune together will typically each vary the melody a bit and each add different ornaments.
polyphony: multiple independent melody lines that interweave together, which are sung or played at the same time. Choral music written in the Renaissance music era was typically written in this style. A round, which is a song such as ""Row, Row, Row Your Boat"", which different groups of singers all start to sing at a different time, is a simple example of polyphony.
homophony: a clear melody supported by chordal accompaniment. Most Western popular music songs from the 19th century onward are written in this texture.
Music that contains a large number of independent parts (e.g., a double concerto accompanied by 100 orchestral instruments with many interweaving melodic lines) is generally said to have a ""thicker"" or ""denser"" texture than a work with few parts (e.g., a solo flute melody accompanied by a single cello).

Timbre or ""tone color""
Timbre, sometimes called ""color"" or ""tone color"" is the quality or sound of a voice or instrument. Timbre is what makes a particular musical sound different from another, even when they have the same pitch and loudness. For example, a 440 Hz A note sounds different when it is played on oboe, piano, violin or electric guitar. Even if different players of the same instrument play the same note, their notes might sound different due to differences in instrumental technique (e.g., different embouchures), different types of accessories (e.g., mouthpieces for brass players, reeds for oboe and bassoon players) or strings made out of different materials for string players (e.g., gut strings versus steel strings). Even two instrumentalists playing the same note on the same instrument (one after the other) may sound different due to different ways of playing the instrument (e.g., two string players might hold the bow differently).
The physical characteristics of sound that determine the perception of timbre include the spectrum, envelope and overtones of a note or musical sound. For electric instruments developed in the 20th century, such as electric guitar, electric bass and electric piano, the performer can also change the tone by adjusting equalizer controls, tone controls on the instrument, and by using electronic effects units such as distortion pedals. The tone of the electric Hammond organ is controlled by adjusting drawbars.

Expression
Expressive qualities are those elements in music that create change in music without changing the main pitches or substantially changing the rhythms of the melody and its accompaniment. Performers, including singers and instrumentalists, can add musical expression to a song or piece by adding phrasing, by adding effects such as vibrato (with voice and some instruments, such as guitar, violin, brass instruments and woodwinds), dynamics (the loudness or softness of piece or a section of it), tempo fluctuations (e.g., ritardando or accelerando, which are, respectively slowing down and speeding up the tempo), by adding pauses or fermatas on a cadence, and by changing the articulation of the notes (e.g., making notes more pronounced or accented, by making notes more legato, which means smoothly connected, or by making notes shorter).
Expression is achieved through the manipulation of pitch (such as inflection, vibrato, slides etc.), volume (dynamics, accent, tremolo etc.), duration (tempo fluctuations, rhythmic changes, changing note duration such as with legato and staccato, etc.), timbre (e.g. changing vocal timbre from a light to a resonant voice) and sometimes even texture (e.g. doubling the bass note for a richer effect in a piano piece). Expression therefore can be seen as a manipulation of all elements in order to convey ""an indication of mood, spirit, character etc.""  and as such cannot be included as a unique perceptual element of music, although it can be considered an important rudimentary element of music.

Form
In music, form describes how the overall structure or plan of a song or piece of music, and it describes the layout of a composition as divided into sections. In the early 20th century, Tin Pan Alley songs and Broadway musical songs were often in AABA 32 bar form, in which the A sections repeated the same eight bar melody and the B section provided a contrasting melody and/or harmony for 8 bars. From the 1960s onward, Western pop and rock songs are often in verse-chorus form, which is based around a sequence of verse and chorus (""refrain"") sections, with new lyrics for most verses and repeating lyrics for the choruses. Popular music often makes use of strophic form, sometimes in conjunction with the twelve bar blues.
In the tenth edition of The Oxford Companion to Music, Percy Scholes defines musical form as ""a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration."" Examples of common forms of Western music include the fugue, the invention, sonata-allegro, canon, strophic, theme and variations, and rondo. Scholes states that European classical music had only six stand-alone forms: simple binary, simple ternary, compound binary, rondo, air with variations, and fugue (although musicologist Alfred Mann emphasized that the fugue is primarily a method of composition that has sometimes taken on certain structural conventions.)
Where a piece cannot readily be broken down into sectional units (though it might borrow some form from a poem, story or programme), it is said to be through-composed. Such is often the case with a fantasia, prelude, rhapsody, etude (or study), symphonic poem, Bagatelle, impromptu, etc. Professor Charles Keil classified forms and formal detail as ""sectional, developmental, or variational.""
Sectional form
This form is built from a sequence of clear-cut units that may be referred to by letters but also often have generic names such as introduction and coda, exposition, development and recapitulation, verse, chorus or refrain, and bridge. Introductions and codas, when they are no more than that, are frequently excluded from formal analysis. All such units may typically be eight measures long. Sectional forms include:
Strophic form
This form is defined by its ""unrelieved repetition"" (AAAA...).
Medley
Medley, potpourri is the extreme opposite, that of ""unrelieved variation"": it is simply an indefinite sequence of self-contained sections (ABCD...), sometimes with repeats (AABBCCDD...). Examples include orchestral overtures, which are sometimes no more than a string of the best tunes of the musical theatre show or opera to come.
Binary form

This form uses two sections (AB...), each often repeated (AABB...). In 18th-century Western classical music, ""simple binary"" form was often used for dances and carried with it the convention that the two sections should be in different musical keys but same rhythm, duration and tone. The alternation of two tunes gives enough variety to permit a dance to be extended for as long as desired.
Ternary form
This form has three parts. In Western classical music a simple ternary form has a third section that is a recapitulation of the first (ABA). Often, the first section is repeated (AABA). This approach was popular in the 18th-century operatic aria, and was called da capo (i.e. ""repeat from the top"") form. Later, it gave rise to the 32-bar s",Category:CS1 maint: Multiple names: authors list,3
134,135,Doppler effect,"The Doppler effect (or the Doppler shift) is the change in frequency or wavelength of a wave for an observer who is moving relative to the wave source. It is named after the Austrian physicist Christian Doppler, who described the phenomenon in 1842.
A common example of Doppler shift is the change of pitch heard when a vehicle sounding a horn approaches and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.
The reason for the Doppler effect is that when the source of the waves is moving towards the observer, each successive wave crest is emitted from a position closer to the observer than the previous wave. Therefore, each wave takes slightly less time to reach the observer than the previous wave. Hence, the time between the arrival of successive wave crests at the observer is reduced, causing an increase in the frequency. While they are traveling, the distance between successive wave fronts is reduced, so the waves ""bunch together"". Conversely, if the source of waves is moving away from the observer, each wave is emitted from a position farther from the observer than the previous wave, so the arrival time between successive waves is increased, reducing the frequency. The distance between successive wave fronts is then increased, so the waves ""spread out"".
For waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect may therefore result from motion of the source, motion of the observer, or motion of the medium. Each of these effects are analyzed separately. For waves which do not require a medium, such as light or gravity in general relativity, only the relative difference in velocity between the observer and the source needs to be considered.

History
Doppler first proposed this effect in 1842 in his treatise ""Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels"" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called ""effet Doppler-Fizeau"" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).

General
In classical physics, where the speeds of source and the receiver relative to the medium are lower than the velocity of waves in the medium, the relationship between observed frequency 
  
    
      
        f
      
    
    {\displaystyle f}
   and emitted frequency 
  
    
      
        
          f
          
            0
          
        
      
    
    {\displaystyle f_{\text{0}}}
   is given by:

  
    
      
        f
        =
        
          (
          
            
              
                c
                +
                
                  v
                  
                    r
                  
                
              
              
                c
                +
                
                  v
                  
                    s
                  
                
              
            
          
          )
        
        
          f
          
            0
          
        
        
      
    
    {\displaystyle f=\left({\frac {c+v_{\text{r}}}{c+v_{\text{s}}}}\right)f_{0}\,}
  

where

  
    
      
        c
        
      
    
    {\displaystyle c\;}
   is the velocity of waves in the medium;

  
    
      
        
          v
          
            r
          
        
        
      
    
    {\displaystyle v_{\text{r}}\,}
   is the velocity of the receiver relative to the medium; positive if the receiver is moving towards the source (and negative in the other direction);

  
    
      
        
          v
          
            s
          
        
        
      
    
    {\displaystyle v_{\text{s}}\,}
   is the velocity of the source relative to the medium; positive if the source is moving away from the receiver (and negative in the other direction).

The frequency is decreased if either is moving away from the other.
The above formula assumes that the source is either directly approaching or receding from the observer. If the source approaches the observer at an angle (but still with a constant velocity), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach; but when the wave is received, the source and observer will no longer be at their closest), and a continued monotonic decrease as it recedes from the observer. When the observer is very close to the path of the object, the transition from high to low frequency is very abrupt. When the observer is far from the path of the object, the transition from high to low frequency is gradual.
If the speeds 
  
    
      
        
          v
          
            s
          
        
        
      
    
    {\displaystyle v_{\text{s}}\,}
   and 
  
    
      
        
          v
          
            r
          
        
        
      
    
    {\displaystyle v_{\text{r}}\,}
   are small compared to the speed of the wave, the relationship between observed frequency 
  
    
      
        f
      
    
    {\displaystyle f}
   and emitted frequency 
  
    
      
        
          f
          
            0
          
        
      
    
    {\displaystyle f_{\text{0}}}
   is approximately
where

  
    
      
        ?
        f
        =
        f
        ?
        
          f
          
            0
          
        
        
      
    
    {\displaystyle \Delta f=f-f_{0}\,}
  

  
    
      
        ?
        v
        =
        
          v
          
            r
          
        
        ?
        
          v
          
            s
          
        
        
      
    
    {\displaystyle \Delta v=v_{\text{r}}-v_{\text{s}}\,}
   is the velocity of the receiver relative to the source: it is positive when the source and the receiver are moving towards each other.

Analysis
To understand what happens, consider the following analogy. Someone throws one ball every second at a man. Assume that balls travel with constant velocity. If the thrower is stationary, the man will receive one ball every second. However, if the thrower is moving towards the man, he will receive balls more frequently because the balls will be less spaced out. The inverse is true if the thrower is moving away from the man. So it is actually the wavelength which is affected; as a consequence, the received frequency is also affected. It may also be said that the velocity of the wave remains constant whereas wavelength changes; hence frequency also changes.
With an observer stationary relative to the medium, if a moving source is emitting waves with an actual frequency 
  
    
      
        
          f
          
            0
          
        
      
    
    {\displaystyle f_{\text{0}}}
   (in this case, the wavelength is changed, the transmission velocity of the wave keeps constant 
  
    
      
        
          --
        
      
    
    {\displaystyle {\text{--}}}
   note that the transmission velocity of the wave does not depend on the velocity of the source), then the observer detects waves with a frequency 
  
    
      
        f
      
    
    {\displaystyle f}
   given by

  
    
      
        f
        =
        
          (
          
            
              c
              
                c
                +
                
                  v
                  
                    s
                  
                
              
            
          
          )
        
        
          f
          
            0
          
        
      
    
    {\displaystyle f=\left({\frac {c}{c+v_{\text{s}}}}\right)f_{0}}
  
A similar analysis for a moving observer and a stationary source (in this case, the wavelength keeps constant, but due to the motion, the rate at which the observer receives waves 
  
    
      
        
          --
        
      
    
    {\displaystyle {\text{--}}}
   and hence the transmission velocity of the wave [with respect to the observer] 
  
    
      
        
          --
        
      
    
    {\displaystyle {\text{--}}}
   is changed) yields the observed frequency:

  
    
      
        f
        =
        
          (
          
            
              
                c
                +
                
                  v
                  
                    r
                  
                
              
              c
            
          
          )
        
        
          f
          
            0
          
        
      
    
    {\displaystyle f=\left({\frac {c+v_{\text{r}}}{c}}\right)f_{0}}
  
These can be generalized into the equation that was presented in the previous section.

  
    
      
        f
        =
        
          (
          
            
              
                c
                +
                
                  v
                  
                    r
                  
                
              
              
                c
                +
                
                  v
                  
                    s
                  
                
              
            
          
          )
        
        
          f
          
            0
          
        
      
    
    {\displaystyle f=\left({\frac {c+v_{\text{r}}}{c+v_{\text{s}}}}\right)f_{0}}
  
An interesting effect was predicted by Lord Rayleigh in his classic book on sound: if the source is moving toward the observer at twice the speed of sound, a musical piece emitted by that source would be heard in correct time and tune, but backwards. The Doppler effect with sound is only clearly heard with objects moving at high speed, as change in frequency of musical tone involves a speed of around 40 meters per second, and smaller changes in frequency can easily be confused by changes in the amplitude of the sounds from moving emitters. Neil A Downie has demonstrated  how the Doppler effect can be made much more easily audible by using an ultrasonic (e.g. 40 kHz) emitter on the moving object. The observer then uses a heterodyne frequency converter, as used in many bat detectors, to listen to a band around 40 kHz. In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second.

Application
Sirens
The siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus:
""The reason the siren slides is because it doesn't hit you.""
In other words, if the siren approached the observer directly, the pitch would remain constant until the vehicle hit him, and then immediately jump to a new lower pitch. Because the vehicle passes by the observer, the radial velocity does not remain constant, but instead varies as a function of the angle between his line of sight and the siren's velocity:

  
    
      
        
          v
          
            radial
          
        
        =
        
          v
          
            s
          
        
        ?
        cos
        ?
        
          ?
        
      
    
    {\displaystyle v_{\text{radial}}=v_{\text{s}}\cdot \cos {\theta }}
  
where 
  
    
      
        ?
      
    
    {\displaystyle \theta }
   is the angle between the object's forward velocity and the line of sight from the object to the observer.

Astronomy
The Doppler effect for electromagnetic waves such as light is of great use in astronomy and results in either a so-called redshift or blueshift. It has been used to measure the speed at which stars and galaxies are approaching or receding from us; that is, their radial velocities. This may be used to detect if an apparently single star is, in reality, a close binary, to measure the rotational speed of stars and galaxies, or to detect exoplanets. This redshift and blueshift happens on a very small scale, if an object is moving toward earth, there would not be a noticeable difference in visible light [1].
Note that redshift is also used to measure the expansion of space, but that this is not truly a Doppler effect. Rather, redshifting due to the expansion of space is known as cosmological redshift, which can be derived purely from the Robertson-Walker metric under the formalism of General Relativity. Having said this, it also happens that there are detectable Doppler effects on cosmological scales, which, if incorrectly interpreted as cosmological in origin, lead to the observation of redshift-space distortions.
The use of the Doppler effect for light in astronomy depends on our knowledge that the spectra of stars are not homogeneous. They exhibit absorption lines at well defined frequencies that are correlated with the energies required to excite electrons in various elements from one level to another. The Doppler effect is recognizable in the fact that the absorption lines are not always at the frequencies that are obtained from the spectrum of a stationary light source. Since blue light has a higher frequency than red light, the spectral lines of an approaching astronomical light source exhibit a blueshift and those of a receding astronomical light source exhibit a redshift.
Among the nearby stars, the largest radial velocities with respect to the Sun are +308 km/s (BD-15°4041, also known as LHS 52, 81.7 light-years away) and -260 km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial velocity means the star is receding from the Sun, negative that it is approaching.

Radar
The Doppler effect is used in some types of radar, to measure the velocity of detected objects. A radar beam is fired at a moving target — e.g. a motor car, as police use radar to detect speeding motorists — as it approaches or recedes from the radar source. Each successive radar wave has to travel farther to reach the car, before being reflected and re-detected near the source. As each wave has to move farther, the gap between each wave increases, increasing the wavelength. In some situations, the radar beam is fired at the moving car as it approaches, in which case each successive wave travels a lesser distance, decreasing the wavelength. In either situation, calculations from the Doppler effect accurately determine the car's velocity. Moreover, the proximity fuze, developed during World War II, relies upon Doppler radar to detonate explosives at the correct time, height, distance, etc.
Because the doppler shift affects the wave incident upon the target as well as the wave reflected back to the radar, the change in frequency observed by a radar due to a target moving at relative velocity 
  
    
      
        ?
        v
      
    
    {\displaystyle \Delta v}
   is twice that from the same target emitting a wave:

  
    
      
        ?
        f
        =
        
          
            
              2
              ?
              v
            
            c
          
        
        
          f
          
            0
          
        
      
    
    {\displaystyle \Delta f={\frac {2\Delta v}{c}}f_{0}}
  .

Medical
An echocardiogram can, within certain limits, produce an accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, abnormal communications between the left and right side of the heart, leaking of blood through the valves (valvular regurgitation), and calculation of the cardiac output. Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.
Although ""Doppler"" has become synonymous with ""velocity measurement"" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift (when the received signal arrives).
Velocity measurements of blood flow are also used in other fields of medical ultrasonography, such as obstetric ultrasonography and neurology. Velocity measurement of blood flow in arteries and veins based on Doppler effect is an effective tool for diagnosis of vascular problems like stenosis.

Flow measurement
Instruments such as the laser Doppler velocimeter (LDV), and acoustic Doppler velocimeter (ADV) have been developed to measure velocities in a fluid flow. The LDV emits a light beam and the ADV emits an ultrasonic acoustic burst, and measure the Doppler shift in wavelengths of reflections from particles moving with the flow. The actual flow is computed as a function of the water velocity and phase. This technique allows non-intrusive flow measurements, at high precision and high frequency.

Velocity profile measurement
Developed originally for velocity measurements in medical applications (blood flow), Ultrasonic Doppler Velocimetry (UDV) can measure in real time complete velocity profile in almost any liquids containing particles in suspension such as dust, gas bubbles, emulsions. Flows can be pulsating, oscillating, laminar or turbulent, stationary or transient. This technique is fully non-invasive.

Satellite communication
Fast moving satellites can have a Doppler shift of dozens of kilohertz relative to a ground station. The speed, thus magnitude of Doppler effect, changes due to earth curvature. Dynamic Doppler compensation, where the frequency of a signal is changed multiple times during transmission, is used so the satellite receives a constant frequency signal.

Audio
The Leslie speaker, most commonly associated with and predominantly used with the famous Hammond organ, takes advantage of the Doppler effect by using an electric motor to rotate an acoustic horn around a loudspeaker, sending its sound in a circle. This results at the listener's ear in rapidly fluctuating frequencies of a keyboard note.

Vibration measurement
A laser Doppler vibrometer (LDV) is a non-contact instrument for measuring vibration. The laser beam from the LDV is directed at the surface of interest, and the vibration amplitude and frequency are extracted from the Doppler shift of the laser beam frequency due to the motion of the surface.

Developmental biology
During the segmentation of vertebrate embryos, waves of gene expression sweep across the presomitic mesoderm, the tissue from which the precursors of the vertebrae (somites) are formed. A new somite is formed upon arrival of a wave at the anterior end of the presomitic mesoderm. In zebrafish, it has been shown that the shortening of the presomitic mesoderm during segmentation leads to a Doppler effect as the anterior end of the tissue moves into the waves. This Doppler effect contributes to the period of segmentation.

Inverse Doppler effect
Since 1968 scientists such as Victor Veselago have speculated about the possibility of an inverse Doppler effect. The experiment that claimed to have detected this effect was conducted by Nigel Seddon and Trevor Bearpark in Bristol, United Kingdom in 2003.

See also
Differential Doppler effect
Doppler cooling
Dopplergraph
Fading
Fizeau experiment
Photoacoustic Doppler effect
Rayleigh fading
Redshift
Relativistic Doppler effect

References
Further reading
Doppler, C. (1842). Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels (About the coloured light of the binary stars and some other stars of the heavens). Publisher: Abhandlungen der Königl. Böhm. Gesellschaft der Wissenschaften (V. Folge, Bd. 2, S. 465-482) [Proceedings of the Royal Bohemian Society of Sciences (Part V, Vol 2)]; Prague: 1842 (Reissued 1903). Some sources mention 1843 as year of publication because in that year the article was published in the Proceedings of the Bohemian Society of Sciences. Doppler himself referred to the publication as ""Prag 1842 bei Borrosch und André"", because in 1842 he had a preliminary edition printed that he distributed independently.
""Doppler and the Doppler effect"", E. N. da C. Andrade, Endeavour Vol. XVIII No. 69, January 1959 (published by ICI London). Historical account of Doppler's original paper and subsequent developments.
Adrian, Eleni (24 June 1995). ""Doppler Effect"". NCSA. Retrieved 2008-07-13.

External links
Doppler Effect, [ScienceWorld]
Java simulation of Doppler effect
Doppler Shift for Sound and Light at MathPages
Flash simulation and game of Doppler effect of sound at Scratch (programming language)
The Doppler Effect and Sonic Booms (D.A. Russell, Kettering University)
Video Mashup with Doppler Effect videos
Wave Propagation from John de Pillis. An animation showing that the speed of a moving wave source does not affect the speed of the wave.
EM Wave Animation from John de Pillis. How an electromagnetic wave propagates through a vacuum
Doppler Shift Demo - Interactive flash simulation for demonstrating Doppler shift.
Interactive applets at Physics 2000",Category:Radar signal processing,3
